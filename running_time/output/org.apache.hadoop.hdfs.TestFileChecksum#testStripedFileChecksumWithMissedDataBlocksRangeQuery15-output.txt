2020-12-03 07:19:28,224 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(493)) - starting cluster: numNameNodes=1, numDataNodes=11
Formatting using clusterid: testClusterID
2020-12-03 07:19:29,127 [main] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(229)) - Edit logging is async:true
2020-12-03 07:19:29,143 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(755)) - KeyProvider: null
2020-12-03 07:19:29,145 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(123)) - fsLock is fair: true
2020-12-03 07:19:29,145 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(141)) - Detailed lock hold time metrics enabled: false
2020-12-03 07:19:29,171 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(780)) - fsOwner             = root (auth:SIMPLE)
2020-12-03 07:19:29,172 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(781)) - supergroup          = supergroup
2020-12-03 07:19:29,172 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(782)) - isPermissionEnabled = true
2020-12-03 07:19:29,173 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(793)) - HA Enabled: false
2020-12-03 07:19:29,246 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:29,254 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - hadoop.configured.node.mapping is deprecated. Instead, use net.topology.configured.node.mapping
2020-12-03 07:19:29,255 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(303)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-12-03 07:19:29,255 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(311)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-12-03 07:19:29,264 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-12-03 07:19:29,265 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Dec 03 07:19:29
2020-12-03 07:19:29,268 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-12-03 07:19:29,270 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:19:29,272 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.9 GB = 39.3 MB
2020-12-03 07:19:29,272 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-12-03 07:19:29,293 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createSPSManager(5183)) - Storage policy satisfier is disabled
2020-12-03 07:19:29,294 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(595)) - dfs.block.access.token.enable = true
2020-12-03 07:19:29,294 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(617)) - dfs.block.access.key.update.interval=600 min(s), dfs.block.access.token.lifetime=600 min(s), dfs.encrypt.data.transfer.algorithm=null
2020-12-03 07:19:29,318 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-12-03 07:19:29,318 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-12-03 07:19:29,319 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-12-03 07:19:29,319 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-12-03 07:19:29,320 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(581)) - defaultReplication         = 3
2020-12-03 07:19:29,320 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(582)) - maxReplication             = 512
2020-12-03 07:19:29,321 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(583)) - minReplication             = 1
2020-12-03 07:19:29,321 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(584)) - maxReplicationStreams      = 0
2020-12-03 07:19:29,321 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(585)) - redundancyRecheckInterval  = 3000ms
2020-12-03 07:19:29,321 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(586)) - encryptDataTransfer        = false
2020-12-03 07:19:29,323 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(587)) - maxNumBlocksToLog          = 1000
2020-12-03 07:19:29,358 [main] INFO  namenode.FSDirectory (SerialNumberManager.java:<clinit>(51)) - GLOBAL serial map: bits=29 maxEntries=536870911
2020-12-03 07:19:29,359 [main] INFO  namenode.FSDirectory (SerialNumberManager.java:<clinit>(51)) - USER serial map: bits=24 maxEntries=16777215
2020-12-03 07:19:29,359 [main] INFO  namenode.FSDirectory (SerialNumberManager.java:<clinit>(51)) - GROUP serial map: bits=24 maxEntries=16777215
2020-12-03 07:19:29,359 [main] INFO  namenode.FSDirectory (SerialNumberManager.java:<clinit>(51)) - XATTR serial map: bits=24 maxEntries=16777215
2020-12-03 07:19:29,375 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-12-03 07:19:29,376 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:19:29,376 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.9 GB = 19.6 MB
2020-12-03 07:19:29,377 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-12-03 07:19:29,385 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(290)) - ACLs enabled? false
2020-12-03 07:19:29,386 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(294)) - POSIX ACL inheritance enabled? true
2020-12-03 07:19:29,386 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(298)) - XAttrs enabled? true
2020-12-03 07:19:29,387 [main] INFO  namenode.NameNode (FSDirectory.java:<init>(362)) - Caching file names occurring more than 10 times
2020-12-03 07:19:29,396 [main] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-12-03 07:19:29,401 [main] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-12-03 07:19:29,409 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-12-03 07:19:29,409 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:19:29,410 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.9 GB = 4.9 MB
2020-12-03 07:19:29,410 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-12-03 07:19:29,422 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-12-03 07:19:29,422 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-12-03 07:19:29,423 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-12-03 07:19:29,429 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(996)) - Retry cache on namenode is enabled
2020-12-03 07:19:29,430 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(1004)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-12-03 07:19:29,433 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-12-03 07:19:29,433 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:19:29,434 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.9 GB = 603.0 KB
2020-12-03 07:19:29,434 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-12-03 07:19:29,483 [main] INFO  namenode.FSImage (FSImage.java:format(185)) - Allocated new BlockPoolId: BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:29,640 [main] INFO  common.Storage (NNStorage.java:format(595)) - Storage directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1 has been successfully formatted.
2020-12-03 07:19:29,816 [main] INFO  common.Storage (NNStorage.java:format(595)) - Storage directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2 has been successfully formatted.
2020-12-03 07:19:29,858 [FSImageSaver for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(512)) - Saving image file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/fsimage.ckpt_0000000000000000000 using no compression
2020-12-03 07:19:29,863 [FSImageSaver for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(512)) - Saving image file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage.ckpt_0000000000000000000 using no compression
2020-12-03 07:19:30,021 [FSImageSaver for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(516)) - Image file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .
2020-12-03 07:19:30,021 [FSImageSaver for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(516)) - Image file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .
2020-12-03 07:19:30,087 [main] INFO  namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:getImageTxIdToRetain(203)) - Going to retain 1 images with txid >= 0
2020-12-03 07:19:30,093 [main] INFO  namenode.NameNode (NameNode.java:createNameNode(1632)) - createNameNode []
2020-12-03 07:19:30,614 [main] INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(118)) - Loaded properties from hadoop-metrics2.properties
2020-12-03 07:19:30,758 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 0 second(s).
2020-12-03 07:19:30,758 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2020-12-03 07:19:30,801 [main] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://127.0.0.1:0
2020-12-03 07:19:30,847 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@7bab3f1a] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:19:30,863 [main] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1641)) - Starting Web-server for hdfs at: http://localhost:0
2020-12-03 07:19:30,869 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:30,888 [main] INFO  util.log (Log.java:initialized(192)) - Logging initialized @3709ms
2020-12-03 07:19:31,024 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:19:31,029 [main] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-12-03 07:19:31,029 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:31,039 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:19:31,041 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-12-03 07:19:31,042 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:19:31,042 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:19:31,088 [main] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(102)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-12-03 07:19:31,088 [main] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(806)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-12-03 07:19:31,103 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 38604
2020-12-03 07:19:31,106 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:19:31,188 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@1b66c0fb{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:19:31,196 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@24c1b2d2{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:19:31,267 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@62833051{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-12-03 07:19:31,293 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@7fc4780b{HTTP/1.1,[http/1.1]}{localhost:38604}
2020-12-03 07:19:31,293 [main] INFO  server.Server (Server.java:doStart(419)) - Started @4115ms
2020-12-03 07:19:31,310 [main] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(229)) - Edit logging is async:true
2020-12-03 07:19:31,311 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(755)) - KeyProvider: null
2020-12-03 07:19:31,311 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(123)) - fsLock is fair: true
2020-12-03 07:19:31,311 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(141)) - Detailed lock hold time metrics enabled: false
2020-12-03 07:19:31,316 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(780)) - fsOwner             = root (auth:SIMPLE)
2020-12-03 07:19:31,316 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(781)) - supergroup          = supergroup
2020-12-03 07:19:31,316 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(782)) - isPermissionEnabled = true
2020-12-03 07:19:31,317 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(793)) - HA Enabled: false
2020-12-03 07:19:31,318 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:31,319 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(303)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-12-03 07:19:31,319 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(311)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-12-03 07:19:31,320 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-12-03 07:19:31,320 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Dec 03 07:19:31
2020-12-03 07:19:31,321 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-12-03 07:19:31,321 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:19:31,322 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.8 GB = 36.4 MB
2020-12-03 07:19:31,322 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-12-03 07:19:31,330 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createSPSManager(5183)) - Storage policy satisfier is disabled
2020-12-03 07:19:31,330 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(595)) - dfs.block.access.token.enable = true
2020-12-03 07:19:31,331 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(617)) - dfs.block.access.key.update.interval=600 min(s), dfs.block.access.token.lifetime=600 min(s), dfs.encrypt.data.transfer.algorithm=null
2020-12-03 07:19:31,332 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-12-03 07:19:31,332 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-12-03 07:19:31,333 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-12-03 07:19:31,333 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-12-03 07:19:31,334 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(581)) - defaultReplication         = 3
2020-12-03 07:19:31,334 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(582)) - maxReplication             = 512
2020-12-03 07:19:31,334 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(583)) - minReplication             = 1
2020-12-03 07:19:31,335 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(584)) - maxReplicationStreams      = 0
2020-12-03 07:19:31,335 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(585)) - redundancyRecheckInterval  = 3000ms
2020-12-03 07:19:31,336 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(586)) - encryptDataTransfer        = false
2020-12-03 07:19:31,336 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(587)) - maxNumBlocksToLog          = 1000
2020-12-03 07:19:31,337 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-12-03 07:19:31,337 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:19:31,338 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.8 GB = 18.2 MB
2020-12-03 07:19:31,338 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-12-03 07:19:31,361 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(290)) - ACLs enabled? false
2020-12-03 07:19:31,361 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(294)) - POSIX ACL inheritance enabled? true
2020-12-03 07:19:31,362 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(298)) - XAttrs enabled? true
2020-12-03 07:19:31,362 [main] INFO  namenode.NameNode (FSDirectory.java:<init>(362)) - Caching file names occurring more than 10 times
2020-12-03 07:19:31,363 [main] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-12-03 07:19:31,363 [main] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-12-03 07:19:31,363 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-12-03 07:19:31,364 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:19:31,365 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.8 GB = 4.6 MB
2020-12-03 07:19:31,365 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-12-03 07:19:31,367 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-12-03 07:19:31,368 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-12-03 07:19:31,369 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-12-03 07:19:31,369 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(996)) - Retry cache on namenode is enabled
2020-12-03 07:19:31,370 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(1004)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-12-03 07:19:31,370 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-12-03 07:19:31,370 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:19:31,371 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.8 GB = 559.3 KB
2020-12-03 07:19:31,371 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-12-03 07:19:31,409 [main] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/in_use.lock acquired by nodename 275@bef41f9e9427
2020-12-03 07:19:31,433 [main] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/in_use.lock acquired by nodename 275@bef41f9e9427
2020-12-03 07:19:31,447 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(428)) - Recovering unfinalized segments in /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current
2020-12-03 07:19:31,447 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(428)) - Recovering unfinalized segments in /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current
2020-12-03 07:19:31,448 [main] INFO  namenode.FSImage (FSImage.java:loadFSImage(733)) - No edit log streams selected.
2020-12-03 07:19:31,448 [main] INFO  namenode.FSImage (FSImage.java:loadFSImageFile(797)) - Planning to load image: FSImageFile(file=/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2020-12-03 07:19:31,494 [main] INFO  namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(234)) - Loading 1 INodes.
2020-12-03 07:19:31,504 [main] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(246)) - Loaded FSImage in 0 seconds.
2020-12-03 07:19:31,505 [main] INFO  namenode.FSImage (FSImage.java:loadFSImage(978)) - Loaded image for txid 0 from /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000000
2020-12-03 07:19:31,516 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1110)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-12-03 07:19:31,518 [main] INFO  namenode.FSEditLog (FSEditLog.java:startLogSegment(1365)) - Starting log segment at 1
2020-12-03 07:19:31,638 [main] INFO  namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2020-12-03 07:19:31,638 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(727)) - Finished loading FSImage in 264 msecs
2020-12-03 07:19:31,863 [main] INFO  namenode.NameNode (NameNodeRpcServer.java:<init>(448)) - RPC server is binding to localhost:0
2020-12-03 07:19:31,935 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:19:31,954 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:19:32,329 [Listener at localhost/44779] INFO  namenode.NameNode (NameNode.java:initialize(722)) - Clients are to use localhost:44779 to access this namenode/service.
2020-12-03 07:19:32,333 [Listener at localhost/44779] INFO  namenode.FSNamesystem (FSNamesystem.java:registerMBean(5090)) - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2020-12-03 07:19:32,373 [Listener at localhost/44779] INFO  namenode.LeaseManager (LeaseManager.java:getNumUnderConstructionBlocks(171)) - Number of blocks under construction: 0
2020-12-03 07:19:32,389 [org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager$Monitor@29a5f4e7] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:updateKeys(263)) - Updating block keys
2020-12-03 07:19:32,393 [Listener at localhost/44779] INFO  blockmanagement.BlockManager (BlockManager.java:initializeReplQueues(4922)) - initializing replication queues
2020-12-03 07:19:32,395 [Listener at localhost/44779] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(400)) - STATE* Leaving safe mode after 0 secs
2020-12-03 07:19:32,396 [Listener at localhost/44779] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(406)) - STATE* Network topology has 0 racks and 0 datanodes
2020-12-03 07:19:32,396 [Listener at localhost/44779] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(408)) - STATE* UnderReplicatedBlocks has 0 blocks
2020-12-03 07:19:32,401 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3585)) - Total number of blocks            = 0
2020-12-03 07:19:32,401 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3586)) - Number of invalid blocks          = 0
2020-12-03 07:19:32,402 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3587)) - Number of under-replicated blocks = 0
2020-12-03 07:19:32,402 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3588)) - Number of  over-replicated blocks = 0
2020-12-03 07:19:32,402 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3590)) - Number of blocks being written    = 0
2020-12-03 07:19:32,402 [Reconstruction Queue Initializer] INFO  hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(3593)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 6 msec
2020-12-03 07:19:32,488 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:19:32,502 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:19:32,518 [Listener at localhost/44779] INFO  namenode.NameNode (NameNode.java:startCommonServices(828)) - NameNode RPC up at: localhost/127.0.0.1:44779
2020-12-03 07:19:32,522 [Listener at localhost/44779] INFO  namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1222)) - Starting services required for active state
2020-12-03 07:19:32,522 [Listener at localhost/44779] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(777)) - Initializing quota with 4 thread(s)
2020-12-03 07:19:32,533 [Listener at localhost/44779] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(786)) - Quota initialization completed in 10 milliseconds
name space=1
storage space=0
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2020-12-03 07:19:32,547 [CacheReplicationMonitor(919427203)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-12-03 07:19:32,553 [Listener at localhost/44779] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1,[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:19:32,651 [Listener at localhost/44779] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:19:32,677 [Listener at localhost/44779] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:19:32,703 [Listener at localhost/44779] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:19:32,712 [Listener at localhost/44779] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:32,716 [Listener at localhost/44779] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:19:32,750 [Listener at localhost/44779] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:19:32,751 [Listener at localhost/44779] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:32,757 [Listener at localhost/44779] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:19:32,768 [Listener at localhost/44779] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:40990
2020-12-03 07:19:32,771 [Listener at localhost/44779] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:19:32,772 [Listener at localhost/44779] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:19:32,797 [Listener at localhost/44779] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:32,800 [Listener at localhost/44779] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:19:32,802 [Listener at localhost/44779] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:19:32,803 [Listener at localhost/44779] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:32,821 [Listener at localhost/44779] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:19:32,823 [Listener at localhost/44779] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:19:32,823 [Listener at localhost/44779] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:19:32,823 [Listener at localhost/44779] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:19:32,832 [Listener at localhost/44779] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 36768
2020-12-03 07:19:32,832 [Listener at localhost/44779] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:19:32,835 [Listener at localhost/44779] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@569bf9eb{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:19:32,836 [Listener at localhost/44779] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@274872f8{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:19:32,848 [Listener at localhost/44779] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@7fcbe147{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:19:32,849 [Listener at localhost/44779] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@235f4c10{HTTP/1.1,[http/1.1]}{localhost:36768}
2020-12-03 07:19:32,850 [Listener at localhost/44779] INFO  server.Server (Server.java:doStart(419)) - Started @5671ms
2020-12-03 07:19:33,336 [Listener at localhost/44779] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:36857
2020-12-03 07:19:33,338 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@56b78e55] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:19:33,339 [Listener at localhost/44779] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:19:33,339 [Listener at localhost/44779] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:19:34,016 [Listener at localhost/44779] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:19:34,018 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:19:34,031 [Listener at localhost/40094] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:40094
2020-12-03 07:19:34,052 [Listener at localhost/40094] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:19:34,054 [Listener at localhost/40094] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:19:34,066 [Thread-59] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:44779 starting to offer service
2020-12-03 07:19:34,073 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:19:34,073 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:19:34,078 [Listener at localhost/40094] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 1 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3,[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:19:34,080 [Listener at localhost/40094] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
2020-12-03 07:19:34,081 [Listener at localhost/40094] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:19:34,082 [Listener at localhost/40094] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:19:34,084 [Listener at localhost/40094] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:34,084 [Listener at localhost/40094] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:19:34,085 [Listener at localhost/40094] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:19:34,085 [Listener at localhost/40094] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:34,086 [Listener at localhost/40094] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:19:34,087 [Listener at localhost/40094] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:36972
2020-12-03 07:19:34,087 [Listener at localhost/40094] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:19:34,087 [Listener at localhost/40094] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:19:34,093 [Listener at localhost/40094] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:34,096 [Listener at localhost/40094] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:19:34,098 [Listener at localhost/40094] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:19:34,098 [Listener at localhost/40094] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:34,102 [Listener at localhost/40094] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:19:34,104 [Listener at localhost/40094] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:19:34,105 [Listener at localhost/40094] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:19:34,105 [Listener at localhost/40094] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:19:34,106 [Listener at localhost/40094] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 43192
2020-12-03 07:19:34,107 [Listener at localhost/40094] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:19:34,115 [Listener at localhost/40094] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@71e9a896{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:19:34,116 [Listener at localhost/40094] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@408b35bf{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:19:34,125 [Listener at localhost/40094] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@615f972{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:19:34,140 [Listener at localhost/40094] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@285f09de{HTTP/1.1,[http/1.1]}{localhost:43192}
2020-12-03 07:19:34,140 [Listener at localhost/40094] INFO  server.Server (Server.java:doStart(419)) - Started @6962ms
2020-12-03 07:19:34,322 [Listener at localhost/40094] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:42569
2020-12-03 07:19:34,322 [Listener at localhost/40094] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:19:34,323 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@31500940] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:19:34,323 [Listener at localhost/40094] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:19:34,324 [Listener at localhost/40094] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:19:34,326 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:19:34,340 [Listener at localhost/43158] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:43158
2020-12-03 07:19:34,358 [Listener at localhost/43158] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:19:34,359 [Listener at localhost/43158] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:19:34,360 [Thread-83] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:44779 starting to offer service
2020-12-03 07:19:34,370 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:19:34,375 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:19:34,378 [Listener at localhost/43158] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 2 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5,[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6
2020-12-03 07:19:34,380 [Listener at localhost/43158] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5
2020-12-03 07:19:34,381 [Listener at localhost/43158] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6
2020-12-03 07:19:34,382 [Listener at localhost/43158] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:19:34,383 [Listener at localhost/43158] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:34,383 [Listener at localhost/43158] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:19:34,385 [Listener at localhost/43158] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:19:34,385 [Listener at localhost/43158] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:34,386 [Listener at localhost/43158] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:19:34,387 [Listener at localhost/43158] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:43468
2020-12-03 07:19:34,387 [Listener at localhost/43158] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:19:34,388 [Listener at localhost/43158] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:19:34,391 [Listener at localhost/43158] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:34,393 [Listener at localhost/43158] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:19:34,394 [Listener at localhost/43158] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:19:34,394 [Listener at localhost/43158] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:34,398 [Listener at localhost/43158] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:19:34,399 [Listener at localhost/43158] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:19:34,399 [Listener at localhost/43158] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:19:34,401 [Listener at localhost/43158] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:19:34,407 [Listener at localhost/43158] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 41837
2020-12-03 07:19:34,410 [Listener at localhost/43158] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:19:34,413 [Listener at localhost/43158] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@41382722{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:19:34,414 [Listener at localhost/43158] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@425357dd{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:19:34,433 [Listener at localhost/43158] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@54ec8cc9{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:19:34,434 [Listener at localhost/43158] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@52eacb4b{HTTP/1.1,[http/1.1]}{localhost:41837}
2020-12-03 07:19:34,434 [Listener at localhost/43158] INFO  server.Server (Server.java:doStart(419)) - Started @7256ms
2020-12-03 07:19:34,507 [Listener at localhost/43158] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:37309
2020-12-03 07:19:34,508 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@2a551a63] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:19:34,508 [Listener at localhost/43158] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:19:34,509 [Listener at localhost/43158] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:19:34,509 [Listener at localhost/43158] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:19:34,511 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:19:34,521 [Listener at localhost/43239] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:43239
2020-12-03 07:19:34,527 [Listener at localhost/43239] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:19:34,527 [Listener at localhost/43239] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:19:34,528 [Thread-105] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:44779 starting to offer service
2020-12-03 07:19:34,529 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:19:34,529 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:19:34,536 [Listener at localhost/43239] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 3 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7,[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8
2020-12-03 07:19:34,541 [Listener at localhost/43239] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7
2020-12-03 07:19:34,541 [Listener at localhost/43239] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8
2020-12-03 07:19:34,543 [Listener at localhost/43239] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:19:34,544 [Listener at localhost/43239] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:34,544 [Listener at localhost/43239] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:19:34,545 [Listener at localhost/43239] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:19:34,545 [Listener at localhost/43239] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:34,546 [Listener at localhost/43239] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:19:34,547 [Listener at localhost/43239] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:43233
2020-12-03 07:19:34,547 [Listener at localhost/43239] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:19:34,547 [Listener at localhost/43239] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:19:34,549 [Listener at localhost/43239] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:34,551 [Listener at localhost/43239] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:19:34,553 [Listener at localhost/43239] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:19:34,553 [Listener at localhost/43239] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:34,602 [Listener at localhost/43239] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:19:34,603 [Listener at localhost/43239] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:19:34,603 [Listener at localhost/43239] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:19:34,603 [Listener at localhost/43239] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:19:34,605 [Listener at localhost/43239] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 36382
2020-12-03 07:19:34,605 [Listener at localhost/43239] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:19:34,607 [Listener at localhost/43239] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@ecf9fb3{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:19:34,610 [Listener at localhost/43239] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@27f9e982{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:19:34,619 [Listener at localhost/43239] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@588ab592{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:19:34,622 [Listener at localhost/43239] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@c8b96ec{HTTP/1.1,[http/1.1]}{localhost:36382}
2020-12-03 07:19:34,622 [Listener at localhost/43239] INFO  server.Server (Server.java:doStart(419)) - Started @7443ms
2020-12-03 07:19:34,727 [Listener at localhost/43239] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:41049
2020-12-03 07:19:34,728 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@2d8f2f3a] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:19:34,728 [Listener at localhost/43239] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:19:34,731 [Listener at localhost/43239] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:19:34,733 [Listener at localhost/43239] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:19:34,735 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:19:34,745 [Listener at localhost/41306] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:41306
2020-12-03 07:19:34,770 [Listener at localhost/41306] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:19:34,770 [Listener at localhost/41306] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:19:34,771 [Thread-127] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:44779 starting to offer service
2020-12-03 07:19:34,773 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:19:34,773 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:19:34,782 [Listener at localhost/41306] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 4 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9,[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10
2020-12-03 07:19:34,790 [Listener at localhost/41306] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9
2020-12-03 07:19:34,791 [Listener at localhost/41306] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10
2020-12-03 07:19:34,798 [Listener at localhost/41306] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:19:34,799 [Listener at localhost/41306] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:34,800 [Listener at localhost/41306] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:19:34,801 [Listener at localhost/41306] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:19:34,801 [Listener at localhost/41306] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:34,801 [Listener at localhost/41306] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:19:34,802 [Listener at localhost/41306] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:40369
2020-12-03 07:19:34,802 [Listener at localhost/41306] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:19:34,802 [Listener at localhost/41306] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:19:34,816 [Thread-83] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:44779
2020-12-03 07:19:34,816 [Listener at localhost/41306] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:34,817 [Thread-127] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:44779
2020-12-03 07:19:34,817 [Thread-59] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:44779
2020-12-03 07:19:34,821 [Listener at localhost/41306] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:19:34,820 [Thread-83] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:19:34,820 [Thread-127] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:19:34,819 [Thread-105] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:44779
2020-12-03 07:19:34,822 [Thread-59] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:19:34,823 [Thread-105] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:19:34,824 [Listener at localhost/41306] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:19:34,825 [Listener at localhost/41306] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:34,827 [Listener at localhost/41306] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:19:34,828 [Listener at localhost/41306] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:19:34,828 [Listener at localhost/41306] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:19:34,828 [Listener at localhost/41306] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:19:34,829 [Listener at localhost/41306] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 38353
2020-12-03 07:19:34,829 [Listener at localhost/41306] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:19:34,833 [Listener at localhost/41306] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@3a7b503d{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:19:34,833 [Listener at localhost/41306] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@62c5bbdc{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:19:34,854 [Listener at localhost/41306] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@3c321bdb{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:19:34,855 [Listener at localhost/41306] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@24855019{HTTP/1.1,[http/1.1]}{localhost:38353}
2020-12-03 07:19:34,856 [Listener at localhost/41306] INFO  server.Server (Server.java:doStart(419)) - Started @7677ms
2020-12-03 07:19:34,878 [Listener at localhost/41306] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:34120
2020-12-03 07:19:34,879 [Listener at localhost/41306] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:19:34,879 [Listener at localhost/41306] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:19:34,880 [Listener at localhost/41306] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:19:34,880 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@4d4d8fcf] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:19:34,881 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:19:34,885 [Listener at localhost/41056] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:41056
2020-12-03 07:19:34,893 [Listener at localhost/41056] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:19:34,893 [Listener at localhost/41056] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:19:34,895 [Thread-149] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:44779 starting to offer service
2020-12-03 07:19:34,899 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:19:34,900 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:19:34,904 [Listener at localhost/41056] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 5 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11,[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12
2020-12-03 07:19:34,906 [Listener at localhost/41056] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11
2020-12-03 07:19:34,907 [Thread-149] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:44779
2020-12-03 07:19:34,907 [Listener at localhost/41056] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12
2020-12-03 07:19:34,908 [Thread-59] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/in_use.lock acquired by nodename 275@bef41f9e9427
2020-12-03 07:19:34,908 [Thread-105] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/in_use.lock acquired by nodename 275@bef41f9e9427
2020-12-03 07:19:34,908 [Thread-83] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/in_use.lock acquired by nodename 275@bef41f9e9427
2020-12-03 07:19:34,908 [Thread-59] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1 is not formatted for namespace 1133803897. Formatting...
2020-12-03 07:19:34,908 [Thread-105] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5 is not formatted for namespace 1133803897. Formatting...
2020-12-03 07:19:34,909 [Thread-127] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7/in_use.lock acquired by nodename 275@bef41f9e9427
2020-12-03 07:19:34,909 [Listener at localhost/41056] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:19:34,910 [Thread-127] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7 is not formatted for namespace 1133803897. Formatting...
2020-12-03 07:19:34,910 [Listener at localhost/41056] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:34,910 [Thread-105] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-34e32a14-5782-4fe9-b945-76fbaab513a9 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5 
2020-12-03 07:19:34,910 [Thread-127] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-11ae3bf8-6838-4dfc-af89-3c76d111c85e for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7 
2020-12-03 07:19:34,910 [Thread-59] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-3bec7c54-7709-4f3d-b486-35765741195a for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1 
2020-12-03 07:19:34,911 [Thread-149] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:19:34,911 [Thread-83] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3 is not formatted for namespace 1133803897. Formatting...
2020-12-03 07:19:34,910 [Listener at localhost/41056] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:19:34,912 [Thread-83] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-c85716ef-a148-4295-b2b7-bca92bf0548a for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3 
2020-12-03 07:19:34,912 [Listener at localhost/41056] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:19:34,913 [Listener at localhost/41056] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:34,913 [Listener at localhost/41056] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:19:34,914 [Listener at localhost/41056] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:35598
2020-12-03 07:19:34,914 [Listener at localhost/41056] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:19:34,914 [Listener at localhost/41056] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:19:34,916 [Listener at localhost/41056] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:34,922 [Listener at localhost/41056] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:19:34,923 [Listener at localhost/41056] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:19:34,924 [Listener at localhost/41056] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:34,927 [Listener at localhost/41056] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:19:34,928 [Listener at localhost/41056] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:19:34,928 [Listener at localhost/41056] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:19:34,928 [Listener at localhost/41056] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:19:34,929 [Listener at localhost/41056] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 35546
2020-12-03 07:19:34,929 [Listener at localhost/41056] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:19:34,931 [Listener at localhost/41056] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@3a7704c{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:19:34,932 [Listener at localhost/41056] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@619bd14c{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:19:34,939 [Listener at localhost/41056] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@7dd712e8{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:19:34,940 [Listener at localhost/41056] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@2c282004{HTTP/1.1,[http/1.1]}{localhost:35546}
2020-12-03 07:19:34,940 [Listener at localhost/41056] INFO  server.Server (Server.java:doStart(419)) - Started @7761ms
2020-12-03 07:19:34,969 [Listener at localhost/41056] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:44165
2020-12-03 07:19:34,970 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@7bfc3126] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:19:34,970 [Listener at localhost/41056] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:19:34,970 [Listener at localhost/41056] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:19:34,971 [Listener at localhost/41056] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:19:34,971 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:19:34,977 [Listener at localhost/44302] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:44302
2020-12-03 07:19:34,982 [Listener at localhost/44302] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:19:34,983 [Listener at localhost/44302] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:19:34,984 [Thread-171] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:44779 starting to offer service
2020-12-03 07:19:34,990 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:19:34,991 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:19:34,994 [Thread-171] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:44779
2020-12-03 07:19:34,995 [Thread-171] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:19:34,995 [Listener at localhost/44302] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 6 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13,[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14
2020-12-03 07:19:34,997 [Listener at localhost/44302] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13
2020-12-03 07:19:34,998 [Listener at localhost/44302] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14
2020-12-03 07:19:35,000 [Listener at localhost/44302] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:19:35,001 [Listener at localhost/44302] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:35,001 [Listener at localhost/44302] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:19:35,002 [Listener at localhost/44302] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:19:35,002 [Listener at localhost/44302] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:35,002 [Listener at localhost/44302] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:19:35,003 [Listener at localhost/44302] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:33826
2020-12-03 07:19:35,004 [Listener at localhost/44302] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:19:35,004 [Listener at localhost/44302] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:19:35,005 [Listener at localhost/44302] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:35,007 [Listener at localhost/44302] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:19:35,012 [Listener at localhost/44302] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:19:35,012 [Listener at localhost/44302] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:35,014 [Listener at localhost/44302] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:19:35,015 [Listener at localhost/44302] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:19:35,015 [Listener at localhost/44302] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:19:35,015 [Listener at localhost/44302] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:19:35,018 [Listener at localhost/44302] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 39059
2020-12-03 07:19:35,021 [Listener at localhost/44302] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:19:35,026 [Listener at localhost/44302] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@77b325b3{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:19:35,028 [Listener at localhost/44302] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@7e8e8651{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:19:35,038 [Listener at localhost/44302] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@3be8821f{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:19:35,039 [Listener at localhost/44302] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@64b31700{HTTP/1.1,[http/1.1]}{localhost:39059}
2020-12-03 07:19:35,043 [Listener at localhost/44302] INFO  server.Server (Server.java:doStart(419)) - Started @7864ms
2020-12-03 07:19:35,064 [Listener at localhost/44302] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:37619
2020-12-03 07:19:35,065 [Listener at localhost/44302] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:19:35,065 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@bae47a0] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:19:35,065 [Listener at localhost/44302] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:19:35,066 [Listener at localhost/44302] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:19:35,067 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:19:35,071 [Listener at localhost/34339] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:34339
2020-12-03 07:19:35,078 [Listener at localhost/34339] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:19:35,078 [Listener at localhost/34339] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:19:35,080 [Thread-193] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:44779 starting to offer service
2020-12-03 07:19:35,082 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:19:35,082 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:19:35,085 [Thread-149] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9/in_use.lock acquired by nodename 275@bef41f9e9427
2020-12-03 07:19:35,086 [Thread-149] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9 is not formatted for namespace 1133803897. Formatting...
2020-12-03 07:19:35,088 [Thread-193] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:44779
2020-12-03 07:19:35,088 [Thread-193] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:19:35,135 [Thread-149] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-035cd092-cc9b-40a4-8e6a-3004ff2553ce for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9 
2020-12-03 07:19:35,144 [Listener at localhost/34339] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 7 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15,[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16
2020-12-03 07:19:35,149 [Listener at localhost/34339] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15
2020-12-03 07:19:35,149 [Listener at localhost/34339] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16
2020-12-03 07:19:35,151 [Listener at localhost/34339] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:19:35,152 [Listener at localhost/34339] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:35,152 [Listener at localhost/34339] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:19:35,153 [Listener at localhost/34339] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:19:35,153 [Listener at localhost/34339] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:35,156 [Listener at localhost/34339] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:19:35,158 [Listener at localhost/34339] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:40799
2020-12-03 07:19:35,158 [Listener at localhost/34339] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:19:35,158 [Listener at localhost/34339] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:19:35,160 [Listener at localhost/34339] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:35,162 [Listener at localhost/34339] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:19:35,168 [Listener at localhost/34339] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:19:35,168 [Listener at localhost/34339] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:35,171 [Listener at localhost/34339] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:19:35,172 [Listener at localhost/34339] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:19:35,173 [Listener at localhost/34339] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:19:35,173 [Listener at localhost/34339] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:19:35,175 [Listener at localhost/34339] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 39375
2020-12-03 07:19:35,175 [Listener at localhost/34339] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:19:35,177 [Listener at localhost/34339] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@3add81c4{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:19:35,178 [Listener at localhost/34339] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@1c65121{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:19:35,185 [Listener at localhost/34339] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@7668d560{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:19:35,187 [Listener at localhost/34339] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@46292372{HTTP/1.1,[http/1.1]}{localhost:39375}
2020-12-03 07:19:35,187 [Listener at localhost/34339] INFO  server.Server (Server.java:doStart(419)) - Started @8008ms
2020-12-03 07:19:35,249 [Thread-171] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11/in_use.lock acquired by nodename 275@bef41f9e9427
2020-12-03 07:19:35,250 [Thread-171] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11 is not formatted for namespace 1133803897. Formatting...
2020-12-03 07:19:35,254 [Thread-171] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-d3e3d4e2-c7e5-44e4-9c40-1b0d2f6aaca7 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11 
2020-12-03 07:19:35,263 [Listener at localhost/34339] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:42978
2020-12-03 07:19:35,264 [Listener at localhost/34339] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:19:35,264 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@6c44052e] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:19:35,264 [Listener at localhost/34339] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:19:35,265 [Listener at localhost/34339] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:19:35,266 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:19:35,274 [Listener at localhost/43524] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:43524
2020-12-03 07:19:35,282 [Listener at localhost/43524] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:19:35,282 [Listener at localhost/43524] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:19:35,283 [Thread-215] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:44779 starting to offer service
2020-12-03 07:19:35,309 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:19:35,331 [Thread-215] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:44779
2020-12-03 07:19:35,332 [Thread-215] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:19:35,333 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:19:35,338 [Listener at localhost/43524] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 8 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17,[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18
2020-12-03 07:19:35,340 [Listener at localhost/43524] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17
2020-12-03 07:19:35,341 [Listener at localhost/43524] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18
2020-12-03 07:19:35,342 [Listener at localhost/43524] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:19:35,342 [Listener at localhost/43524] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:35,342 [Listener at localhost/43524] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:19:35,343 [Listener at localhost/43524] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:19:35,343 [Listener at localhost/43524] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:35,343 [Listener at localhost/43524] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:19:35,345 [Listener at localhost/43524] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:33393
2020-12-03 07:19:35,346 [Listener at localhost/43524] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:19:35,346 [Listener at localhost/43524] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:19:35,347 [Listener at localhost/43524] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:35,349 [Listener at localhost/43524] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:19:35,350 [Listener at localhost/43524] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:19:35,350 [Listener at localhost/43524] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:35,352 [Listener at localhost/43524] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:19:35,352 [Thread-193] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13/in_use.lock acquired by nodename 275@bef41f9e9427
2020-12-03 07:19:35,353 [Thread-193] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13 is not formatted for namespace 1133803897. Formatting...
2020-12-03 07:19:35,353 [Listener at localhost/43524] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:19:35,354 [Listener at localhost/43524] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:19:35,354 [Listener at localhost/43524] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:19:35,355 [Listener at localhost/43524] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 41745
2020-12-03 07:19:35,356 [Listener at localhost/43524] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:19:35,358 [Listener at localhost/43524] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@54e81b21{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:19:35,359 [Listener at localhost/43524] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@6650813a{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:19:35,369 [Thread-193] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-2ce634e7-1c83-4eb2-9adf-081181b40a1c for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13 
2020-12-03 07:19:35,377 [Listener at localhost/43524] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@64d7b720{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:19:35,379 [Listener at localhost/43524] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@30272916{HTTP/1.1,[http/1.1]}{localhost:41745}
2020-12-03 07:19:35,380 [Listener at localhost/43524] INFO  server.Server (Server.java:doStart(419)) - Started @8201ms
2020-12-03 07:19:35,421 [Listener at localhost/43524] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:42572
2020-12-03 07:19:35,428 [Listener at localhost/43524] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:19:35,429 [Listener at localhost/43524] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:19:35,429 [Listener at localhost/43524] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:19:35,430 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@5bf61e67] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:19:35,436 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:19:35,453 [Listener at localhost/36667] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:36667
2020-12-03 07:19:35,480 [Listener at localhost/36667] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:19:35,480 [Listener at localhost/36667] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:19:35,481 [Thread-237] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:44779 starting to offer service
2020-12-03 07:19:35,482 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:19:35,483 [Thread-215] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15/in_use.lock acquired by nodename 275@bef41f9e9427
2020-12-03 07:19:35,491 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:19:35,493 [Thread-215] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15 is not formatted for namespace 1133803897. Formatting...
2020-12-03 07:19:35,494 [Thread-215] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-997db01c-04dd-4559-92d6-ffa544c717ec for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15 
2020-12-03 07:19:35,499 [Listener at localhost/36667] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 9 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19,[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20
2020-12-03 07:19:35,500 [Listener at localhost/36667] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19
2020-12-03 07:19:35,501 [Listener at localhost/36667] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20
2020-12-03 07:19:35,501 [Thread-237] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:44779
2020-12-03 07:19:35,508 [Thread-237] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:19:35,512 [Listener at localhost/36667] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:19:35,513 [Listener at localhost/36667] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:35,513 [Listener at localhost/36667] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:19:35,514 [Listener at localhost/36667] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:19:35,514 [Listener at localhost/36667] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:35,514 [Listener at localhost/36667] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:19:35,516 [Listener at localhost/36667] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:46334
2020-12-03 07:19:35,516 [Listener at localhost/36667] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:19:35,516 [Listener at localhost/36667] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:19:35,518 [Listener at localhost/36667] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:35,520 [Listener at localhost/36667] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:19:35,521 [Listener at localhost/36667] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:19:35,521 [Listener at localhost/36667] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:35,524 [Listener at localhost/36667] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:19:35,525 [Listener at localhost/36667] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:19:35,525 [Listener at localhost/36667] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:19:35,525 [Listener at localhost/36667] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:19:35,526 [Listener at localhost/36667] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 41412
2020-12-03 07:19:35,527 [Listener at localhost/36667] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:19:35,533 [Listener at localhost/36667] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@9fecdf1{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:19:35,534 [Listener at localhost/36667] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@3b0f7d9d{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:19:35,545 [Listener at localhost/36667] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@3e1162e7{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:19:35,546 [Listener at localhost/36667] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@79c3f01f{HTTP/1.1,[http/1.1]}{localhost:41412}
2020-12-03 07:19:35,546 [Listener at localhost/36667] INFO  server.Server (Server.java:doStart(419)) - Started @8367ms
2020-12-03 07:19:35,579 [Listener at localhost/36667] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:35491
2020-12-03 07:19:35,580 [Listener at localhost/36667] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:19:35,580 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@350b3a17] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:19:35,580 [Listener at localhost/36667] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:19:35,581 [Listener at localhost/36667] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:19:35,582 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:19:35,586 [Listener at localhost/39541] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:39541
2020-12-03 07:19:35,593 [Listener at localhost/39541] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:19:35,594 [Listener at localhost/39541] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:19:35,595 [Thread-259] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:44779 starting to offer service
2020-12-03 07:19:35,608 [Thread-127] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/in_use.lock acquired by nodename 275@bef41f9e9427
2020-12-03 07:19:35,608 [Thread-127] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8 is not formatted for namespace 1133803897. Formatting...
2020-12-03 07:19:35,608 [Thread-59] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/in_use.lock acquired by nodename 275@bef41f9e9427
2020-12-03 07:19:35,609 [Thread-59] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2 is not formatted for namespace 1133803897. Formatting...
2020-12-03 07:19:35,610 [Thread-83] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/in_use.lock acquired by nodename 275@bef41f9e9427
2020-12-03 07:19:35,610 [Thread-83] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4 is not formatted for namespace 1133803897. Formatting...
2020-12-03 07:19:35,611 [Thread-105] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/in_use.lock acquired by nodename 275@bef41f9e9427
2020-12-03 07:19:35,611 [Thread-105] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6 is not formatted for namespace 1133803897. Formatting...
2020-12-03 07:19:35,611 [Thread-83] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-72b6d766-f12d-46df-9b78-3689fbb0ff3f for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4 
2020-12-03 07:19:35,611 [Thread-105] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-462ced07-43f8-4720-afda-b3153be81947 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6 
2020-12-03 07:19:35,613 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:19:35,614 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:19:35,614 [Thread-127] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-4d752d77-dc01-4782-88b7-fac90ddd2643 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8 
2020-12-03 07:19:35,615 [Thread-59] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-c396f456-72b2-4eec-a6af-559f05d184b3 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2 
2020-12-03 07:19:35,625 [Thread-259] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:44779
2020-12-03 07:19:35,626 [Thread-259] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:19:35,630 [Listener at localhost/39541] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 10 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21,[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22
2020-12-03 07:19:35,632 [Listener at localhost/39541] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21
2020-12-03 07:19:35,632 [Listener at localhost/39541] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22
2020-12-03 07:19:35,634 [Listener at localhost/39541] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:19:35,635 [Listener at localhost/39541] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:35,635 [Listener at localhost/39541] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:19:35,652 [Listener at localhost/39541] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:19:35,653 [Listener at localhost/39541] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:35,653 [Listener at localhost/39541] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:19:35,654 [Listener at localhost/39541] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:36213
2020-12-03 07:19:35,654 [Listener at localhost/39541] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:19:35,654 [Listener at localhost/39541] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:19:35,655 [Listener at localhost/39541] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:35,657 [Listener at localhost/39541] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:19:35,658 [Listener at localhost/39541] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:19:35,658 [Listener at localhost/39541] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:35,660 [Listener at localhost/39541] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:19:35,660 [Listener at localhost/39541] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:19:35,661 [Listener at localhost/39541] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:19:35,661 [Listener at localhost/39541] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:19:35,662 [Listener at localhost/39541] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 36302
2020-12-03 07:19:35,662 [Listener at localhost/39541] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:19:35,664 [Listener at localhost/39541] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@3e681bc{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:19:35,665 [Listener at localhost/39541] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@23aae55{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:19:35,671 [Listener at localhost/39541] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@56bc3fac{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:19:35,671 [Listener at localhost/39541] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@df4b72{HTTP/1.1,[http/1.1]}{localhost:36302}
2020-12-03 07:19:35,672 [Listener at localhost/39541] INFO  server.Server (Server.java:doStart(419)) - Started @8493ms
2020-12-03 07:19:35,694 [Listener at localhost/39541] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:38285
2020-12-03 07:19:35,695 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@37ff4054] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:19:35,695 [Listener at localhost/39541] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:19:35,695 [Listener at localhost/39541] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:19:35,696 [Listener at localhost/39541] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:19:35,697 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:19:35,702 [Listener at localhost/32858] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:32858
2020-12-03 07:19:35,712 [Listener at localhost/32858] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:19:35,713 [Listener at localhost/32858] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:19:35,714 [Thread-281] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:44779 starting to offer service
2020-12-03 07:19:35,724 [Thread-237] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17/in_use.lock acquired by nodename 275@bef41f9e9427
2020-12-03 07:19:35,725 [Thread-237] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17 is not formatted for namespace 1133803897. Formatting...
2020-12-03 07:19:35,725 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:19:35,725 [Thread-237] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-7a239de6-36ad-4a59-8b2f-83eb72cddab6 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17 
2020-12-03 07:19:35,725 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:19:35,736 [Thread-281] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:44779
2020-12-03 07:19:35,739 [Thread-281] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:19:35,857 [Thread-149] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10/in_use.lock acquired by nodename 275@bef41f9e9427
2020-12-03 07:19:35,858 [Thread-149] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10 is not formatted for namespace 1133803897. Formatting...
2020-12-03 07:19:35,860 [Thread-259] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19/in_use.lock acquired by nodename 275@bef41f9e9427
2020-12-03 07:19:35,860 [Thread-259] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19 is not formatted for namespace 1133803897. Formatting...
2020-12-03 07:19:35,863 [Thread-149] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-5167c021-7d28-44ed-982a-8cb38c70870d for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10 
2020-12-03 07:19:35,863 [Thread-259] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-3c77b156-23e2-4fc9-9db2-bac28e329697 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19 
2020-12-03 07:19:35,990 [Thread-281] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21/in_use.lock acquired by nodename 275@bef41f9e9427
2020-12-03 07:19:35,990 [Thread-281] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21 is not formatted for namespace 1133803897. Formatting...
2020-12-03 07:19:35,991 [Thread-171] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12/in_use.lock acquired by nodename 275@bef41f9e9427
2020-12-03 07:19:35,992 [Thread-171] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12 is not formatted for namespace 1133803897. Formatting...
2020-12-03 07:19:35,995 [Thread-281] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-04af9d40-9522-498c-835c-e506c824a39f for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21 
2020-12-03 07:19:35,995 [Thread-171] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-265e643d-40cf-4be7-923e-86955d3df769 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12 
2020-12-03 07:19:36,117 [Thread-193] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14/in_use.lock acquired by nodename 275@bef41f9e9427
2020-12-03 07:19:36,117 [Thread-193] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14 is not formatted for namespace 1133803897. Formatting...
2020-12-03 07:19:36,123 [Thread-193] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-023f6a39-4389-43da-a1df-f7cccb6bcc41 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14 
2020-12-03 07:19:36,138 [Thread-59] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:36,139 [Thread-83] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:36,139 [Thread-83] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:36,144 [Thread-59] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:36,145 [Thread-59] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1 and block pool id BP-390774184-172.17.0.6-1606979969464 is not formatted. Formatting ...
2020-12-03 07:19:36,145 [Thread-59] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-390774184-172.17.0.6-1606979969464 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-390774184-172.17.0.6-1606979969464/current
2020-12-03 07:19:36,145 [Thread-127] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:36,146 [Thread-105] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:36,146 [Thread-105] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:36,147 [Thread-105] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5 and block pool id BP-390774184-172.17.0.6-1606979969464 is not formatted. Formatting ...
2020-12-03 07:19:36,147 [Thread-105] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-390774184-172.17.0.6-1606979969464 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-390774184-172.17.0.6-1606979969464/current
2020-12-03 07:19:36,145 [Thread-83] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3 and block pool id BP-390774184-172.17.0.6-1606979969464 is not formatted. Formatting ...
2020-12-03 07:19:36,148 [Thread-83] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-390774184-172.17.0.6-1606979969464 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-390774184-172.17.0.6-1606979969464/current
2020-12-03 07:19:36,146 [Thread-127] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7/current/BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:36,152 [Thread-127] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7 and block pool id BP-390774184-172.17.0.6-1606979969464 is not formatted. Formatting ...
2020-12-03 07:19:36,152 [Thread-127] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-390774184-172.17.0.6-1606979969464 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7/current/BP-390774184-172.17.0.6-1606979969464/current
2020-12-03 07:19:36,272 [Thread-215] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16/in_use.lock acquired by nodename 275@bef41f9e9427
2020-12-03 07:19:36,273 [Thread-215] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16 is not formatted for namespace 1133803897. Formatting...
2020-12-03 07:19:36,277 [Thread-215] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-353fdb6d-79d4-40b3-891b-044f5d75a5de for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16 
2020-12-03 07:19:36,343 [IPC Server handler 4 on default port 44779] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:36,352 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:36,352 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:36,386 [Thread-149] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:36,387 [Thread-149] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9/current/BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:36,387 [Thread-149] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9 and block pool id BP-390774184-172.17.0.6-1606979969464 is not formatted. Formatting ...
2020-12-03 07:19:36,387 [Thread-149] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-390774184-172.17.0.6-1606979969464 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9/current/BP-390774184-172.17.0.6-1606979969464/current
2020-12-03 07:19:36,455 [IPC Server handler 7 on default port 44779] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:36,457 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:36,457 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:36,498 [Thread-237] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18/in_use.lock acquired by nodename 275@bef41f9e9427
2020-12-03 07:19:36,499 [Thread-237] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18 is not formatted for namespace 1133803897. Formatting...
2020-12-03 07:19:36,502 [Thread-237] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-6c847cf4-ff4b-4f0f-91b4-4b65310e2c06 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18 
2020-12-03 07:19:36,510 [Thread-171] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:36,510 [Thread-171] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11/current/BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:36,510 [Thread-171] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11 and block pool id BP-390774184-172.17.0.6-1606979969464 is not formatted. Formatting ...
2020-12-03 07:19:36,510 [Thread-171] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-390774184-172.17.0.6-1606979969464 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11/current/BP-390774184-172.17.0.6-1606979969464/current
2020-12-03 07:19:36,560 [IPC Server handler 8 on default port 44779] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:36,560 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:36,561 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:36,622 [Thread-259] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20/in_use.lock acquired by nodename 275@bef41f9e9427
2020-12-03 07:19:36,623 [Thread-259] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20 is not formatted for namespace 1133803897. Formatting...
2020-12-03 07:19:36,629 [Thread-259] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-aa4eafc1-6206-4cf4-beac-6f7c6e133413 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20 
2020-12-03 07:19:36,633 [Thread-83] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:36,634 [Thread-83] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:36,634 [Thread-127] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:36,634 [Thread-193] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:36,634 [Thread-127] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/current/BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:36,634 [Thread-83] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4 and block pool id BP-390774184-172.17.0.6-1606979969464 is not formatted. Formatting ...
2020-12-03 07:19:36,635 [Thread-127] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8 and block pool id BP-390774184-172.17.0.6-1606979969464 is not formatted. Formatting ...
2020-12-03 07:19:36,634 [Thread-193] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13/current/BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:36,635 [Thread-127] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-390774184-172.17.0.6-1606979969464 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/current/BP-390774184-172.17.0.6-1606979969464/current
2020-12-03 07:19:36,635 [Thread-83] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-390774184-172.17.0.6-1606979969464 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-390774184-172.17.0.6-1606979969464/current
2020-12-03 07:19:36,635 [Thread-193] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13 and block pool id BP-390774184-172.17.0.6-1606979969464 is not formatted. Formatting ...
2020-12-03 07:19:36,636 [Thread-193] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-390774184-172.17.0.6-1606979969464 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13/current/BP-390774184-172.17.0.6-1606979969464/current
2020-12-03 07:19:36,641 [Thread-59] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:36,641 [Thread-59] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:36,641 [Thread-59] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2 and block pool id BP-390774184-172.17.0.6-1606979969464 is not formatted. Formatting ...
2020-12-03 07:19:36,641 [Thread-59] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-390774184-172.17.0.6-1606979969464 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-390774184-172.17.0.6-1606979969464/current
2020-12-03 07:19:36,644 [Thread-105] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:36,644 [Thread-105] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/current/BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:36,644 [Thread-105] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6 and block pool id BP-390774184-172.17.0.6-1606979969464 is not formatted. Formatting ...
2020-12-03 07:19:36,644 [Thread-105] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-390774184-172.17.0.6-1606979969464 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/current/BP-390774184-172.17.0.6-1606979969464/current
2020-12-03 07:19:36,665 [IPC Server handler 5 on default port 44779] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:36,666 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:36,666 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:36,765 [Thread-281] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22/in_use.lock acquired by nodename 275@bef41f9e9427
2020-12-03 07:19:36,766 [Thread-281] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22 is not formatted for namespace 1133803897. Formatting...
2020-12-03 07:19:36,769 [IPC Server handler 0 on default port 44779] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:36,770 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:36,770 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:36,771 [Thread-281] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-6b4155c2-6b43-450c-b668-a1919719e233 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22 
2020-12-03 07:19:36,775 [Thread-215] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:36,776 [Thread-215] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15/current/BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:36,776 [Thread-215] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15 and block pool id BP-390774184-172.17.0.6-1606979969464 is not formatted. Formatting ...
2020-12-03 07:19:36,776 [Thread-215] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-390774184-172.17.0.6-1606979969464 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15/current/BP-390774184-172.17.0.6-1606979969464/current
2020-12-03 07:19:36,873 [IPC Server handler 3 on default port 44779] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:36,873 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:36,873 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:36,899 [Thread-149] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:36,899 [Thread-149] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10/current/BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:36,899 [Thread-149] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10 and block pool id BP-390774184-172.17.0.6-1606979969464 is not formatted. Formatting ...
2020-12-03 07:19:36,900 [Thread-149] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-390774184-172.17.0.6-1606979969464 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10/current/BP-390774184-172.17.0.6-1606979969464/current
2020-12-03 07:19:36,976 [IPC Server handler 6 on default port 44779] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:36,977 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:36,977 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:37,036 [Thread-237] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:37,036 [Thread-237] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17/current/BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:37,037 [Thread-237] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17 and block pool id BP-390774184-172.17.0.6-1606979969464 is not formatted. Formatting ...
2020-12-03 07:19:37,037 [Thread-237] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-390774184-172.17.0.6-1606979969464 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17/current/BP-390774184-172.17.0.6-1606979969464/current
2020-12-03 07:19:37,052 [Thread-171] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:37,053 [Thread-171] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12/current/BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:37,053 [Thread-171] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12 and block pool id BP-390774184-172.17.0.6-1606979969464 is not formatted. Formatting ...
2020-12-03 07:19:37,053 [Thread-171] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-390774184-172.17.0.6-1606979969464 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12/current/BP-390774184-172.17.0.6-1606979969464/current
2020-12-03 07:19:37,080 [IPC Server handler 9 on default port 44779] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:37,081 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:37,081 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:37,159 [Thread-83] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=1133803897;bpid=BP-390774184-172.17.0.6-1606979969464;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=1133803897;c=1606979969464;bpid=BP-390774184-172.17.0.6-1606979969464;dnuuid=null
2020-12-03 07:19:37,160 [Thread-59] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=1133803897;bpid=BP-390774184-172.17.0.6-1606979969464;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=1133803897;c=1606979969464;bpid=BP-390774184-172.17.0.6-1606979969464;dnuuid=null
2020-12-03 07:19:37,166 [Thread-105] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=1133803897;bpid=BP-390774184-172.17.0.6-1606979969464;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=1133803897;c=1606979969464;bpid=BP-390774184-172.17.0.6-1606979969464;dnuuid=null
2020-12-03 07:19:37,167 [Thread-127] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=1133803897;bpid=BP-390774184-172.17.0.6-1606979969464;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=1133803897;c=1606979969464;bpid=BP-390774184-172.17.0.6-1606979969464;dnuuid=null
2020-12-03 07:19:37,174 [Thread-193] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:37,174 [Thread-193] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14/current/BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:37,174 [Thread-193] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14 and block pool id BP-390774184-172.17.0.6-1606979969464 is not formatted. Formatting ...
2020-12-03 07:19:37,175 [Thread-193] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-390774184-172.17.0.6-1606979969464 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14/current/BP-390774184-172.17.0.6-1606979969464/current
2020-12-03 07:19:37,177 [Thread-259] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:37,177 [Thread-259] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19/current/BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:37,177 [Thread-259] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19 and block pool id BP-390774184-172.17.0.6-1606979969464 is not formatted. Formatting ...
2020-12-03 07:19:37,178 [Thread-259] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-390774184-172.17.0.6-1606979969464 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19/current/BP-390774184-172.17.0.6-1606979969464/current
2020-12-03 07:19:37,188 [IPC Server handler 4 on default port 44779] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:37,189 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:37,189 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:37,291 [IPC Server handler 7 on default port 44779] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:37,292 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:37,293 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:37,357 [Thread-215] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:37,357 [Thread-215] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16/current/BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:37,357 [Thread-215] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16 and block pool id BP-390774184-172.17.0.6-1606979969464 is not formatted. Formatting ...
2020-12-03 07:19:37,358 [Thread-215] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-390774184-172.17.0.6-1606979969464 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16/current/BP-390774184-172.17.0.6-1606979969464/current
2020-12-03 07:19:37,358 [Thread-281] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:37,358 [Thread-281] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21/current/BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:37,358 [Thread-281] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21 and block pool id BP-390774184-172.17.0.6-1606979969464 is not formatted. Formatting ...
2020-12-03 07:19:37,358 [Thread-281] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-390774184-172.17.0.6-1606979969464 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21/current/BP-390774184-172.17.0.6-1606979969464/current
2020-12-03 07:19:37,395 [IPC Server handler 8 on default port 44779] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:37,396 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:37,396 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:37,468 [Thread-149] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=1133803897;bpid=BP-390774184-172.17.0.6-1606979969464;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=1133803897;c=1606979969464;bpid=BP-390774184-172.17.0.6-1606979969464;dnuuid=null
2020-12-03 07:19:37,499 [IPC Server handler 5 on default port 44779] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:37,499 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:37,500 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:37,602 [IPC Server handler 0 on default port 44779] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:37,603 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:37,603 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:37,615 [Thread-171] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=1133803897;bpid=BP-390774184-172.17.0.6-1606979969464;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=1133803897;c=1606979969464;bpid=BP-390774184-172.17.0.6-1606979969464;dnuuid=null
2020-12-03 07:19:37,622 [Thread-237] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:37,623 [Thread-237] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18/current/BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:37,623 [Thread-237] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18 and block pool id BP-390774184-172.17.0.6-1606979969464 is not formatted. Formatting ...
2020-12-03 07:19:37,623 [Thread-237] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-390774184-172.17.0.6-1606979969464 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18/current/BP-390774184-172.17.0.6-1606979969464/current
2020-12-03 07:19:37,706 [IPC Server handler 3 on default port 44779] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:37,707 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:37,707 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:37,764 [Thread-105] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID 22bd5bbb-5aec-4015-af14-16567f170d84
2020-12-03 07:19:37,764 [Thread-59] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID 7a7e8fbe-c521-4997-b64d-db194b2bc262
2020-12-03 07:19:37,765 [Thread-193] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=1133803897;bpid=BP-390774184-172.17.0.6-1606979969464;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=1133803897;c=1606979969464;bpid=BP-390774184-172.17.0.6-1606979969464;dnuuid=null
2020-12-03 07:19:37,770 [Thread-127] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID 6e5d8d03-7267-400e-9598-80d9941dd61b
2020-12-03 07:19:37,771 [Thread-83] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID c39c5c86-f8fb-43b7-9a7e-df2bf8e2ff45
2020-12-03 07:19:37,777 [Thread-259] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:37,777 [Thread-259] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20/current/BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:37,778 [Thread-259] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20 and block pool id BP-390774184-172.17.0.6-1606979969464 is not formatted. Formatting ...
2020-12-03 07:19:37,778 [Thread-259] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-390774184-172.17.0.6-1606979969464 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20/current/BP-390774184-172.17.0.6-1606979969464/current
2020-12-03 07:19:37,813 [IPC Server handler 2 on default port 44779] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:37,814 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:37,814 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:37,907 [Thread-215] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=1133803897;bpid=BP-390774184-172.17.0.6-1606979969464;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=1133803897;c=1606979969464;bpid=BP-390774184-172.17.0.6-1606979969464;dnuuid=null
2020-12-03 07:19:37,920 [Thread-83] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-c85716ef-a148-4295-b2b7-bca92bf0548a
2020-12-03 07:19:37,920 [Thread-83] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3, StorageType: DISK
2020-12-03 07:19:37,920 [Thread-59] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-3bec7c54-7709-4f3d-b486-35765741195a
2020-12-03 07:19:37,920 [Thread-127] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-11ae3bf8-6838-4dfc-af89-3c76d111c85e
2020-12-03 07:19:37,922 [Thread-127] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7, StorageType: DISK
2020-12-03 07:19:37,922 [Thread-59] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, StorageType: DISK
2020-12-03 07:19:37,920 [Thread-105] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-34e32a14-5782-4fe9-b945-76fbaab513a9
2020-12-03 07:19:37,923 [Thread-105] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5, StorageType: DISK
2020-12-03 07:19:37,923 [IPC Server handler 1 on default port 44779] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:37,924 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:37,925 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:37,924 [Thread-83] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-72b6d766-f12d-46df-9b78-3689fbb0ff3f
2020-12-03 07:19:37,925 [Thread-83] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, StorageType: DISK
2020-12-03 07:19:37,925 [Thread-105] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-462ced07-43f8-4720-afda-b3153be81947
2020-12-03 07:19:37,925 [Thread-105] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6, StorageType: DISK
2020-12-03 07:19:37,930 [Thread-59] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-c396f456-72b2-4eec-a6af-559f05d184b3
2020-12-03 07:19:37,930 [Thread-59] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, StorageType: DISK
2020-12-03 07:19:37,930 [Thread-127] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-4d752d77-dc01-4782-88b7-fac90ddd2643
2020-12-03 07:19:37,932 [Thread-127] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8, StorageType: DISK
2020-12-03 07:19:37,940 [Thread-281] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:37,941 [Thread-281] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22/current/BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:37,941 [Thread-281] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22 and block pool id BP-390774184-172.17.0.6-1606979969464 is not formatted. Formatting ...
2020-12-03 07:19:37,941 [Thread-281] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-390774184-172.17.0.6-1606979969464 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22/current/BP-390774184-172.17.0.6-1606979969464/current
2020-12-03 07:19:37,957 [Thread-127] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:19:37,959 [Thread-105] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:19:37,959 [Thread-83] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:19:37,959 [Thread-59] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:19:37,965 [Thread-127] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7
2020-12-03 07:19:37,968 [Thread-59] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:19:37,969 [Thread-83] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
2020-12-03 07:19:37,970 [Thread-105] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5
2020-12-03 07:19:37,974 [Thread-59] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:19:37,975 [Thread-127] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7
2020-12-03 07:19:37,980 [Thread-105] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5
2020-12-03 07:19:37,981 [Thread-83] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
2020-12-03 07:19:37,982 [Thread-59] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:19:37,982 [Thread-59] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:19:37,982 [Thread-59] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:37,983 [Thread-105] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6
2020-12-03 07:19:37,983 [Thread-83] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:19:37,983 [Thread-127] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8
2020-12-03 07:19:37,983 [Thread-83] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:19:37,984 [Thread-303] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2020-12-03 07:19:37,984 [Thread-304] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2020-12-03 07:19:37,984 [Thread-127] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8
2020-12-03 07:19:37,983 [Thread-105] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6
2020-12-03 07:19:37,985 [Thread-105] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:37,985 [Thread-127] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:37,986 [Thread-305] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5...
2020-12-03 07:19:37,986 [Thread-306] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6...
2020-12-03 07:19:37,986 [Thread-307] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7...
2020-12-03 07:19:37,986 [Thread-308] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8...
2020-12-03 07:19:38,007 [Thread-83] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:38,008 [Thread-310] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4...
2020-12-03 07:19:38,008 [Thread-309] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3...
2020-12-03 07:19:38,030 [IPC Server handler 9 on default port 44779] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:38,031 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:38,031 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:38,071 [Thread-149] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID 55b8e545-b17d-4893-81a8-bea205778f84
2020-12-03 07:19:38,075 [Thread-149] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-035cd092-cc9b-40a4-8e6a-3004ff2553ce
2020-12-03 07:19:38,075 [Thread-149] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9, StorageType: DISK
2020-12-03 07:19:38,077 [Thread-149] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-5167c021-7d28-44ed-982a-8cb38c70870d
2020-12-03 07:19:38,078 [Thread-149] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10, StorageType: DISK
2020-12-03 07:19:38,079 [Thread-149] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:19:38,080 [Thread-307] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-390774184-172.17.0.6-1606979969464 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7: 94ms
2020-12-03 07:19:38,081 [Thread-310] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-390774184-172.17.0.6-1606979969464 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4: 72ms
2020-12-03 07:19:38,081 [Thread-149] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9
2020-12-03 07:19:38,084 [Thread-303] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-390774184-172.17.0.6-1606979969464 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 99ms
2020-12-03 07:19:38,090 [Thread-149] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9
2020-12-03 07:19:38,090 [Thread-149] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10
2020-12-03 07:19:38,090 [Thread-149] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10
2020-12-03 07:19:38,104 [Thread-149] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:38,106 [Thread-321] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9...
2020-12-03 07:19:38,106 [Thread-322] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10...
2020-12-03 07:19:38,111 [Thread-305] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-390774184-172.17.0.6-1606979969464 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5: 125ms
2020-12-03 07:19:38,111 [Thread-304] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-390774184-172.17.0.6-1606979969464 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 126ms
2020-12-03 07:19:38,111 [Thread-59] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-390774184-172.17.0.6-1606979969464: 129ms
2020-12-03 07:19:38,113 [Thread-306] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-390774184-172.17.0.6-1606979969464 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6: 127ms
2020-12-03 07:19:38,113 [Thread-105] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-390774184-172.17.0.6-1606979969464: 128ms
2020-12-03 07:19:38,114 [Thread-323] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5...
2020-12-03 07:19:38,115 [Thread-323] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-390774184-172.17.0.6-1606979969464/current/replicas doesn't exist 
2020-12-03 07:19:38,115 [Thread-309] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-390774184-172.17.0.6-1606979969464 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3: 106ms
2020-12-03 07:19:38,115 [Thread-83] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-390774184-172.17.0.6-1606979969464: 107ms
2020-12-03 07:19:38,115 [Thread-325] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6...
2020-12-03 07:19:38,120 [Thread-324] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2020-12-03 07:19:38,120 [Thread-325] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/current/BP-390774184-172.17.0.6-1606979969464/current/replicas doesn't exist 
2020-12-03 07:19:38,121 [Thread-327] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3...
2020-12-03 07:19:38,122 [Thread-328] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4...
2020-12-03 07:19:38,122 [Thread-326] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2020-12-03 07:19:38,122 [Thread-326] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-390774184-172.17.0.6-1606979969464/current/replicas doesn't exist 
2020-12-03 07:19:38,122 [Thread-327] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-390774184-172.17.0.6-1606979969464/current/replicas doesn't exist 
2020-12-03 07:19:38,122 [Thread-324] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-390774184-172.17.0.6-1606979969464/current/replicas doesn't exist 
2020-12-03 07:19:38,123 [Thread-325] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6: 3ms
2020-12-03 07:19:38,122 [Thread-328] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-390774184-172.17.0.6-1606979969464/current/replicas doesn't exist 
2020-12-03 07:19:38,127 [Thread-326] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 1ms
2020-12-03 07:19:38,133 [Thread-324] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 13ms
2020-12-03 07:19:38,127 [Thread-323] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5: 8ms
2020-12-03 07:19:38,134 [Thread-59] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-390774184-172.17.0.6-1606979969464: 20ms
2020-12-03 07:19:38,134 [Thread-327] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3: 12ms
2020-12-03 07:19:38,133 [Thread-328] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4: 11ms
2020-12-03 07:19:38,134 [Thread-105] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-390774184-172.17.0.6-1606979969464: 20ms
2020-12-03 07:19:38,136 [IPC Server handler 4 on default port 44779] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:38,137 [Thread-308] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-390774184-172.17.0.6-1606979969464 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8: 151ms
2020-12-03 07:19:38,137 [Thread-127] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-390774184-172.17.0.6-1606979969464: 151ms
2020-12-03 07:19:38,138 [Thread-83] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-390774184-172.17.0.6-1606979969464: 24ms
2020-12-03 07:19:38,139 [Thread-331] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7...
2020-12-03 07:19:38,139 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:19:38,144 [Thread-331] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7/current/BP-390774184-172.17.0.6-1606979969464/current/replicas doesn't exist 
2020-12-03 07:19:38,144 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:19:38,144 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
2020-12-03 07:19:38,146 [Thread-331] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7: 7ms
2020-12-03 07:19:38,144 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6
2020-12-03 07:19:38,139 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:19:38,139 [Thread-332] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8...
2020-12-03 07:19:38,148 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:38,148 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:38,144 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5
2020-12-03 07:19:38,148 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, DS-72b6d766-f12d-46df-9b78-3689fbb0ff3f): finished scanning block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:38,148 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-c396f456-72b2-4eec-a6af-559f05d184b3): finished scanning block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:38,148 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3, DS-c85716ef-a148-4295-b2b7-bca92bf0548a): finished scanning block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:38,148 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6, DS-462ced07-43f8-4720-afda-b3153be81947): finished scanning block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:38,148 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-3bec7c54-7709-4f3d-b486-35765741195a): finished scanning block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:38,148 [Thread-332] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/current/BP-390774184-172.17.0.6-1606979969464/current/replicas doesn't exist 
2020-12-03 07:19:38,151 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5, DS-34e32a14-5782-4fe9-b945-76fbaab513a9): finished scanning block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:38,153 [Thread-332] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8: 5ms
2020-12-03 07:19:38,156 [Thread-127] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-390774184-172.17.0.6-1606979969464: 17ms
2020-12-03 07:19:38,156 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7
2020-12-03 07:19:38,157 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8
2020-12-03 07:19:38,157 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7, DS-11ae3bf8-6838-4dfc-af89-3c76d111c85e): finished scanning block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:38,157 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8, DS-4d752d77-dc01-4782-88b7-fac90ddd2643): finished scanning block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:38,161 [Thread-321] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-390774184-172.17.0.6-1606979969464 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9: 55ms
2020-12-03 07:19:38,161 [Thread-322] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-390774184-172.17.0.6-1606979969464 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10: 54ms
2020-12-03 07:19:38,161 [Thread-149] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-390774184-172.17.0.6-1606979969464: 57ms
2020-12-03 07:19:38,162 [Thread-341] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9...
2020-12-03 07:19:38,214 [Thread-341] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9/current/BP-390774184-172.17.0.6-1606979969464/current/replicas doesn't exist 
2020-12-03 07:19:38,215 [Thread-341] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9: 1ms
2020-12-03 07:19:38,215 [Thread-342] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10...
2020-12-03 07:19:38,225 [Thread-342] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10/current/BP-390774184-172.17.0.6-1606979969464/current/replicas doesn't exist 
2020-12-03 07:19:38,249 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, DS-72b6d766-f12d-46df-9b78-3689fbb0ff3f): no suitable block pools found to scan.  Waiting 1814399890 ms.
2020-12-03 07:19:38,249 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5, DS-34e32a14-5782-4fe9-b945-76fbaab513a9): no suitable block pools found to scan.  Waiting 1814399890 ms.
2020-12-03 07:19:38,248 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8, DS-4d752d77-dc01-4782-88b7-fac90ddd2643): no suitable block pools found to scan.  Waiting 1814399909 ms.
2020-12-03 07:19:38,248 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6, DS-462ced07-43f8-4720-afda-b3153be81947): no suitable block pools found to scan.  Waiting 1814399891 ms.
2020-12-03 07:19:38,248 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3, DS-c85716ef-a148-4295-b2b7-bca92bf0548a): no suitable block pools found to scan.  Waiting 1814399891 ms.
2020-12-03 07:19:38,248 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7, DS-11ae3bf8-6838-4dfc-af89-3c76d111c85e): no suitable block pools found to scan.  Waiting 1814399908 ms.
2020-12-03 07:19:38,248 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-c396f456-72b2-4eec-a6af-559f05d184b3): no suitable block pools found to scan.  Waiting 1814399891 ms.
2020-12-03 07:19:38,240 [Thread-237] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=1133803897;bpid=BP-390774184-172.17.0.6-1606979969464;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=1133803897;c=1606979969464;bpid=BP-390774184-172.17.0.6-1606979969464;dnuuid=null
2020-12-03 07:19:38,230 [Thread-171] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID dc2fc2d4-8a32-4acd-8e94-f1a1b85207c7
2020-12-03 07:19:38,256 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-3bec7c54-7709-4f3d-b486-35765741195a): no suitable block pools found to scan.  Waiting 1814399883 ms.
2020-12-03 07:19:38,255 [Thread-342] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10: 31ms
2020-12-03 07:19:38,254 [IPC Server handler 7 on default port 44779] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:38,257 [Thread-149] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-390774184-172.17.0.6-1606979969464: 95ms
2020-12-03 07:19:38,258 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9
2020-12-03 07:19:38,258 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9, DS-035cd092-cc9b-40a4-8e6a-3004ff2553ce): finished scanning block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:38,259 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9, DS-035cd092-cc9b-40a4-8e6a-3004ff2553ce): no suitable block pools found to scan.  Waiting 1814399999 ms.
2020-12-03 07:19:38,258 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:38,259 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:38,259 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10
2020-12-03 07:19:38,260 [Thread-171] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-d3e3d4e2-c7e5-44e4-9c40-1b0d2f6aaca7
2020-12-03 07:19:38,260 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10, DS-5167c021-7d28-44ed-982a-8cb38c70870d): finished scanning block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:38,260 [Thread-171] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11, StorageType: DISK
2020-12-03 07:19:38,264 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10, DS-5167c021-7d28-44ed-982a-8cb38c70870d): no suitable block pools found to scan.  Waiting 1814399994 ms.
2020-12-03 07:19:38,265 [Thread-83] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 8:24 AM with interval of 21600000ms
2020-12-03 07:19:38,265 [Thread-105] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 11:54 AM with interval of 21600000ms
2020-12-03 07:19:38,267 [Thread-59] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 11:58 AM with interval of 21600000ms
2020-12-03 07:19:38,267 [Thread-171] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-265e643d-40cf-4be7-923e-86955d3df769
2020-12-03 07:19:38,267 [Thread-171] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12, StorageType: DISK
2020-12-03 07:19:38,268 [Thread-171] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:19:38,270 [Thread-127] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 8:12 AM with interval of 21600000ms
2020-12-03 07:19:38,270 [Thread-149] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 9:02 AM with interval of 21600000ms
2020-12-03 07:19:38,271 [Thread-171] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11
2020-12-03 07:19:38,276 [Thread-171] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11
2020-12-03 07:19:38,276 [Thread-171] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12
2020-12-03 07:19:38,276 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid c39c5c86-f8fb-43b7-9a7e-df2bf8e2ff45) service to localhost/127.0.0.1:44779 beginning handshake with NN
2020-12-03 07:19:38,276 [Thread-171] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12
2020-12-03 07:19:38,276 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid 22bd5bbb-5aec-4015-af14-16567f170d84) service to localhost/127.0.0.1:44779 beginning handshake with NN
2020-12-03 07:19:38,276 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid 55b8e545-b17d-4893-81a8-bea205778f84) service to localhost/127.0.0.1:44779 beginning handshake with NN
2020-12-03 07:19:38,276 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid 6e5d8d03-7267-400e-9598-80d9941dd61b) service to localhost/127.0.0.1:44779 beginning handshake with NN
2020-12-03 07:19:38,277 [Thread-171] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:38,278 [Thread-352] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11...
2020-12-03 07:19:38,278 [Thread-353] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12...
2020-12-03 07:19:38,282 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid 7a7e8fbe-c521-4997-b64d-db194b2bc262) service to localhost/127.0.0.1:44779 beginning handshake with NN
2020-12-03 07:19:38,293 [IPC Server handler 5 on default port 44779] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:43233, datanodeUuid=6e5d8d03-7267-400e-9598-80d9941dd61b, infoPort=41049, infoSecurePort=0, ipcPort=41306, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464) storage 6e5d8d03-7267-400e-9598-80d9941dd61b
2020-12-03 07:19:38,295 [IPC Server handler 5 on default port 44779] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:43233
2020-12-03 07:19:38,296 [IPC Server handler 5 on default port 44779] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 6e5d8d03-7267-400e-9598-80d9941dd61b (127.0.0.1:43233).
2020-12-03 07:19:38,305 [IPC Server handler 2 on default port 44779] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:40369, datanodeUuid=55b8e545-b17d-4893-81a8-bea205778f84, infoPort=34120, infoSecurePort=0, ipcPort=41056, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464) storage 55b8e545-b17d-4893-81a8-bea205778f84
2020-12-03 07:19:38,305 [IPC Server handler 2 on default port 44779] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:40369
2020-12-03 07:19:38,307 [IPC Server handler 2 on default port 44779] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 55b8e545-b17d-4893-81a8-bea205778f84 (127.0.0.1:40369).
2020-12-03 07:19:38,315 [IPC Server handler 0 on default port 44779] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:36972, datanodeUuid=c39c5c86-f8fb-43b7-9a7e-df2bf8e2ff45, infoPort=42569, infoSecurePort=0, ipcPort=43158, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464) storage c39c5c86-f8fb-43b7-9a7e-df2bf8e2ff45
2020-12-03 07:19:38,315 [IPC Server handler 0 on default port 44779] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:36972
2020-12-03 07:19:38,315 [IPC Server handler 0 on default port 44779] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN c39c5c86-f8fb-43b7-9a7e-df2bf8e2ff45 (127.0.0.1:36972).
2020-12-03 07:19:38,318 [IPC Server handler 8 on default port 44779] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:43468, datanodeUuid=22bd5bbb-5aec-4015-af14-16567f170d84, infoPort=37309, infoSecurePort=0, ipcPort=43239, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464) storage 22bd5bbb-5aec-4015-af14-16567f170d84
2020-12-03 07:19:38,319 [IPC Server handler 8 on default port 44779] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:43468
2020-12-03 07:19:38,319 [IPC Server handler 8 on default port 44779] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 22bd5bbb-5aec-4015-af14-16567f170d84 (127.0.0.1:43468).
2020-12-03 07:19:38,319 [IPC Server handler 3 on default port 44779] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:40990, datanodeUuid=7a7e8fbe-c521-4997-b64d-db194b2bc262, infoPort=36857, infoSecurePort=0, ipcPort=40094, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464) storage 7a7e8fbe-c521-4997-b64d-db194b2bc262
2020-12-03 07:19:38,320 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid 6e5d8d03-7267-400e-9598-80d9941dd61b) service to localhost/127.0.0.1:44779 successfully registered with NN
2020-12-03 07:19:38,320 [IPC Server handler 3 on default port 44779] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:40990
2020-12-03 07:19:38,320 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (DataNode.java:registerBlockPoolWithSecretManager(1615)) - Block token params received from NN: for block pool BP-390774184-172.17.0.6-1606979969464 keyUpdateInterval=600 min(s), tokenLifetime=600 min(s)
2020-12-03 07:19:38,320 [IPC Server handler 3 on default port 44779] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 7a7e8fbe-c521-4997-b64d-db194b2bc262 (127.0.0.1:40990).
2020-12-03 07:19:38,321 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:19:38,321 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:44779 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:19:38,328 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid c39c5c86-f8fb-43b7-9a7e-df2bf8e2ff45) service to localhost/127.0.0.1:44779 successfully registered with NN
2020-12-03 07:19:38,328 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid 55b8e545-b17d-4893-81a8-bea205778f84) service to localhost/127.0.0.1:44779 successfully registered with NN
2020-12-03 07:19:38,328 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (DataNode.java:registerBlockPoolWithSecretManager(1615)) - Block token params received from NN: for block pool BP-390774184-172.17.0.6-1606979969464 keyUpdateInterval=600 min(s), tokenLifetime=600 min(s)
2020-12-03 07:19:38,329 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:19:38,329 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:44779 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:19:38,329 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid 22bd5bbb-5aec-4015-af14-16567f170d84) service to localhost/127.0.0.1:44779 successfully registered with NN
2020-12-03 07:19:38,337 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (DataNode.java:registerBlockPoolWithSecretManager(1615)) - Block token params received from NN: for block pool BP-390774184-172.17.0.6-1606979969464 keyUpdateInterval=600 min(s), tokenLifetime=600 min(s)
2020-12-03 07:19:38,337 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (DataNode.java:registerBlockPoolWithSecretManager(1615)) - Block token params received from NN: for block pool BP-390774184-172.17.0.6-1606979969464 keyUpdateInterval=600 min(s), tokenLifetime=600 min(s)
2020-12-03 07:19:38,338 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:19:38,338 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:44779 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:19:38,337 [Thread-259] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=1133803897;bpid=BP-390774184-172.17.0.6-1606979969464;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=1133803897;c=1606979969464;bpid=BP-390774184-172.17.0.6-1606979969464;dnuuid=null
2020-12-03 07:19:38,337 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid 7a7e8fbe-c521-4997-b64d-db194b2bc262) service to localhost/127.0.0.1:44779 successfully registered with NN
2020-12-03 07:19:38,341 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (DataNode.java:registerBlockPoolWithSecretManager(1615)) - Block token params received from NN: for block pool BP-390774184-172.17.0.6-1606979969464 keyUpdateInterval=600 min(s), tokenLifetime=600 min(s)
2020-12-03 07:19:38,342 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:19:38,342 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:44779 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:19:38,341 [Thread-193] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID 288350fe-5f15-4f60-866b-b391b14a3073
2020-12-03 07:19:38,337 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:19:38,343 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:44779 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:19:38,349 [Thread-352] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-390774184-172.17.0.6-1606979969464 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11: 71ms
2020-12-03 07:19:38,350 [Thread-193] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-2ce634e7-1c83-4eb2-9adf-081181b40a1c
2020-12-03 07:19:38,350 [Thread-193] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13, StorageType: DISK
2020-12-03 07:19:38,352 [Thread-353] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-390774184-172.17.0.6-1606979969464 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12: 74ms
2020-12-03 07:19:38,353 [Thread-171] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-390774184-172.17.0.6-1606979969464: 76ms
2020-12-03 07:19:38,353 [Thread-193] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-023f6a39-4389-43da-a1df-f7cccb6bcc41
2020-12-03 07:19:38,354 [Thread-193] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14, StorageType: DISK
2020-12-03 07:19:38,355 [Thread-358] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11...
2020-12-03 07:19:38,355 [Thread-358] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11/current/BP-390774184-172.17.0.6-1606979969464/current/replicas doesn't exist 
2020-12-03 07:19:38,355 [Thread-193] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:19:38,356 [Thread-358] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11: 1ms
2020-12-03 07:19:38,357 [Thread-359] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12...
2020-12-03 07:19:38,357 [Thread-359] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12/current/BP-390774184-172.17.0.6-1606979969464/current/replicas doesn't exist 
2020-12-03 07:19:38,357 [Thread-193] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13
2020-12-03 07:19:38,357 [Thread-359] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12: 0ms
2020-12-03 07:19:38,360 [Thread-171] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-390774184-172.17.0.6-1606979969464: 6ms
2020-12-03 07:19:38,360 [Thread-193] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13
2020-12-03 07:19:38,360 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12
2020-12-03 07:19:38,361 [Thread-193] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14
2020-12-03 07:19:38,361 [Thread-193] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14
2020-12-03 07:19:38,361 [Thread-171] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 9:34 AM with interval of 21600000ms
2020-12-03 07:19:38,361 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12, DS-265e643d-40cf-4be7-923e-86955d3df769): finished scanning block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:38,361 [Thread-193] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:38,362 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11
2020-12-03 07:19:38,362 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12, DS-265e643d-40cf-4be7-923e-86955d3df769): no suitable block pools found to scan.  Waiting 1814399998 ms.
2020-12-03 07:19:38,368 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11, DS-d3e3d4e2-c7e5-44e4-9c40-1b0d2f6aaca7): finished scanning block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:38,369 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11, DS-d3e3d4e2-c7e5-44e4-9c40-1b0d2f6aaca7): no suitable block pools found to scan.  Waiting 1814399991 ms.
2020-12-03 07:19:38,372 [Thread-363] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13...
2020-12-03 07:19:38,381 [Thread-364] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14...
2020-12-03 07:19:38,381 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid dc2fc2d4-8a32-4acd-8e94-f1a1b85207c7) service to localhost/127.0.0.1:44779 beginning handshake with NN
2020-12-03 07:19:38,384 [IPC Server handler 1 on default port 44779] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:38,392 [IPC Server handler 6 on default port 44779] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-11ae3bf8-6838-4dfc-af89-3c76d111c85e for DN 127.0.0.1:43233
2020-12-03 07:19:38,405 [IPC Server handler 6 on default port 44779] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-4d752d77-dc01-4782-88b7-fac90ddd2643 for DN 127.0.0.1:43233
2020-12-03 07:19:38,406 [IPC Server handler 9 on default port 44779] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-34e32a14-5782-4fe9-b945-76fbaab513a9 for DN 127.0.0.1:43468
2020-12-03 07:19:38,409 [IPC Server handler 9 on default port 44779] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-462ced07-43f8-4720-afda-b3153be81947 for DN 127.0.0.1:43468
2020-12-03 07:19:38,415 [IPC Server handler 4 on default port 44779] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-c85716ef-a148-4295-b2b7-bca92bf0548a for DN 127.0.0.1:36972
2020-12-03 07:19:38,415 [IPC Server handler 4 on default port 44779] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-72b6d766-f12d-46df-9b78-3689fbb0ff3f for DN 127.0.0.1:36972
2020-12-03 07:19:38,415 [IPC Server handler 7 on default port 44779] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-3bec7c54-7709-4f3d-b486-35765741195a for DN 127.0.0.1:40990
2020-12-03 07:19:38,416 [IPC Server handler 7 on default port 44779] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-c396f456-72b2-4eec-a6af-559f05d184b3 for DN 127.0.0.1:40990
2020-12-03 07:19:38,418 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:38,419 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:38,418 [Thread-281] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=1133803897;bpid=BP-390774184-172.17.0.6-1606979969464;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=1133803897;c=1606979969464;bpid=BP-390774184-172.17.0.6-1606979969464;dnuuid=null
2020-12-03 07:19:38,418 [IPC Server handler 5 on default port 44779] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:35598, datanodeUuid=dc2fc2d4-8a32-4acd-8e94-f1a1b85207c7, infoPort=44165, infoSecurePort=0, ipcPort=44302, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464) storage dc2fc2d4-8a32-4acd-8e94-f1a1b85207c7
2020-12-03 07:19:38,421 [IPC Server handler 5 on default port 44779] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:35598
2020-12-03 07:19:38,421 [IPC Server handler 5 on default port 44779] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN dc2fc2d4-8a32-4acd-8e94-f1a1b85207c7 (127.0.0.1:35598).
2020-12-03 07:19:38,422 [IPC Server handler 0 on default port 44779] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-035cd092-cc9b-40a4-8e6a-3004ff2553ce for DN 127.0.0.1:40369
2020-12-03 07:19:38,422 [IPC Server handler 0 on default port 44779] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-5167c021-7d28-44ed-982a-8cb38c70870d for DN 127.0.0.1:40369
2020-12-03 07:19:38,422 [Thread-215] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID 24857262-68d9-4b98-abb9-9105d4bfc034
2020-12-03 07:19:38,424 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid dc2fc2d4-8a32-4acd-8e94-f1a1b85207c7) service to localhost/127.0.0.1:44779 successfully registered with NN
2020-12-03 07:19:38,424 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (DataNode.java:registerBlockPoolWithSecretManager(1615)) - Block token params received from NN: for block pool BP-390774184-172.17.0.6-1606979969464 keyUpdateInterval=600 min(s), tokenLifetime=600 min(s)
2020-12-03 07:19:38,424 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:19:38,425 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:44779 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:19:38,431 [IPC Server handler 2 on default port 44779] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-d3e3d4e2-c7e5-44e4-9c40-1b0d2f6aaca7 for DN 127.0.0.1:35598
2020-12-03 07:19:38,431 [IPC Server handler 2 on default port 44779] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-265e643d-40cf-4be7-923e-86955d3df769 for DN 127.0.0.1:35598
2020-12-03 07:19:38,443 [Thread-364] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-390774184-172.17.0.6-1606979969464 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14: 62ms
2020-12-03 07:19:38,447 [Thread-215] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-997db01c-04dd-4559-92d6-ffa544c717ec
2020-12-03 07:19:38,467 [Thread-215] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15, StorageType: DISK
2020-12-03 07:19:38,513 [Thread-215] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-353fdb6d-79d4-40b3-891b-044f5d75a5de
2020-12-03 07:19:38,513 [Thread-215] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16, StorageType: DISK
2020-12-03 07:19:38,514 [Thread-215] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:19:38,532 [Thread-363] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-390774184-172.17.0.6-1606979969464 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13: 160ms
2020-12-03 07:19:38,535 [Thread-193] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-390774184-172.17.0.6-1606979969464: 174ms
2020-12-03 07:19:38,551 [Thread-369] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13...
2020-12-03 07:19:38,552 [IPC Server handler 3 on default port 44779] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:38,552 [Thread-369] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13/current/BP-390774184-172.17.0.6-1606979969464/current/replicas doesn't exist 
2020-12-03 07:19:38,552 [Thread-215] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15
2020-12-03 07:19:38,553 [Thread-369] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13: 1ms
2020-12-03 07:19:38,554 [Thread-370] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14...
2020-12-03 07:19:38,554 [Thread-370] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14/current/BP-390774184-172.17.0.6-1606979969464/current/replicas doesn't exist 
2020-12-03 07:19:38,555 [Thread-370] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14: 1ms
2020-12-03 07:19:38,555 [Thread-193] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-390774184-172.17.0.6-1606979969464: 20ms
2020-12-03 07:19:38,556 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13
2020-12-03 07:19:38,560 [Thread-215] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15
2020-12-03 07:19:38,560 [Thread-193] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 10:07 AM with interval of 21600000ms
2020-12-03 07:19:38,556 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14
2020-12-03 07:19:38,580 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid 288350fe-5f15-4f60-866b-b391b14a3073) service to localhost/127.0.0.1:44779 beginning handshake with NN
2020-12-03 07:19:38,581 [Thread-215] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16
2020-12-03 07:19:38,581 [Thread-237] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID a1eb1197-6763-4116-b38c-3702cdacda3d
2020-12-03 07:19:38,581 [Thread-215] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16
2020-12-03 07:19:38,581 [Thread-215] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:38,581 [Thread-374] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15...
2020-12-03 07:19:38,582 [Thread-375] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16...
2020-12-03 07:19:38,586 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x334613d0796bf6ab: Processing first storage report for DS-c396f456-72b2-4eec-a6af-559f05d184b3 from datanode 7a7e8fbe-c521-4997-b64d-db194b2bc262
2020-12-03 07:19:38,616 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:38,616 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:38,622 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x334613d0796bf6ab: from storage DS-c396f456-72b2-4eec-a6af-559f05d184b3 node DatanodeRegistration(127.0.0.1:40990, datanodeUuid=7a7e8fbe-c521-4997-b64d-db194b2bc262, infoPort=36857, infoSecurePort=0, ipcPort=40094, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464), blocks: 0, hasStaleStorage: true, processing time: 36 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,623 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x1d78e80a3e6a007f: Processing first storage report for DS-34e32a14-5782-4fe9-b945-76fbaab513a9 from datanode 22bd5bbb-5aec-4015-af14-16567f170d84
2020-12-03 07:19:38,623 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x1d78e80a3e6a007f: from storage DS-34e32a14-5782-4fe9-b945-76fbaab513a9 node DatanodeRegistration(127.0.0.1:43468, datanodeUuid=22bd5bbb-5aec-4015-af14-16567f170d84, infoPort=37309, infoSecurePort=0, ipcPort=43239, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,623 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0xf2f3690f1b7df9ba: Processing first storage report for DS-d3e3d4e2-c7e5-44e4-9c40-1b0d2f6aaca7 from datanode dc2fc2d4-8a32-4acd-8e94-f1a1b85207c7
2020-12-03 07:19:38,624 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0xf2f3690f1b7df9ba: from storage DS-d3e3d4e2-c7e5-44e4-9c40-1b0d2f6aaca7 node DatanodeRegistration(127.0.0.1:35598, datanodeUuid=dc2fc2d4-8a32-4acd-8e94-f1a1b85207c7, infoPort=44165, infoSecurePort=0, ipcPort=44302, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,624 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x51fa1e1b82899da9: Processing first storage report for DS-72b6d766-f12d-46df-9b78-3689fbb0ff3f from datanode c39c5c86-f8fb-43b7-9a7e-df2bf8e2ff45
2020-12-03 07:19:38,624 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x51fa1e1b82899da9: from storage DS-72b6d766-f12d-46df-9b78-3689fbb0ff3f node DatanodeRegistration(127.0.0.1:36972, datanodeUuid=c39c5c86-f8fb-43b7-9a7e-df2bf8e2ff45, infoPort=42569, infoSecurePort=0, ipcPort=43158, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464), blocks: 0, hasStaleStorage: true, processing time: 1 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,625 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0xeadce6decf90dfa9: Processing first storage report for DS-5167c021-7d28-44ed-982a-8cb38c70870d from datanode 55b8e545-b17d-4893-81a8-bea205778f84
2020-12-03 07:19:38,625 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0xeadce6decf90dfa9: from storage DS-5167c021-7d28-44ed-982a-8cb38c70870d node DatanodeRegistration(127.0.0.1:40369, datanodeUuid=55b8e545-b17d-4893-81a8-bea205778f84, infoPort=34120, infoSecurePort=0, ipcPort=41056, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,626 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14, DS-023f6a39-4389-43da-a1df-f7cccb6bcc41): finished scanning block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:38,627 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14, DS-023f6a39-4389-43da-a1df-f7cccb6bcc41): no suitable block pools found to scan.  Waiting 1814399929 ms.
2020-12-03 07:19:38,629 [Thread-237] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-7a239de6-36ad-4a59-8b2f-83eb72cddab6
2020-12-03 07:19:38,629 [Thread-237] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17, StorageType: DISK
2020-12-03 07:19:38,631 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x34803805a9a7455f: Processing first storage report for DS-4d752d77-dc01-4782-88b7-fac90ddd2643 from datanode 6e5d8d03-7267-400e-9598-80d9941dd61b
2020-12-03 07:19:38,632 [Thread-237] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-6c847cf4-ff4b-4f0f-91b4-4b65310e2c06
2020-12-03 07:19:38,632 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x34803805a9a7455f: from storage DS-4d752d77-dc01-4782-88b7-fac90ddd2643 node DatanodeRegistration(127.0.0.1:43233, datanodeUuid=6e5d8d03-7267-400e-9598-80d9941dd61b, infoPort=41049, infoSecurePort=0, ipcPort=41306, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464), blocks: 0, hasStaleStorage: true, processing time: 7 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,644 [Thread-259] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID 1671bb40-be61-48b3-8dfe-d4cda3f68f8f
2020-12-03 07:19:38,632 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13, DS-2ce634e7-1c83-4eb2-9adf-081181b40a1c): finished scanning block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:38,632 [Thread-237] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18, StorageType: DISK
2020-12-03 07:19:38,653 [Thread-237] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:19:38,656 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13, DS-2ce634e7-1c83-4eb2-9adf-081181b40a1c): no suitable block pools found to scan.  Waiting 1814399900 ms.
2020-12-03 07:19:38,656 [IPC Server handler 6 on default port 44779] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:33826, datanodeUuid=288350fe-5f15-4f60-866b-b391b14a3073, infoPort=37619, infoSecurePort=0, ipcPort=34339, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464) storage 288350fe-5f15-4f60-866b-b391b14a3073
2020-12-03 07:19:38,657 [IPC Server handler 6 on default port 44779] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:33826
2020-12-03 07:19:38,657 [IPC Server handler 6 on default port 44779] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 288350fe-5f15-4f60-866b-b391b14a3073 (127.0.0.1:33826).
2020-12-03 07:19:38,658 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0xf2f3690f1b7df9ba: Processing first storage report for DS-265e643d-40cf-4be7-923e-86955d3df769 from datanode dc2fc2d4-8a32-4acd-8e94-f1a1b85207c7
2020-12-03 07:19:38,658 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0xf2f3690f1b7df9ba: from storage DS-265e643d-40cf-4be7-923e-86955d3df769 node DatanodeRegistration(127.0.0.1:35598, datanodeUuid=dc2fc2d4-8a32-4acd-8e94-f1a1b85207c7, infoPort=44165, infoSecurePort=0, ipcPort=44302, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,658 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x1d78e80a3e6a007f: Processing first storage report for DS-462ced07-43f8-4720-afda-b3153be81947 from datanode 22bd5bbb-5aec-4015-af14-16567f170d84
2020-12-03 07:19:38,658 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x1d78e80a3e6a007f: from storage DS-462ced07-43f8-4720-afda-b3153be81947 node DatanodeRegistration(127.0.0.1:43468, datanodeUuid=22bd5bbb-5aec-4015-af14-16567f170d84, infoPort=37309, infoSecurePort=0, ipcPort=43239, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,658 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0xeadce6decf90dfa9: Processing first storage report for DS-035cd092-cc9b-40a4-8e6a-3004ff2553ce from datanode 55b8e545-b17d-4893-81a8-bea205778f84
2020-12-03 07:19:38,659 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0xeadce6decf90dfa9: from storage DS-035cd092-cc9b-40a4-8e6a-3004ff2553ce node DatanodeRegistration(127.0.0.1:40369, datanodeUuid=55b8e545-b17d-4893-81a8-bea205778f84, infoPort=34120, infoSecurePort=0, ipcPort=41056, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,659 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x334613d0796bf6ab: Processing first storage report for DS-3bec7c54-7709-4f3d-b486-35765741195a from datanode 7a7e8fbe-c521-4997-b64d-db194b2bc262
2020-12-03 07:19:38,659 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x334613d0796bf6ab: from storage DS-3bec7c54-7709-4f3d-b486-35765741195a node DatanodeRegistration(127.0.0.1:40990, datanodeUuid=7a7e8fbe-c521-4997-b64d-db194b2bc262, infoPort=36857, infoSecurePort=0, ipcPort=40094, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,659 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x51fa1e1b82899da9: Processing first storage report for DS-c85716ef-a148-4295-b2b7-bca92bf0548a from datanode c39c5c86-f8fb-43b7-9a7e-df2bf8e2ff45
2020-12-03 07:19:38,659 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x51fa1e1b82899da9: from storage DS-c85716ef-a148-4295-b2b7-bca92bf0548a node DatanodeRegistration(127.0.0.1:36972, datanodeUuid=c39c5c86-f8fb-43b7-9a7e-df2bf8e2ff45, infoPort=42569, infoSecurePort=0, ipcPort=43158, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,659 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x34803805a9a7455f: Processing first storage report for DS-11ae3bf8-6838-4dfc-af89-3c76d111c85e from datanode 6e5d8d03-7267-400e-9598-80d9941dd61b
2020-12-03 07:19:38,660 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x34803805a9a7455f: from storage DS-11ae3bf8-6838-4dfc-af89-3c76d111c85e node DatanodeRegistration(127.0.0.1:43233, datanodeUuid=6e5d8d03-7267-400e-9598-80d9941dd61b, infoPort=41049, infoSecurePort=0, ipcPort=41306, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,661 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid 288350fe-5f15-4f60-866b-b391b14a3073) service to localhost/127.0.0.1:44779 successfully registered with NN
2020-12-03 07:19:38,667 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (DataNode.java:registerBlockPoolWithSecretManager(1615)) - Block token params received from NN: for block pool BP-390774184-172.17.0.6-1606979969464 keyUpdateInterval=600 min(s), tokenLifetime=600 min(s)
2020-12-03 07:19:38,667 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:19:38,668 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:44779 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:19:38,680 [Thread-375] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-390774184-172.17.0.6-1606979969464 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16: 99ms
2020-12-03 07:19:38,683 [Thread-237] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17
2020-12-03 07:19:38,685 [Thread-237] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17
2020-12-03 07:19:38,685 [Thread-237] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18
2020-12-03 07:19:38,685 [Thread-237] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18
2020-12-03 07:19:38,687 [IPC Server handler 7 on default port 44779] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-2ce634e7-1c83-4eb2-9adf-081181b40a1c for DN 127.0.0.1:33826
2020-12-03 07:19:38,687 [IPC Server handler 7 on default port 44779] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-023f6a39-4389-43da-a1df-f7cccb6bcc41 for DN 127.0.0.1:33826
2020-12-03 07:19:38,689 [Thread-259] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-3c77b156-23e2-4fc9-9db2-bac28e329697
2020-12-03 07:19:38,700 [Thread-259] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19, StorageType: DISK
2020-12-03 07:19:38,701 [Thread-281] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID 68a96d2b-90b0-4ee3-b0be-e2a02cebb74b
2020-12-03 07:19:38,710 [Thread-259] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-aa4eafc1-6206-4cf4-beac-6f7c6e133413
2020-12-03 07:19:38,710 [Thread-259] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20, StorageType: DISK
2020-12-03 07:19:38,711 [Thread-259] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:19:38,714 [Thread-281] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-04af9d40-9522-498c-835c-e506c824a39f
2020-12-03 07:19:38,714 [Thread-281] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21, StorageType: DISK
2020-12-03 07:19:38,721 [Thread-237] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:38,722 [Thread-383] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17...
2020-12-03 07:19:38,724 [Thread-384] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18...
2020-12-03 07:19:38,725 [Thread-259] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19
2020-12-03 07:19:38,728 [IPC Server handler 4 on default port 44779] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:38,732 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:38,733 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:38,733 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x778887e34c936d70: Processing first storage report for DS-023f6a39-4389-43da-a1df-f7cccb6bcc41 from datanode 288350fe-5f15-4f60-866b-b391b14a3073
2020-12-03 07:19:38,733 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x778887e34c936d70: from storage DS-023f6a39-4389-43da-a1df-f7cccb6bcc41 node DatanodeRegistration(127.0.0.1:33826, datanodeUuid=288350fe-5f15-4f60-866b-b391b14a3073, infoPort=37619, infoSecurePort=0, ipcPort=34339, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,735 [Thread-259] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19
2020-12-03 07:19:38,736 [Thread-259] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20
2020-12-03 07:19:38,736 [Thread-259] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20
2020-12-03 07:19:38,738 [Thread-259] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:38,738 [Thread-281] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-6b4155c2-6b43-450c-b668-a1919719e233
2020-12-03 07:19:38,739 [Thread-281] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22, StorageType: DISK
2020-12-03 07:19:38,739 [Thread-281] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:19:38,740 [Thread-386] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19...
2020-12-03 07:19:38,740 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x778887e34c936d70: Processing first storage report for DS-2ce634e7-1c83-4eb2-9adf-081181b40a1c from datanode 288350fe-5f15-4f60-866b-b391b14a3073
2020-12-03 07:19:38,741 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x778887e34c936d70: from storage DS-2ce634e7-1c83-4eb2-9adf-081181b40a1c node DatanodeRegistration(127.0.0.1:33826, datanodeUuid=288350fe-5f15-4f60-866b-b391b14a3073, infoPort=37619, infoSecurePort=0, ipcPort=34339, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,742 [Thread-387] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20...
2020-12-03 07:19:38,742 [Thread-281] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21
2020-12-03 07:19:38,754 [Thread-281] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21
2020-12-03 07:19:38,755 [Thread-281] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22
2020-12-03 07:19:38,755 [Thread-281] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22
2020-12-03 07:19:38,774 [Thread-281] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:38,775 [Thread-388] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21...
2020-12-03 07:19:38,778 [Thread-374] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-390774184-172.17.0.6-1606979969464 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15: 197ms
2020-12-03 07:19:38,779 [Thread-215] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-390774184-172.17.0.6-1606979969464: 198ms
2020-12-03 07:19:38,784 [Thread-389] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22...
2020-12-03 07:19:38,785 [Thread-392] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15...
2020-12-03 07:19:38,785 [Thread-394] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16...
2020-12-03 07:19:38,786 [Thread-392] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15/current/BP-390774184-172.17.0.6-1606979969464/current/replicas doesn't exist 
2020-12-03 07:19:38,786 [Thread-394] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16/current/BP-390774184-172.17.0.6-1606979969464/current/replicas doesn't exist 
2020-12-03 07:19:38,788 [Thread-392] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15: 2ms
2020-12-03 07:19:38,788 [Thread-394] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16: 2ms
2020-12-03 07:19:38,789 [Thread-215] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-390774184-172.17.0.6-1606979969464: 10ms
2020-12-03 07:19:38,794 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0xf2f3690f1b7df9ba,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 19 msec to generate and 263 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:19:38,796 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x51fa1e1b82899da9,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 25 msec to generate and 268 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:19:38,796 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x778887e34c936d70,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 8 msec to generate and 55 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:19:38,796 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:38,796 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x34803805a9a7455f,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 37 msec to generate and 258 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:19:38,797 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:38,796 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15
2020-12-03 07:19:38,797 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15, DS-997db01c-04dd-4559-92d6-ffa544c717ec): finished scanning block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:38,798 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15, DS-997db01c-04dd-4559-92d6-ffa544c717ec): no suitable block pools found to scan.  Waiting 1814399997 ms.
2020-12-03 07:19:38,796 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16
2020-12-03 07:19:38,794 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x334613d0796bf6ab,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 262 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:19:38,798 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:38,794 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x1d78e80a3e6a007f,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 19 msec to generate and 269 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:19:38,799 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:38,799 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16, DS-353fdb6d-79d4-40b3-891b-044f5d75a5de): finished scanning block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:38,799 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16, DS-353fdb6d-79d4-40b3-891b-044f5d75a5de): no suitable block pools found to scan.  Waiting 1814399995 ms.
2020-12-03 07:19:38,800 [Thread-215] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 10:48 AM with interval of 21600000ms
2020-12-03 07:19:38,796 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:38,796 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:38,836 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0xeadce6decf90dfa9,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 1 msec to generate and 273 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:19:38,837 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:38,841 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid 24857262-68d9-4b98-abb9-9105d4bfc034) service to localhost/127.0.0.1:44779 beginning handshake with NN
2020-12-03 07:19:38,854 [IPC Server handler 6 on default port 44779] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:38,855 [IPC Server handler 7 on default port 44779] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:40799, datanodeUuid=24857262-68d9-4b98-abb9-9105d4bfc034, infoPort=42978, infoSecurePort=0, ipcPort=43524, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464) storage 24857262-68d9-4b98-abb9-9105d4bfc034
2020-12-03 07:19:38,855 [IPC Server handler 7 on default port 44779] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:40799
2020-12-03 07:19:38,856 [IPC Server handler 7 on default port 44779] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 24857262-68d9-4b98-abb9-9105d4bfc034 (127.0.0.1:40799).
2020-12-03 07:19:38,857 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:38,857 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:38,859 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid 24857262-68d9-4b98-abb9-9105d4bfc034) service to localhost/127.0.0.1:44779 successfully registered with NN
2020-12-03 07:19:38,859 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (DataNode.java:registerBlockPoolWithSecretManager(1615)) - Block token params received from NN: for block pool BP-390774184-172.17.0.6-1606979969464 keyUpdateInterval=600 min(s), tokenLifetime=600 min(s)
2020-12-03 07:19:38,859 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:19:38,859 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:44779 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:19:38,870 [Thread-384] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-390774184-172.17.0.6-1606979969464 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18: 146ms
2020-12-03 07:19:38,879 [Thread-386] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-390774184-172.17.0.6-1606979969464 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19: 140ms
2020-12-03 07:19:38,880 [IPC Server handler 4 on default port 44779] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-997db01c-04dd-4559-92d6-ffa544c717ec for DN 127.0.0.1:40799
2020-12-03 07:19:38,880 [IPC Server handler 4 on default port 44779] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-353fdb6d-79d4-40b3-891b-044f5d75a5de for DN 127.0.0.1:40799
2020-12-03 07:19:38,888 [Thread-387] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-390774184-172.17.0.6-1606979969464 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20: 147ms
2020-12-03 07:19:38,891 [Thread-383] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-390774184-172.17.0.6-1606979969464 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17: 169ms
2020-12-03 07:19:38,892 [Thread-259] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-390774184-172.17.0.6-1606979969464: 153ms
2020-12-03 07:19:38,893 [Thread-237] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-390774184-172.17.0.6-1606979969464: 171ms
2020-12-03 07:19:38,893 [Thread-401] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19...
2020-12-03 07:19:38,896 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0xdb578ad523d8de52: Processing first storage report for DS-997db01c-04dd-4559-92d6-ffa544c717ec from datanode 24857262-68d9-4b98-abb9-9105d4bfc034
2020-12-03 07:19:38,896 [Thread-401] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19/current/BP-390774184-172.17.0.6-1606979969464/current/replicas doesn't exist 
2020-12-03 07:19:38,896 [Thread-403] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17...
2020-12-03 07:19:38,896 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0xdb578ad523d8de52: from storage DS-997db01c-04dd-4559-92d6-ffa544c717ec node DatanodeRegistration(127.0.0.1:40799, datanodeUuid=24857262-68d9-4b98-abb9-9105d4bfc034, infoPort=42978, infoSecurePort=0, ipcPort=43524, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,897 [Thread-403] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17/current/BP-390774184-172.17.0.6-1606979969464/current/replicas doesn't exist 
2020-12-03 07:19:38,897 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0xdb578ad523d8de52: Processing first storage report for DS-353fdb6d-79d4-40b3-891b-044f5d75a5de from datanode 24857262-68d9-4b98-abb9-9105d4bfc034
2020-12-03 07:19:38,897 [Thread-401] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19: 2ms
2020-12-03 07:19:38,897 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0xdb578ad523d8de52: from storage DS-353fdb6d-79d4-40b3-891b-044f5d75a5de node DatanodeRegistration(127.0.0.1:40799, datanodeUuid=24857262-68d9-4b98-abb9-9105d4bfc034, infoPort=42978, infoSecurePort=0, ipcPort=43524, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,897 [Thread-403] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17: 0ms
2020-12-03 07:19:38,910 [Thread-404] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18...
2020-12-03 07:19:38,910 [Thread-404] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18/current/BP-390774184-172.17.0.6-1606979969464/current/replicas doesn't exist 
2020-12-03 07:19:38,914 [Thread-388] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-390774184-172.17.0.6-1606979969464 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21: 138ms
2020-12-03 07:19:38,914 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0xdb578ad523d8de52,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 28 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:19:38,914 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:38,915 [Thread-389] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-390774184-172.17.0.6-1606979969464 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22: 131ms
2020-12-03 07:19:38,915 [Thread-281] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-390774184-172.17.0.6-1606979969464: 141ms
2020-12-03 07:19:38,916 [Thread-402] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20...
2020-12-03 07:19:38,916 [Thread-402] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20/current/BP-390774184-172.17.0.6-1606979969464/current/replicas doesn't exist 
2020-12-03 07:19:38,916 [Thread-405] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21...
2020-12-03 07:19:38,916 [Thread-405] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21/current/BP-390774184-172.17.0.6-1606979969464/current/replicas doesn't exist 
2020-12-03 07:19:38,917 [Thread-402] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20: 1ms
2020-12-03 07:19:38,917 [Thread-259] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-390774184-172.17.0.6-1606979969464: 25ms
2020-12-03 07:19:38,915 [Thread-404] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18: 6ms
2020-12-03 07:19:38,925 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20
2020-12-03 07:19:38,926 [Thread-237] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-390774184-172.17.0.6-1606979969464: 33ms
2020-12-03 07:19:38,926 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20, DS-aa4eafc1-6206-4cf4-beac-6f7c6e133413): finished scanning block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:38,926 [Thread-259] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 8:31 AM with interval of 21600000ms
2020-12-03 07:19:38,926 [Thread-237] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 7:38 AM with interval of 21600000ms
2020-12-03 07:19:38,927 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20, DS-aa4eafc1-6206-4cf4-beac-6f7c6e133413): no suitable block pools found to scan.  Waiting 1814399998 ms.
2020-12-03 07:19:38,928 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid a1eb1197-6763-4116-b38c-3702cdacda3d) service to localhost/127.0.0.1:44779 beginning handshake with NN
2020-12-03 07:19:38,931 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18
2020-12-03 07:19:38,932 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18, DS-6c847cf4-ff4b-4f0f-91b4-4b65310e2c06): finished scanning block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:38,933 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18, DS-6c847cf4-ff4b-4f0f-91b4-4b65310e2c06): no suitable block pools found to scan.  Waiting 1814399993 ms.
2020-12-03 07:19:38,933 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17
2020-12-03 07:19:38,933 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17, DS-7a239de6-36ad-4a59-8b2f-83eb72cddab6): finished scanning block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:38,934 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17, DS-7a239de6-36ad-4a59-8b2f-83eb72cddab6): no suitable block pools found to scan.  Waiting 1814399992 ms.
2020-12-03 07:19:38,925 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19
2020-12-03 07:19:38,921 [Thread-405] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21: 5ms
2020-12-03 07:19:38,935 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19, DS-3c77b156-23e2-4fc9-9db2-bac28e329697): finished scanning block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:38,935 [IPC Server handler 0 on default port 44779] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:33393, datanodeUuid=a1eb1197-6763-4116-b38c-3702cdacda3d, infoPort=42572, infoSecurePort=0, ipcPort=36667, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464) storage a1eb1197-6763-4116-b38c-3702cdacda3d
2020-12-03 07:19:38,935 [IPC Server handler 0 on default port 44779] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:33393
2020-12-03 07:19:38,936 [IPC Server handler 0 on default port 44779] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN a1eb1197-6763-4116-b38c-3702cdacda3d (127.0.0.1:33393).
2020-12-03 07:19:38,916 [Thread-406] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22...
2020-12-03 07:19:38,937 [Thread-406] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22/current/BP-390774184-172.17.0.6-1606979969464/current/replicas doesn't exist 
2020-12-03 07:19:38,936 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19, DS-3c77b156-23e2-4fc9-9db2-bac28e329697): no suitable block pools found to scan.  Waiting 1814399981 ms.
2020-12-03 07:19:38,929 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid 1671bb40-be61-48b3-8dfe-d4cda3f68f8f) service to localhost/127.0.0.1:44779 beginning handshake with NN
2020-12-03 07:19:38,937 [Thread-406] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22: 0ms
2020-12-03 07:19:38,937 [Thread-281] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-390774184-172.17.0.6-1606979969464: 22ms
2020-12-03 07:19:38,938 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid a1eb1197-6763-4116-b38c-3702cdacda3d) service to localhost/127.0.0.1:44779 successfully registered with NN
2020-12-03 07:19:38,938 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (DataNode.java:registerBlockPoolWithSecretManager(1615)) - Block token params received from NN: for block pool BP-390774184-172.17.0.6-1606979969464 keyUpdateInterval=600 min(s), tokenLifetime=600 min(s)
2020-12-03 07:19:38,938 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21
2020-12-03 07:19:38,938 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-390774184-172.17.0.6-1606979969464 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22
2020-12-03 07:19:38,938 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21, DS-04af9d40-9522-498c-835c-e506c824a39f): finished scanning block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:38,938 [Thread-281] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 11:05 AM with interval of 21600000ms
2020-12-03 07:19:38,938 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:19:38,938 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22, DS-6b4155c2-6b43-450c-b668-a1919719e233): finished scanning block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:38,939 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:44779 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:19:38,940 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid 68a96d2b-90b0-4ee3-b0be-e2a02cebb74b) service to localhost/127.0.0.1:44779 beginning handshake with NN
2020-12-03 07:19:38,944 [IPC Server handler 8 on default port 44779] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:46334, datanodeUuid=1671bb40-be61-48b3-8dfe-d4cda3f68f8f, infoPort=35491, infoSecurePort=0, ipcPort=39541, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464) storage 1671bb40-be61-48b3-8dfe-d4cda3f68f8f
2020-12-03 07:19:38,945 [IPC Server handler 8 on default port 44779] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:46334
2020-12-03 07:19:38,945 [IPC Server handler 8 on default port 44779] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 1671bb40-be61-48b3-8dfe-d4cda3f68f8f (127.0.0.1:46334).
2020-12-03 07:19:38,946 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21, DS-04af9d40-9522-498c-835c-e506c824a39f): no suitable block pools found to scan.  Waiting 1814399992 ms.
2020-12-03 07:19:38,946 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid 1671bb40-be61-48b3-8dfe-d4cda3f68f8f) service to localhost/127.0.0.1:44779 successfully registered with NN
2020-12-03 07:19:38,947 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (DataNode.java:registerBlockPoolWithSecretManager(1615)) - Block token params received from NN: for block pool BP-390774184-172.17.0.6-1606979969464 keyUpdateInterval=600 min(s), tokenLifetime=600 min(s)
2020-12-03 07:19:38,947 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:19:38,947 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:44779 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:19:38,946 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22, DS-6b4155c2-6b43-450c-b668-a1919719e233): no suitable block pools found to scan.  Waiting 1814399992 ms.
2020-12-03 07:19:38,950 [IPC Server handler 9 on default port 44779] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:36213, datanodeUuid=68a96d2b-90b0-4ee3-b0be-e2a02cebb74b, infoPort=38285, infoSecurePort=0, ipcPort=32858, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464) storage 68a96d2b-90b0-4ee3-b0be-e2a02cebb74b
2020-12-03 07:19:38,956 [IPC Server handler 9 on default port 44779] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:36213
2020-12-03 07:19:38,956 [IPC Server handler 9 on default port 44779] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 68a96d2b-90b0-4ee3-b0be-e2a02cebb74b (127.0.0.1:36213).
2020-12-03 07:19:38,959 [IPC Server handler 2 on default port 44779] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-7a239de6-36ad-4a59-8b2f-83eb72cddab6 for DN 127.0.0.1:33393
2020-12-03 07:19:38,959 [IPC Server handler 2 on default port 44779] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-6c847cf4-ff4b-4f0f-91b4-4b65310e2c06 for DN 127.0.0.1:33393
2020-12-03 07:19:38,959 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid 68a96d2b-90b0-4ee3-b0be-e2a02cebb74b) service to localhost/127.0.0.1:44779 successfully registered with NN
2020-12-03 07:19:38,959 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (DataNode.java:registerBlockPoolWithSecretManager(1615)) - Block token params received from NN: for block pool BP-390774184-172.17.0.6-1606979969464 keyUpdateInterval=600 min(s), tokenLifetime=600 min(s)
2020-12-03 07:19:38,961 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:19:38,961 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:44779 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:19:38,960 [IPC Server handler 1 on default port 44779] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-3c77b156-23e2-4fc9-9db2-bac28e329697 for DN 127.0.0.1:46334
2020-12-03 07:19:38,964 [IPC Server handler 1 on default port 44779] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-aa4eafc1-6206-4cf4-beac-6f7c6e133413 for DN 127.0.0.1:46334
2020-12-03 07:19:38,964 [IPC Server handler 3 on default port 44779] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:38,964 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x252800b46bed3276: Processing first storage report for DS-6c847cf4-ff4b-4f0f-91b4-4b65310e2c06 from datanode a1eb1197-6763-4116-b38c-3702cdacda3d
2020-12-03 07:19:38,964 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x252800b46bed3276: from storage DS-6c847cf4-ff4b-4f0f-91b4-4b65310e2c06 node DatanodeRegistration(127.0.0.1:33393, datanodeUuid=a1eb1197-6763-4116-b38c-3702cdacda3d, infoPort=42572, infoSecurePort=0, ipcPort=36667, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464), blocks: 0, hasStaleStorage: true, processing time: 1 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,965 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x252800b46bed3276: Processing first storage report for DS-7a239de6-36ad-4a59-8b2f-83eb72cddab6 from datanode a1eb1197-6763-4116-b38c-3702cdacda3d
2020-12-03 07:19:38,966 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x252800b46bed3276: from storage DS-7a239de6-36ad-4a59-8b2f-83eb72cddab6 node DatanodeRegistration(127.0.0.1:33393, datanodeUuid=a1eb1197-6763-4116-b38c-3702cdacda3d, infoPort=42572, infoSecurePort=0, ipcPort=36667, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464), blocks: 0, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,967 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x252800b46bed3276,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 5 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:19:38,967 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:38,970 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2798)) - No heartbeat from DataNode: 127.0.0.1:36213
2020-12-03 07:19:38,971 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:38,975 [IPC Server handler 4 on default port 44779] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-04af9d40-9522-498c-835c-e506c824a39f for DN 127.0.0.1:36213
2020-12-03 07:19:38,975 [IPC Server handler 4 on default port 44779] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-6b4155c2-6b43-450c-b668-a1919719e233 for DN 127.0.0.1:36213
2020-12-03 07:19:38,976 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0xa39058208ec29e6b: Processing first storage report for DS-3c77b156-23e2-4fc9-9db2-bac28e329697 from datanode 1671bb40-be61-48b3-8dfe-d4cda3f68f8f
2020-12-03 07:19:38,976 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0xa39058208ec29e6b: from storage DS-3c77b156-23e2-4fc9-9db2-bac28e329697 node DatanodeRegistration(127.0.0.1:46334, datanodeUuid=1671bb40-be61-48b3-8dfe-d4cda3f68f8f, infoPort=35491, infoSecurePort=0, ipcPort=39541, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,976 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0xa39058208ec29e6b: Processing first storage report for DS-aa4eafc1-6206-4cf4-beac-6f7c6e133413 from datanode 1671bb40-be61-48b3-8dfe-d4cda3f68f8f
2020-12-03 07:19:38,976 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0xa39058208ec29e6b: from storage DS-aa4eafc1-6206-4cf4-beac-6f7c6e133413 node DatanodeRegistration(127.0.0.1:46334, datanodeUuid=1671bb40-be61-48b3-8dfe-d4cda3f68f8f, infoPort=35491, infoSecurePort=0, ipcPort=39541, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,977 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0xa39058208ec29e6b,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 10 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:19:38,977 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:38,982 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x96cb10cf26bd2eda: Processing first storage report for DS-6b4155c2-6b43-450c-b668-a1919719e233 from datanode 68a96d2b-90b0-4ee3-b0be-e2a02cebb74b
2020-12-03 07:19:38,983 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x96cb10cf26bd2eda: from storage DS-6b4155c2-6b43-450c-b668-a1919719e233 node DatanodeRegistration(127.0.0.1:36213, datanodeUuid=68a96d2b-90b0-4ee3-b0be-e2a02cebb74b, infoPort=38285, infoSecurePort=0, ipcPort=32858, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464), blocks: 0, hasStaleStorage: true, processing time: 1 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,983 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x96cb10cf26bd2eda: Processing first storage report for DS-04af9d40-9522-498c-835c-e506c824a39f from datanode 68a96d2b-90b0-4ee3-b0be-e2a02cebb74b
2020-12-03 07:19:38,983 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x96cb10cf26bd2eda: from storage DS-04af9d40-9522-498c-835c-e506c824a39f node DatanodeRegistration(127.0.0.1:36213, datanodeUuid=68a96d2b-90b0-4ee3-b0be-e2a02cebb74b, infoPort=38285, infoSecurePort=0, ipcPort=32858, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,986 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x96cb10cf26bd2eda,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 10 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:19:38,986 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:39,078 [IPC Server handler 0 on default port 44779] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:39,083 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2758)) - Cluster is active
2020-12-03 07:19:39,112 [IPC Server handler 8 on default port 44779] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/striped	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-12-03 07:19:39,150 [IPC Server handler 9 on default port 44779] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setErasureCodingPolicy	src=/striped	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-12-03 07:19:39,159 [IPC Server handler 2 on default port 44779] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=enableErasureCodingPolicy	src=RS-6-3-1024k	dst=null	perm=null	proto=rpc
2020-12-03 07:19:39,165 [Thread-416] INFO  hdfs.TestFileChecksum (TestFileChecksum.java:testStripedFileChecksumWithMissedDataBlocksRangeQuery(290)) - Checksum file:/striped/stripedFileChecksum1, requested length:754974720
2020-12-03 07:19:40,237 [IPC Server handler 1 on default port 44779] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/striped/stripedFileChecksum1	dst=null	perm=null	proto=rpc
2020-12-03 07:19:40,270 [IPC Server handler 7 on default port 44779] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/striped/stripedFileChecksum1	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2020-12-03 07:19:40,709 [IPC Server handler 3 on default port 44779] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_-9223372036854775792_1001, replicas=127.0.0.1:33393, 127.0.0.1:40369, 127.0.0.1:40799, 127.0.0.1:40990, 127.0.0.1:36213, 127.0.0.1:35598, 127.0.0.1:43233, 127.0.0.1:33826, 127.0.0.1:46334 for /striped/stripedFileChecksum1
2020-12-03 07:19:40,743 [Thread-417] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:40,780 [Thread-418] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:40,821 [Thread-419] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:40,848 [Thread-420] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:40,881 [Thread-422] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:40,909 [Thread-421] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:40,924 [Thread-423] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:40,989 [Thread-424] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:41,008 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:56168 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775790_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775790_1001 src: /127.0.0.1:56168 dest: /127.0.0.1:40799
2020-12-03 07:19:41,036 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:55958 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001 src: /127.0.0.1:55958 dest: /127.0.0.1:36213
2020-12-03 07:19:41,037 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:40980 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001 src: /127.0.0.1:40980 dest: /127.0.0.1:40990
2020-12-03 07:19:41,037 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:36744 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001 src: /127.0.0.1:36744 dest: /127.0.0.1:35598
2020-12-03 07:19:41,037 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:59816 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001 src: /127.0.0.1:59816 dest: /127.0.0.1:33393
2020-12-03 07:19:41,037 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:49756 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001 src: /127.0.0.1:49756 dest: /127.0.0.1:40369
2020-12-03 07:19:41,037 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:49238 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775785_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775785_1001 src: /127.0.0.1:49238 dest: /127.0.0.1:33826
2020-12-03 07:19:41,042 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:33054 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001 src: /127.0.0.1:33054 dest: /127.0.0.1:43233
2020-12-03 07:19:41,055 [Thread-425] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:41,121 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:56016 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775784_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775784_1001 src: /127.0.0.1:56016 dest: /127.0.0.1:46334
2020-12-03 07:19:43,980 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@2d8f2f3a] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(205)) - Detected pause in JVM or host machine (eg GC): pause of approximately 2715ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=970ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1793ms
2020-12-03 07:19:43,981 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@6c44052e] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(205)) - Detected pause in JVM or host machine (eg GC): pause of approximately 2682ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=970ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1793ms
2020-12-03 07:19:43,981 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@31500940] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(205)) - Detected pause in JVM or host machine (eg GC): pause of approximately 2614ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=970ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1793ms
2020-12-03 07:19:43,982 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@4d4d8fcf] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(205)) - Detected pause in JVM or host machine (eg GC): pause of approximately 2554ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=970ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1793ms
2020-12-03 07:19:43,982 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@5bf61e67] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(205)) - Detected pause in JVM or host machine (eg GC): pause of approximately 2510ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=970ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1793ms
2020-12-03 07:19:43,982 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@56b78e55] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(205)) - Detected pause in JVM or host machine (eg GC): pause of approximately 2478ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=970ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1793ms
2020-12-03 07:19:43,982 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@7bab3f1a] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(205)) - Detected pause in JVM or host machine (eg GC): pause of approximately 2477ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=970ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1793ms
2020-12-03 07:19:43,982 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@2a551a63] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(205)) - Detected pause in JVM or host machine (eg GC): pause of approximately 2428ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=970ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1793ms
2020-12-03 07:19:43,982 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@bae47a0] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(205)) - Detected pause in JVM or host machine (eg GC): pause of approximately 2362ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=970ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1793ms
2020-12-03 07:19:43,980 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@350b3a17] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(205)) - Detected pause in JVM or host machine (eg GC): pause of approximately 2359ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=970ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1793ms
2020-12-03 07:19:43,982 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@37ff4054] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(205)) - Detected pause in JVM or host machine (eg GC): pause of approximately 2730ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=970ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1793ms
2020-12-03 07:19:43,982 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@7bfc3126] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(205)) - Detected pause in JVM or host machine (eg GC): pause of approximately 2468ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=970ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1793ms
2020-12-03 07:19:44,504 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775784_1001, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:56016, dest: /127.0.0.1:46334, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 1671bb40-be61-48b3-8dfe-d4cda3f68f8f, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775784_1001, duration(ns): 3360391357
2020-12-03 07:19:44,505 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775784_1001, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775784_1001, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:44,504 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:59816, dest: /127.0.0.1:33393, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: a1eb1197-6763-4116-b38c-3702cdacda3d, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001, duration(ns): 3341055957
2020-12-03 07:19:44,516 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:44,505 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775790_1001, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:56168, dest: /127.0.0.1:40799, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 24857262-68d9-4b98-abb9-9105d4bfc034, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775790_1001, duration(ns): 3347768893
2020-12-03 07:19:44,517 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:49756, dest: /127.0.0.1:40369, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 55b8e545-b17d-4893-81a8-bea205778f84, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001, duration(ns): 3343284206
2020-12-03 07:19:44,517 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:44,508 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:55958, dest: /127.0.0.1:36213, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 68a96d2b-90b0-4ee3-b0be-e2a02cebb74b, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001, duration(ns): 3345993062
2020-12-03 07:19:44,524 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775785_1001, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:49238, dest: /127.0.0.1:33826, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 288350fe-5f15-4f60-866b-b391b14a3073, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775785_1001, duration(ns): 543760826
2020-12-03 07:19:44,527 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:44,517 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775790_1001, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775790_1001, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:44,527 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775785_1001, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775785_1001, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:44,546 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:40980, dest: /127.0.0.1:40990, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 7a7e8fbe-c521-4997-b64d-db194b2bc262, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001, duration(ns): 3388631329
2020-12-03 07:19:44,547 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:44,547 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:36744, dest: /127.0.0.1:35598, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: dc2fc2d4-8a32-4acd-8e94-f1a1b85207c7, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001, duration(ns): 559693898
2020-12-03 07:19:44,547 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:44,547 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:33054, dest: /127.0.0.1:43233, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 6e5d8d03-7267-400e-9598-80d9941dd61b, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001, duration(ns): 3388453160
2020-12-03 07:19:44,548 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:44,562 [IPC Server handler 3 on default port 44779] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_-9223372036854775776_1002, replicas=127.0.0.1:46334, 127.0.0.1:43233, 127.0.0.1:40990, 127.0.0.1:43468, 127.0.0.1:33393, 127.0.0.1:33826, 127.0.0.1:40799, 127.0.0.1:35598, 127.0.0.1:40369 for /striped/stripedFileChecksum1
2020-12-03 07:19:44,572 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:44,578 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:44,578 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:56114 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775776_1002]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775776_1002 src: /127.0.0.1:56114 dest: /127.0.0.1:46334
2020-12-03 07:19:44,581 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:33160 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775775_1002]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775775_1002 src: /127.0.0.1:33160 dest: /127.0.0.1:43233
2020-12-03 07:19:44,594 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:44,599 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:44,601 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:44,607 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:59952 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775772_1002]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775772_1002 src: /127.0.0.1:59952 dest: /127.0.0.1:33393
2020-12-03 07:19:44,617 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:44,620 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:41098 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775774_1002]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775774_1002 src: /127.0.0.1:41098 dest: /127.0.0.1:40990
2020-12-03 07:19:44,620 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:49360 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775771_1002]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775771_1002 src: /127.0.0.1:49360 dest: /127.0.0.1:33826
2020-12-03 07:19:44,626 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:45898 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775773_1002]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775773_1002 src: /127.0.0.1:45898 dest: /127.0.0.1:43468
2020-12-03 07:19:44,665 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:44,673 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:44,674 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:56316 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775770_1002]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775770_1002 src: /127.0.0.1:56316 dest: /127.0.0.1:40799
2020-12-03 07:19:44,684 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:36888 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775769_1002]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775769_1002 src: /127.0.0.1:36888 dest: /127.0.0.1:35598
2020-12-03 07:19:44,684 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:44,689 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:49918 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775768_1002]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775768_1002 src: /127.0.0.1:49918 dest: /127.0.0.1:40369
2020-12-03 07:19:44,898 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775776_1002, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:56114, dest: /127.0.0.1:46334, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 1671bb40-be61-48b3-8dfe-d4cda3f68f8f, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775776_1002, duration(ns): 309669768
2020-12-03 07:19:44,898 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775776_1002, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775776_1002, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:44,900 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775773_1002, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:45898, dest: /127.0.0.1:43468, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 22bd5bbb-5aec-4015-af14-16567f170d84, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775773_1002, duration(ns): 264226326
2020-12-03 07:19:44,900 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775771_1002, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:49360, dest: /127.0.0.1:33826, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 288350fe-5f15-4f60-866b-b391b14a3073, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775771_1002, duration(ns): 250843000
2020-12-03 07:19:44,901 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775775_1002, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:33160, dest: /127.0.0.1:43233, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 6e5d8d03-7267-400e-9598-80d9941dd61b, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775775_1002, duration(ns): 312351027
2020-12-03 07:19:44,901 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775775_1002, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775775_1002, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:44,905 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775773_1002, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775773_1002, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:44,909 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775772_1002, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:59952, dest: /127.0.0.1:33393, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: a1eb1197-6763-4116-b38c-3702cdacda3d, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775772_1002, duration(ns): 280376267
2020-12-03 07:19:44,909 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775771_1002, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775771_1002, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:44,909 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775769_1002, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:36888, dest: /127.0.0.1:35598, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: dc2fc2d4-8a32-4acd-8e94-f1a1b85207c7, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775769_1002, duration(ns): 207929503
2020-12-03 07:19:44,909 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775774_1002, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:41098, dest: /127.0.0.1:40990, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 7a7e8fbe-c521-4997-b64d-db194b2bc262, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775774_1002, duration(ns): 271594158
2020-12-03 07:19:44,911 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775772_1002, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775772_1002, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:44,918 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775774_1002, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775774_1002, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:44,920 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775768_1002, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:49918, dest: /127.0.0.1:40369, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 55b8e545-b17d-4893-81a8-bea205778f84, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775768_1002, duration(ns): 223336253
2020-12-03 07:19:44,930 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775770_1002, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:56316, dest: /127.0.0.1:40799, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 24857262-68d9-4b98-abb9-9105d4bfc034, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775770_1002, duration(ns): 231012165
2020-12-03 07:19:44,930 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775770_1002, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775770_1002, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:44,932 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775769_1002, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775769_1002, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:44,943 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775768_1002, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775768_1002, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:44,962 [IPC Server handler 6 on default port 44779] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_-9223372036854775760_1003, replicas=127.0.0.1:36972, 127.0.0.1:43468, 127.0.0.1:40799, 127.0.0.1:46334, 127.0.0.1:43233, 127.0.0.1:35598, 127.0.0.1:33393, 127.0.0.1:40369, 127.0.0.1:40990 for /striped/stripedFileChecksum1
2020-12-03 07:19:44,970 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:44,971 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:44,974 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:45980 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775759_1003]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775759_1003 src: /127.0.0.1:45980 dest: /127.0.0.1:43468
2020-12-03 07:19:44,980 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:45048 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775760_1003]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775760_1003 src: /127.0.0.1:45048 dest: /127.0.0.1:36972
2020-12-03 07:19:44,984 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:44,990 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:56382 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775758_1003]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775758_1003 src: /127.0.0.1:56382 dest: /127.0.0.1:40799
2020-12-03 07:19:44,991 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,018 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,028 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,035 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,018 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:56216 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775757_1003]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775757_1003 src: /127.0.0.1:56216 dest: /127.0.0.1:46334
2020-12-03 07:19:45,036 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:60060 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775754_1003]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775754_1003 src: /127.0.0.1:60060 dest: /127.0.0.1:33393
2020-12-03 07:19:45,036 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:33266 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775756_1003]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775756_1003 src: /127.0.0.1:33266 dest: /127.0.0.1:43233
2020-12-03 07:19:45,043 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:36964 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775755_1003]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775755_1003 src: /127.0.0.1:36964 dest: /127.0.0.1:35598
2020-12-03 07:19:45,044 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,062 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,065 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:41214 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775752_1003]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775752_1003 src: /127.0.0.1:41214 dest: /127.0.0.1:40990
2020-12-03 07:19:45,074 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:49994 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775753_1003]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775753_1003 src: /127.0.0.1:49994 dest: /127.0.0.1:40369
2020-12-03 07:19:45,277 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775757_1003, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:56216, dest: /127.0.0.1:46334, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 1671bb40-be61-48b3-8dfe-d4cda3f68f8f, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775757_1003, duration(ns): 219938197
2020-12-03 07:19:45,278 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775757_1003, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775757_1003, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,278 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775753_1003, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:49994, dest: /127.0.0.1:40369, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 55b8e545-b17d-4893-81a8-bea205778f84, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775753_1003, duration(ns): 171464476
2020-12-03 07:19:45,278 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775753_1003, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775753_1003, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,279 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775752_1003, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:41214, dest: /127.0.0.1:40990, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 7a7e8fbe-c521-4997-b64d-db194b2bc262, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775752_1003, duration(ns): 175597184
2020-12-03 07:19:45,279 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775752_1003, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775752_1003, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,279 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775760_1003, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:45048, dest: /127.0.0.1:36972, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: c39c5c86-f8fb-43b7-9a7e-df2bf8e2ff45, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775760_1003, duration(ns): 267290700
2020-12-03 07:19:45,280 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775760_1003, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775760_1003, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,280 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775758_1003, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:56382, dest: /127.0.0.1:40799, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 24857262-68d9-4b98-abb9-9105d4bfc034, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775758_1003, duration(ns): 267159172
2020-12-03 07:19:45,280 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775759_1003, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:45980, dest: /127.0.0.1:43468, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 22bd5bbb-5aec-4015-af14-16567f170d84, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775759_1003, duration(ns): 273145167
2020-12-03 07:19:45,280 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775758_1003, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775758_1003, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,280 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775756_1003, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:33266, dest: /127.0.0.1:43233, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 6e5d8d03-7267-400e-9598-80d9941dd61b, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775756_1003, duration(ns): 202501235
2020-12-03 07:19:45,280 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775759_1003, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775759_1003, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,281 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775756_1003, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775756_1003, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,281 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775755_1003, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:36964, dest: /127.0.0.1:35598, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: dc2fc2d4-8a32-4acd-8e94-f1a1b85207c7, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775755_1003, duration(ns): 215900304
2020-12-03 07:19:45,281 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775755_1003, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775755_1003, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,281 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775754_1003, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:60060, dest: /127.0.0.1:33393, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: a1eb1197-6763-4116-b38c-3702cdacda3d, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775754_1003, duration(ns): 201509150
2020-12-03 07:19:45,282 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775754_1003, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775754_1003, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,299 [IPC Server handler 5 on default port 44779] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_-9223372036854775744_1004, replicas=127.0.0.1:40369, 127.0.0.1:33393, 127.0.0.1:43468, 127.0.0.1:36213, 127.0.0.1:40990, 127.0.0.1:43233, 127.0.0.1:46334, 127.0.0.1:33826, 127.0.0.1:40799 for /striped/stripedFileChecksum1
2020-12-03 07:19:45,320 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,321 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,322 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:50042 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775744_1004]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775744_1004 src: /127.0.0.1:50042 dest: /127.0.0.1:40369
2020-12-03 07:19:45,323 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,325 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,325 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:60112 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775743_1004]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775743_1004 src: /127.0.0.1:60112 dest: /127.0.0.1:33393
2020-12-03 07:19:45,326 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,327 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:56240 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775741_1004]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775741_1004 src: /127.0.0.1:56240 dest: /127.0.0.1:36213
2020-12-03 07:19:45,327 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:41268 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775740_1004]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775740_1004 src: /127.0.0.1:41268 dest: /127.0.0.1:40990
2020-12-03 07:19:45,327 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,355 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:33336 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775739_1004]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775739_1004 src: /127.0.0.1:33336 dest: /127.0.0.1:43233
2020-12-03 07:19:45,364 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:46062 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775742_1004]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775742_1004 src: /127.0.0.1:46062 dest: /127.0.0.1:43468
2020-12-03 07:19:45,361 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,395 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,398 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,398 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:49528 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775737_1004]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775737_1004 src: /127.0.0.1:49528 dest: /127.0.0.1:33826
2020-12-03 07:19:45,399 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:56476 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775736_1004]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775736_1004 src: /127.0.0.1:56476 dest: /127.0.0.1:40799
2020-12-03 07:19:45,399 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:56300 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775738_1004]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775738_1004 src: /127.0.0.1:56300 dest: /127.0.0.1:46334
2020-12-03 07:19:45,592 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775740_1004, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:41268, dest: /127.0.0.1:40990, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 7a7e8fbe-c521-4997-b64d-db194b2bc262, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775740_1004, duration(ns): 226367774
2020-12-03 07:19:45,592 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775736_1004, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:56476, dest: /127.0.0.1:40799, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 24857262-68d9-4b98-abb9-9105d4bfc034, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775736_1004, duration(ns): 169899134
2020-12-03 07:19:45,593 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775740_1004, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775740_1004, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,593 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775736_1004, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775736_1004, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,605 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775737_1004, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:49528, dest: /127.0.0.1:33826, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 288350fe-5f15-4f60-866b-b391b14a3073, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775737_1004, duration(ns): 181669147
2020-12-03 07:19:45,605 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775737_1004, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775737_1004, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,605 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775739_1004, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:33336, dest: /127.0.0.1:43233, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 6e5d8d03-7267-400e-9598-80d9941dd61b, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775739_1004, duration(ns): 181863720
2020-12-03 07:19:45,606 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775739_1004, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775739_1004, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,606 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775741_1004, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:56240, dest: /127.0.0.1:36213, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 68a96d2b-90b0-4ee3-b0be-e2a02cebb74b, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775741_1004, duration(ns): 226376668
2020-12-03 07:19:45,609 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775741_1004, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775741_1004, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,609 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775744_1004, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:50042, dest: /127.0.0.1:40369, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 55b8e545-b17d-4893-81a8-bea205778f84, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775744_1004, duration(ns): 203880167
2020-12-03 07:19:45,608 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775743_1004, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:60112, dest: /127.0.0.1:33393, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: a1eb1197-6763-4116-b38c-3702cdacda3d, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775743_1004, duration(ns): 221621455
2020-12-03 07:19:45,610 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775744_1004, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775744_1004, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,610 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775743_1004, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775743_1004, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,618 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775738_1004, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:56300, dest: /127.0.0.1:46334, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 1671bb40-be61-48b3-8dfe-d4cda3f68f8f, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775738_1004, duration(ns): 191575150
2020-12-03 07:19:45,618 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775738_1004, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775738_1004, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,620 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775742_1004, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:46062, dest: /127.0.0.1:43468, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 22bd5bbb-5aec-4015-af14-16567f170d84, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775742_1004, duration(ns): 209368174
2020-12-03 07:19:45,621 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775742_1004, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775742_1004, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,635 [IPC Server handler 1 on default port 44779] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_-9223372036854775728_1005, replicas=127.0.0.1:40369, 127.0.0.1:35598, 127.0.0.1:43233, 127.0.0.1:36972, 127.0.0.1:33826, 127.0.0.1:40799, 127.0.0.1:46334, 127.0.0.1:33393, 127.0.0.1:40990 for /striped/stripedFileChecksum1
2020-12-03 07:19:45,655 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,657 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:50144 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775728_1005]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775728_1005 src: /127.0.0.1:50144 dest: /127.0.0.1:40369
2020-12-03 07:19:45,661 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,662 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,664 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:37126 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775727_1005]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775727_1005 src: /127.0.0.1:37126 dest: /127.0.0.1:35598
2020-12-03 07:19:45,664 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,668 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:33432 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775726_1005]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775726_1005 src: /127.0.0.1:33432 dest: /127.0.0.1:43233
2020-12-03 07:19:45,668 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:45240 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775725_1005]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775725_1005 src: /127.0.0.1:45240 dest: /127.0.0.1:36972
2020-12-03 07:19:45,784 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,786 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:49634 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775724_1005]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775724_1005 src: /127.0.0.1:49634 dest: /127.0.0.1:33826
2020-12-03 07:19:45,787 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,789 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:56582 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775723_1005]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775723_1005 src: /127.0.0.1:56582 dest: /127.0.0.1:40799
2020-12-03 07:19:45,794 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,799 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,801 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:60246 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775721_1005]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775721_1005 src: /127.0.0.1:60246 dest: /127.0.0.1:33393
2020-12-03 07:19:45,803 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:56416 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775722_1005]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775722_1005 src: /127.0.0.1:56416 dest: /127.0.0.1:46334
2020-12-03 07:19:45,804 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,812 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:41398 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775720_1005]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775720_1005 src: /127.0.0.1:41398 dest: /127.0.0.1:40990
2020-12-03 07:19:46,127 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775723_1005, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:56582, dest: /127.0.0.1:40799, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 24857262-68d9-4b98-abb9-9105d4bfc034, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775723_1005, duration(ns): 322566305
2020-12-03 07:19:46,128 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775723_1005, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775723_1005, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,128 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775728_1005, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:50144, dest: /127.0.0.1:40369, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 55b8e545-b17d-4893-81a8-bea205778f84, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775728_1005, duration(ns): 447792688
2020-12-03 07:19:46,135 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775728_1005, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775728_1005, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,135 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775727_1005, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:37126, dest: /127.0.0.1:35598, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: dc2fc2d4-8a32-4acd-8e94-f1a1b85207c7, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775727_1005, duration(ns): 436170210
2020-12-03 07:19:46,136 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775720_1005, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:41398, dest: /127.0.0.1:40990, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 7a7e8fbe-c521-4997-b64d-db194b2bc262, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775720_1005, duration(ns): 310247321
2020-12-03 07:19:46,136 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775720_1005, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775720_1005, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,136 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775727_1005, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775727_1005, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,137 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775724_1005, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:49634, dest: /127.0.0.1:33826, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 288350fe-5f15-4f60-866b-b391b14a3073, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775724_1005, duration(ns): 334712955
2020-12-03 07:19:46,138 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775724_1005, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775724_1005, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,138 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775725_1005, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:45240, dest: /127.0.0.1:36972, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: c39c5c86-f8fb-43b7-9a7e-df2bf8e2ff45, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775725_1005, duration(ns): 341141298
2020-12-03 07:19:46,138 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775725_1005, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775725_1005, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,137 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775726_1005, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:33432, dest: /127.0.0.1:43233, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 6e5d8d03-7267-400e-9598-80d9941dd61b, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775726_1005, duration(ns): 358435308
2020-12-03 07:19:46,139 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775726_1005, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775726_1005, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,142 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775722_1005, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:56416, dest: /127.0.0.1:46334, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 1671bb40-be61-48b3-8dfe-d4cda3f68f8f, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775722_1005, duration(ns): 322370018
2020-12-03 07:19:46,143 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775722_1005, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775722_1005, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,143 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775721_1005, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:60246, dest: /127.0.0.1:33393, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: a1eb1197-6763-4116-b38c-3702cdacda3d, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775721_1005, duration(ns): 325963974
2020-12-03 07:19:46,143 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775721_1005, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775721_1005, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,158 [IPC Server handler 6 on default port 44779] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_-9223372036854775712_1006, replicas=127.0.0.1:35598, 127.0.0.1:40369, 127.0.0.1:40990, 127.0.0.1:36972, 127.0.0.1:33826, 127.0.0.1:40799, 127.0.0.1:43233, 127.0.0.1:33393, 127.0.0.1:46334 for /striped/stripedFileChecksum1
2020-12-03 07:19:46,181 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:46,183 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:37234 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775712_1006]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775712_1006 src: /127.0.0.1:37234 dest: /127.0.0.1:35598
2020-12-03 07:19:46,188 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:46,190 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:46,191 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:46,196 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:46,196 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:50260 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775711_1006]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775711_1006 src: /127.0.0.1:50260 dest: /127.0.0.1:40369
2020-12-03 07:19:46,198 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:46,216 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:46,219 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:46,232 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:60342 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775705_1006]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775705_1006 src: /127.0.0.1:60342 dest: /127.0.0.1:33393
2020-12-03 07:19:46,232 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:56680 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775707_1006]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775707_1006 src: /127.0.0.1:56680 dest: /127.0.0.1:40799
2020-12-03 07:19:46,233 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:45352 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775709_1006]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775709_1006 src: /127.0.0.1:45352 dest: /127.0.0.1:36972
2020-12-03 07:19:46,232 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:49734 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775708_1006]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775708_1006 src: /127.0.0.1:49734 dest: /127.0.0.1:33826
2020-12-03 07:19:46,247 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:46,295 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:56524 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775704_1006]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775704_1006 src: /127.0.0.1:56524 dest: /127.0.0.1:46334
2020-12-03 07:19:46,297 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:33556 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775706_1006]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775706_1006 src: /127.0.0.1:33556 dest: /127.0.0.1:43233
2020-12-03 07:19:46,295 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:41480 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775710_1006]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775710_1006 src: /127.0.0.1:41480 dest: /127.0.0.1:40990
2020-12-03 07:19:46,510 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775707_1006, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:56680, dest: /127.0.0.1:40799, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 24857262-68d9-4b98-abb9-9105d4bfc034, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775707_1006, duration(ns): 183905444
2020-12-03 07:19:46,510 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775708_1006, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:49734, dest: /127.0.0.1:33826, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 288350fe-5f15-4f60-866b-b391b14a3073, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775708_1006, duration(ns): 214209945
2020-12-03 07:19:46,510 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775711_1006, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:50260, dest: /127.0.0.1:40369, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 55b8e545-b17d-4893-81a8-bea205778f84, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775711_1006, duration(ns): 277670574
2020-12-03 07:19:46,510 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775704_1006, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:56524, dest: /127.0.0.1:46334, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 1671bb40-be61-48b3-8dfe-d4cda3f68f8f, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775704_1006, duration(ns): 179602379
2020-12-03 07:19:46,510 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775712_1006, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:37234, dest: /127.0.0.1:35598, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: dc2fc2d4-8a32-4acd-8e94-f1a1b85207c7, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775712_1006, duration(ns): 323465803
2020-12-03 07:19:46,511 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775712_1006, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775712_1006, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,511 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775705_1006, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:60342, dest: /127.0.0.1:33393, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: a1eb1197-6763-4116-b38c-3702cdacda3d, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775705_1006, duration(ns): 184184408
2020-12-03 07:19:46,511 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775705_1006, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775705_1006, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,510 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775708_1006, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775708_1006, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,510 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775707_1006, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775707_1006, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,511 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775709_1006, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:45352, dest: /127.0.0.1:36972, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: c39c5c86-f8fb-43b7-9a7e-df2bf8e2ff45, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775709_1006, duration(ns): 214589196
2020-12-03 07:19:46,510 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775711_1006, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775711_1006, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,510 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775704_1006, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775704_1006, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,510 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775706_1006, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:33556, dest: /127.0.0.1:43233, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 6e5d8d03-7267-400e-9598-80d9941dd61b, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775706_1006, duration(ns): 182185284
2020-12-03 07:19:46,512 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775706_1006, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775706_1006, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,512 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775710_1006, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:41480, dest: /127.0.0.1:40990, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 7a7e8fbe-c521-4997-b64d-db194b2bc262, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775710_1006, duration(ns): 179903475
2020-12-03 07:19:46,512 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775709_1006, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775709_1006, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,513 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775710_1006, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775710_1006, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,524 [IPC Server handler 6 on default port 44779] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_-9223372036854775696_1007, replicas=127.0.0.1:43233, 127.0.0.1:33393, 127.0.0.1:35598, 127.0.0.1:43468, 127.0.0.1:40369, 127.0.0.1:40990, 127.0.0.1:33826, 127.0.0.1:46334, 127.0.0.1:36972 for /striped/stripedFileChecksum1
2020-12-03 07:19:46,535 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:46,537 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:46,538 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:46,539 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:33616 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775696_1007]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775696_1007 src: /127.0.0.1:33616 dest: /127.0.0.1:43233
2020-12-03 07:19:46,539 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:60402 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775695_1007]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775695_1007 src: /127.0.0.1:60402 dest: /127.0.0.1:33393
2020-12-03 07:19:46,540 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:37312 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775694_1007]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775694_1007 src: /127.0.0.1:37312 dest: /127.0.0.1:35598
2020-12-03 07:19:46,542 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:46,543 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:46354 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775693_1007]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775693_1007 src: /127.0.0.1:46354 dest: /127.0.0.1:43468
2020-12-03 07:19:46,545 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:46,548 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:46,553 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:46,555 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:41560 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775691_1007]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775691_1007 src: /127.0.0.1:41560 dest: /127.0.0.1:40990
2020-12-03 07:19:46,555 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:46,555 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:50340 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775692_1007]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775692_1007 src: /127.0.0.1:50340 dest: /127.0.0.1:40369
2020-12-03 07:19:46,555 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:49810 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775690_1007]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775690_1007 src: /127.0.0.1:49810 dest: /127.0.0.1:33826
2020-12-03 07:19:46,558 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:56588 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775689_1007]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775689_1007 src: /127.0.0.1:56588 dest: /127.0.0.1:46334
2020-12-03 07:19:46,568 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:46,569 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:45436 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775688_1007]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775688_1007 src: /127.0.0.1:45436 dest: /127.0.0.1:36972
2020-12-03 07:19:46,821 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775695_1007, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:60402, dest: /127.0.0.1:33393, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: a1eb1197-6763-4116-b38c-3702cdacda3d, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775695_1007, duration(ns): 279367721
2020-12-03 07:19:46,821 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775695_1007, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775695_1007, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,821 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775696_1007, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:33616, dest: /127.0.0.1:43233, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 6e5d8d03-7267-400e-9598-80d9941dd61b, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775696_1007, duration(ns): 279651619
2020-12-03 07:19:46,822 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775696_1007, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775696_1007, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,821 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775690_1007, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:49810, dest: /127.0.0.1:33826, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 288350fe-5f15-4f60-866b-b391b14a3073, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775690_1007, duration(ns): 258791033
2020-12-03 07:19:46,821 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775692_1007, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:50340, dest: /127.0.0.1:40369, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 55b8e545-b17d-4893-81a8-bea205778f84, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775692_1007, duration(ns): 262372204
2020-12-03 07:19:46,822 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775692_1007, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775692_1007, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,821 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775694_1007, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:37312, dest: /127.0.0.1:35598, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: dc2fc2d4-8a32-4acd-8e94-f1a1b85207c7, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775694_1007, duration(ns): 279088873
2020-12-03 07:19:46,823 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775688_1007, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:45436, dest: /127.0.0.1:36972, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: c39c5c86-f8fb-43b7-9a7e-df2bf8e2ff45, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775688_1007, duration(ns): 251638891
2020-12-03 07:19:46,823 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775694_1007, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775694_1007, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,822 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775690_1007, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775690_1007, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,821 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775689_1007, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:56588, dest: /127.0.0.1:46334, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 1671bb40-be61-48b3-8dfe-d4cda3f68f8f, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775689_1007, duration(ns): 260248332
2020-12-03 07:19:46,823 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775689_1007, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775689_1007, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,823 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775688_1007, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775688_1007, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,826 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775693_1007, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:46354, dest: /127.0.0.1:43468, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 22bd5bbb-5aec-4015-af14-16567f170d84, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775693_1007, duration(ns): 277461474
2020-12-03 07:19:46,827 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775691_1007, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:41560, dest: /127.0.0.1:40990, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 7a7e8fbe-c521-4997-b64d-db194b2bc262, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775691_1007, duration(ns): 266875568
2020-12-03 07:19:46,827 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775691_1007, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775691_1007, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,827 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775693_1007, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775693_1007, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,831 [IPC Server handler 0 on default port 44779] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_-9223372036854775680_1008, replicas=127.0.0.1:36972, 127.0.0.1:43468, 127.0.0.1:43233, 127.0.0.1:46334, 127.0.0.1:40799, 127.0.0.1:33393, 127.0.0.1:33826, 127.0.0.1:40369, 127.0.0.1:40990 for /striped/stripedFileChecksum1
2020-12-03 07:19:46,843 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:46,843 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:46,845 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:45494 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775680_1008]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775680_1008 src: /127.0.0.1:45494 dest: /127.0.0.1:36972
2020-12-03 07:19:46,845 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:46424 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775679_1008]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775679_1008 src: /127.0.0.1:46424 dest: /127.0.0.1:43468
2020-12-03 07:19:46,845 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:46,847 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:33694 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775678_1008]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775678_1008 src: /127.0.0.1:33694 dest: /127.0.0.1:43233
2020-12-03 07:19:46,849 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:46,851 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:46,856 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:46,858 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:60484 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775675_1008]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775675_1008 src: /127.0.0.1:60484 dest: /127.0.0.1:33393
2020-12-03 07:19:46,858 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:56824 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775676_1008]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775676_1008 src: /127.0.0.1:56824 dest: /127.0.0.1:40799
2020-12-03 07:19:47,186 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:56654 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775677_1008]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775677_1008 src: /127.0.0.1:56654 dest: /127.0.0.1:46334
2020-12-03 07:19:47,188 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:47,192 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:49972 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775674_1008]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775674_1008 src: /127.0.0.1:49972 dest: /127.0.0.1:33826
2020-12-03 07:19:47,194 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:47,197 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:47,210 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:41734 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775672_1008]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775672_1008 src: /127.0.0.1:41734 dest: /127.0.0.1:40990
2020-12-03 07:19:47,214 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:50510 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775673_1008]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775673_1008 src: /127.0.0.1:50510 dest: /127.0.0.1:40369
2020-12-03 07:19:47,215 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775678_1008, type=LAST_IN_PIPELINE] WARN  datanode.DataNode (BlockReceiver.java:sendAckUpstreamUnprotected(1639)) - Slow PacketResponder send ack to upstream took 326ms (threshold=300ms), PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775678_1008, type=LAST_IN_PIPELINE, replyAck=seqno: 705 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0, downstream DNs=[], blockId=-9223372036854775678
2020-12-03 07:19:47,462 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775674_1008, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:49972, dest: /127.0.0.1:33826, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 288350fe-5f15-4f60-866b-b391b14a3073, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775674_1008, duration(ns): 267755826
2020-12-03 07:19:47,462 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775676_1008, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:56824, dest: /127.0.0.1:40799, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 24857262-68d9-4b98-abb9-9105d4bfc034, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775676_1008, duration(ns): 601876055
2020-12-03 07:19:47,462 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775676_1008, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775676_1008, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,462 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775677_1008, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:56654, dest: /127.0.0.1:46334, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 1671bb40-be61-48b3-8dfe-d4cda3f68f8f, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775677_1008, duration(ns): 271546201
2020-12-03 07:19:47,462 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775679_1008, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:46424, dest: /127.0.0.1:43468, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 22bd5bbb-5aec-4015-af14-16567f170d84, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775679_1008, duration(ns): 613258162
2020-12-03 07:19:47,463 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775679_1008, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775679_1008, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,463 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775677_1008, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775677_1008, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,462 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775672_1008, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:41734, dest: /127.0.0.1:40990, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 7a7e8fbe-c521-4997-b64d-db194b2bc262, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775672_1008, duration(ns): 211083369
2020-12-03 07:19:47,464 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775672_1008, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775672_1008, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,464 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775673_1008, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:50510, dest: /127.0.0.1:40369, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 55b8e545-b17d-4893-81a8-bea205778f84, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775673_1008, duration(ns): 248690238
2020-12-03 07:19:47,462 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775680_1008, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:45494, dest: /127.0.0.1:36972, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: c39c5c86-f8fb-43b7-9a7e-df2bf8e2ff45, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775680_1008, duration(ns): 615263555
2020-12-03 07:19:47,465 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775680_1008, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775680_1008, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,464 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775673_1008, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775673_1008, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,463 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775678_1008, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:33694, dest: /127.0.0.1:43233, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 6e5d8d03-7267-400e-9598-80d9941dd61b, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775678_1008, duration(ns): 612080495
2020-12-03 07:19:47,463 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775675_1008, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:60484, dest: /127.0.0.1:33393, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: a1eb1197-6763-4116-b38c-3702cdacda3d, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775675_1008, duration(ns): 602503711
2020-12-03 07:19:47,462 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775674_1008, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775674_1008, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,465 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775675_1008, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775675_1008, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,465 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775678_1008, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775678_1008, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,470 [IPC Server handler 0 on default port 44779] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_-9223372036854775664_1009, replicas=127.0.0.1:36213, 127.0.0.1:43233, 127.0.0.1:35598, 127.0.0.1:36972, 127.0.0.1:33826, 127.0.0.1:40799, 127.0.0.1:46334, 127.0.0.1:40990, 127.0.0.1:40369 for /striped/stripedFileChecksum1
2020-12-03 07:19:47,476 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:47,478 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:47,478 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:56770 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775664_1009]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775664_1009 src: /127.0.0.1:56770 dest: /127.0.0.1:36213
2020-12-03 07:19:47,480 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:47,481 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:33864 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775663_1009]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775663_1009 src: /127.0.0.1:33864 dest: /127.0.0.1:43233
2020-12-03 07:19:47,482 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:47,483 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:47,485 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:47,489 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:37558 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775662_1009]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775662_1009 src: /127.0.0.1:37558 dest: /127.0.0.1:35598
2020-12-03 07:19:47,489 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:45672 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775661_1009]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775661_1009 src: /127.0.0.1:45672 dest: /127.0.0.1:36972
2020-12-03 07:19:47,490 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:56998 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775659_1009]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775659_1009 src: /127.0.0.1:56998 dest: /127.0.0.1:40799
2020-12-03 07:19:47,496 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:47,499 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:50052 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775660_1009]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775660_1009 src: /127.0.0.1:50052 dest: /127.0.0.1:33826
2020-12-03 07:19:47,499 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:56838 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775658_1009]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775658_1009 src: /127.0.0.1:56838 dest: /127.0.0.1:46334
2020-12-03 07:19:47,500 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:47,503 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:41818 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775657_1009]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775657_1009 src: /127.0.0.1:41818 dest: /127.0.0.1:40990
2020-12-03 07:19:47,506 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:47,513 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:50604 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775656_1009]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775656_1009 src: /127.0.0.1:50604 dest: /127.0.0.1:40369
2020-12-03 07:19:47,691 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775658_1009, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:56838, dest: /127.0.0.1:46334, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 1671bb40-be61-48b3-8dfe-d4cda3f68f8f, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775658_1009, duration(ns): 178127017
2020-12-03 07:19:47,691 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775658_1009, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775658_1009, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,692 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775657_1009, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:41818, dest: /127.0.0.1:40990, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 7a7e8fbe-c521-4997-b64d-db194b2bc262, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775657_1009, duration(ns): 183736736
2020-12-03 07:19:47,692 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775657_1009, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775657_1009, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,692 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775662_1009, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:37558, dest: /127.0.0.1:35598, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: dc2fc2d4-8a32-4acd-8e94-f1a1b85207c7, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775662_1009, duration(ns): 200490478
2020-12-03 07:19:47,693 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775661_1009, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:45672, dest: /127.0.0.1:36972, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: c39c5c86-f8fb-43b7-9a7e-df2bf8e2ff45, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775661_1009, duration(ns): 193772984
2020-12-03 07:19:47,693 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775661_1009, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775661_1009, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,693 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775663_1009, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:33864, dest: /127.0.0.1:43233, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 6e5d8d03-7267-400e-9598-80d9941dd61b, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775663_1009, duration(ns): 195601855
2020-12-03 07:19:47,691 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775659_1009, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:56998, dest: /127.0.0.1:40799, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 24857262-68d9-4b98-abb9-9105d4bfc034, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775659_1009, duration(ns): 191550952
2020-12-03 07:19:47,691 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775656_1009, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:50604, dest: /127.0.0.1:40369, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 55b8e545-b17d-4893-81a8-bea205778f84, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775656_1009, duration(ns): 175096677
2020-12-03 07:19:47,691 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775664_1009, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:56770, dest: /127.0.0.1:36213, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 68a96d2b-90b0-4ee3-b0be-e2a02cebb74b, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775664_1009, duration(ns): 210801191
2020-12-03 07:19:47,693 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775656_1009, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775656_1009, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,691 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775660_1009, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:50052, dest: /127.0.0.1:33826, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 288350fe-5f15-4f60-866b-b391b14a3073, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775660_1009, duration(ns): 177543795
2020-12-03 07:19:47,694 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775660_1009, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775660_1009, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,694 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775664_1009, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775664_1009, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,693 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775663_1009, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775663_1009, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,693 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775659_1009, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775659_1009, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,693 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775662_1009, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775662_1009, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,698 [IPC Server handler 0 on default port 44779] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_-9223372036854775648_1010, replicas=127.0.0.1:40990, 127.0.0.1:33826, 127.0.0.1:36213, 127.0.0.1:40799, 127.0.0.1:35598, 127.0.0.1:40369, 127.0.0.1:43233, 127.0.0.1:46334, 127.0.0.1:33393 for /striped/stripedFileChecksum1
2020-12-03 07:19:47,708 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:47,710 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:41902 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775648_1010]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775648_1010 src: /127.0.0.1:41902 dest: /127.0.0.1:40990
2020-12-03 07:19:47,710 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:47,712 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:50152 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775647_1010]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775647_1010 src: /127.0.0.1:50152 dest: /127.0.0.1:33826
2020-12-03 07:19:47,712 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:47,716 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:47,718 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:37670 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775644_1010]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775644_1010 src: /127.0.0.1:37670 dest: /127.0.0.1:35598
2020-12-03 07:19:47,719 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:47,720 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:50696 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775643_1010]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775643_1010 src: /127.0.0.1:50696 dest: /127.0.0.1:40369
2020-12-03 07:19:47,721 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:47,721 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:56882 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775646_1010]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775646_1010 src: /127.0.0.1:56882 dest: /127.0.0.1:36213
2020-12-03 07:19:47,724 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:57102 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775645_1010]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775645_1010 src: /127.0.0.1:57102 dest: /127.0.0.1:40799
2020-12-03 07:19:47,727 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:47,730 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:33982 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775642_1010]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775642_1010 src: /127.0.0.1:33982 dest: /127.0.0.1:43233
2020-12-03 07:19:47,732 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:47,734 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:56942 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775641_1010]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775641_1010 src: /127.0.0.1:56942 dest: /127.0.0.1:46334
2020-12-03 07:19:47,738 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:47,742 [DataXceiver for client DFSClient_NONMAPREDUCE_1967344085_1 at /127.0.0.1:60770 [Receiving block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775640_1010]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775640_1010 src: /127.0.0.1:60770 dest: /127.0.0.1:33393
2020-12-03 07:19:47,910 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775644_1010, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:37670, dest: /127.0.0.1:35598, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: dc2fc2d4-8a32-4acd-8e94-f1a1b85207c7, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775644_1010, duration(ns): 189791356
2020-12-03 07:19:47,910 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775648_1010, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:41902, dest: /127.0.0.1:40990, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 7a7e8fbe-c521-4997-b64d-db194b2bc262, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775648_1010, duration(ns): 198063072
2020-12-03 07:19:47,910 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775648_1010, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775648_1010, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,910 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775647_1010, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:50152, dest: /127.0.0.1:33826, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 288350fe-5f15-4f60-866b-b391b14a3073, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775647_1010, duration(ns): 196347627
2020-12-03 07:19:47,910 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775646_1010, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:56882, dest: /127.0.0.1:36213, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 68a96d2b-90b0-4ee3-b0be-e2a02cebb74b, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775646_1010, duration(ns): 187175929
2020-12-03 07:19:47,911 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775647_1010, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775647_1010, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,911 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775646_1010, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775646_1010, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,911 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775642_1010, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:33982, dest: /127.0.0.1:43233, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 6e5d8d03-7267-400e-9598-80d9941dd61b, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775642_1010, duration(ns): 169383347
2020-12-03 07:19:47,911 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775642_1010, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775642_1010, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,912 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775643_1010, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:50696, dest: /127.0.0.1:40369, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 55b8e545-b17d-4893-81a8-bea205778f84, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775643_1010, duration(ns): 187683084
2020-12-03 07:19:47,912 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775643_1010, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775643_1010, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,910 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775645_1010, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:57102, dest: /127.0.0.1:40799, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 24857262-68d9-4b98-abb9-9105d4bfc034, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775645_1010, duration(ns): 183658909
2020-12-03 07:19:47,910 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775640_1010, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:60770, dest: /127.0.0.1:33393, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: a1eb1197-6763-4116-b38c-3702cdacda3d, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775640_1010, duration(ns): 166254493
2020-12-03 07:19:47,916 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775645_1010, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775645_1010, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,916 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775640_1010, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775640_1010, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,910 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775644_1010, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775644_1010, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,912 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775641_1010, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:56942, dest: /127.0.0.1:46334, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1967344085_1, offset: 0, srvID: 1671bb40-be61-48b3-8dfe-d4cda3f68f8f, blockid: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775641_1010, duration(ns): 171828582
2020-12-03 07:19:47,918 [PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775641_1010, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775641_1010, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,925 [IPC Server handler 0 on default port 44779] INFO  hdfs.StateChange (FSNamesystem.java:completeFile(2948)) - DIR* completeFile: /striped/stripedFileChecksum1 is closed by DFSClient_NONMAPREDUCE_1967344085_1
2020-12-03 07:19:47,945 [IPC Server handler 7 on default port 44779] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/striped/stripedFileChecksum1	dst=null	perm=null	proto=rpc
2020-12-03 07:19:47,980 [Thread-416] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:33393,DS-7a239de6-36ad-4a59-8b2f-83eb72cddab6,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33393,DS-7a239de6-36ad-4a59-8b2f-83eb72cddab6,DISK], DatanodeInfoWithStorage[127.0.0.1:40369,DS-035cd092-cc9b-40a4-8e6a-3004ff2553ce,DISK], DatanodeInfoWithStorage[127.0.0.1:40799,DS-997db01c-04dd-4559-92d6-ffa544c717ec,DISK], DatanodeInfoWithStorage[127.0.0.1:40990,DS-3bec7c54-7709-4f3d-b486-35765741195a,DISK], DatanodeInfoWithStorage[127.0.0.1:36213,DS-04af9d40-9522-498c-835c-e506c824a39f,DISK], DatanodeInfoWithStorage[127.0.0.1:35598,DS-d3e3d4e2-c7e5-44e4-9c40-1b0d2f6aaca7,DISK], DatanodeInfoWithStorage[127.0.0.1:43233,DS-11ae3bf8-6838-4dfc-af89-3c76d111c85e,DISK], DatanodeInfoWithStorage[127.0.0.1:33826,DS-2ce634e7-1c83-4eb2-9adf-081181b40a1c,DISK], DatanodeInfoWithStorage[127.0.0.1:46334,DS-3c77b156-23e2-4fc9-9db2-bac28e329697,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7, 8]}
2020-12-03 07:19:48,052 [Thread-416] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:extractChecksumProperties(430)) - set bytesPerCRC=512, crcPerBlock=12288
2020-12-03 07:19:48,052 [Thread-416] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(718)) - got reply from DatanodeInfoWithStorage[127.0.0.1:33393,DS-7a239de6-36ad-4a59-8b2f-83eb72cddab6,DISK]: blockChecksum=e199faa66ca34a63cc13e2c4448e3716, blockChecksumType=MD5CRC
2020-12-03 07:19:48,053 [Thread-416] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:46334,DS-aa4eafc1-6206-4cf4-beac-6f7c6e133413,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=37748736; locs=[DatanodeInfoWithStorage[127.0.0.1:46334,DS-aa4eafc1-6206-4cf4-beac-6f7c6e133413,DISK], DatanodeInfoWithStorage[127.0.0.1:43233,DS-4d752d77-dc01-4782-88b7-fac90ddd2643,DISK], DatanodeInfoWithStorage[127.0.0.1:40990,DS-c396f456-72b2-4eec-a6af-559f05d184b3,DISK], DatanodeInfoWithStorage[127.0.0.1:43468,DS-34e32a14-5782-4fe9-b945-76fbaab513a9,DISK], DatanodeInfoWithStorage[127.0.0.1:33393,DS-6c847cf4-ff4b-4f0f-91b4-4b65310e2c06,DISK], DatanodeInfoWithStorage[127.0.0.1:33826,DS-023f6a39-4389-43da-a1df-f7cccb6bcc41,DISK], DatanodeInfoWithStorage[127.0.0.1:40799,DS-353fdb6d-79d4-40b3-891b-044f5d75a5de,DISK], DatanodeInfoWithStorage[127.0.0.1:35598,DS-265e643d-40cf-4be7-923e-86955d3df769,DISK], DatanodeInfoWithStorage[127.0.0.1:40369,DS-5167c021-7d28-44ed-982a-8cb38c70870d,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7, 8]}
2020-12-03 07:19:48,077 [Thread-416] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(718)) - got reply from DatanodeInfoWithStorage[127.0.0.1:46334,DS-aa4eafc1-6206-4cf4-beac-6f7c6e133413,DISK]: blockChecksum=4b648ddfb75e52fc4ab8acf75e7116cf, blockChecksumType=MD5CRC
2020-12-03 07:19:48,077 [Thread-416] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:36972,DS-c85716ef-a148-4295-b2b7-bca92bf0548a,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775760_1003; getBlockSize()=37748736; corrupt=false; offset=75497472; locs=[DatanodeInfoWithStorage[127.0.0.1:36972,DS-c85716ef-a148-4295-b2b7-bca92bf0548a,DISK], DatanodeInfoWithStorage[127.0.0.1:43468,DS-462ced07-43f8-4720-afda-b3153be81947,DISK], DatanodeInfoWithStorage[127.0.0.1:40799,DS-997db01c-04dd-4559-92d6-ffa544c717ec,DISK], DatanodeInfoWithStorage[127.0.0.1:46334,DS-3c77b156-23e2-4fc9-9db2-bac28e329697,DISK], DatanodeInfoWithStorage[127.0.0.1:43233,DS-11ae3bf8-6838-4dfc-af89-3c76d111c85e,DISK], DatanodeInfoWithStorage[127.0.0.1:35598,DS-d3e3d4e2-c7e5-44e4-9c40-1b0d2f6aaca7,DISK], DatanodeInfoWithStorage[127.0.0.1:33393,DS-7a239de6-36ad-4a59-8b2f-83eb72cddab6,DISK], DatanodeInfoWithStorage[127.0.0.1:40369,DS-035cd092-cc9b-40a4-8e6a-3004ff2553ce,DISK], DatanodeInfoWithStorage[127.0.0.1:40990,DS-3bec7c54-7709-4f3d-b486-35765741195a,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7, 8]}
2020-12-03 07:19:48,105 [Thread-416] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(718)) - got reply from DatanodeInfoWithStorage[127.0.0.1:36972,DS-c85716ef-a148-4295-b2b7-bca92bf0548a,DISK]: blockChecksum=67708555cca0bd9ee68540924fa61776, blockChecksumType=MD5CRC
2020-12-03 07:19:48,106 [Thread-416] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:40369,DS-5167c021-7d28-44ed-982a-8cb38c70870d,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775744_1004; getBlockSize()=37748736; corrupt=false; offset=113246208; locs=[DatanodeInfoWithStorage[127.0.0.1:40369,DS-5167c021-7d28-44ed-982a-8cb38c70870d,DISK], DatanodeInfoWithStorage[127.0.0.1:33393,DS-6c847cf4-ff4b-4f0f-91b4-4b65310e2c06,DISK], DatanodeInfoWithStorage[127.0.0.1:43468,DS-34e32a14-5782-4fe9-b945-76fbaab513a9,DISK], DatanodeInfoWithStorage[127.0.0.1:36213,DS-6b4155c2-6b43-450c-b668-a1919719e233,DISK], DatanodeInfoWithStorage[127.0.0.1:40990,DS-c396f456-72b2-4eec-a6af-559f05d184b3,DISK], DatanodeInfoWithStorage[127.0.0.1:43233,DS-4d752d77-dc01-4782-88b7-fac90ddd2643,DISK], DatanodeInfoWithStorage[127.0.0.1:46334,DS-aa4eafc1-6206-4cf4-beac-6f7c6e133413,DISK], DatanodeInfoWithStorage[127.0.0.1:33826,DS-2ce634e7-1c83-4eb2-9adf-081181b40a1c,DISK], DatanodeInfoWithStorage[127.0.0.1:40799,DS-353fdb6d-79d4-40b3-891b-044f5d75a5de,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7, 8]}
2020-12-03 07:19:48,124 [Thread-416] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(718)) - got reply from DatanodeInfoWithStorage[127.0.0.1:40369,DS-5167c021-7d28-44ed-982a-8cb38c70870d,DISK]: blockChecksum=4b0b124b13c998a25d246802f733b088, blockChecksumType=MD5CRC
2020-12-03 07:19:48,125 [Thread-416] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:40369,DS-035cd092-cc9b-40a4-8e6a-3004ff2553ce,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775728_1005; getBlockSize()=37748736; corrupt=false; offset=150994944; locs=[DatanodeInfoWithStorage[127.0.0.1:40369,DS-035cd092-cc9b-40a4-8e6a-3004ff2553ce,DISK], DatanodeInfoWithStorage[127.0.0.1:35598,DS-265e643d-40cf-4be7-923e-86955d3df769,DISK], DatanodeInfoWithStorage[127.0.0.1:43233,DS-11ae3bf8-6838-4dfc-af89-3c76d111c85e,DISK], DatanodeInfoWithStorage[127.0.0.1:36972,DS-72b6d766-f12d-46df-9b78-3689fbb0ff3f,DISK], DatanodeInfoWithStorage[127.0.0.1:33826,DS-023f6a39-4389-43da-a1df-f7cccb6bcc41,DISK], DatanodeInfoWithStorage[127.0.0.1:40799,DS-997db01c-04dd-4559-92d6-ffa544c717ec,DISK], DatanodeInfoWithStorage[127.0.0.1:46334,DS-3c77b156-23e2-4fc9-9db2-bac28e329697,DISK], DatanodeInfoWithStorage[127.0.0.1:33393,DS-7a239de6-36ad-4a59-8b2f-83eb72cddab6,DISK], DatanodeInfoWithStorage[127.0.0.1:40990,DS-3bec7c54-7709-4f3d-b486-35765741195a,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7, 8]}
2020-12-03 07:19:48,151 [Thread-416] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(718)) - got reply from DatanodeInfoWithStorage[127.0.0.1:40369,DS-035cd092-cc9b-40a4-8e6a-3004ff2553ce,DISK]: blockChecksum=51cd424ab95d87cdb83537c2851632ce, blockChecksumType=MD5CRC
2020-12-03 07:19:48,152 [Thread-416] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:35598,DS-d3e3d4e2-c7e5-44e4-9c40-1b0d2f6aaca7,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775712_1006; getBlockSize()=37748736; corrupt=false; offset=188743680; locs=[DatanodeInfoWithStorage[127.0.0.1:35598,DS-d3e3d4e2-c7e5-44e4-9c40-1b0d2f6aaca7,DISK], DatanodeInfoWithStorage[127.0.0.1:40369,DS-5167c021-7d28-44ed-982a-8cb38c70870d,DISK], DatanodeInfoWithStorage[127.0.0.1:40990,DS-c396f456-72b2-4eec-a6af-559f05d184b3,DISK], DatanodeInfoWithStorage[127.0.0.1:36972,DS-c85716ef-a148-4295-b2b7-bca92bf0548a,DISK], DatanodeInfoWithStorage[127.0.0.1:33826,DS-2ce634e7-1c83-4eb2-9adf-081181b40a1c,DISK], DatanodeInfoWithStorage[127.0.0.1:40799,DS-353fdb6d-79d4-40b3-891b-044f5d75a5de,DISK], DatanodeInfoWithStorage[127.0.0.1:43233,DS-4d752d77-dc01-4782-88b7-fac90ddd2643,DISK], DatanodeInfoWithStorage[127.0.0.1:33393,DS-6c847cf4-ff4b-4f0f-91b4-4b65310e2c06,DISK], DatanodeInfoWithStorage[127.0.0.1:46334,DS-aa4eafc1-6206-4cf4-beac-6f7c6e133413,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7, 8]}
2020-12-03 07:19:48,190 [Thread-416] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(718)) - got reply from DatanodeInfoWithStorage[127.0.0.1:35598,DS-d3e3d4e2-c7e5-44e4-9c40-1b0d2f6aaca7,DISK]: blockChecksum=89aa04e67e713dc683b754d2f2ef667b, blockChecksumType=MD5CRC
2020-12-03 07:19:48,191 [Thread-416] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:43233,DS-11ae3bf8-6838-4dfc-af89-3c76d111c85e,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775696_1007; getBlockSize()=37748736; corrupt=false; offset=226492416; locs=[DatanodeInfoWithStorage[127.0.0.1:43233,DS-11ae3bf8-6838-4dfc-af89-3c76d111c85e,DISK], DatanodeInfoWithStorage[127.0.0.1:33393,DS-7a239de6-36ad-4a59-8b2f-83eb72cddab6,DISK], DatanodeInfoWithStorage[127.0.0.1:35598,DS-265e643d-40cf-4be7-923e-86955d3df769,DISK], DatanodeInfoWithStorage[127.0.0.1:43468,DS-462ced07-43f8-4720-afda-b3153be81947,DISK], DatanodeInfoWithStorage[127.0.0.1:40369,DS-035cd092-cc9b-40a4-8e6a-3004ff2553ce,DISK], DatanodeInfoWithStorage[127.0.0.1:40990,DS-3bec7c54-7709-4f3d-b486-35765741195a,DISK], DatanodeInfoWithStorage[127.0.0.1:33826,DS-023f6a39-4389-43da-a1df-f7cccb6bcc41,DISK], DatanodeInfoWithStorage[127.0.0.1:46334,DS-3c77b156-23e2-4fc9-9db2-bac28e329697,DISK], DatanodeInfoWithStorage[127.0.0.1:36972,DS-72b6d766-f12d-46df-9b78-3689fbb0ff3f,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7, 8]}
2020-12-03 07:19:48,215 [Thread-416] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(718)) - got reply from DatanodeInfoWithStorage[127.0.0.1:43233,DS-11ae3bf8-6838-4dfc-af89-3c76d111c85e,DISK]: blockChecksum=dfe7bac951dcb0667d7f7df4eb6cf90a, blockChecksumType=MD5CRC
2020-12-03 07:19:48,222 [Thread-416] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:36972,DS-c85716ef-a148-4295-b2b7-bca92bf0548a,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775680_1008; getBlockSize()=37748736; corrupt=false; offset=264241152; locs=[DatanodeInfoWithStorage[127.0.0.1:36972,DS-c85716ef-a148-4295-b2b7-bca92bf0548a,DISK], DatanodeInfoWithStorage[127.0.0.1:43468,DS-34e32a14-5782-4fe9-b945-76fbaab513a9,DISK], DatanodeInfoWithStorage[127.0.0.1:43233,DS-4d752d77-dc01-4782-88b7-fac90ddd2643,DISK], DatanodeInfoWithStorage[127.0.0.1:46334,DS-aa4eafc1-6206-4cf4-beac-6f7c6e133413,DISK], DatanodeInfoWithStorage[127.0.0.1:40799,DS-997db01c-04dd-4559-92d6-ffa544c717ec,DISK], DatanodeInfoWithStorage[127.0.0.1:33393,DS-6c847cf4-ff4b-4f0f-91b4-4b65310e2c06,DISK], DatanodeInfoWithStorage[127.0.0.1:33826,DS-2ce634e7-1c83-4eb2-9adf-081181b40a1c,DISK], DatanodeInfoWithStorage[127.0.0.1:40369,DS-5167c021-7d28-44ed-982a-8cb38c70870d,DISK], DatanodeInfoWithStorage[127.0.0.1:40990,DS-c396f456-72b2-4eec-a6af-559f05d184b3,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7, 8]}
2020-12-03 07:19:48,250 [Thread-416] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(718)) - got reply from DatanodeInfoWithStorage[127.0.0.1:36972,DS-c85716ef-a148-4295-b2b7-bca92bf0548a,DISK]: blockChecksum=9c3cec3f3a04864d00f8b7af0c6afe6d, blockChecksumType=MD5CRC
2020-12-03 07:19:48,251 [Thread-416] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:36213,DS-04af9d40-9522-498c-835c-e506c824a39f,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775664_1009; getBlockSize()=37748736; corrupt=false; offset=301989888; locs=[DatanodeInfoWithStorage[127.0.0.1:36213,DS-04af9d40-9522-498c-835c-e506c824a39f,DISK], DatanodeInfoWithStorage[127.0.0.1:43233,DS-11ae3bf8-6838-4dfc-af89-3c76d111c85e,DISK], DatanodeInfoWithStorage[127.0.0.1:35598,DS-d3e3d4e2-c7e5-44e4-9c40-1b0d2f6aaca7,DISK], DatanodeInfoWithStorage[127.0.0.1:36972,DS-72b6d766-f12d-46df-9b78-3689fbb0ff3f,DISK], DatanodeInfoWithStorage[127.0.0.1:33826,DS-023f6a39-4389-43da-a1df-f7cccb6bcc41,DISK], DatanodeInfoWithStorage[127.0.0.1:40799,DS-353fdb6d-79d4-40b3-891b-044f5d75a5de,DISK], DatanodeInfoWithStorage[127.0.0.1:46334,DS-3c77b156-23e2-4fc9-9db2-bac28e329697,DISK], DatanodeInfoWithStorage[127.0.0.1:40990,DS-3bec7c54-7709-4f3d-b486-35765741195a,DISK], DatanodeInfoWithStorage[127.0.0.1:40369,DS-035cd092-cc9b-40a4-8e6a-3004ff2553ce,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7, 8]}
2020-12-03 07:19:48,269 [Thread-416] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(718)) - got reply from DatanodeInfoWithStorage[127.0.0.1:36213,DS-04af9d40-9522-498c-835c-e506c824a39f,DISK]: blockChecksum=eacc1d4fa7716608ec43c4a99fd6b039, blockChecksumType=MD5CRC
2020-12-03 07:19:48,269 [Thread-416] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:40990,DS-c396f456-72b2-4eec-a6af-559f05d184b3,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775648_1010; getBlockSize()=37748736; corrupt=false; offset=339738624; locs=[DatanodeInfoWithStorage[127.0.0.1:40990,DS-c396f456-72b2-4eec-a6af-559f05d184b3,DISK], DatanodeInfoWithStorage[127.0.0.1:33826,DS-2ce634e7-1c83-4eb2-9adf-081181b40a1c,DISK], DatanodeInfoWithStorage[127.0.0.1:36213,DS-6b4155c2-6b43-450c-b668-a1919719e233,DISK], DatanodeInfoWithStorage[127.0.0.1:40799,DS-997db01c-04dd-4559-92d6-ffa544c717ec,DISK], DatanodeInfoWithStorage[127.0.0.1:35598,DS-265e643d-40cf-4be7-923e-86955d3df769,DISK], DatanodeInfoWithStorage[127.0.0.1:40369,DS-5167c021-7d28-44ed-982a-8cb38c70870d,DISK], DatanodeInfoWithStorage[127.0.0.1:43233,DS-4d752d77-dc01-4782-88b7-fac90ddd2643,DISK], DatanodeInfoWithStorage[127.0.0.1:46334,DS-aa4eafc1-6206-4cf4-beac-6f7c6e133413,DISK], DatanodeInfoWithStorage[127.0.0.1:33393,DS-7a239de6-36ad-4a59-8b2f-83eb72cddab6,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7, 8]}
2020-12-03 07:19:48,299 [Thread-416] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(718)) - got reply from DatanodeInfoWithStorage[127.0.0.1:40990,DS-c396f456-72b2-4eec-a6af-559f05d184b3,DISK]: blockChecksum=d727de70e1a448a98a33a89b5579fb44, blockChecksumType=MD5CRC
2020-12-03 07:19:48,317 [IPC Server handler 1 on default port 44779] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/striped/stripedFileChecksum1	dst=null	perm=null	proto=rpc
2020-12-03 07:19:48,324 [Thread-416] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:19:48,324 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@71652c98] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:19:48,326 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16, DS-353fdb6d-79d4-40b3-891b-044f5d75a5de) exiting.
2020-12-03 07:19:48,327 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15, DS-997db01c-04dd-4559-92d6-ffa544c717ec) exiting.
2020-12-03 07:19:48,449 [Thread-416] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@7668d560{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:19:48,457 [Thread-416] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@46292372{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:19:48,460 [Thread-416] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@1c65121{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:19:48,462 [Thread-416] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@3add81c4{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:19:48,471 [Thread-416] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 43524
2020-12-03 07:19:48,479 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:19:48,480 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:19:48,486 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:19:48,486 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid 24857262-68d9-4b98-abb9-9105d4bfc034) service to localhost/127.0.0.1:44779
2020-12-03 07:19:48,487 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid 24857262-68d9-4b98-abb9-9105d4bfc034)
2020-12-03 07:19:48,487 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:48,489 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15/current/BP-390774184-172.17.0.6-1606979969464] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:48,489 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16/current/BP-390774184-172.17.0.6-1606979969464] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:48,497 [Thread-416] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:19:48,498 [Thread-416] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:19:48,498 [Thread-416] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:19:48,498 [Thread-416] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:19:48,508 [Thread-416] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:19:48,511 [Thread-416] INFO  hdfs.StateChange (DatanodeManager.java:removeDeadDatanode(754)) - BLOCK* removeDeadDatanode: lost heartbeat from 127.0.0.1:40799, removeBlocksFromBlockMap true
2020-12-03 07:19:48,517 [Thread-416] INFO  net.NetworkTopology (NetworkTopology.java:remove(219)) - Removing a node: /default-rack/127.0.0.1:40799
2020-12-03 07:19:48,527 [IPC Server handler 9 on default port 44779] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/striped/stripedFileChecksum1	dst=null	perm=null	proto=rpc
2020-12-03 07:19:48,551 [Thread-416] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:33393,DS-7a239de6-36ad-4a59-8b2f-83eb72cddab6,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33393,DS-7a239de6-36ad-4a59-8b2f-83eb72cddab6,DISK], DatanodeInfoWithStorage[127.0.0.1:40369,DS-035cd092-cc9b-40a4-8e6a-3004ff2553ce,DISK], DatanodeInfoWithStorage[127.0.0.1:40990,DS-3bec7c54-7709-4f3d-b486-35765741195a,DISK], DatanodeInfoWithStorage[127.0.0.1:36213,DS-04af9d40-9522-498c-835c-e506c824a39f,DISK], DatanodeInfoWithStorage[127.0.0.1:35598,DS-d3e3d4e2-c7e5-44e4-9c40-1b0d2f6aaca7,DISK], DatanodeInfoWithStorage[127.0.0.1:43233,DS-11ae3bf8-6838-4dfc-af89-3c76d111c85e,DISK], DatanodeInfoWithStorage[127.0.0.1:33826,DS-2ce634e7-1c83-4eb2-9adf-081181b40a1c,DISK], DatanodeInfoWithStorage[127.0.0.1:46334,DS-3c77b156-23e2-4fc9-9db2-bac28e329697,DISK]]; indices=[0, 1, 3, 4, 5, 6, 7, 8]}
2020-12-03 07:19:48,585 [DataXceiver for client /127.0.0.1:32934 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,621 [DataXceiver for client /127.0.0.1:32934 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,626 [DataXceiver for client /127.0.0.1:32934 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,629 [DataXceiver for client /127.0.0.1:32934 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,635 [DataXceiver for client /127.0.0.1:32934 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,638 [DataXceiver for client /127.0.0.1:32934 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,654 [Thread-416] WARN  hdfs.FileChecksumHelper (FileChecksumHelper.java:checksumBlockGroup(679)) - src=/striped/stripedFileChecksum1, datanodes[0]=DatanodeInfoWithStorage[127.0.0.1:33393,DS-7a239de6-36ad-4a59-8b2f-83eb72cddab6,DISK]
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:550)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.tryDatanode(FileChecksumHelper.java:709)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlockGroup(FileChecksumHelper.java:664)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:638)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery15(TestFileChecksum.java:465)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-12-03 07:19:48,664 [DataXceiver for client /127.0.0.1:32934 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:33393:DataXceiver error processing BLOCK_GROUP_CHECKSUM operation  src: /127.0.0.1:32934 dst: /127.0.0.1:33393
java.lang.UnsupportedOperationException
	at java.nio.ByteBuffer.array(ByteBuffer.java:994)
	at org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockChecksumReconstructor.reconstruct(StripedBlockChecksumReconstructor.java:90)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.recalculateChecksum(BlockChecksumHelper.java:711)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.compute(BlockChecksumHelper.java:489)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.blockGroupChecksum(DataXceiver.java:1047)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opStripedBlockChecksum(Receiver.java:327)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:119)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:48,666 [Thread-416] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:40369,DS-035cd092-cc9b-40a4-8e6a-3004ff2553ce,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33393,DS-7a239de6-36ad-4a59-8b2f-83eb72cddab6,DISK], DatanodeInfoWithStorage[127.0.0.1:40369,DS-035cd092-cc9b-40a4-8e6a-3004ff2553ce,DISK], DatanodeInfoWithStorage[127.0.0.1:40990,DS-3bec7c54-7709-4f3d-b486-35765741195a,DISK], DatanodeInfoWithStorage[127.0.0.1:36213,DS-04af9d40-9522-498c-835c-e506c824a39f,DISK], DatanodeInfoWithStorage[127.0.0.1:35598,DS-d3e3d4e2-c7e5-44e4-9c40-1b0d2f6aaca7,DISK], DatanodeInfoWithStorage[127.0.0.1:43233,DS-11ae3bf8-6838-4dfc-af89-3c76d111c85e,DISK], DatanodeInfoWithStorage[127.0.0.1:33826,DS-2ce634e7-1c83-4eb2-9adf-081181b40a1c,DISK], DatanodeInfoWithStorage[127.0.0.1:46334,DS-3c77b156-23e2-4fc9-9db2-bac28e329697,DISK]]; indices=[0, 1, 3, 4, 5, 6, 7, 8]}
2020-12-03 07:19:48,679 [DataXceiver for client /127.0.0.1:51158 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,681 [DataXceiver for client /127.0.0.1:51158 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,683 [DataXceiver for client /127.0.0.1:51158 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,687 [DataXceiver for client /127.0.0.1:51158 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,688 [DataXceiver for client /127.0.0.1:51158 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,690 [DataXceiver for client /127.0.0.1:51158 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,695 [DataXceiver for client /127.0.0.1:51158 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:40369:DataXceiver error processing BLOCK_GROUP_CHECKSUM operation  src: /127.0.0.1:51158 dst: /127.0.0.1:40369
java.lang.UnsupportedOperationException
	at java.nio.ByteBuffer.array(ByteBuffer.java:994)
	at org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockChecksumReconstructor.reconstruct(StripedBlockChecksumReconstructor.java:90)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.recalculateChecksum(BlockChecksumHelper.java:711)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.compute(BlockChecksumHelper.java:489)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.blockGroupChecksum(DataXceiver.java:1047)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opStripedBlockChecksum(Receiver.java:327)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:119)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:48,695 [Thread-416] WARN  hdfs.FileChecksumHelper (FileChecksumHelper.java:checksumBlockGroup(679)) - src=/striped/stripedFileChecksum1, datanodes[1]=DatanodeInfoWithStorage[127.0.0.1:40369,DS-035cd092-cc9b-40a4-8e6a-3004ff2553ce,DISK]
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:550)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.tryDatanode(FileChecksumHelper.java:709)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlockGroup(FileChecksumHelper.java:664)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:638)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery15(TestFileChecksum.java:465)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-12-03 07:19:48,699 [Thread-416] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:40990,DS-3bec7c54-7709-4f3d-b486-35765741195a,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33393,DS-7a239de6-36ad-4a59-8b2f-83eb72cddab6,DISK], DatanodeInfoWithStorage[127.0.0.1:40369,DS-035cd092-cc9b-40a4-8e6a-3004ff2553ce,DISK], DatanodeInfoWithStorage[127.0.0.1:40990,DS-3bec7c54-7709-4f3d-b486-35765741195a,DISK], DatanodeInfoWithStorage[127.0.0.1:36213,DS-04af9d40-9522-498c-835c-e506c824a39f,DISK], DatanodeInfoWithStorage[127.0.0.1:35598,DS-d3e3d4e2-c7e5-44e4-9c40-1b0d2f6aaca7,DISK], DatanodeInfoWithStorage[127.0.0.1:43233,DS-11ae3bf8-6838-4dfc-af89-3c76d111c85e,DISK], DatanodeInfoWithStorage[127.0.0.1:33826,DS-2ce634e7-1c83-4eb2-9adf-081181b40a1c,DISK], DatanodeInfoWithStorage[127.0.0.1:46334,DS-3c77b156-23e2-4fc9-9db2-bac28e329697,DISK]]; indices=[0, 1, 3, 4, 5, 6, 7, 8]}
2020-12-03 07:19:48,707 [DataXceiver for client /127.0.0.1:42430 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,710 [DataXceiver for client /127.0.0.1:42430 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,712 [DataXceiver for client /127.0.0.1:42430 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,716 [DataXceiver for client /127.0.0.1:42430 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,722 [DataXceiver for client /127.0.0.1:42430 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,728 [DataXceiver for client /127.0.0.1:42430 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,732 [DataXceiver for client /127.0.0.1:42430 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:40990:DataXceiver error processing BLOCK_GROUP_CHECKSUM operation  src: /127.0.0.1:42430 dst: /127.0.0.1:40990
java.lang.UnsupportedOperationException
	at java.nio.ByteBuffer.array(ByteBuffer.java:994)
	at org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockChecksumReconstructor.reconstruct(StripedBlockChecksumReconstructor.java:90)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.recalculateChecksum(BlockChecksumHelper.java:711)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.compute(BlockChecksumHelper.java:489)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.blockGroupChecksum(DataXceiver.java:1047)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opStripedBlockChecksum(Receiver.java:327)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:119)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:48,732 [Thread-416] WARN  hdfs.FileChecksumHelper (FileChecksumHelper.java:checksumBlockGroup(679)) - src=/striped/stripedFileChecksum1, datanodes[2]=DatanodeInfoWithStorage[127.0.0.1:40990,DS-3bec7c54-7709-4f3d-b486-35765741195a,DISK]
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:550)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.tryDatanode(FileChecksumHelper.java:709)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlockGroup(FileChecksumHelper.java:664)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:638)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery15(TestFileChecksum.java:465)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-12-03 07:19:48,743 [Thread-416] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:36213,DS-04af9d40-9522-498c-835c-e506c824a39f,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33393,DS-7a239de6-36ad-4a59-8b2f-83eb72cddab6,DISK], DatanodeInfoWithStorage[127.0.0.1:40369,DS-035cd092-cc9b-40a4-8e6a-3004ff2553ce,DISK], DatanodeInfoWithStorage[127.0.0.1:40990,DS-3bec7c54-7709-4f3d-b486-35765741195a,DISK], DatanodeInfoWithStorage[127.0.0.1:36213,DS-04af9d40-9522-498c-835c-e506c824a39f,DISK], DatanodeInfoWithStorage[127.0.0.1:35598,DS-d3e3d4e2-c7e5-44e4-9c40-1b0d2f6aaca7,DISK], DatanodeInfoWithStorage[127.0.0.1:43233,DS-11ae3bf8-6838-4dfc-af89-3c76d111c85e,DISK], DatanodeInfoWithStorage[127.0.0.1:33826,DS-2ce634e7-1c83-4eb2-9adf-081181b40a1c,DISK], DatanodeInfoWithStorage[127.0.0.1:46334,DS-3c77b156-23e2-4fc9-9db2-bac28e329697,DISK]]; indices=[0, 1, 3, 4, 5, 6, 7, 8]}
2020-12-03 07:19:48,800 [DataXceiver for client /127.0.0.1:57446 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,807 [DataXceiver for client /127.0.0.1:57446 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,816 [DataXceiver for client /127.0.0.1:57446 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,888 [DataXceiver for client /127.0.0.1:57446 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,904 [DataXceiver for client /127.0.0.1:57446 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,906 [DataXceiver for client /127.0.0.1:57446 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,919 [DataXceiver for client /127.0.0.1:57446 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:36213:DataXceiver error processing BLOCK_GROUP_CHECKSUM operation  src: /127.0.0.1:57446 dst: /127.0.0.1:36213
java.lang.UnsupportedOperationException
	at java.nio.ByteBuffer.array(ByteBuffer.java:994)
	at org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockChecksumReconstructor.reconstruct(StripedBlockChecksumReconstructor.java:90)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.recalculateChecksum(BlockChecksumHelper.java:711)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.compute(BlockChecksumHelper.java:489)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.blockGroupChecksum(DataXceiver.java:1047)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opStripedBlockChecksum(Receiver.java:327)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:119)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:48,919 [Thread-416] WARN  hdfs.FileChecksumHelper (FileChecksumHelper.java:checksumBlockGroup(679)) - src=/striped/stripedFileChecksum1, datanodes[3]=DatanodeInfoWithStorage[127.0.0.1:36213,DS-04af9d40-9522-498c-835c-e506c824a39f,DISK]
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:550)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.tryDatanode(FileChecksumHelper.java:709)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlockGroup(FileChecksumHelper.java:664)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:638)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery15(TestFileChecksum.java:465)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-12-03 07:19:48,922 [Thread-416] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:35598,DS-d3e3d4e2-c7e5-44e4-9c40-1b0d2f6aaca7,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33393,DS-7a239de6-36ad-4a59-8b2f-83eb72cddab6,DISK], DatanodeInfoWithStorage[127.0.0.1:40369,DS-035cd092-cc9b-40a4-8e6a-3004ff2553ce,DISK], DatanodeInfoWithStorage[127.0.0.1:40990,DS-3bec7c54-7709-4f3d-b486-35765741195a,DISK], DatanodeInfoWithStorage[127.0.0.1:36213,DS-04af9d40-9522-498c-835c-e506c824a39f,DISK], DatanodeInfoWithStorage[127.0.0.1:35598,DS-d3e3d4e2-c7e5-44e4-9c40-1b0d2f6aaca7,DISK], DatanodeInfoWithStorage[127.0.0.1:43233,DS-11ae3bf8-6838-4dfc-af89-3c76d111c85e,DISK], DatanodeInfoWithStorage[127.0.0.1:33826,DS-2ce634e7-1c83-4eb2-9adf-081181b40a1c,DISK], DatanodeInfoWithStorage[127.0.0.1:46334,DS-3c77b156-23e2-4fc9-9db2-bac28e329697,DISK]]; indices=[0, 1, 3, 4, 5, 6, 7, 8]}
2020-12-03 07:19:48,933 [DataXceiver for client /127.0.0.1:38308 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,939 [DataXceiver for client /127.0.0.1:38308 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,941 [DataXceiver for client /127.0.0.1:38308 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,947 [DataXceiver for client /127.0.0.1:38308 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,959 [DataXceiver for client /127.0.0.1:38308 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,961 [DataXceiver for client /127.0.0.1:38308 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,980 [DataXceiver for client /127.0.0.1:38308 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:35598:DataXceiver error processing BLOCK_GROUP_CHECKSUM operation  src: /127.0.0.1:38308 dst: /127.0.0.1:35598
java.lang.UnsupportedOperationException
	at java.nio.ByteBuffer.array(ByteBuffer.java:994)
	at org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockChecksumReconstructor.reconstruct(StripedBlockChecksumReconstructor.java:90)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.recalculateChecksum(BlockChecksumHelper.java:711)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.compute(BlockChecksumHelper.java:489)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.blockGroupChecksum(DataXceiver.java:1047)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opStripedBlockChecksum(Receiver.java:327)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:119)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:48,982 [Thread-416] WARN  hdfs.FileChecksumHelper (FileChecksumHelper.java:checksumBlockGroup(679)) - src=/striped/stripedFileChecksum1, datanodes[4]=DatanodeInfoWithStorage[127.0.0.1:35598,DS-d3e3d4e2-c7e5-44e4-9c40-1b0d2f6aaca7,DISK]
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:550)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.tryDatanode(FileChecksumHelper.java:709)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlockGroup(FileChecksumHelper.java:664)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:638)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery15(TestFileChecksum.java:465)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-12-03 07:19:48,985 [Thread-416] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:43233,DS-11ae3bf8-6838-4dfc-af89-3c76d111c85e,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33393,DS-7a239de6-36ad-4a59-8b2f-83eb72cddab6,DISK], DatanodeInfoWithStorage[127.0.0.1:40369,DS-035cd092-cc9b-40a4-8e6a-3004ff2553ce,DISK], DatanodeInfoWithStorage[127.0.0.1:40990,DS-3bec7c54-7709-4f3d-b486-35765741195a,DISK], DatanodeInfoWithStorage[127.0.0.1:36213,DS-04af9d40-9522-498c-835c-e506c824a39f,DISK], DatanodeInfoWithStorage[127.0.0.1:35598,DS-d3e3d4e2-c7e5-44e4-9c40-1b0d2f6aaca7,DISK], DatanodeInfoWithStorage[127.0.0.1:43233,DS-11ae3bf8-6838-4dfc-af89-3c76d111c85e,DISK], DatanodeInfoWithStorage[127.0.0.1:33826,DS-2ce634e7-1c83-4eb2-9adf-081181b40a1c,DISK], DatanodeInfoWithStorage[127.0.0.1:46334,DS-3c77b156-23e2-4fc9-9db2-bac28e329697,DISK]]; indices=[0, 1, 3, 4, 5, 6, 7, 8]}
2020-12-03 07:19:48,992 [DataXceiver for client /127.0.0.1:34710 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,994 [DataXceiver for client /127.0.0.1:34710 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:49,010 [DataXceiver for client /127.0.0.1:34710 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:49,017 [DataXceiver for client /127.0.0.1:34710 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:49,026 [DataXceiver for client /127.0.0.1:34710 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:49,036 [DataXceiver for client /127.0.0.1:34710 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:49,048 [DataXceiver for client /127.0.0.1:34710 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:43233:DataXceiver error processing BLOCK_GROUP_CHECKSUM operation  src: /127.0.0.1:34710 dst: /127.0.0.1:43233
java.lang.UnsupportedOperationException
	at java.nio.ByteBuffer.array(ByteBuffer.java:994)
	at org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockChecksumReconstructor.reconstruct(StripedBlockChecksumReconstructor.java:90)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.recalculateChecksum(BlockChecksumHelper.java:711)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.compute(BlockChecksumHelper.java:489)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.blockGroupChecksum(DataXceiver.java:1047)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opStripedBlockChecksum(Receiver.java:327)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:119)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,051 [Thread-416] WARN  hdfs.FileChecksumHelper (FileChecksumHelper.java:checksumBlockGroup(679)) - src=/striped/stripedFileChecksum1, datanodes[5]=DatanodeInfoWithStorage[127.0.0.1:43233,DS-11ae3bf8-6838-4dfc-af89-3c76d111c85e,DISK]
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:550)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.tryDatanode(FileChecksumHelper.java:709)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlockGroup(FileChecksumHelper.java:664)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:638)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery15(TestFileChecksum.java:465)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-12-03 07:19:49,055 [Thread-416] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:33826,DS-2ce634e7-1c83-4eb2-9adf-081181b40a1c,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33393,DS-7a239de6-36ad-4a59-8b2f-83eb72cddab6,DISK], DatanodeInfoWithStorage[127.0.0.1:40369,DS-035cd092-cc9b-40a4-8e6a-3004ff2553ce,DISK], DatanodeInfoWithStorage[127.0.0.1:40990,DS-3bec7c54-7709-4f3d-b486-35765741195a,DISK], DatanodeInfoWithStorage[127.0.0.1:36213,DS-04af9d40-9522-498c-835c-e506c824a39f,DISK], DatanodeInfoWithStorage[127.0.0.1:35598,DS-d3e3d4e2-c7e5-44e4-9c40-1b0d2f6aaca7,DISK], DatanodeInfoWithStorage[127.0.0.1:43233,DS-11ae3bf8-6838-4dfc-af89-3c76d111c85e,DISK], DatanodeInfoWithStorage[127.0.0.1:33826,DS-2ce634e7-1c83-4eb2-9adf-081181b40a1c,DISK], DatanodeInfoWithStorage[127.0.0.1:46334,DS-3c77b156-23e2-4fc9-9db2-bac28e329697,DISK]]; indices=[0, 1, 3, 4, 5, 6, 7, 8]}
2020-12-03 07:19:49,074 [DataXceiver for client /127.0.0.1:51006 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:49,092 [DataXceiver for client /127.0.0.1:51006 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:49,105 [DataXceiver for client /127.0.0.1:51006 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:49,117 [DataXceiver for client /127.0.0.1:51006 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:49,119 [DataXceiver for client /127.0.0.1:51006 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:49,121 [DataXceiver for client /127.0.0.1:51006 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:49,123 [DataXceiver for client /127.0.0.1:51006 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:33826:DataXceiver error processing BLOCK_GROUP_CHECKSUM operation  src: /127.0.0.1:51006 dst: /127.0.0.1:33826
java.lang.UnsupportedOperationException
	at java.nio.ByteBuffer.array(ByteBuffer.java:994)
	at org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockChecksumReconstructor.reconstruct(StripedBlockChecksumReconstructor.java:90)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.recalculateChecksum(BlockChecksumHelper.java:711)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.compute(BlockChecksumHelper.java:489)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.blockGroupChecksum(DataXceiver.java:1047)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opStripedBlockChecksum(Receiver.java:327)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:119)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,124 [Thread-416] WARN  hdfs.FileChecksumHelper (FileChecksumHelper.java:checksumBlockGroup(679)) - src=/striped/stripedFileChecksum1, datanodes[6]=DatanodeInfoWithStorage[127.0.0.1:33826,DS-2ce634e7-1c83-4eb2-9adf-081181b40a1c,DISK]
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:550)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.tryDatanode(FileChecksumHelper.java:709)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlockGroup(FileChecksumHelper.java:664)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:638)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery15(TestFileChecksum.java:465)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-12-03 07:19:49,127 [Thread-416] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:46334,DS-3c77b156-23e2-4fc9-9db2-bac28e329697,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33393,DS-7a239de6-36ad-4a59-8b2f-83eb72cddab6,DISK], DatanodeInfoWithStorage[127.0.0.1:40369,DS-035cd092-cc9b-40a4-8e6a-3004ff2553ce,DISK], DatanodeInfoWithStorage[127.0.0.1:40990,DS-3bec7c54-7709-4f3d-b486-35765741195a,DISK], DatanodeInfoWithStorage[127.0.0.1:36213,DS-04af9d40-9522-498c-835c-e506c824a39f,DISK], DatanodeInfoWithStorage[127.0.0.1:35598,DS-d3e3d4e2-c7e5-44e4-9c40-1b0d2f6aaca7,DISK], DatanodeInfoWithStorage[127.0.0.1:43233,DS-11ae3bf8-6838-4dfc-af89-3c76d111c85e,DISK], DatanodeInfoWithStorage[127.0.0.1:33826,DS-2ce634e7-1c83-4eb2-9adf-081181b40a1c,DISK], DatanodeInfoWithStorage[127.0.0.1:46334,DS-3c77b156-23e2-4fc9-9db2-bac28e329697,DISK]]; indices=[0, 1, 3, 4, 5, 6, 7, 8]}
2020-12-03 07:19:49,132 [DataXceiver for client /127.0.0.1:57838 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:49,134 [DataXceiver for client /127.0.0.1:57838 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:49,135 [DataXceiver for client /127.0.0.1:57838 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:49,137 [DataXceiver for client /127.0.0.1:57838 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:49,141 [DataXceiver for client /127.0.0.1:57838 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:49,142 [DataXceiver for client /127.0.0.1:57838 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:49,145 [DataXceiver for client /127.0.0.1:57838 [Getting checksum for block groupBP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:46334:DataXceiver error processing BLOCK_GROUP_CHECKSUM operation  src: /127.0.0.1:57838 dst: /127.0.0.1:46334
java.lang.UnsupportedOperationException
	at java.nio.ByteBuffer.array(ByteBuffer.java:994)
	at org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockChecksumReconstructor.reconstruct(StripedBlockChecksumReconstructor.java:90)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.recalculateChecksum(BlockChecksumHelper.java:711)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.compute(BlockChecksumHelper.java:489)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.blockGroupChecksum(DataXceiver.java:1047)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opStripedBlockChecksum(Receiver.java:327)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:119)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,146 [Thread-416] WARN  hdfs.FileChecksumHelper (FileChecksumHelper.java:checksumBlockGroup(679)) - src=/striped/stripedFileChecksum1, datanodes[7]=DatanodeInfoWithStorage[127.0.0.1:46334,DS-3c77b156-23e2-4fc9-9db2-bac28e329697,DISK]
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:550)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.tryDatanode(FileChecksumHelper.java:709)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlockGroup(FileChecksumHelper.java:664)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:638)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery15(TestFileChecksum.java:465)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-12-03 07:19:49,150 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(2049)) - Shutting down the Mini HDFS Cluster
2020-12-03 07:19:49,152 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 10
2020-12-03 07:19:49,153 [Listener at localhost/32858] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:19:49,153 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@6aa648b9] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:19:49,155 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22, DS-6b4155c2-6b43-450c-b668-a1919719e233) exiting.
2020-12-03 07:19:49,155 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21, DS-04af9d40-9522-498c-835c-e506c824a39f) exiting.
2020-12-03 07:19:49,218 [Listener at localhost/32858] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@56bc3fac{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:19:49,220 [Listener at localhost/32858] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@df4b72{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:19:49,220 [Listener at localhost/32858] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@23aae55{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:19:49,221 [Listener at localhost/32858] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@3e681bc{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:19:49,227 [DataXceiver for client  at /127.0.0.1:57382 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,227 [DataXceiver for client  at /127.0.0.1:57382 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001 on DS-04af9d40-9522-498c-835c-e506c824a39f, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,230 [DataXceiver for client  at /127.0.0.1:57382 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:36213, datanodeUuid=68a96d2b-90b0-4ee3-b0be-e2a02cebb74b, infoPort=38285, infoSecurePort=0, ipcPort=32858, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001 to /127.0.0.1:57382
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,238 [DataXceiver for client  at /127.0.0.1:57434 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,238 [DataXceiver for client  at /127.0.0.1:57382 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:36213:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:57382 dst: /127.0.0.1:36213
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,241 [DataXceiver for client  at /127.0.0.1:57500 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,241 [DataXceiver for client  at /127.0.0.1:57672 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,238 [DataXceiver for client  at /127.0.0.1:57434 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001 on DS-04af9d40-9522-498c-835c-e506c824a39f, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,241 [DataXceiver for client  at /127.0.0.1:57556 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,242 [Listener at localhost/32858] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 32858
2020-12-03 07:19:49,242 [DataXceiver for client  at /127.0.0.1:57810 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,242 [DataXceiver for client  at /127.0.0.1:57774 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,242 [DataXceiver for client  at /127.0.0.1:57672 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001 on DS-04af9d40-9522-498c-835c-e506c824a39f, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,242 [DataXceiver for client  at /127.0.0.1:57434 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:36213, datanodeUuid=68a96d2b-90b0-4ee3-b0be-e2a02cebb74b, infoPort=38285, infoSecurePort=0, ipcPort=32858, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001 to /127.0.0.1:57434
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,255 [DataXceiver for client  at /127.0.0.1:57500 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001 on DS-04af9d40-9522-498c-835c-e506c824a39f, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,255 [DataXceiver for client  at /127.0.0.1:57434 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:36213:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:57434 dst: /127.0.0.1:36213
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,255 [DataXceiver for client  at /127.0.0.1:57672 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:36213, datanodeUuid=68a96d2b-90b0-4ee3-b0be-e2a02cebb74b, infoPort=38285, infoSecurePort=0, ipcPort=32858, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001 to /127.0.0.1:57672
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,254 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:19:49,246 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:19:49,263 [DataXceiver for client  at /127.0.0.1:57672 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:36213:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:57672 dst: /127.0.0.1:36213
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,258 [DataXceiver for client  at /127.0.0.1:57774 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001 on DS-04af9d40-9522-498c-835c-e506c824a39f, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,255 [DataXceiver for client  at /127.0.0.1:57500 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:36213, datanodeUuid=68a96d2b-90b0-4ee3-b0be-e2a02cebb74b, infoPort=38285, infoSecurePort=0, ipcPort=32858, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001 to /127.0.0.1:57500
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,286 [DataXceiver for client  at /127.0.0.1:57500 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:36213:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:57500 dst: /127.0.0.1:36213
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,286 [DataXceiver for client  at /127.0.0.1:57774 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:36213, datanodeUuid=68a96d2b-90b0-4ee3-b0be-e2a02cebb74b, infoPort=38285, infoSecurePort=0, ipcPort=32858, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001 to /127.0.0.1:57774
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,286 [DataXceiver for client  at /127.0.0.1:57810 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001 on DS-04af9d40-9522-498c-835c-e506c824a39f, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,275 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:19:49,299 [DataXceiver for client  at /127.0.0.1:57810 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:36213, datanodeUuid=68a96d2b-90b0-4ee3-b0be-e2a02cebb74b, infoPort=38285, infoSecurePort=0, ipcPort=32858, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001 to /127.0.0.1:57810
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,296 [DataXceiver for client  at /127.0.0.1:57556 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001 on DS-04af9d40-9522-498c-835c-e506c824a39f, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,299 [DataXceiver for client  at /127.0.0.1:57556 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:36213, datanodeUuid=68a96d2b-90b0-4ee3-b0be-e2a02cebb74b, infoPort=38285, infoSecurePort=0, ipcPort=32858, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001 to /127.0.0.1:57556
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,299 [DataXceiver for client  at /127.0.0.1:57556 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:36213:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:57556 dst: /127.0.0.1:36213
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,296 [DataXceiver for client  at /127.0.0.1:57774 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:36213:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:57774 dst: /127.0.0.1:36213
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,299 [DataXceiver for client  at /127.0.0.1:57810 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775788_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:36213:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:57810 dst: /127.0.0.1:36213
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,299 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid 68a96d2b-90b0-4ee3-b0be-e2a02cebb74b) service to localhost/127.0.0.1:44779
2020-12-03 07:19:49,309 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid 68a96d2b-90b0-4ee3-b0be-e2a02cebb74b)
2020-12-03 07:19:49,309 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:49,324 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21/current/BP-390774184-172.17.0.6-1606979969464] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:49,324 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22/current/BP-390774184-172.17.0.6-1606979969464] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:49,336 [Listener at localhost/32858] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:19:49,338 [Listener at localhost/32858] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:19:49,340 [Listener at localhost/32858] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:19:49,352 [Listener at localhost/32858] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:19:49,358 [Listener at localhost/32858] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:19:49,358 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 9
2020-12-03 07:19:49,358 [Listener at localhost/32858] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:19:49,358 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@777c9dc9] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:19:49,361 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20, DS-aa4eafc1-6206-4cf4-beac-6f7c6e133413) exiting.
2020-12-03 07:19:49,361 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19, DS-3c77b156-23e2-4fc9-9db2-bac28e329697) exiting.
2020-12-03 07:19:49,389 [Listener at localhost/32858] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@3e1162e7{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:19:49,400 [Listener at localhost/32858] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@79c3f01f{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:19:49,400 [Listener at localhost/32858] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@3b0f7d9d{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:19:49,401 [Listener at localhost/32858] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@9fecdf1{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:19:49,402 [Listener at localhost/32858] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 39541
2020-12-03 07:19:49,403 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:19:49,403 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:19:49,407 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:19:49,408 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid 1671bb40-be61-48b3-8dfe-d4cda3f68f8f) service to localhost/127.0.0.1:44779
2020-12-03 07:19:49,408 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid 1671bb40-be61-48b3-8dfe-d4cda3f68f8f)
2020-12-03 07:19:49,408 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:49,409 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19/current/BP-390774184-172.17.0.6-1606979969464] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:49,409 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20/current/BP-390774184-172.17.0.6-1606979969464] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:49,422 [Listener at localhost/32858] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:19:49,422 [Listener at localhost/32858] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:19:49,425 [Listener at localhost/32858] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:19:49,425 [Listener at localhost/32858] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:19:49,430 [Listener at localhost/32858] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:19:49,431 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 8
2020-12-03 07:19:49,431 [Listener at localhost/32858] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:19:49,431 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@54f5f647] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:19:49,434 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17, DS-7a239de6-36ad-4a59-8b2f-83eb72cddab6) exiting.
2020-12-03 07:19:49,434 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18, DS-6c847cf4-ff4b-4f0f-91b4-4b65310e2c06) exiting.
2020-12-03 07:19:49,471 [Listener at localhost/32858] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@64d7b720{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:19:49,472 [Listener at localhost/32858] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@30272916{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:19:49,473 [Listener at localhost/32858] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@6650813a{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:19:49,473 [Listener at localhost/32858] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@54e81b21{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:19:49,476 [DataXceiver for client  at /127.0.0.1:32946 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,476 [DataXceiver for client  at /127.0.0.1:32946 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001 on DS-7a239de6-36ad-4a59-8b2f-83eb72cddab6, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,477 [DataXceiver for client  at /127.0.0.1:32946 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:33393, datanodeUuid=a1eb1197-6763-4116-b38c-3702cdacda3d, infoPort=42572, infoSecurePort=0, ipcPort=36667, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001 to /127.0.0.1:32946
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,476 [DataXceiver for client  at /127.0.0.1:33062 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,477 [DataXceiver for client  at /127.0.0.1:33062 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001 on DS-7a239de6-36ad-4a59-8b2f-83eb72cddab6, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,478 [DataXceiver for client  at /127.0.0.1:33062 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:33393, datanodeUuid=a1eb1197-6763-4116-b38c-3702cdacda3d, infoPort=42572, infoSecurePort=0, ipcPort=36667, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001 to /127.0.0.1:33062
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,476 [DataXceiver for client  at /127.0.0.1:33442 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,487 [DataXceiver for client  at /127.0.0.1:33442 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001 on DS-7a239de6-36ad-4a59-8b2f-83eb72cddab6, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,476 [DataXceiver for client  at /127.0.0.1:33278 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,512 [DataXceiver for client  at /127.0.0.1:33278 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001 on DS-7a239de6-36ad-4a59-8b2f-83eb72cddab6, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,512 [DataXceiver for client  at /127.0.0.1:33278 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:33393, datanodeUuid=a1eb1197-6763-4116-b38c-3702cdacda3d, infoPort=42572, infoSecurePort=0, ipcPort=36667, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001 to /127.0.0.1:33278
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,513 [DataXceiver for client  at /127.0.0.1:33278 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:33393:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:33278 dst: /127.0.0.1:33393
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,476 [Listener at localhost/32858] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 36667
2020-12-03 07:19:49,476 [DataXceiver for client  at /127.0.0.1:33178 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,476 [DataXceiver for client  at /127.0.0.1:33010 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,476 [DataXceiver for client  at /127.0.0.1:33110 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,524 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:19:49,524 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:19:49,524 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:19:49,523 [DataXceiver for client  at /127.0.0.1:33178 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001 on DS-7a239de6-36ad-4a59-8b2f-83eb72cddab6, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,488 [DataXceiver for client  at /127.0.0.1:33392 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,487 [DataXceiver for client  at /127.0.0.1:33442 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:33393, datanodeUuid=a1eb1197-6763-4116-b38c-3702cdacda3d, infoPort=42572, infoSecurePort=0, ipcPort=36667, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001 to /127.0.0.1:33442
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,486 [DataXceiver for client  at /127.0.0.1:33062 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:33393:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:33062 dst: /127.0.0.1:33393
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,477 [DataXceiver for client  at /127.0.0.1:32946 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:33393:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:32946 dst: /127.0.0.1:33393
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,537 [DataXceiver for client  at /127.0.0.1:33442 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:33393:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:33442 dst: /127.0.0.1:33393
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,535 [DataXceiver for client  at /127.0.0.1:33178 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:33393, datanodeUuid=a1eb1197-6763-4116-b38c-3702cdacda3d, infoPort=42572, infoSecurePort=0, ipcPort=36667, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001 to /127.0.0.1:33178
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,535 [DataXceiver for client  at /127.0.0.1:33110 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001 on DS-7a239de6-36ad-4a59-8b2f-83eb72cddab6, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,554 [DataXceiver for client  at /127.0.0.1:33178 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:33393:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:33178 dst: /127.0.0.1:33393
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,531 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid a1eb1197-6763-4116-b38c-3702cdacda3d) service to localhost/127.0.0.1:44779
2020-12-03 07:19:49,566 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid a1eb1197-6763-4116-b38c-3702cdacda3d)
2020-12-03 07:19:49,566 [DataXceiver for client  at /127.0.0.1:33010 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001 on DS-7a239de6-36ad-4a59-8b2f-83eb72cddab6, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,570 [DataXceiver for client  at /127.0.0.1:33110 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:33393, datanodeUuid=a1eb1197-6763-4116-b38c-3702cdacda3d, infoPort=42572, infoSecurePort=0, ipcPort=36667, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001 to /127.0.0.1:33110
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,570 [DataXceiver for client  at /127.0.0.1:33010 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:33393, datanodeUuid=a1eb1197-6763-4116-b38c-3702cdacda3d, infoPort=42572, infoSecurePort=0, ipcPort=36667, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001 to /127.0.0.1:33010
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,570 [DataXceiver for client  at /127.0.0.1:33392 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001 on DS-7a239de6-36ad-4a59-8b2f-83eb72cddab6, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,570 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:49,576 [DataXceiver for client  at /127.0.0.1:33392 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:33393, datanodeUuid=a1eb1197-6763-4116-b38c-3702cdacda3d, infoPort=42572, infoSecurePort=0, ipcPort=36667, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001 to /127.0.0.1:33392
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,573 [DataXceiver for client  at /127.0.0.1:33010 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:33393:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:33010 dst: /127.0.0.1:33393
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,571 [DataXceiver for client  at /127.0.0.1:33110 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:33393:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:33110 dst: /127.0.0.1:33393
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,581 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17/current/BP-390774184-172.17.0.6-1606979969464] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:49,577 [DataXceiver for client  at /127.0.0.1:33392 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:33393:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:33392 dst: /127.0.0.1:33393
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,584 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18/current/BP-390774184-172.17.0.6-1606979969464] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:49,604 [Listener at localhost/32858] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:19:49,604 [Listener at localhost/32858] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:19:49,606 [Listener at localhost/32858] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:19:49,606 [Listener at localhost/32858] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:19:49,613 [Listener at localhost/32858] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:19:49,614 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 7
2020-12-03 07:19:49,614 [Listener at localhost/32858] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(341)) - DirectoryScanner: shutdown has been called, but periodic scanner not started
2020-12-03 07:19:49,614 [Listener at localhost/32858] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 43524
2020-12-03 07:19:49,615 [Listener at localhost/32858] WARN  util.MBeans (MBeans.java:unregister(145)) - Error unregistering Hadoop:service=DataNode,name=FSDatasetState-24857262-68d9-4b98-abb9-9105d4bfc034
javax.management.InstanceNotFoundException: Hadoop:service=DataNode,name=FSDatasetState-24857262-68d9-4b98-abb9-9105d4bfc034
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1095)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.exclusiveUnregisterMBean(DefaultMBeanServerInterceptor.java:427)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.unregisterMBean(DefaultMBeanServerInterceptor.java:415)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.unregisterMBean(JmxMBeanServer.java:546)
	at org.apache.hadoop.metrics2.util.MBeans.unregister(MBeans.java:143)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.shutdown(FsDatasetImpl.java:2293)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdown(DataNode.java:2151)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdownDataNode(MiniDFSCluster.java:2099)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdownDataNodes(MiniDFSCluster.java:2089)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2068)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2042)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2035)
	at org.apache.hadoop.hdfs.TestFileChecksum.tearDown(TestFileChecksum.java:111)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:168)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-12-03 07:19:49,616 [Listener at localhost/32858] WARN  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(191)) - AsyncDiskService has already shut down.
2020-12-03 07:19:49,616 [Listener at localhost/32858] WARN  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(175)) - AsyncLazyPersistService has already shut down.
2020-12-03 07:19:49,622 [Listener at localhost/32858] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:19:49,622 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 6
2020-12-03 07:19:49,623 [Listener at localhost/32858] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:19:49,623 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@11ee02f8] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:19:49,627 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13, DS-2ce634e7-1c83-4eb2-9adf-081181b40a1c) exiting.
2020-12-03 07:19:49,627 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14, DS-023f6a39-4389-43da-a1df-f7cccb6bcc41) exiting.
2020-12-03 07:19:49,656 [Listener at localhost/32858] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@3be8821f{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:19:49,657 [Listener at localhost/32858] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@64b31700{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:19:49,657 [Listener at localhost/32858] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@7e8e8651{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:19:49,658 [Listener at localhost/32858] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@77b325b3{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:19:49,666 [Listener at localhost/32858] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 34339
2020-12-03 07:19:49,667 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:19:49,667 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:19:49,668 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:19:49,668 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid 288350fe-5f15-4f60-866b-b391b14a3073) service to localhost/127.0.0.1:44779
2020-12-03 07:19:49,668 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid 288350fe-5f15-4f60-866b-b391b14a3073)
2020-12-03 07:19:49,668 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:49,674 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13/current/BP-390774184-172.17.0.6-1606979969464] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:49,674 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14/current/BP-390774184-172.17.0.6-1606979969464] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:49,686 [Listener at localhost/32858] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:19:49,687 [Listener at localhost/32858] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:19:49,690 [Listener at localhost/32858] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:19:49,690 [Listener at localhost/32858] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:19:49,697 [Listener at localhost/32858] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:19:49,698 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 5
2020-12-03 07:19:49,698 [Listener at localhost/32858] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:19:49,698 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@6f6a7463] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:19:49,703 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11, DS-d3e3d4e2-c7e5-44e4-9c40-1b0d2f6aaca7) exiting.
2020-12-03 07:19:49,703 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12, DS-265e643d-40cf-4be7-923e-86955d3df769) exiting.
2020-12-03 07:19:49,733 [Listener at localhost/32858] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@7dd712e8{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:19:49,734 [Listener at localhost/32858] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@2c282004{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:19:49,734 [Listener at localhost/32858] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@619bd14c{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:19:49,735 [Listener at localhost/32858] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@3a7704c{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:19:49,736 [Listener at localhost/32858] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 44302
2020-12-03 07:19:49,736 [DataXceiver for client  at /127.0.0.1:38598 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,736 [DataXceiver for client  at /127.0.0.1:38168 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,736 [DataXceiver for client  at /127.0.0.1:38474 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,736 [DataXceiver for client  at /127.0.0.1:38116 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,736 [DataXceiver for client  at /127.0.0.1:38298 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,736 [DataXceiver for client  at /127.0.0.1:38356 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,736 [DataXceiver for client  at /127.0.0.1:38562 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,736 [DataXceiver for client  at /127.0.0.1:38220 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,755 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:19:49,737 [DataXceiver for client  at /127.0.0.1:38598 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001 on DS-d3e3d4e2-c7e5-44e4-9c40-1b0d2f6aaca7, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,737 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:19:49,737 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:19:49,759 [DataXceiver for client  at /127.0.0.1:38598 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:35598, datanodeUuid=dc2fc2d4-8a32-4acd-8e94-f1a1b85207c7, infoPort=44165, infoSecurePort=0, ipcPort=44302, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001 to /127.0.0.1:38598
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,759 [DataXceiver for client  at /127.0.0.1:38220 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001 on DS-d3e3d4e2-c7e5-44e4-9c40-1b0d2f6aaca7, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,759 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid dc2fc2d4-8a32-4acd-8e94-f1a1b85207c7) service to localhost/127.0.0.1:44779
2020-12-03 07:19:49,769 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid dc2fc2d4-8a32-4acd-8e94-f1a1b85207c7)
2020-12-03 07:19:49,768 [DataXceiver for client  at /127.0.0.1:38598 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:35598:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:38598 dst: /127.0.0.1:35598
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,764 [DataXceiver for client  at /127.0.0.1:38562 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001 on DS-d3e3d4e2-c7e5-44e4-9c40-1b0d2f6aaca7, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,778 [DataXceiver for client  at /127.0.0.1:38220 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:35598, datanodeUuid=dc2fc2d4-8a32-4acd-8e94-f1a1b85207c7, infoPort=44165, infoSecurePort=0, ipcPort=44302, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001 to /127.0.0.1:38220
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,778 [DataXceiver for client  at /127.0.0.1:38562 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:35598, datanodeUuid=dc2fc2d4-8a32-4acd-8e94-f1a1b85207c7, infoPort=44165, infoSecurePort=0, ipcPort=44302, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001 to /127.0.0.1:38562
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,778 [DataXceiver for client  at /127.0.0.1:38356 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001 on DS-d3e3d4e2-c7e5-44e4-9c40-1b0d2f6aaca7, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,782 [DataXceiver for client  at /127.0.0.1:38562 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:35598:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:38562 dst: /127.0.0.1:35598
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,782 [DataXceiver for client  at /127.0.0.1:38220 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:35598:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:38220 dst: /127.0.0.1:35598
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,793 [DataXceiver for client  at /127.0.0.1:38356 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:35598, datanodeUuid=dc2fc2d4-8a32-4acd-8e94-f1a1b85207c7, infoPort=44165, infoSecurePort=0, ipcPort=44302, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001 to /127.0.0.1:38356
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,793 [DataXceiver for client  at /127.0.0.1:38298 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001 on DS-d3e3d4e2-c7e5-44e4-9c40-1b0d2f6aaca7, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,794 [DataXceiver for client  at /127.0.0.1:38356 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:35598:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:38356 dst: /127.0.0.1:35598
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,794 [DataXceiver for client  at /127.0.0.1:38298 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:35598, datanodeUuid=dc2fc2d4-8a32-4acd-8e94-f1a1b85207c7, infoPort=44165, infoSecurePort=0, ipcPort=44302, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001 to /127.0.0.1:38298
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,794 [DataXceiver for client  at /127.0.0.1:38116 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001 on DS-d3e3d4e2-c7e5-44e4-9c40-1b0d2f6aaca7, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,848 [DataXceiver for client  at /127.0.0.1:38298 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:35598:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:38298 dst: /127.0.0.1:35598
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,848 [DataXceiver for client  at /127.0.0.1:38474 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001 on DS-d3e3d4e2-c7e5-44e4-9c40-1b0d2f6aaca7, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,860 [DataXceiver for client  at /127.0.0.1:38168 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001 on DS-d3e3d4e2-c7e5-44e4-9c40-1b0d2f6aaca7, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,860 [DataXceiver for client  at /127.0.0.1:38474 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:35598, datanodeUuid=dc2fc2d4-8a32-4acd-8e94-f1a1b85207c7, infoPort=44165, infoSecurePort=0, ipcPort=44302, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001 to /127.0.0.1:38474
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,860 [DataXceiver for client  at /127.0.0.1:38116 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:35598, datanodeUuid=dc2fc2d4-8a32-4acd-8e94-f1a1b85207c7, infoPort=44165, infoSecurePort=0, ipcPort=44302, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001 to /127.0.0.1:38116
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,861 [DataXceiver for client  at /127.0.0.1:38474 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:35598:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:38474 dst: /127.0.0.1:35598
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,861 [DataXceiver for client  at /127.0.0.1:38168 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:35598, datanodeUuid=dc2fc2d4-8a32-4acd-8e94-f1a1b85207c7, infoPort=44165, infoSecurePort=0, ipcPort=44302, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001 to /127.0.0.1:38168
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,861 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:49,867 [DataXceiver for client  at /127.0.0.1:38116 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:35598:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:38116 dst: /127.0.0.1:35598
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,880 [DataXceiver for client  at /127.0.0.1:38168 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775787_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:35598:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:38168 dst: /127.0.0.1:35598
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,881 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11/current/BP-390774184-172.17.0.6-1606979969464] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:49,881 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12/current/BP-390774184-172.17.0.6-1606979969464] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:49,895 [Listener at localhost/32858] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:19:49,895 [Listener at localhost/32858] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:19:49,901 [Listener at localhost/32858] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:19:49,901 [Listener at localhost/32858] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:19:49,909 [Listener at localhost/32858] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:19:49,909 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 4
2020-12-03 07:19:49,910 [Listener at localhost/32858] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:19:49,910 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@50b8ae8d] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:19:49,915 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10, DS-5167c021-7d28-44ed-982a-8cb38c70870d) exiting.
2020-12-03 07:19:49,915 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9, DS-035cd092-cc9b-40a4-8e6a-3004ff2553ce) exiting.
2020-12-03 07:19:49,936 [Listener at localhost/32858] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@3c321bdb{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:19:49,937 [Listener at localhost/32858] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@24855019{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:19:49,938 [Listener at localhost/32858] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@62c5bbdc{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:19:49,938 [Listener at localhost/32858] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@3a7b503d{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:19:49,940 [Listener at localhost/32858] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 41056
2020-12-03 07:19:49,940 [DataXceiver for client  at /127.0.0.1:51566 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,940 [DataXceiver for client  at /127.0.0.1:51566 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001 on DS-035cd092-cc9b-40a4-8e6a-3004ff2553ce, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,941 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:19:49,940 [DataXceiver for client  at /127.0.0.1:51610 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,940 [DataXceiver for client  at /127.0.0.1:51234 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,940 [DataXceiver for client  at /127.0.0.1:51122 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,940 [DataXceiver for client  at /127.0.0.1:51448 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,940 [DataXceiver for client  at /127.0.0.1:51278 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,940 [DataXceiver for client  at /127.0.0.1:51350 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,940 [DataXceiver for client  at /127.0.0.1:51180 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,946 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:19:49,946 [DataXceiver for client  at /127.0.0.1:51610 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001 on DS-035cd092-cc9b-40a4-8e6a-3004ff2553ce, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,946 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:19:49,941 [DataXceiver for client  at /127.0.0.1:51566 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:40369, datanodeUuid=55b8e545-b17d-4893-81a8-bea205778f84, infoPort=34120, infoSecurePort=0, ipcPort=41056, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001 to /127.0.0.1:51566
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,949 [DataXceiver for client  at /127.0.0.1:51610 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:40369, datanodeUuid=55b8e545-b17d-4893-81a8-bea205778f84, infoPort=34120, infoSecurePort=0, ipcPort=41056, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001 to /127.0.0.1:51610
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,949 [DataXceiver for client  at /127.0.0.1:51180 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001 on DS-035cd092-cc9b-40a4-8e6a-3004ff2553ce, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,948 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid 55b8e545-b17d-4893-81a8-bea205778f84) service to localhost/127.0.0.1:44779
2020-12-03 07:19:49,965 [DataXceiver for client  at /127.0.0.1:51180 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:40369, datanodeUuid=55b8e545-b17d-4893-81a8-bea205778f84, infoPort=34120, infoSecurePort=0, ipcPort=41056, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001 to /127.0.0.1:51180
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,965 [DataXceiver for client  at /127.0.0.1:51350 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001 on DS-035cd092-cc9b-40a4-8e6a-3004ff2553ce, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,960 [DataXceiver for client  at /127.0.0.1:51610 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:40369:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:51610 dst: /127.0.0.1:40369
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,949 [DataXceiver for client  at /127.0.0.1:51566 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:40369:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:51566 dst: /127.0.0.1:40369
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,970 [DataXceiver for client  at /127.0.0.1:51350 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:40369, datanodeUuid=55b8e545-b17d-4893-81a8-bea205778f84, infoPort=34120, infoSecurePort=0, ipcPort=41056, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001 to /127.0.0.1:51350
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,991 [DataXceiver for client  at /127.0.0.1:51350 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:40369:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:51350 dst: /127.0.0.1:40369
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,966 [DataXceiver for client  at /127.0.0.1:51278 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001 on DS-035cd092-cc9b-40a4-8e6a-3004ff2553ce, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,966 [DataXceiver for client  at /127.0.0.1:51180 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:40369:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:51180 dst: /127.0.0.1:40369
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,965 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid 55b8e545-b17d-4893-81a8-bea205778f84)
2020-12-03 07:19:50,001 [DataXceiver for client  at /127.0.0.1:51278 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:40369, datanodeUuid=55b8e545-b17d-4893-81a8-bea205778f84, infoPort=34120, infoSecurePort=0, ipcPort=41056, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001 to /127.0.0.1:51278
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,001 [DataXceiver for client  at /127.0.0.1:51448 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001 on DS-035cd092-cc9b-40a4-8e6a-3004ff2553ce, because there is no volume scanner for that storageId.
2020-12-03 07:19:50,002 [DataXceiver for client  at /127.0.0.1:51122 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001 on DS-035cd092-cc9b-40a4-8e6a-3004ff2553ce, because there is no volume scanner for that storageId.
2020-12-03 07:19:50,007 [DataXceiver for client  at /127.0.0.1:51234 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001 on DS-035cd092-cc9b-40a4-8e6a-3004ff2553ce, because there is no volume scanner for that storageId.
2020-12-03 07:19:50,007 [DataXceiver for client  at /127.0.0.1:51278 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:40369:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:51278 dst: /127.0.0.1:40369
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,015 [DataXceiver for client  at /127.0.0.1:51234 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:40369, datanodeUuid=55b8e545-b17d-4893-81a8-bea205778f84, infoPort=34120, infoSecurePort=0, ipcPort=41056, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001 to /127.0.0.1:51234
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,015 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:50,015 [DataXceiver for client  at /127.0.0.1:51448 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:40369, datanodeUuid=55b8e545-b17d-4893-81a8-bea205778f84, infoPort=34120, infoSecurePort=0, ipcPort=41056, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001 to /127.0.0.1:51448
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,007 [DataXceiver for client  at /127.0.0.1:51122 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:40369, datanodeUuid=55b8e545-b17d-4893-81a8-bea205778f84, infoPort=34120, infoSecurePort=0, ipcPort=41056, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001 to /127.0.0.1:51122
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,030 [DataXceiver for client  at /127.0.0.1:51448 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:40369:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:51448 dst: /127.0.0.1:40369
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,024 [DataXceiver for client  at /127.0.0.1:51234 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:40369:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:51234 dst: /127.0.0.1:40369
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,060 [DataXceiver for client  at /127.0.0.1:51122 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775791_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:40369:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:51122 dst: /127.0.0.1:40369
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,060 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9/current/BP-390774184-172.17.0.6-1606979969464] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:50,061 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10/current/BP-390774184-172.17.0.6-1606979969464] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:50,087 [Listener at localhost/32858] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:19:50,087 [Listener at localhost/32858] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:19:50,091 [Listener at localhost/32858] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:19:50,091 [Listener at localhost/32858] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:19:50,098 [Listener at localhost/32858] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:19:50,098 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 3
2020-12-03 07:19:50,099 [Listener at localhost/32858] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:19:50,099 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@324dcd31] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:19:50,102 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7, DS-11ae3bf8-6838-4dfc-af89-3c76d111c85e) exiting.
2020-12-03 07:19:50,102 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8, DS-4d752d77-dc01-4782-88b7-fac90ddd2643) exiting.
2020-12-03 07:19:50,130 [Listener at localhost/32858] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@588ab592{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:19:50,130 [Listener at localhost/32858] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@c8b96ec{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:19:50,131 [Listener at localhost/32858] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@27f9e982{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:19:50,131 [Listener at localhost/32858] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@ecf9fb3{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:19:50,132 [Listener at localhost/32858] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 41306
2020-12-03 07:19:50,132 [DataXceiver for client  at /127.0.0.1:34908 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,132 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:19:50,132 [DataXceiver for client  at /127.0.0.1:34874 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,132 [DataXceiver for client  at /127.0.0.1:34608 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,132 [DataXceiver for client  at /127.0.0.1:34430 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,132 [DataXceiver for client  at /127.0.0.1:34534 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,132 [DataXceiver for client  at /127.0.0.1:34480 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,132 [DataXceiver for client  at /127.0.0.1:34666 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,132 [DataXceiver for client  at /127.0.0.1:34806 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,138 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:19:50,133 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:19:50,132 [DataXceiver for client  at /127.0.0.1:34908 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001 on DS-11ae3bf8-6838-4dfc-af89-3c76d111c85e, because there is no volume scanner for that storageId.
2020-12-03 07:19:50,140 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid 6e5d8d03-7267-400e-9598-80d9941dd61b) service to localhost/127.0.0.1:44779
2020-12-03 07:19:50,140 [DataXceiver for client  at /127.0.0.1:34908 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:43233, datanodeUuid=6e5d8d03-7267-400e-9598-80d9941dd61b, infoPort=41049, infoSecurePort=0, ipcPort=41306, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001 to /127.0.0.1:34908
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,141 [DataXceiver for client  at /127.0.0.1:34908 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:43233:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:34908 dst: /127.0.0.1:43233
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,141 [DataXceiver for client  at /127.0.0.1:34806 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001 on DS-11ae3bf8-6838-4dfc-af89-3c76d111c85e, because there is no volume scanner for that storageId.
2020-12-03 07:19:50,140 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid 6e5d8d03-7267-400e-9598-80d9941dd61b)
2020-12-03 07:19:50,141 [DataXceiver for client  at /127.0.0.1:34806 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:43233, datanodeUuid=6e5d8d03-7267-400e-9598-80d9941dd61b, infoPort=41049, infoSecurePort=0, ipcPort=41306, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001 to /127.0.0.1:34806
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,141 [DataXceiver for client  at /127.0.0.1:34666 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001 on DS-11ae3bf8-6838-4dfc-af89-3c76d111c85e, because there is no volume scanner for that storageId.
2020-12-03 07:19:50,142 [DataXceiver for client  at /127.0.0.1:34480 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001 on DS-11ae3bf8-6838-4dfc-af89-3c76d111c85e, because there is no volume scanner for that storageId.
2020-12-03 07:19:50,142 [DataXceiver for client  at /127.0.0.1:34666 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:43233, datanodeUuid=6e5d8d03-7267-400e-9598-80d9941dd61b, infoPort=41049, infoSecurePort=0, ipcPort=41306, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001 to /127.0.0.1:34666
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,142 [DataXceiver for client  at /127.0.0.1:34534 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001 on DS-11ae3bf8-6838-4dfc-af89-3c76d111c85e, because there is no volume scanner for that storageId.
2020-12-03 07:19:50,142 [DataXceiver for client  at /127.0.0.1:34480 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:43233, datanodeUuid=6e5d8d03-7267-400e-9598-80d9941dd61b, infoPort=41049, infoSecurePort=0, ipcPort=41306, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001 to /127.0.0.1:34480
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,142 [DataXceiver for client  at /127.0.0.1:34534 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:43233, datanodeUuid=6e5d8d03-7267-400e-9598-80d9941dd61b, infoPort=41049, infoSecurePort=0, ipcPort=41306, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001 to /127.0.0.1:34534
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,142 [DataXceiver for client  at /127.0.0.1:34430 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001 on DS-11ae3bf8-6838-4dfc-af89-3c76d111c85e, because there is no volume scanner for that storageId.
2020-12-03 07:19:50,143 [DataXceiver for client  at /127.0.0.1:34608 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001 on DS-11ae3bf8-6838-4dfc-af89-3c76d111c85e, because there is no volume scanner for that storageId.
2020-12-03 07:19:50,143 [DataXceiver for client  at /127.0.0.1:34430 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:43233, datanodeUuid=6e5d8d03-7267-400e-9598-80d9941dd61b, infoPort=41049, infoSecurePort=0, ipcPort=41306, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001 to /127.0.0.1:34430
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,143 [DataXceiver for client  at /127.0.0.1:34874 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001 on DS-11ae3bf8-6838-4dfc-af89-3c76d111c85e, because there is no volume scanner for that storageId.
2020-12-03 07:19:50,143 [DataXceiver for client  at /127.0.0.1:34608 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:43233, datanodeUuid=6e5d8d03-7267-400e-9598-80d9941dd61b, infoPort=41049, infoSecurePort=0, ipcPort=41306, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001 to /127.0.0.1:34608
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,143 [DataXceiver for client  at /127.0.0.1:34874 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:43233, datanodeUuid=6e5d8d03-7267-400e-9598-80d9941dd61b, infoPort=41049, infoSecurePort=0, ipcPort=41306, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001 to /127.0.0.1:34874
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,143 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:50,154 [DataXceiver for client  at /127.0.0.1:34480 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:43233:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:34480 dst: /127.0.0.1:43233
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,154 [DataXceiver for client  at /127.0.0.1:34666 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:43233:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:34666 dst: /127.0.0.1:43233
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,154 [DataXceiver for client  at /127.0.0.1:34806 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:43233:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:34806 dst: /127.0.0.1:43233
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,154 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7/current/BP-390774184-172.17.0.6-1606979969464] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:50,155 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/current/BP-390774184-172.17.0.6-1606979969464] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:50,155 [DataXceiver for client  at /127.0.0.1:34534 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:43233:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:34534 dst: /127.0.0.1:43233
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,155 [DataXceiver for client  at /127.0.0.1:34874 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:43233:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:34874 dst: /127.0.0.1:43233
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,155 [DataXceiver for client  at /127.0.0.1:34608 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:43233:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:34608 dst: /127.0.0.1:43233
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,155 [DataXceiver for client  at /127.0.0.1:34430 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775786_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:43233:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:34430 dst: /127.0.0.1:43233
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,189 [Listener at localhost/32858] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:19:50,215 [Listener at localhost/32858] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:19:50,220 [Listener at localhost/32858] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:19:50,230 [Listener at localhost/32858] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:19:50,251 [Listener at localhost/32858] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:19:50,251 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 2
2020-12-03 07:19:50,252 [Listener at localhost/32858] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:19:50,252 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@473b3b7a] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:19:50,259 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5, DS-34e32a14-5782-4fe9-b945-76fbaab513a9) exiting.
2020-12-03 07:19:50,259 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6, DS-462ced07-43f8-4720-afda-b3153be81947) exiting.
2020-12-03 07:19:50,284 [Listener at localhost/32858] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@54ec8cc9{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:19:50,285 [Listener at localhost/32858] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@52eacb4b{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:19:50,286 [Listener at localhost/32858] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@425357dd{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:19:50,286 [Listener at localhost/32858] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@41382722{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:19:50,287 [Listener at localhost/32858] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 43239
2020-12-03 07:19:50,288 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:19:50,288 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:19:50,295 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:19:50,295 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid 22bd5bbb-5aec-4015-af14-16567f170d84) service to localhost/127.0.0.1:44779
2020-12-03 07:19:50,296 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid 22bd5bbb-5aec-4015-af14-16567f170d84)
2020-12-03 07:19:50,296 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:50,296 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-390774184-172.17.0.6-1606979969464] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:50,297 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/current/BP-390774184-172.17.0.6-1606979969464] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:50,331 [Listener at localhost/32858] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:19:50,333 [Listener at localhost/32858] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:19:50,336 [Listener at localhost/32858] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:19:50,336 [Listener at localhost/32858] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:19:50,359 [Listener at localhost/32858] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:19:50,359 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 1
2020-12-03 07:19:50,360 [Listener at localhost/32858] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:19:50,360 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@2449cff7] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:19:50,365 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, DS-72b6d766-f12d-46df-9b78-3689fbb0ff3f) exiting.
2020-12-03 07:19:50,365 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3, DS-c85716ef-a148-4295-b2b7-bca92bf0548a) exiting.
2020-12-03 07:19:50,437 [Listener at localhost/32858] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@615f972{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:19:50,437 [Listener at localhost/32858] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@285f09de{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:19:50,438 [Listener at localhost/32858] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@408b35bf{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:19:50,438 [Listener at localhost/32858] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@71e9a896{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:19:50,440 [Listener at localhost/32858] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 43158
2020-12-03 07:19:50,440 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:19:50,446 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:19:50,447 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:19:50,447 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid c39c5c86-f8fb-43b7-9a7e-df2bf8e2ff45) service to localhost/127.0.0.1:44779
2020-12-03 07:19:50,447 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid c39c5c86-f8fb-43b7-9a7e-df2bf8e2ff45)
2020-12-03 07:19:50,447 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:50,448 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-390774184-172.17.0.6-1606979969464] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:50,448 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-390774184-172.17.0.6-1606979969464] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:50,454 [Listener at localhost/32858] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:19:50,455 [Listener at localhost/32858] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:19:50,460 [Listener at localhost/32858] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:19:50,460 [Listener at localhost/32858] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:19:50,461 [Listener at localhost/32858] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:19:50,461 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 0
2020-12-03 07:19:50,461 [Listener at localhost/32858] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:19:50,461 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@4ae33a11] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:19:50,466 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-3bec7c54-7709-4f3d-b486-35765741195a) exiting.
2020-12-03 07:19:50,466 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-c396f456-72b2-4eec-a6af-559f05d184b3) exiting.
2020-12-03 07:19:50,502 [Listener at localhost/32858] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@7fcbe147{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:19:50,503 [Listener at localhost/32858] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@235f4c10{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:19:50,504 [Listener at localhost/32858] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@274872f8{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:19:50,504 [Listener at localhost/32858] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@569bf9eb{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:19:50,505 [Listener at localhost/32858] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 40094
2020-12-03 07:19:50,505 [DataXceiver for client  at /127.0.0.1:42678 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,505 [DataXceiver for client  at /127.0.0.1:42402 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,505 [DataXceiver for client  at /127.0.0.1:42832 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,505 [DataXceiver for client  at /127.0.0.1:42788 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,505 [DataXceiver for client  at /127.0.0.1:42458 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,505 [DataXceiver for client  at /127.0.0.1:42500 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,505 [DataXceiver for client  at /127.0.0.1:42348 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,518 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:19:50,517 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:19:50,512 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:19:50,506 [DataXceiver for client  at /127.0.0.1:42678 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001 on DS-3bec7c54-7709-4f3d-b486-35765741195a, because there is no volume scanner for that storageId.
2020-12-03 07:19:50,629 [DataXceiver for client  at /127.0.0.1:42348 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001 on DS-3bec7c54-7709-4f3d-b486-35765741195a, because there is no volume scanner for that storageId.
2020-12-03 07:19:50,629 [DataXceiver for client  at /127.0.0.1:42678 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:40990, datanodeUuid=7a7e8fbe-c521-4997-b64d-db194b2bc262, infoPort=36857, infoSecurePort=0, ipcPort=40094, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001 to /127.0.0.1:42678
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,629 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid 7a7e8fbe-c521-4997-b64d-db194b2bc262) service to localhost/127.0.0.1:44779
2020-12-03 07:19:50,629 [DataXceiver for client  at /127.0.0.1:42348 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:40990, datanodeUuid=7a7e8fbe-c521-4997-b64d-db194b2bc262, infoPort=36857, infoSecurePort=0, ipcPort=40094, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001 to /127.0.0.1:42348
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,629 [DataXceiver for client  at /127.0.0.1:42500 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001 on DS-3bec7c54-7709-4f3d-b486-35765741195a, because there is no volume scanner for that storageId.
2020-12-03 07:19:50,630 [DataXceiver for client  at /127.0.0.1:42458 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001 on DS-3bec7c54-7709-4f3d-b486-35765741195a, because there is no volume scanner for that storageId.
2020-12-03 07:19:50,630 [DataXceiver for client  at /127.0.0.1:42500 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:40990, datanodeUuid=7a7e8fbe-c521-4997-b64d-db194b2bc262, infoPort=36857, infoSecurePort=0, ipcPort=40094, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001 to /127.0.0.1:42500
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,630 [DataXceiver for client  at /127.0.0.1:42458 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:40990, datanodeUuid=7a7e8fbe-c521-4997-b64d-db194b2bc262, infoPort=36857, infoSecurePort=0, ipcPort=40094, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001 to /127.0.0.1:42458
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,630 [DataXceiver for client  at /127.0.0.1:42788 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001 on DS-3bec7c54-7709-4f3d-b486-35765741195a, because there is no volume scanner for that storageId.
2020-12-03 07:19:50,631 [DataXceiver for client  at /127.0.0.1:42788 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:40990, datanodeUuid=7a7e8fbe-c521-4997-b64d-db194b2bc262, infoPort=36857, infoSecurePort=0, ipcPort=40094, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001 to /127.0.0.1:42788
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,684 [DataXceiver for client  at /127.0.0.1:42832 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001 on DS-3bec7c54-7709-4f3d-b486-35765741195a, because there is no volume scanner for that storageId.
2020-12-03 07:19:50,684 [DataXceiver for client  at /127.0.0.1:42788 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:40990:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:42788 dst: /127.0.0.1:40990
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,685 [DataXceiver for client  at /127.0.0.1:42832 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:40990, datanodeUuid=7a7e8fbe-c521-4997-b64d-db194b2bc262, infoPort=36857, infoSecurePort=0, ipcPort=40094, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001 to /127.0.0.1:42832
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,685 [DataXceiver for client  at /127.0.0.1:42402 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001 on DS-3bec7c54-7709-4f3d-b486-35765741195a, because there is no volume scanner for that storageId.
2020-12-03 07:19:50,685 [DataXceiver for client  at /127.0.0.1:42402 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:40990, datanodeUuid=7a7e8fbe-c521-4997-b64d-db194b2bc262, infoPort=36857, infoSecurePort=0, ipcPort=40094, storageInfo=lv=-57;cid=testClusterID;nsid=1133803897;c=1606979969464):Got exception while serving BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001 to /127.0.0.1:42402
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,685 [DataXceiver for client  at /127.0.0.1:42832 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:40990:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:42832 dst: /127.0.0.1:40990
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,686 [DataXceiver for client  at /127.0.0.1:42678 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:40990:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:42678 dst: /127.0.0.1:40990
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,686 [DataXceiver for client  at /127.0.0.1:42402 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:40990:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:42402 dst: /127.0.0.1:40990
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,687 [DataXceiver for client  at /127.0.0.1:42348 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:40990:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:42348 dst: /127.0.0.1:40990
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,688 [DataXceiver for client  at /127.0.0.1:42500 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:40990:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:42500 dst: /127.0.0.1:40990
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,688 [DataXceiver for client  at /127.0.0.1:42458 [Sending block BP-390774184-172.17.0.6-1606979969464:blk_-9223372036854775789_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:40990:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:42458 dst: /127.0.0.1:40990
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,769 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-390774184-172.17.0.6-1606979969464 (Datanode Uuid 7a7e8fbe-c521-4997-b64d-db194b2bc262)
2020-12-03 07:19:50,769 [BP-390774184-172.17.0.6-1606979969464 heartbeating to localhost/127.0.0.1:44779] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-390774184-172.17.0.6-1606979969464
2020-12-03 07:19:50,770 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-390774184-172.17.0.6-1606979969464] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:50,771 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-390774184-172.17.0.6-1606979969464] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:50,776 [Listener at localhost/32858] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:19:50,776 [Listener at localhost/32858] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:19:50,779 [Listener at localhost/32858] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:19:50,779 [Listener at localhost/32858] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:19:50,780 [Listener at localhost/32858] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2020-12-03 07:19:50,781 [Listener at localhost/32858] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2020-12-03 07:19:50,782 [Listener at localhost/32858] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - DataNode metrics system shutdown complete.
2020-12-03 07:19:50,782 [Listener at localhost/32858] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:19:50,783 [Listener at localhost/32858] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:stopAndJoinNameNode(2130)) - Shutting down the namenode
2020-12-03 07:19:50,783 [Listener at localhost/32858] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1334)) - Stopping services started for active state
2020-12-03 07:19:50,783 [Listener at localhost/32858] INFO  namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1410)) - Ending log segment 1, 36
2020-12-03 07:19:50,783 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@150ab4ed] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4198)) - LazyPersistFileScrubber was interrupted, exiting
2020-12-03 07:19:50,784 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@a8c1f44] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4107)) - NameNodeEditLogRoller was interrupted, exiting
2020-12-03 07:19:50,784 [Listener at localhost/32858] INFO  namenode.FSEditLog (FSEditLog.java:printStatistics(778)) - Number of transactions: 37 Total time for transactions(ms): 31 Number of transactions batched in Syncs: 10 Number of syncs: 28 SyncTimes(ms): 6 2 
2020-12-03 07:19:51,713 [Listener at localhost/32858] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(145)) - Finalizing edits file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_inprogress_0000000000000000001 -> /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000001-0000000000000000037
2020-12-03 07:19:51,714 [Listener at localhost/32858] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(145)) - Finalizing edits file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_inprogress_0000000000000000001 -> /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000001-0000000000000000037
2020-12-03 07:19:51,716 [FSEditLogAsync] INFO  namenode.FSEditLog (FSEditLogAsync.java:run(253)) - FSEditLogAsync was interrupted, exiting
2020-12-03 07:19:51,716 [CacheReplicationMonitor(919427203)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2020-12-03 07:19:51,717 [Listener at localhost/32858] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 44779
2020-12-03 07:19:51,718 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:19:51,718 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:19:51,732 [RedundancyMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4687)) - Stopping RedundancyMonitor.
2020-12-03 07:19:51,732 [StorageInfoMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4722)) - Stopping thread.
2020-12-03 07:19:51,778 [Listener at localhost/32858] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1334)) - Stopping services started for active state
2020-12-03 07:19:51,779 [Listener at localhost/32858] INFO  namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1434)) - Stopping services started for standby state
2020-12-03 07:19:51,800 [Listener at localhost/32858] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@62833051{/,null,UNAVAILABLE}{/hdfs}
2020-12-03 07:19:51,802 [Listener at localhost/32858] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@7fc4780b{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:19:51,802 [Listener at localhost/32858] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@24c1b2d2{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:19:51,802 [Listener at localhost/32858] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@1b66c0fb{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
msx-rc 1
