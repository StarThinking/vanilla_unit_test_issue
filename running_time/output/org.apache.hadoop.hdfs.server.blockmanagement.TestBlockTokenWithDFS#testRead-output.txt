2020-12-03 07:21:53,429 [main] INFO  net.ServerSocketUtil (ServerSocketUtil.java:getPort(53)) - Using port 18020
2020-12-03 07:21:53,436 [main] INFO  net.ServerSocketUtil (ServerSocketUtil.java:getPort(53)) - Using port 19870
2020-12-03 07:21:53,450 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(493)) - starting cluster: numNameNodes=1, numDataNodes=2
Formatting using clusterid: testClusterID
2020-12-03 07:21:54,187 [main] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(229)) - Edit logging is async:true
2020-12-03 07:21:54,201 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(755)) - KeyProvider: null
2020-12-03 07:21:54,203 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(123)) - fsLock is fair: true
2020-12-03 07:21:54,203 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(141)) - Detailed lock hold time metrics enabled: false
2020-12-03 07:21:54,228 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(780)) - fsOwner             = root (auth:SIMPLE)
2020-12-03 07:21:54,229 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(781)) - supergroup          = supergroup
2020-12-03 07:21:54,229 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(782)) - isPermissionEnabled = true
2020-12-03 07:21:54,230 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(793)) - HA Enabled: false
2020-12-03 07:21:54,293 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:21:54,301 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - hadoop.configured.node.mapping is deprecated. Instead, use net.topology.configured.node.mapping
2020-12-03 07:21:54,301 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:21:54,302 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(303)) - dfs.block.invalidate.limit: configured=1000, counted=20, effected=1000
2020-12-03 07:21:54,302 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(311)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-12-03 07:21:54,304 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:21:54,311 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-12-03 07:21:54,311 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Dec 03 07:21:54
2020-12-03 07:21:54,314 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-12-03 07:21:54,314 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:21:54,316 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.9 GB = 39.3 MB
2020-12-03 07:21:54,316 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-12-03 07:21:54,336 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:21:54,337 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:21:54,341 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createSPSManager(5183)) - Storage policy satisfier is disabled
2020-12-03 07:21:54,341 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(595)) - dfs.block.access.token.enable = true
2020-12-03 07:21:54,341 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(617)) - dfs.block.access.key.update.interval=600 min(s), dfs.block.access.token.lifetime=600 min(s), dfs.encrypt.data.transfer.algorithm=null
2020-12-03 07:21:54,362 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:21:54,367 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-12-03 07:21:54,367 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-12-03 07:21:54,367 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-12-03 07:21:54,368 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-12-03 07:21:54,369 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(581)) - defaultReplication         = 2
2020-12-03 07:21:54,369 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(582)) - maxReplication             = 512
2020-12-03 07:21:54,369 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(583)) - minReplication             = 1
2020-12-03 07:21:54,369 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(584)) - maxReplicationStreams      = 2
2020-12-03 07:21:54,370 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(585)) - redundancyRecheckInterval  = 3000ms
2020-12-03 07:21:54,370 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(586)) - encryptDataTransfer        = false
2020-12-03 07:21:54,370 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(587)) - maxNumBlocksToLog          = 1000
2020-12-03 07:21:54,404 [main] INFO  namenode.FSDirectory (SerialNumberManager.java:<clinit>(51)) - GLOBAL serial map: bits=29 maxEntries=536870911
2020-12-03 07:21:54,405 [main] INFO  namenode.FSDirectory (SerialNumberManager.java:<clinit>(51)) - USER serial map: bits=24 maxEntries=16777215
2020-12-03 07:21:54,405 [main] INFO  namenode.FSDirectory (SerialNumberManager.java:<clinit>(51)) - GROUP serial map: bits=24 maxEntries=16777215
2020-12-03 07:21:54,406 [main] INFO  namenode.FSDirectory (SerialNumberManager.java:<clinit>(51)) - XATTR serial map: bits=24 maxEntries=16777215
2020-12-03 07:21:54,421 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-12-03 07:21:54,422 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:21:54,423 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.9 GB = 19.6 MB
2020-12-03 07:21:54,423 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-12-03 07:21:54,431 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(290)) - ACLs enabled? false
2020-12-03 07:21:54,431 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(294)) - POSIX ACL inheritance enabled? true
2020-12-03 07:21:54,432 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(298)) - XAttrs enabled? true
2020-12-03 07:21:54,432 [main] INFO  namenode.NameNode (FSDirectory.java:<init>(362)) - Caching file names occurring more than 10 times
2020-12-03 07:21:54,439 [main] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-12-03 07:21:54,442 [main] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-12-03 07:21:54,447 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-12-03 07:21:54,448 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:21:54,448 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.9 GB = 4.9 MB
2020-12-03 07:21:54,448 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-12-03 07:21:54,459 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-12-03 07:21:54,459 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-12-03 07:21:54,459 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-12-03 07:21:54,464 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(996)) - Retry cache on namenode is enabled
2020-12-03 07:21:54,464 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(1004)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-12-03 07:21:54,468 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-12-03 07:21:54,469 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:21:54,469 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.9 GB = 603.0 KB
2020-12-03 07:21:54,470 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-12-03 07:21:54,504 [main] INFO  namenode.FSImage (FSImage.java:format(185)) - Allocated new BlockPoolId: BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:21:54,663 [main] INFO  common.Storage (NNStorage.java:format(595)) - Storage directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1 has been successfully formatted.
2020-12-03 07:21:54,841 [main] INFO  common.Storage (NNStorage.java:format(595)) - Storage directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2 has been successfully formatted.
2020-12-03 07:21:54,874 [FSImageSaver for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(512)) - Saving image file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/fsimage.ckpt_0000000000000000000 using no compression
2020-12-03 07:21:54,874 [FSImageSaver for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(512)) - Saving image file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage.ckpt_0000000000000000000 using no compression
2020-12-03 07:21:55,001 [FSImageSaver for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(516)) - Image file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .
2020-12-03 07:21:55,001 [FSImageSaver for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(516)) - Image file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .
2020-12-03 07:21:55,075 [main] INFO  namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:getImageTxIdToRetain(203)) - Going to retain 1 images with txid >= 0
2020-12-03 07:21:55,079 [main] INFO  namenode.NameNode (NameNode.java:createNameNode(1632)) - createNameNode []
2020-12-03 07:21:55,184 [main] INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(118)) - Loaded properties from hadoop-metrics2.properties
2020-12-03 07:21:55,392 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 0 second(s).
2020-12-03 07:21:55,393 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2020-12-03 07:21:55,427 [main] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://127.0.0.1:18020
2020-12-03 07:21:55,427 [main] INFO  namenode.NameNode (NameNode.java:<init>(944)) - Clients should use 127.0.0.1:18020 to access this namenode/service.
2020-12-03 07:21:55,476 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@51e69659] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:21:55,497 [main] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1641)) - Starting Web-server for hdfs at: http://localhost:19870
2020-12-03 07:21:55,502 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:21:55,518 [main] INFO  util.log (Log.java:initialized(192)) - Logging initialized @2947ms
2020-12-03 07:21:55,640 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:21:55,644 [main] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-12-03 07:21:55,644 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:21:55,652 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:21:55,654 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-12-03 07:21:55,655 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:21:55,655 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:21:55,683 [main] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(102)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-12-03 07:21:55,683 [main] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(806)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-12-03 07:21:55,693 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 19870
2020-12-03 07:21:55,695 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:21:55,738 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@5efa40fe{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:21:55,739 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@1c9b0314{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:21:55,778 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@28b46423{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-12-03 07:21:55,787 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@4a3631f8{HTTP/1.1,[http/1.1]}{localhost:19870}
2020-12-03 07:21:55,787 [main] INFO  server.Server (Server.java:doStart(419)) - Started @3216ms
2020-12-03 07:21:55,798 [main] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(229)) - Edit logging is async:true
2020-12-03 07:21:55,798 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(755)) - KeyProvider: null
2020-12-03 07:21:55,799 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(123)) - fsLock is fair: true
2020-12-03 07:21:55,799 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(141)) - Detailed lock hold time metrics enabled: false
2020-12-03 07:21:55,799 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(780)) - fsOwner             = root (auth:SIMPLE)
2020-12-03 07:21:55,799 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(781)) - supergroup          = supergroup
2020-12-03 07:21:55,800 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(782)) - isPermissionEnabled = true
2020-12-03 07:21:55,800 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(793)) - HA Enabled: false
2020-12-03 07:21:55,801 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:21:55,801 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:21:55,801 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(303)) - dfs.block.invalidate.limit: configured=1000, counted=20, effected=1000
2020-12-03 07:21:55,802 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(311)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-12-03 07:21:55,802 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:21:55,802 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-12-03 07:21:55,803 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Dec 03 07:21:55
2020-12-03 07:21:55,803 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-12-03 07:21:55,803 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:21:55,803 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.8 GB = 36.4 MB
2020-12-03 07:21:55,804 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-12-03 07:21:55,810 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:21:55,810 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:21:55,811 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createSPSManager(5183)) - Storage policy satisfier is disabled
2020-12-03 07:21:55,811 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(595)) - dfs.block.access.token.enable = true
2020-12-03 07:21:55,811 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(617)) - dfs.block.access.key.update.interval=600 min(s), dfs.block.access.token.lifetime=600 min(s), dfs.encrypt.data.transfer.algorithm=null
2020-12-03 07:21:55,812 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:21:55,813 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-12-03 07:21:55,813 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-12-03 07:21:55,813 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-12-03 07:21:55,813 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-12-03 07:21:55,814 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(581)) - defaultReplication         = 2
2020-12-03 07:21:55,814 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(582)) - maxReplication             = 512
2020-12-03 07:21:55,814 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(583)) - minReplication             = 1
2020-12-03 07:21:55,814 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(584)) - maxReplicationStreams      = 2
2020-12-03 07:21:55,815 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(585)) - redundancyRecheckInterval  = 3000ms
2020-12-03 07:21:55,815 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(586)) - encryptDataTransfer        = false
2020-12-03 07:21:55,815 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(587)) - maxNumBlocksToLog          = 1000
2020-12-03 07:21:55,816 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-12-03 07:21:55,816 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:21:55,816 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.8 GB = 18.2 MB
2020-12-03 07:21:55,817 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-12-03 07:21:55,818 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(290)) - ACLs enabled? false
2020-12-03 07:21:55,819 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(294)) - POSIX ACL inheritance enabled? true
2020-12-03 07:21:55,819 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(298)) - XAttrs enabled? true
2020-12-03 07:21:55,819 [main] INFO  namenode.NameNode (FSDirectory.java:<init>(362)) - Caching file names occurring more than 10 times
2020-12-03 07:21:55,819 [main] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-12-03 07:21:55,820 [main] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-12-03 07:21:55,820 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-12-03 07:21:55,820 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:21:55,821 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.8 GB = 4.6 MB
2020-12-03 07:21:55,821 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-12-03 07:21:55,822 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-12-03 07:21:55,822 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-12-03 07:21:55,823 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-12-03 07:21:55,823 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(996)) - Retry cache on namenode is enabled
2020-12-03 07:21:55,823 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(1004)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-12-03 07:21:55,823 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-12-03 07:21:55,824 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:21:55,824 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.8 GB = 559.3 KB
2020-12-03 07:21:55,824 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-12-03 07:21:55,867 [main] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/in_use.lock acquired by nodename 5792@13788a696671
2020-12-03 07:21:55,901 [main] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/in_use.lock acquired by nodename 5792@13788a696671
2020-12-03 07:21:55,904 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(428)) - Recovering unfinalized segments in /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current
2020-12-03 07:21:55,905 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(428)) - Recovering unfinalized segments in /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current
2020-12-03 07:21:55,906 [main] INFO  namenode.FSImage (FSImage.java:loadFSImage(733)) - No edit log streams selected.
2020-12-03 07:21:55,906 [main] INFO  namenode.FSImage (FSImage.java:loadFSImageFile(797)) - Planning to load image: FSImageFile(file=/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2020-12-03 07:21:55,938 [main] INFO  namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(234)) - Loading 1 INodes.
2020-12-03 07:21:55,947 [main] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(246)) - Loaded FSImage in 0 seconds.
2020-12-03 07:21:55,948 [main] INFO  namenode.FSImage (FSImage.java:loadFSImage(978)) - Loaded image for txid 0 from /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000000
2020-12-03 07:21:55,953 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1110)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-12-03 07:21:55,953 [main] INFO  namenode.FSEditLog (FSEditLog.java:startLogSegment(1365)) - Starting log segment at 1
2020-12-03 07:21:56,035 [main] INFO  namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2020-12-03 07:21:56,036 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(727)) - Finished loading FSImage in 209 msecs
2020-12-03 07:21:56,250 [main] INFO  namenode.NameNode (NameNodeRpcServer.java:<init>(448)) - RPC server is binding to localhost:18020
2020-12-03 07:21:56,303 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:21:56,318 [Socket Reader #1 for port 18020] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 18020
2020-12-03 07:21:56,589 [Listener at localhost/18020] INFO  namenode.FSNamesystem (FSNamesystem.java:registerMBean(5090)) - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2020-12-03 07:21:56,603 [Listener at localhost/18020] INFO  namenode.LeaseManager (LeaseManager.java:getNumUnderConstructionBlocks(171)) - Number of blocks under construction: 0
2020-12-03 07:21:56,612 [org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager$Monitor@404bbcbd] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:updateKeys(263)) - Updating block keys
2020-12-03 07:21:56,615 [Listener at localhost/18020] INFO  blockmanagement.BlockManager (BlockManager.java:initializeReplQueues(4922)) - initializing replication queues
2020-12-03 07:21:56,616 [Listener at localhost/18020] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(400)) - STATE* Leaving safe mode after 0 secs
2020-12-03 07:21:56,616 [Listener at localhost/18020] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(406)) - STATE* Network topology has 0 racks and 0 datanodes
2020-12-03 07:21:56,616 [Listener at localhost/18020] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(408)) - STATE* UnderReplicatedBlocks has 0 blocks
2020-12-03 07:21:56,619 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3585)) - Total number of blocks            = 0
2020-12-03 07:21:56,619 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3586)) - Number of invalid blocks          = 0
2020-12-03 07:21:56,620 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3587)) - Number of under-replicated blocks = 0
2020-12-03 07:21:56,620 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3588)) - Number of  over-replicated blocks = 0
2020-12-03 07:21:56,620 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3590)) - Number of blocks being written    = 0
2020-12-03 07:21:56,620 [Reconstruction Queue Initializer] INFO  hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(3593)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 4 msec
2020-12-03 07:21:56,645 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:21:56,646 [IPC Server listener on 18020] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 18020: starting
2020-12-03 07:21:56,648 [Listener at localhost/18020] INFO  namenode.NameNode (NameNode.java:startCommonServices(828)) - NameNode RPC up at: localhost/127.0.0.1:18020
2020-12-03 07:21:56,651 [Listener at localhost/18020] INFO  namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1222)) - Starting services required for active state
2020-12-03 07:21:56,652 [Listener at localhost/18020] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(777)) - Initializing quota with 4 thread(s)
2020-12-03 07:21:56,659 [Listener at localhost/18020] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(786)) - Quota initialization completed in 7 milliseconds
name space=1
storage space=0
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2020-12-03 07:21:56,663 [CacheReplicationMonitor(1368553405)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-12-03 07:21:56,671 [Listener at localhost/18020] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1,[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:21:56,733 [Listener at localhost/18020] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:21:56,748 [Listener at localhost/18020] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:21:56,766 [Listener at localhost/18020] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:21:56,771 [Listener at localhost/18020] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:21:56,773 [Listener at localhost/18020] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:21:56,777 [Listener at localhost/18020] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:21:56,778 [Listener at localhost/18020] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:21:56,778 [Listener at localhost/18020] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:21:56,779 [Listener at localhost/18020] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:21:56,782 [Listener at localhost/18020] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:21:56,788 [Listener at localhost/18020] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:33955
2020-12-03 07:21:56,790 [Listener at localhost/18020] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:21:56,791 [Listener at localhost/18020] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:21:56,808 [Listener at localhost/18020] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:21:56,810 [Listener at localhost/18020] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:21:56,811 [Listener at localhost/18020] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:21:56,812 [Listener at localhost/18020] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:21:56,814 [Listener at localhost/18020] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:21:56,815 [Listener at localhost/18020] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:21:56,815 [Listener at localhost/18020] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:21:56,815 [Listener at localhost/18020] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:21:56,818 [Listener at localhost/18020] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 37861
2020-12-03 07:21:56,819 [Listener at localhost/18020] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:21:56,820 [Listener at localhost/18020] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@410ae9a3{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:21:56,821 [Listener at localhost/18020] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@d5ae57e{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:21:56,828 [Listener at localhost/18020] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@fd0e5b6{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:21:56,829 [Listener at localhost/18020] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@4eed46ee{HTTP/1.1,[http/1.1]}{localhost:37861}
2020-12-03 07:21:56,829 [Listener at localhost/18020] INFO  server.Server (Server.java:doStart(419)) - Started @4258ms
2020-12-03 07:21:57,211 [Listener at localhost/18020] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:34498
2020-12-03 07:21:57,211 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@2d72f75e] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:21:57,212 [Listener at localhost/18020] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:21:57,212 [Listener at localhost/18020] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:21:57,229 [Listener at localhost/18020] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:21:57,230 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:21:57,236 [Listener at localhost/46144] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:46144
2020-12-03 07:21:57,251 [Listener at localhost/46144] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:21:57,252 [Listener at localhost/46144] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:21:57,263 [Thread-59] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:18020 starting to offer service
2020-12-03 07:21:57,269 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:21:57,269 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:21:57,273 [Listener at localhost/46144] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 1 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3,[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:21:57,275 [Listener at localhost/46144] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
2020-12-03 07:21:57,276 [Listener at localhost/46144] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:21:57,278 [Listener at localhost/46144] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:21:57,278 [Listener at localhost/46144] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:21:57,278 [Listener at localhost/46144] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:21:57,279 [Listener at localhost/46144] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:21:57,279 [Listener at localhost/46144] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:21:57,279 [Listener at localhost/46144] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:21:57,279 [Listener at localhost/46144] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:21:57,280 [Listener at localhost/46144] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:21:57,280 [Listener at localhost/46144] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:32825
2020-12-03 07:21:57,281 [Listener at localhost/46144] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:21:57,281 [Listener at localhost/46144] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:21:57,282 [Listener at localhost/46144] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:21:57,284 [Listener at localhost/46144] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:21:57,284 [Listener at localhost/46144] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:21:57,285 [Listener at localhost/46144] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:21:57,288 [Listener at localhost/46144] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:21:57,289 [Listener at localhost/46144] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:21:57,289 [Listener at localhost/46144] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:21:57,289 [Listener at localhost/46144] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:21:57,291 [Listener at localhost/46144] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 36806
2020-12-03 07:21:57,291 [Listener at localhost/46144] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:21:57,293 [Listener at localhost/46144] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@14bb2297{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:21:57,295 [Listener at localhost/46144] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@797501a{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:21:57,302 [Listener at localhost/46144] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@200606de{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:21:57,303 [Listener at localhost/46144] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@750fe12e{HTTP/1.1,[http/1.1]}{localhost:36806}
2020-12-03 07:21:57,304 [Listener at localhost/46144] INFO  server.Server (Server.java:doStart(419)) - Started @4733ms
2020-12-03 07:21:57,383 [Listener at localhost/46144] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:41308
2020-12-03 07:21:57,384 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@3e587920] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:21:57,384 [Listener at localhost/46144] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:21:57,384 [Listener at localhost/46144] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:21:57,385 [Listener at localhost/46144] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:21:57,386 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:21:57,392 [Listener at localhost/43105] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:43105
2020-12-03 07:21:57,398 [Listener at localhost/43105] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:21:57,398 [Listener at localhost/43105] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:21:57,399 [Thread-83] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:18020 starting to offer service
2020-12-03 07:21:57,400 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:21:57,401 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:21:57,408 [Listener at localhost/43105] DEBUG hdfs.DFSClient (DFSClient.java:<init>(318)) - Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
2020-12-03 07:21:57,543 [Thread-83] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:18020
2020-12-03 07:21:57,543 [Thread-59] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:18020
2020-12-03 07:21:57,545 [Thread-59] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:21:57,545 [Thread-83] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:21:57,790 [Thread-83] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/in_use.lock acquired by nodename 5792@13788a696671
2020-12-03 07:21:57,790 [Thread-59] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/in_use.lock acquired by nodename 5792@13788a696671
2020-12-03 07:21:57,791 [Thread-83] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3 is not formatted for namespace 1772939335. Formatting...
2020-12-03 07:21:57,791 [Thread-59] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1 is not formatted for namespace 1772939335. Formatting...
2020-12-03 07:21:57,792 [Thread-83] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-ef772597-ccee-4e3e-8242-d1e42fd95974 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3 
2020-12-03 07:21:57,792 [Thread-59] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1 
2020-12-03 07:21:57,814 [IPC Server handler 0 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:57,821 [Listener at localhost/43105] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:21:57,822 [Listener at localhost/43105] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:21:57,917 [Thread-83] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/in_use.lock acquired by nodename 5792@13788a696671
2020-12-03 07:21:57,917 [Thread-59] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/in_use.lock acquired by nodename 5792@13788a696671
2020-12-03 07:21:57,917 [Thread-83] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4 is not formatted for namespace 1772939335. Formatting...
2020-12-03 07:21:57,918 [Thread-59] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2 is not formatted for namespace 1772939335. Formatting...
2020-12-03 07:21:57,918 [Thread-83] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4 
2020-12-03 07:21:57,918 [Thread-59] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-5b5d6366-8003-4c96-a269-54331e87d267 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2 
2020-12-03 07:21:57,924 [IPC Server handler 1 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:57,925 [Listener at localhost/43105] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:21:57,925 [Listener at localhost/43105] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:21:58,007 [Thread-83] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:21:58,008 [Thread-83] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:21:58,009 [Thread-59] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:21:58,009 [Thread-59] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:21:58,009 [Thread-83] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3 and block pool id BP-1338698873-172.17.0.5-1606980114492 is not formatted. Formatting ...
2020-12-03 07:21:58,009 [Thread-59] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1 and block pool id BP-1338698873-172.17.0.5-1606980114492 is not formatted. Formatting ...
2020-12-03 07:21:58,009 [Thread-83] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1338698873-172.17.0.5-1606980114492 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1338698873-172.17.0.5-1606980114492/current
2020-12-03 07:21:58,009 [Thread-59] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1338698873-172.17.0.5-1606980114492 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1338698873-172.17.0.5-1606980114492/current
2020-12-03 07:21:58,027 [IPC Server handler 2 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:58,028 [Listener at localhost/43105] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:21:58,028 [Listener at localhost/43105] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:21:58,105 [Thread-83] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:21:58,106 [Thread-83] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:21:58,106 [Thread-83] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4 and block pool id BP-1338698873-172.17.0.5-1606980114492 is not formatted. Formatting ...
2020-12-03 07:21:58,106 [Thread-83] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1338698873-172.17.0.5-1606980114492 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1338698873-172.17.0.5-1606980114492/current
2020-12-03 07:21:58,107 [Thread-59] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:21:58,107 [Thread-59] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:21:58,108 [Thread-59] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2 and block pool id BP-1338698873-172.17.0.5-1606980114492 is not formatted. Formatting ...
2020-12-03 07:21:58,108 [Thread-59] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1338698873-172.17.0.5-1606980114492 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1338698873-172.17.0.5-1606980114492/current
2020-12-03 07:21:58,130 [IPC Server handler 3 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:58,130 [Listener at localhost/43105] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:21:58,131 [Listener at localhost/43105] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:21:58,232 [IPC Server handler 4 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:58,233 [Listener at localhost/43105] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:21:58,233 [Listener at localhost/43105] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:21:58,248 [Thread-83] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=1772939335;bpid=BP-1338698873-172.17.0.5-1606980114492;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=1772939335;c=1606980114492;bpid=BP-1338698873-172.17.0.5-1606980114492;dnuuid=null
2020-12-03 07:21:58,293 [Thread-59] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=1772939335;bpid=BP-1338698873-172.17.0.5-1606980114492;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=1772939335;c=1606980114492;bpid=BP-1338698873-172.17.0.5-1606980114492;dnuuid=null
2020-12-03 07:21:58,335 [IPC Server handler 5 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:58,336 [Listener at localhost/43105] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:21:58,336 [Listener at localhost/43105] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:21:58,436 [Thread-83] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID fcfa3dd6-e926-44de-87fa-c54cedb559cc
2020-12-03 07:21:58,441 [IPC Server handler 6 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:58,442 [Listener at localhost/43105] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:21:58,442 [Listener at localhost/43105] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:21:58,485 [Thread-59] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID bb4cfced-0bdc-453a-99a9-88d3aea04105
2020-12-03 07:21:58,544 [IPC Server handler 7 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:58,545 [Listener at localhost/43105] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:21:58,545 [Listener at localhost/43105] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:21:58,571 [Thread-59] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85
2020-12-03 07:21:58,571 [Thread-83] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-ef772597-ccee-4e3e-8242-d1e42fd95974
2020-12-03 07:21:58,571 [Thread-59] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, StorageType: DISK
2020-12-03 07:21:58,571 [Thread-83] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3, StorageType: DISK
2020-12-03 07:21:58,574 [Thread-59] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-5b5d6366-8003-4c96-a269-54331e87d267
2020-12-03 07:21:58,574 [Thread-59] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, StorageType: DISK
2020-12-03 07:21:58,575 [Thread-83] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd
2020-12-03 07:21:58,575 [Thread-83] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, StorageType: DISK
2020-12-03 07:21:58,579 [Thread-83] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:21:58,579 [Thread-59] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:21:58,584 [Thread-83] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
2020-12-03 07:21:58,585 [Thread-59] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:21:58,593 [Thread-59] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:21:58,593 [Thread-83] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
2020-12-03 07:21:58,594 [Thread-59] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:21:58,594 [Thread-83] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:21:58,594 [Thread-59] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:21:58,595 [Thread-83] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:21:58,595 [Thread-83] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:21:58,595 [Thread-59] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:21:58,596 [Thread-101] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3...
2020-12-03 07:21:58,596 [Thread-102] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4...
2020-12-03 07:21:58,597 [Thread-103] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2020-12-03 07:21:58,597 [Thread-104] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2020-12-03 07:21:58,627 [Thread-101] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1338698873-172.17.0.5-1606980114492 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3: 31ms
2020-12-03 07:21:58,627 [Thread-103] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1338698873-172.17.0.5-1606980114492 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 31ms
2020-12-03 07:21:58,628 [Thread-102] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1338698873-172.17.0.5-1606980114492 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4: 32ms
2020-12-03 07:21:58,628 [Thread-83] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1338698873-172.17.0.5-1606980114492: 33ms
2020-12-03 07:21:58,628 [Thread-104] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1338698873-172.17.0.5-1606980114492 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 31ms
2020-12-03 07:21:58,628 [Thread-59] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1338698873-172.17.0.5-1606980114492: 34ms
2020-12-03 07:21:58,630 [Thread-110] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3...
2020-12-03 07:21:58,630 [Thread-109] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2020-12-03 07:21:58,630 [Thread-112] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2020-12-03 07:21:58,630 [Thread-111] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4...
2020-12-03 07:21:58,631 [Thread-112] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1338698873-172.17.0.5-1606980114492/current/replicas doesn't exist 
2020-12-03 07:21:58,631 [Thread-111] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1338698873-172.17.0.5-1606980114492/current/replicas doesn't exist 
2020-12-03 07:21:58,631 [Thread-109] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1338698873-172.17.0.5-1606980114492/current/replicas doesn't exist 
2020-12-03 07:21:58,631 [Thread-110] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1338698873-172.17.0.5-1606980114492/current/replicas doesn't exist 
2020-12-03 07:21:58,632 [Thread-109] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 1ms
2020-12-03 07:21:58,632 [Thread-112] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 1ms
2020-12-03 07:21:58,632 [Thread-59] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1338698873-172.17.0.5-1606980114492: 2ms
2020-12-03 07:21:58,633 [Thread-110] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3: 2ms
2020-12-03 07:21:58,633 [Thread-111] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4: 2ms
2020-12-03 07:21:58,633 [Thread-83] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1338698873-172.17.0.5-1606980114492: 3ms
2020-12-03 07:21:58,635 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:21:58,635 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:21:58,635 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
2020-12-03 07:21:58,635 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:21:58,637 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85): finished scanning block pool BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:21:58,637 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd): finished scanning block pool BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:21:58,637 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-5b5d6366-8003-4c96-a269-54331e87d267): finished scanning block pool BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:21:58,637 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3, DS-ef772597-ccee-4e3e-8242-d1e42fd95974): finished scanning block pool BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:21:58,648 [IPC Server handler 8 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:58,648 [Listener at localhost/43105] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:21:58,649 [Listener at localhost/43105] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:21:58,658 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd): no suitable block pools found to scan.  Waiting 1814399977 ms.
2020-12-03 07:21:58,658 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85): no suitable block pools found to scan.  Waiting 1814399977 ms.
2020-12-03 07:21:58,658 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3, DS-ef772597-ccee-4e3e-8242-d1e42fd95974): no suitable block pools found to scan.  Waiting 1814399977 ms.
2020-12-03 07:21:58,658 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-5b5d6366-8003-4c96-a269-54331e87d267): no suitable block pools found to scan.  Waiting 1814399977 ms.
2020-12-03 07:21:58,661 [Thread-59] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 8:21 AM with interval of 21600000ms
2020-12-03 07:21:58,661 [Thread-83] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 7:37 AM with interval of 21600000ms
2020-12-03 07:21:58,668 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid fcfa3dd6-e926-44de-87fa-c54cedb559cc) service to localhost/127.0.0.1:18020 beginning handshake with NN
2020-12-03 07:21:58,668 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid bb4cfced-0bdc-453a-99a9-88d3aea04105) service to localhost/127.0.0.1:18020 beginning handshake with NN
2020-12-03 07:21:58,678 [IPC Server handler 0 on default port 18020] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:32825, datanodeUuid=fcfa3dd6-e926-44de-87fa-c54cedb559cc, infoPort=41308, infoSecurePort=0, ipcPort=43105, storageInfo=lv=-57;cid=testClusterID;nsid=1772939335;c=1606980114492) storage fcfa3dd6-e926-44de-87fa-c54cedb559cc
2020-12-03 07:21:58,681 [IPC Server handler 0 on default port 18020] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:32825
2020-12-03 07:21:58,682 [IPC Server handler 0 on default port 18020] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN fcfa3dd6-e926-44de-87fa-c54cedb559cc (127.0.0.1:32825).
2020-12-03 07:21:58,684 [IPC Server handler 1 on default port 18020] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:33955, datanodeUuid=bb4cfced-0bdc-453a-99a9-88d3aea04105, infoPort=34498, infoSecurePort=0, ipcPort=46144, storageInfo=lv=-57;cid=testClusterID;nsid=1772939335;c=1606980114492) storage bb4cfced-0bdc-453a-99a9-88d3aea04105
2020-12-03 07:21:58,685 [IPC Server handler 1 on default port 18020] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:33955
2020-12-03 07:21:58,685 [IPC Server handler 1 on default port 18020] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN bb4cfced-0bdc-453a-99a9-88d3aea04105 (127.0.0.1:33955).
2020-12-03 07:21:58,689 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid fcfa3dd6-e926-44de-87fa-c54cedb559cc) service to localhost/127.0.0.1:18020 successfully registered with NN
2020-12-03 07:21:58,689 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (DataNode.java:registerBlockPoolWithSecretManager(1615)) - Block token params received from NN: for block pool BP-1338698873-172.17.0.5-1606980114492 keyUpdateInterval=600 min(s), tokenLifetime=600 min(s)
2020-12-03 07:21:58,690 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:21:58,690 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:18020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=1000
2020-12-03 07:21:58,689 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid bb4cfced-0bdc-453a-99a9-88d3aea04105) service to localhost/127.0.0.1:18020 successfully registered with NN
2020-12-03 07:21:58,692 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (DataNode.java:registerBlockPoolWithSecretManager(1615)) - Block token params received from NN: for block pool BP-1338698873-172.17.0.5-1606980114492 keyUpdateInterval=600 min(s), tokenLifetime=600 min(s)
2020-12-03 07:21:58,693 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:21:58,693 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:18020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=1000
2020-12-03 07:21:58,708 [IPC Server handler 3 on default port 18020] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85 for DN 127.0.0.1:33955
2020-12-03 07:21:58,709 [IPC Server handler 3 on default port 18020] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-5b5d6366-8003-4c96-a269-54331e87d267 for DN 127.0.0.1:33955
2020-12-03 07:21:58,710 [IPC Server handler 2 on default port 18020] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-ef772597-ccee-4e3e-8242-d1e42fd95974 for DN 127.0.0.1:32825
2020-12-03 07:21:58,710 [IPC Server handler 2 on default port 18020] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd for DN 127.0.0.1:32825
2020-12-03 07:21:58,739 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0xd7da409e26345a2e: Processing first storage report for DS-5b5d6366-8003-4c96-a269-54331e87d267 from datanode bb4cfced-0bdc-453a-99a9-88d3aea04105
2020-12-03 07:21:58,742 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0xd7da409e26345a2e: from storage DS-5b5d6366-8003-4c96-a269-54331e87d267 node DatanodeRegistration(127.0.0.1:33955, datanodeUuid=bb4cfced-0bdc-453a-99a9-88d3aea04105, infoPort=34498, infoSecurePort=0, ipcPort=46144, storageInfo=lv=-57;cid=testClusterID;nsid=1772939335;c=1606980114492), blocks: 0, hasStaleStorage: true, processing time: 3 msecs, invalidatedBlocks: 0
2020-12-03 07:21:58,742 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x16271196db2917c8: Processing first storage report for DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd from datanode fcfa3dd6-e926-44de-87fa-c54cedb559cc
2020-12-03 07:21:58,742 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x16271196db2917c8: from storage DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd node DatanodeRegistration(127.0.0.1:32825, datanodeUuid=fcfa3dd6-e926-44de-87fa-c54cedb559cc, infoPort=41308, infoSecurePort=0, ipcPort=43105, storageInfo=lv=-57;cid=testClusterID;nsid=1772939335;c=1606980114492), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:21:58,742 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0xd7da409e26345a2e: Processing first storage report for DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85 from datanode bb4cfced-0bdc-453a-99a9-88d3aea04105
2020-12-03 07:21:58,743 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0xd7da409e26345a2e: from storage DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85 node DatanodeRegistration(127.0.0.1:33955, datanodeUuid=bb4cfced-0bdc-453a-99a9-88d3aea04105, infoPort=34498, infoSecurePort=0, ipcPort=46144, storageInfo=lv=-57;cid=testClusterID;nsid=1772939335;c=1606980114492), blocks: 0, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
2020-12-03 07:21:58,743 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x16271196db2917c8: Processing first storage report for DS-ef772597-ccee-4e3e-8242-d1e42fd95974 from datanode fcfa3dd6-e926-44de-87fa-c54cedb559cc
2020-12-03 07:21:58,743 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x16271196db2917c8: from storage DS-ef772597-ccee-4e3e-8242-d1e42fd95974 node DatanodeRegistration(127.0.0.1:32825, datanodeUuid=fcfa3dd6-e926-44de-87fa-c54cedb559cc, infoPort=41308, infoSecurePort=0, ipcPort=43105, storageInfo=lv=-57;cid=testClusterID;nsid=1772939335;c=1606980114492), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:21:58,752 [IPC Server handler 6 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:58,761 [Listener at localhost/43105] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2758)) - Cluster is active
2020-12-03 07:21:58,766 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x16271196db2917c8,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 41 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:21:58,766 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0xd7da409e26345a2e,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 41 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:21:58,767 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:21:58,767 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:21:58,767 [Listener at localhost/43105] DEBUG hdfs.DFSClient (DFSClient.java:<init>(318)) - Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
2020-12-03 07:21:58,769 [IPC Server handler 7 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:58,770 [Listener at localhost/43105] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2758)) - Cluster is active
2020-12-03 07:21:58,771 [Listener at localhost/43105] DEBUG hdfs.DFSClient (DFSClient.java:<init>(318)) - Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
2020-12-03 07:21:58,778 [Listener at localhost/43105] DEBUG hdfs.DFSClient (DFSClient.java:create(1216)) - /fileToRead.dat: masked={ masked: rw-r--r--, unmasked: rw-rw-rw- }
2020-12-03 07:21:58,823 [IPC Server handler 8 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/fileToRead.dat	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2020-12-03 07:21:58,840 [Listener at localhost/43105] DEBUG hdfs.DFSClient (DFSOutputStream.java:computePacketChunkSize(410)) - computePacketChunkSize: src=/fileToRead.dat, chunkSize=1028, chunksPerPacket=63, packetSize=64764
2020-12-03 07:21:58,849 [Listener at localhost/43105] DEBUG hdfs.DFSClient (DFSOutputStream.java:writeChunkPrepare(475)) - WriteChunk allocating new packet seqno=0, src=/fileToRead.dat, packetSize=64764, chunksPerPacket=63, bytesCurBlock=0, DFSOutputStream:block==null
2020-12-03 07:21:58,850 [Listener at localhost/43105] DEBUG hdfs.DFSClient (DFSOutputStream.java:computePacketChunkSize(410)) - computePacketChunkSize: src=/fileToRead.dat, chunkSize=1028, chunksPerPacket=1, packetSize=1028
2020-12-03 07:21:58,850 [Listener at localhost/43105] DEBUG hdfs.DFSClient (DFSOutputStream.java:writeChunkPrepare(475)) - WriteChunk allocating new packet seqno=2, src=/fileToRead.dat, packetSize=1028, chunksPerPacket=1, bytesCurBlock=0, DFSOutputStream:block==null
2020-12-03 07:21:58,851 [Listener at localhost/43105] DEBUG hdfs.DFSClient (DFSOutputStream.java:computePacketChunkSize(410)) - computePacketChunkSize: src=/fileToRead.dat, chunkSize=1028, chunksPerPacket=1, packetSize=1028
2020-12-03 07:21:58,865 [IPC Server handler 9 on default port 18020] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_1073741825_1001, replicas=127.0.0.1:32825, 127.0.0.1:33955 for /fileToRead.dat
2020-12-03 07:21:58,882 [Thread-119] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:21:58,950 [DataXceiver for client DFSClient_NONMAPREDUCE_-2074459051_1 at /127.0.0.1:50320 [Receiving block BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001 src: /127.0.0.1:50320 dest: /127.0.0.1:32825
2020-12-03 07:21:58,971 [DataXceiver for client DFSClient_NONMAPREDUCE_-2074459051_1 at /127.0.0.1:50320 [Receiving block BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:21:58,974 [DataXceiver for client DFSClient_NONMAPREDUCE_-2074459051_1 at /127.0.0.1:56688 [Receiving block BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001 src: /127.0.0.1:56688 dest: /127.0.0.1:33955
2020-12-03 07:21:59,004 [PacketResponder: BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:56688, dest: /127.0.0.1:33955, bytes: 1024, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2074459051_1, offset: 0, srvID: bb4cfced-0bdc-453a-99a9-88d3aea04105, blockid: BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001, duration(ns): 10617620
2020-12-03 07:21:59,005 [PacketResponder: BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001, type=LAST_IN_PIPELINE terminating
2020-12-03 07:21:59,007 [PacketResponder: BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:33955]] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:50320, dest: /127.0.0.1:32825, bytes: 1024, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2074459051_1, offset: 0, srvID: fcfa3dd6-e926-44de-87fa-c54cedb559cc, blockid: BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001, duration(ns): 17199132
2020-12-03 07:21:59,008 [PacketResponder: BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:33955]] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:33955] terminating
2020-12-03 07:21:59,018 [IPC Server handler 1 on default port 18020] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_1073741826_1002, replicas=127.0.0.1:32825, 127.0.0.1:33955 for /fileToRead.dat
2020-12-03 07:21:59,021 [DataStreamer for file /fileToRead.dat] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:21:59,024 [DataXceiver for client DFSClient_NONMAPREDUCE_-2074459051_1 at /127.0.0.1:50324 [Receiving block BP-1338698873-172.17.0.5-1606980114492:blk_1073741826_1002]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1338698873-172.17.0.5-1606980114492:blk_1073741826_1002 src: /127.0.0.1:50324 dest: /127.0.0.1:32825
2020-12-03 07:21:59,025 [DataXceiver for client DFSClient_NONMAPREDUCE_-2074459051_1 at /127.0.0.1:50324 [Receiving block BP-1338698873-172.17.0.5-1606980114492:blk_1073741826_1002]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:21:59,027 [DataXceiver for client DFSClient_NONMAPREDUCE_-2074459051_1 at /127.0.0.1:56692 [Receiving block BP-1338698873-172.17.0.5-1606980114492:blk_1073741826_1002]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1338698873-172.17.0.5-1606980114492:blk_1073741826_1002 src: /127.0.0.1:56692 dest: /127.0.0.1:33955
2020-12-03 07:21:59,034 [PacketResponder: BP-1338698873-172.17.0.5-1606980114492:blk_1073741826_1002, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:56692, dest: /127.0.0.1:33955, bytes: 1024, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2074459051_1, offset: 0, srvID: bb4cfced-0bdc-453a-99a9-88d3aea04105, blockid: BP-1338698873-172.17.0.5-1606980114492:blk_1073741826_1002, duration(ns): 4831638
2020-12-03 07:21:59,035 [PacketResponder: BP-1338698873-172.17.0.5-1606980114492:blk_1073741826_1002, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1338698873-172.17.0.5-1606980114492:blk_1073741826_1002, type=LAST_IN_PIPELINE terminating
2020-12-03 07:21:59,038 [PacketResponder: BP-1338698873-172.17.0.5-1606980114492:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:33955]] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:50324, dest: /127.0.0.1:32825, bytes: 1024, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2074459051_1, offset: 0, srvID: fcfa3dd6-e926-44de-87fa-c54cedb559cc, blockid: BP-1338698873-172.17.0.5-1606980114492:blk_1073741826_1002, duration(ns): 8184731
2020-12-03 07:21:59,038 [PacketResponder: BP-1338698873-172.17.0.5-1606980114492:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:33955]] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1338698873-172.17.0.5-1606980114492:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:33955] terminating
2020-12-03 07:21:59,044 [IPC Server handler 7 on default port 18020] INFO  hdfs.StateChange (FSNamesystem.java:completeFile(2948)) - DIR* completeFile: /fileToRead.dat is closed by DFSClient_NONMAPREDUCE_-2074459051_1
2020-12-03 07:21:59,057 [IPC Server handler 8 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/fileToRead.dat	dst=null	perm=null	proto=rpc
2020-12-03 07:21:59,070 [Listener at localhost/43105] DEBUG hdfs.DFSClient (DFSInputStream.java:fetchLocatedBlocksAndGetLastBlockLength(243)) - newInfo = LocatedBlocks{;  fileLength=2048;  underConstruction=false;  blocks=[LocatedBlock{BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001; getBlockSize()=1024; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:32825,DS-ef772597-ccee-4e3e-8242-d1e42fd95974,DISK], DatanodeInfoWithStorage[127.0.0.1:33955,DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85,DISK]]}, LocatedBlock{BP-1338698873-172.17.0.5-1606980114492:blk_1073741826_1002; getBlockSize()=1024; corrupt=false; offset=1024; locs=[DatanodeInfoWithStorage[127.0.0.1:32825,DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd,DISK], DatanodeInfoWithStorage[127.0.0.1:33955,DS-5b5d6366-8003-4c96-a269-54331e87d267,DISK]]}];  lastLocatedBlock=LocatedBlock{BP-1338698873-172.17.0.5-1606980114492:blk_1073741826_1002; getBlockSize()=1024; corrupt=false; offset=1024; locs=[DatanodeInfoWithStorage[127.0.0.1:33955,DS-5b5d6366-8003-4c96-a269-54331e87d267,DISK], DatanodeInfoWithStorage[127.0.0.1:32825,DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd,DISK]]};  isLastBlockComplete=true;  ecPolicy=null}
2020-12-03 07:21:59,073 [Listener at localhost/43105] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:32825
2020-12-03 07:21:59,083 [Listener at localhost/43105] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:21:59,112 [Listener at localhost/43105] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:32825
2020-12-03 07:21:59,116 [IPC Server handler 9 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/fileToRead.dat	dst=null	perm=null	proto=rpc
2020-12-03 07:21:59,118 [Listener at localhost/43105] DEBUG hdfs.DFSClient (DFSInputStream.java:fetchLocatedBlocksAndGetLastBlockLength(243)) - newInfo = LocatedBlocks{;  fileLength=2048;  underConstruction=false;  blocks=[LocatedBlock{BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001; getBlockSize()=1024; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:32825,DS-ef772597-ccee-4e3e-8242-d1e42fd95974,DISK], DatanodeInfoWithStorage[127.0.0.1:33955,DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85,DISK]]}, LocatedBlock{BP-1338698873-172.17.0.5-1606980114492:blk_1073741826_1002; getBlockSize()=1024; corrupt=false; offset=1024; locs=[DatanodeInfoWithStorage[127.0.0.1:32825,DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd,DISK], DatanodeInfoWithStorage[127.0.0.1:33955,DS-5b5d6366-8003-4c96-a269-54331e87d267,DISK]]}];  lastLocatedBlock=LocatedBlock{BP-1338698873-172.17.0.5-1606980114492:blk_1073741826_1002; getBlockSize()=1024; corrupt=false; offset=1024; locs=[DatanodeInfoWithStorage[127.0.0.1:32825,DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd,DISK], DatanodeInfoWithStorage[127.0.0.1:33955,DS-5b5d6366-8003-4c96-a269-54331e87d267,DISK]]};  isLastBlockComplete=true;  ecPolicy=null}
2020-12-03 07:21:59,119 [Listener at localhost/43105] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:32825
2020-12-03 07:21:59,119 [Listener at localhost/43105] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:21:59,121 [Listener at localhost/43105] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:32825
2020-12-03 07:21:59,125 [IPC Server handler 0 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/fileToRead.dat	dst=null	perm=null	proto=rpc
2020-12-03 07:21:59,127 [Listener at localhost/43105] DEBUG hdfs.DFSClient (DFSInputStream.java:fetchLocatedBlocksAndGetLastBlockLength(243)) - newInfo = LocatedBlocks{;  fileLength=2048;  underConstruction=false;  blocks=[LocatedBlock{BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001; getBlockSize()=1024; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33955,DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85,DISK], DatanodeInfoWithStorage[127.0.0.1:32825,DS-ef772597-ccee-4e3e-8242-d1e42fd95974,DISK]]}, LocatedBlock{BP-1338698873-172.17.0.5-1606980114492:blk_1073741826_1002; getBlockSize()=1024; corrupt=false; offset=1024; locs=[DatanodeInfoWithStorage[127.0.0.1:33955,DS-5b5d6366-8003-4c96-a269-54331e87d267,DISK], DatanodeInfoWithStorage[127.0.0.1:32825,DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd,DISK]]}];  lastLocatedBlock=LocatedBlock{BP-1338698873-172.17.0.5-1606980114492:blk_1073741826_1002; getBlockSize()=1024; corrupt=false; offset=1024; locs=[DatanodeInfoWithStorage[127.0.0.1:33955,DS-5b5d6366-8003-4c96-a269-54331e87d267,DISK], DatanodeInfoWithStorage[127.0.0.1:32825,DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd,DISK]]};  isLastBlockComplete=true;  ecPolicy=null}
2020-12-03 07:21:59,128 [Listener at localhost/43105] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:33955
2020-12-03 07:21:59,128 [Listener at localhost/43105] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:21:59,131 [Listener at localhost/43105] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:33955
2020-12-03 07:21:59,134 [Listener at localhost/43105] DEBUG hdfs.DFSClient (DFSClient.java:<init>(318)) - Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
2020-12-03 07:22:00,146 [DataXceiver for client TestBlockTokenWithDFS at /127.0.0.1:50346 [Sending block BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001]] WARN  datanode.DataNode (DataXceiver.java:checkAccess(1449)) - Block token verification failed: op=READ_BLOCK, remoteAddress=/127.0.0.1:50346, message=Block token with block_token_identifier (expiryDate=1606980120136, keyId=38854922, userId=root, blockPoolId=BP-1338698873-172.17.0.5-1606980114492, blockId=1073741825, access modes=[READ], storageTypes= [DISK, DISK], storageIds= [DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85, DS-ef772597-ccee-4e3e-8242-d1e42fd95974]) is expired.
2020-12-03 07:22:00,156 [DataXceiver for client TestBlockTokenWithDFS at /127.0.0.1:50350 [Sending block BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001]] WARN  datanode.DataNode (DataXceiver.java:checkAccess(1449)) - Block token verification failed: op=READ_BLOCK, remoteAddress=/127.0.0.1:50350, message=Block token with block_token_identifier (expiryDate=1606980121150, keyId=38854922, userId=root, blockPoolId=BP-1338698873-172.17.0.5-1606980114492, blockId=1073741826, access modes=[READ], storageTypes= [DISK, DISK], storageIds= [DS-ef772597-ccee-4e3e-8242-d1e42fd95974, DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85]) doesn't apply to block BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001
2020-12-03 07:22:00,158 [DataXceiver for client TestBlockTokenWithDFS at /127.0.0.1:50352 [Sending block BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001]] WARN  datanode.DataNode (DataXceiver.java:checkAccess(1449)) - Block token verification failed: op=READ_BLOCK, remoteAddress=/127.0.0.1:50352, message=Block token with block_token_identifier (expiryDate=1606980121156, keyId=38854922, userId=root, blockPoolId=BP-1338698873-172.17.0.5-1606980114492, blockId=1073741825, access modes=[WRITE], storageTypes= [DISK, DISK], storageIds= [DS-ef772597-ccee-4e3e-8242-d1e42fd95974, DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85]) doesn't have READ permission
2020-12-03 07:22:00,158 [Listener at localhost/43105] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:32825
2020-12-03 07:22:00,160 [DataXceiver for client DFSClient_NONMAPREDUCE_-2074459051_1 at /127.0.0.1:50328 [Sending block BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001]] WARN  datanode.DataNode (DataXceiver.java:checkAccess(1449)) - Block token verification failed: op=READ_BLOCK, remoteAddress=/127.0.0.1:50328, message=Block token with block_token_identifier (expiryDate=1606980120055, keyId=38854922, userId=root, blockPoolId=BP-1338698873-172.17.0.5-1606980114492, blockId=1073741825, access modes=[READ], storageTypes= [DISK, DISK], storageIds= [DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85, DS-ef772597-ccee-4e3e-8242-d1e42fd95974]) is expired.
2020-12-03 07:22:00,161 [Listener at localhost/43105] DEBUG hdfs.DFSClient (DFSInputStream.java:tokenRefetchNeeded(1298)) - Access token was invalid when connecting to /127.0.0.1:32825: {}
org.apache.hadoop.hdfs.security.token.block.InvalidBlockTokenException: Got access token error, status message , for OP_READ_BLOCK, self=/127.0.0.1:50328, remote=/127.0.0.1:32825, for file /fileToRead.dat, for pool BP-1338698873-172.17.0.5-1606980114492 block 1073741825_1001
	at org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:119)
	at org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:110)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.checkSuccess(BlockReaderRemote.java:440)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.newBlockReader(BlockReaderRemote.java:408)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReader(BlockReaderFactory.java:854)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:750)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:380)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:644)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:575)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:757)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:829)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.checkFile1(TestBlockTokenWithDFS.java:102)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.doTestRead(TestBlockTokenWithDFS.java:467)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.testRead(TestBlockTokenWithDFS.java:360)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-12-03 07:22:00,166 [IPC Server handler 1 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/fileToRead.dat	dst=null	perm=null	proto=rpc
2020-12-03 07:22:00,168 [Listener at localhost/43105] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:32825
2020-12-03 07:22:00,169 [Listener at localhost/43105] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:00,172 [Listener at localhost/43105] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:32825
2020-12-03 07:22:00,175 [Listener at localhost/43105] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:33955
2020-12-03 07:22:00,176 [DataXceiver for client DFSClient_NONMAPREDUCE_-2074459051_1 at /127.0.0.1:56698 [Sending block BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001]] WARN  datanode.DataNode (DataXceiver.java:checkAccess(1449)) - Block token verification failed: op=READ_BLOCK, remoteAddress=/127.0.0.1:56698, message=Block token with block_token_identifier (expiryDate=1606980120115, keyId=38854922, userId=root, blockPoolId=BP-1338698873-172.17.0.5-1606980114492, blockId=1073741825, access modes=[READ], storageTypes= [DISK, DISK], storageIds= [DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85, DS-ef772597-ccee-4e3e-8242-d1e42fd95974]) is expired.
2020-12-03 07:22:00,176 [Listener at localhost/43105] DEBUG hdfs.DFSClient (DFSInputStream.java:tokenRefetchNeeded(1298)) - Access token was invalid when connecting to /127.0.0.1:33955: {}
org.apache.hadoop.hdfs.security.token.block.InvalidBlockTokenException: Got access token error, status message , for OP_READ_BLOCK, self=/127.0.0.1:56698, remote=/127.0.0.1:33955, for file /fileToRead.dat, for pool BP-1338698873-172.17.0.5-1606980114492 block 1073741825_1001
	at org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:119)
	at org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:110)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.checkSuccess(BlockReaderRemote.java:440)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.newBlockReader(BlockReaderRemote.java:408)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReader(BlockReaderFactory.java:854)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:750)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:380)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:644)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:575)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToNewSource(DFSInputStream.java:1514)
	at org.apache.hadoop.fs.FSDataInputStream.seekToNewSource(FSDataInputStream.java:131)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.doTestRead(TestBlockTokenWithDFS.java:480)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.testRead(TestBlockTokenWithDFS.java:360)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-12-03 07:22:00,179 [IPC Server handler 3 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/fileToRead.dat	dst=null	perm=null	proto=rpc
2020-12-03 07:22:00,181 [Listener at localhost/43105] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:33955
2020-12-03 07:22:00,182 [Listener at localhost/43105] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:00,185 [Listener at localhost/43105] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:33955
2020-12-03 07:22:00,188 [Listener at localhost/43105] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:33955
2020-12-03 07:22:00,189 [Listener at localhost/43105] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:00,191 [DataXceiver for client DFSClient_NONMAPREDUCE_-2074459051_1 at /127.0.0.1:56724 [Sending block BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001]] WARN  datanode.DataNode (DataXceiver.java:checkAccess(1449)) - Block token verification failed: op=READ_BLOCK, remoteAddress=/127.0.0.1:56724, message=Block token with block_token_identifier (expiryDate=1606980120125, keyId=38854922, userId=root, blockPoolId=BP-1338698873-172.17.0.5-1606980114492, blockId=1073741825, access modes=[READ], storageTypes= [DISK, DISK], storageIds= [DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85, DS-ef772597-ccee-4e3e-8242-d1e42fd95974]) is expired.
2020-12-03 07:22:00,191 [Listener at localhost/43105] DEBUG hdfs.DFSClient (DFSInputStream.java:tokenRefetchNeeded(1298)) - Access token was invalid when connecting to /127.0.0.1:33955: {}
org.apache.hadoop.hdfs.security.token.block.InvalidBlockTokenException: Got access token error, status message , for OP_READ_BLOCK, self=/127.0.0.1:56724, remote=/127.0.0.1:33955, for file /fileToRead.dat, for pool BP-1338698873-172.17.0.5-1606980114492 block 1073741825_1001
	at org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:119)
	at org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:110)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.checkSuccess(BlockReaderRemote.java:440)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.newBlockReader(BlockReaderRemote.java:408)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReader(BlockReaderFactory.java:854)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:750)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:380)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:644)
	at org.apache.hadoop.hdfs.DFSInputStream.actualGetFromOneDataNode(DFSInputStream.java:1049)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchBlockByteRange(DFSInputStream.java:1001)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:92)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.checkFile2(TestBlockTokenWithDFS.java:116)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.doTestRead(TestBlockTokenWithDFS.java:490)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.testRead(TestBlockTokenWithDFS.java:360)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-12-03 07:22:00,195 [IPC Server handler 2 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/fileToRead.dat	dst=null	perm=null	proto=rpc
2020-12-03 07:22:00,199 [Listener at localhost/43105] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:00,202 [Listener at localhost/43105] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:33955
2020-12-03 07:22:00,203 [DataXceiver for client DFSClient_NONMAPREDUCE_-2074459051_1 at /127.0.0.1:56726 [Sending block BP-1338698873-172.17.0.5-1606980114492:blk_1073741826_1002]] WARN  datanode.DataNode (DataXceiver.java:checkAccess(1449)) - Block token verification failed: op=READ_BLOCK, remoteAddress=/127.0.0.1:56726, message=Block token with block_token_identifier (expiryDate=1606980120125, keyId=38854922, userId=root, blockPoolId=BP-1338698873-172.17.0.5-1606980114492, blockId=1073741826, access modes=[READ], storageTypes= [DISK, DISK], storageIds= [DS-5b5d6366-8003-4c96-a269-54331e87d267, DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd]) is expired.
2020-12-03 07:22:00,203 [Listener at localhost/43105] DEBUG hdfs.DFSClient (DFSInputStream.java:tokenRefetchNeeded(1298)) - Access token was invalid when connecting to /127.0.0.1:33955: {}
org.apache.hadoop.hdfs.security.token.block.InvalidBlockTokenException: Got access token error, status message , for OP_READ_BLOCK, self=/127.0.0.1:56726, remote=/127.0.0.1:33955, for file /fileToRead.dat, for pool BP-1338698873-172.17.0.5-1606980114492 block 1073741826_1002
	at org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:119)
	at org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:110)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.checkSuccess(BlockReaderRemote.java:440)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.newBlockReader(BlockReaderRemote.java:408)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReader(BlockReaderFactory.java:854)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:750)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:380)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:644)
	at org.apache.hadoop.hdfs.DFSInputStream.actualGetFromOneDataNode(DFSInputStream.java:1049)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchBlockByteRange(DFSInputStream.java:1001)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:92)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.checkFile2(TestBlockTokenWithDFS.java:116)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.doTestRead(TestBlockTokenWithDFS.java:490)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.testRead(TestBlockTokenWithDFS.java:360)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-12-03 07:22:00,206 [IPC Server handler 6 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/fileToRead.dat	dst=null	perm=null	proto=rpc
2020-12-03 07:22:00,208 [Listener at localhost/43105] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:00,211 [Listener at localhost/43105] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:stopDataNode(2331)) - MiniDFSCluster Stopping DataNode 127.0.0.1:32825 from a total of 2 datanodes.
2020-12-03 07:22:00,213 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@29ad44e3] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:22:00,213 [Listener at localhost/43105] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:22:00,214 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd) exiting.
2020-12-03 07:22:00,214 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3, DS-ef772597-ccee-4e3e-8242-d1e42fd95974) exiting.
2020-12-03 07:22:00,246 [Listener at localhost/43105] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@200606de{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:22:00,252 [Listener at localhost/43105] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@750fe12e{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:22:00,253 [Listener at localhost/43105] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@797501a{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:22:00,254 [Listener at localhost/43105] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@14bb2297{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:22:00,258 [Listener at localhost/43105] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 43105
2020-12-03 07:22:00,260 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:22:00,261 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:22:00,263 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:22:00,263 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid fcfa3dd6-e926-44de-87fa-c54cedb559cc) service to localhost/127.0.0.1:18020
2020-12-03 07:22:00,263 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid fcfa3dd6-e926-44de-87fa-c54cedb559cc)
2020-12-03 07:22:00,263 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:00,265 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1338698873-172.17.0.5-1606980114492] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:00,267 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1338698873-172.17.0.5-1606980114492] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:00,271 [Listener at localhost/43105] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:22:00,271 [Listener at localhost/43105] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:22:00,272 [Listener at localhost/43105] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:22:00,272 [Listener at localhost/43105] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:22:00,278 [Listener at localhost/43105] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:22:00,280 [Listener at localhost/43105] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
2020-12-03 07:22:00,281 [Listener at localhost/43105] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:22:00,282 [Listener at localhost/43105] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:22:00,283 [Listener at localhost/43105] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:22:00,283 [Listener at localhost/43105] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:22:00,283 [Listener at localhost/43105] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:22:00,283 [Listener at localhost/43105] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:22:00,284 [Listener at localhost/43105] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:22:00,284 [Listener at localhost/43105] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:22:00,284 [Listener at localhost/43105] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:22:00,285 [Listener at localhost/43105] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:32825
2020-12-03 07:22:00,285 [Listener at localhost/43105] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:22:00,285 [Listener at localhost/43105] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:22:00,287 [Listener at localhost/43105] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:22:00,289 [Listener at localhost/43105] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:22:00,289 [Listener at localhost/43105] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:22:00,289 [Listener at localhost/43105] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:22:00,292 [Listener at localhost/43105] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:22:00,293 [Listener at localhost/43105] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:22:00,293 [Listener at localhost/43105] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:22:00,293 [Listener at localhost/43105] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:22:00,294 [Listener at localhost/43105] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 37301
2020-12-03 07:22:00,295 [Listener at localhost/43105] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:22:00,296 [Listener at localhost/43105] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@5e840abf{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:22:00,297 [Listener at localhost/43105] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@5972d253{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:22:00,305 [Listener at localhost/43105] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@4bdc8b5d{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:22:00,307 [Listener at localhost/43105] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@3bcd426c{HTTP/1.1,[http/1.1]}{localhost:37301}
2020-12-03 07:22:00,307 [Listener at localhost/43105] INFO  server.Server (Server.java:doStart(419)) - Started @7736ms
2020-12-03 07:22:00,335 [Listener at localhost/43105] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:44763
2020-12-03 07:22:00,336 [Listener at localhost/43105] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:22:00,336 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@726a17c4] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:22:00,336 [Listener at localhost/43105] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:22:00,336 [Listener at localhost/43105] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:22:00,337 [Socket Reader #1 for port 43105] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 43105
2020-12-03 07:22:00,342 [Listener at localhost/43105] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:43105
2020-12-03 07:22:00,348 [Listener at localhost/43105] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:22:00,349 [Listener at localhost/43105] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:22:00,349 [Thread-156] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:18020 starting to offer service
2020-12-03 07:22:00,352 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:22:00,352 [IPC Server listener on 43105] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 43105: starting
2020-12-03 07:22:00,359 [Listener at localhost/43105] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:restartDataNodes(2519)) - Restarted DataNode 1
2020-12-03 07:22:00,359 [Listener at localhost/43105] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:stopDataNode(2331)) - MiniDFSCluster Stopping DataNode 127.0.0.1:33955 from a total of 2 datanodes.
2020-12-03 07:22:00,360 [Listener at localhost/43105] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:22:00,360 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@107ed6fc] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:22:00,362 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-5b5d6366-8003-4c96-a269-54331e87d267) exiting.
2020-12-03 07:22:00,362 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85) exiting.
2020-12-03 07:22:00,363 [Thread-156] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:18020
2020-12-03 07:22:00,364 [Thread-156] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:22:00,388 [Listener at localhost/43105] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@fd0e5b6{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:22:00,389 [Listener at localhost/43105] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@4eed46ee{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:22:00,389 [Listener at localhost/43105] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@d5ae57e{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:22:00,390 [Listener at localhost/43105] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@410ae9a3{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:22:00,391 [Listener at localhost/43105] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 46144
2020-12-03 07:22:00,400 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:22:00,404 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:22:00,405 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:22:00,406 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid bb4cfced-0bdc-453a-99a9-88d3aea04105) service to localhost/127.0.0.1:18020
2020-12-03 07:22:00,406 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid bb4cfced-0bdc-453a-99a9-88d3aea04105)
2020-12-03 07:22:00,406 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:00,408 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1338698873-172.17.0.5-1606980114492] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:00,408 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1338698873-172.17.0.5-1606980114492] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:00,412 [Listener at localhost/43105] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:22:00,412 [Listener at localhost/43105] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:22:00,413 [Listener at localhost/43105] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:22:00,413 [Listener at localhost/43105] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:22:00,414 [Thread-156] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/in_use.lock acquired by nodename 5792@13788a696671
2020-12-03 07:22:00,416 [Listener at localhost/43105] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:22:00,418 [Listener at localhost/43105] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:22:00,418 [Listener at localhost/43105] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:22:00,419 [Listener at localhost/43105] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:22:00,420 [Listener at localhost/43105] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:22:00,420 [Listener at localhost/43105] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:22:00,421 [Listener at localhost/43105] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:22:00,421 [Listener at localhost/43105] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:22:00,421 [Listener at localhost/43105] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:22:00,421 [Listener at localhost/43105] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:22:00,422 [Listener at localhost/43105] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:22:00,423 [Listener at localhost/43105] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:33955
2020-12-03 07:22:00,423 [Listener at localhost/43105] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:22:00,423 [Listener at localhost/43105] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:22:00,424 [Listener at localhost/43105] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:22:00,426 [Listener at localhost/43105] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:22:00,426 [Listener at localhost/43105] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:22:00,426 [Listener at localhost/43105] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:22:00,429 [Listener at localhost/43105] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:22:00,430 [Listener at localhost/43105] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:22:00,430 [Listener at localhost/43105] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:22:00,430 [Listener at localhost/43105] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:22:00,431 [Listener at localhost/43105] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 44443
2020-12-03 07:22:00,431 [Listener at localhost/43105] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:22:00,433 [Listener at localhost/43105] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@73877e19{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:22:00,434 [Listener at localhost/43105] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@5bfc257{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:22:00,440 [Listener at localhost/43105] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@2cfbeac4{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:22:00,442 [Listener at localhost/43105] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@12db3386{HTTP/1.1,[http/1.1]}{localhost:44443}
2020-12-03 07:22:00,442 [Listener at localhost/43105] INFO  server.Server (Server.java:doStart(419)) - Started @7871ms
2020-12-03 07:22:00,529 [Listener at localhost/43105] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:42477
2020-12-03 07:22:00,529 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@79a1728c] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:22:00,529 [Listener at localhost/43105] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:22:00,529 [Listener at localhost/43105] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:22:00,530 [Listener at localhost/43105] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:22:00,531 [Socket Reader #1 for port 46144] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 46144
2020-12-03 07:22:00,536 [Listener at localhost/46144] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:46144
2020-12-03 07:22:00,545 [Listener at localhost/46144] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:22:00,545 [Listener at localhost/46144] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:22:00,546 [Thread-178] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:18020 starting to offer service
2020-12-03 07:22:00,590 [Thread-156] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/in_use.lock acquired by nodename 5792@13788a696671
2020-12-03 07:22:00,591 [IPC Server listener on 46144] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 46144: starting
2020-12-03 07:22:00,591 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:22:00,598 [Listener at localhost/46144] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:restartDataNodes(2519)) - Restarted DataNode 0
2020-12-03 07:22:00,600 [Thread-178] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:18020
2020-12-03 07:22:00,601 [Listener at localhost/46144] DEBUG hdfs.DFSClient (DFSClient.java:<init>(318)) - Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
2020-12-03 07:22:00,602 [Thread-178] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:22:00,609 [IPC Server handler 7 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:22:00,610 [Listener at localhost/46144] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2789)) - !dn.datanode.isDatanodeFullyStarted()
2020-12-03 07:22:00,610 [Listener at localhost/46144] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:22:00,661 [Thread-178] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/in_use.lock acquired by nodename 5792@13788a696671
2020-12-03 07:22:00,673 [Thread-156] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:00,673 [Thread-156] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:00,712 [IPC Server handler 8 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:22:00,713 [Listener at localhost/46144] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2789)) - !dn.datanode.isDatanodeFullyStarted()
2020-12-03 07:22:00,713 [Listener at localhost/46144] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:22:00,756 [Thread-156] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:00,757 [Thread-156] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:00,815 [IPC Server handler 9 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:22:00,817 [Listener at localhost/46144] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2789)) - !dn.datanode.isDatanodeFullyStarted()
2020-12-03 07:22:00,817 [Listener at localhost/46144] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:22:00,827 [Thread-178] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/in_use.lock acquired by nodename 5792@13788a696671
2020-12-03 07:22:00,827 [Thread-156] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=1772939335;bpid=BP-1338698873-172.17.0.5-1606980114492;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=1772939335;c=1606980114492;bpid=BP-1338698873-172.17.0.5-1606980114492;dnuuid=fcfa3dd6-e926-44de-87fa-c54cedb559cc
2020-12-03 07:22:00,830 [Thread-156] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-ef772597-ccee-4e3e-8242-d1e42fd95974
2020-12-03 07:22:00,830 [Thread-156] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3, StorageType: DISK
2020-12-03 07:22:00,835 [Thread-156] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd
2020-12-03 07:22:00,837 [Thread-156] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, StorageType: DISK
2020-12-03 07:22:00,838 [Thread-156] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:22:00,839 [Thread-156] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
2020-12-03 07:22:00,848 [Thread-156] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
2020-12-03 07:22:00,848 [Thread-156] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:22:00,849 [Thread-156] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:22:00,854 [Thread-156] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:00,854 [Thread-191] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3...
2020-12-03 07:22:00,855 [Thread-192] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4...
2020-12-03 07:22:00,859 [Thread-191] INFO  impl.FsDatasetImpl (BlockPoolSlice.java:loadDfsUsed(292)) - Cached dfsUsed found for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1338698873-172.17.0.5-1606980114492/current: 25611
2020-12-03 07:22:00,859 [Thread-192] INFO  impl.FsDatasetImpl (BlockPoolSlice.java:loadDfsUsed(292)) - Cached dfsUsed found for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1338698873-172.17.0.5-1606980114492/current: 25611
2020-12-03 07:22:00,868 [Thread-192] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1338698873-172.17.0.5-1606980114492 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4: 13ms
2020-12-03 07:22:00,868 [Thread-191] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1338698873-172.17.0.5-1606980114492 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3: 13ms
2020-12-03 07:22:00,869 [Thread-156] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1338698873-172.17.0.5-1606980114492: 16ms
2020-12-03 07:22:00,870 [Thread-193] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3...
2020-12-03 07:22:00,870 [Thread-194] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4...
2020-12-03 07:22:00,872 [Thread-193] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(925)) - Successfully read replica from cache file : /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1338698873-172.17.0.5-1606980114492/current/replicas
2020-12-03 07:22:00,872 [Thread-194] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(925)) - Successfully read replica from cache file : /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1338698873-172.17.0.5-1606980114492/current/replicas
2020-12-03 07:22:00,873 [Thread-193] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3: 3ms
2020-12-03 07:22:00,873 [Thread-194] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4: 3ms
2020-12-03 07:22:00,873 [Thread-156] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1338698873-172.17.0.5-1606980114492: 3ms
2020-12-03 07:22:00,888 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd): no suitable block pools found to scan.  Waiting 1814397747 ms.
2020-12-03 07:22:00,888 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3, DS-ef772597-ccee-4e3e-8242-d1e42fd95974): no suitable block pools found to scan.  Waiting 1814397747 ms.
2020-12-03 07:22:00,889 [Thread-156] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 10:01 AM with interval of 21600000ms
2020-12-03 07:22:00,893 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid fcfa3dd6-e926-44de-87fa-c54cedb559cc) service to localhost/127.0.0.1:18020 beginning handshake with NN
2020-12-03 07:22:00,894 [IPC Server handler 0 on default port 18020] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:32825, datanodeUuid=fcfa3dd6-e926-44de-87fa-c54cedb559cc, infoPort=44763, infoSecurePort=0, ipcPort=43105, storageInfo=lv=-57;cid=testClusterID;nsid=1772939335;c=1606980114492) storage fcfa3dd6-e926-44de-87fa-c54cedb559cc
2020-12-03 07:22:00,895 [IPC Server handler 0 on default port 18020] INFO  net.NetworkTopology (NetworkTopology.java:remove(219)) - Removing a node: /default-rack/127.0.0.1:32825
2020-12-03 07:22:00,895 [IPC Server handler 0 on default port 18020] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:32825
2020-12-03 07:22:00,896 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid fcfa3dd6-e926-44de-87fa-c54cedb559cc) service to localhost/127.0.0.1:18020 successfully registered with NN
2020-12-03 07:22:00,896 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (DataNode.java:registerBlockPoolWithSecretManager(1615)) - Block token params received from NN: for block pool BP-1338698873-172.17.0.5-1606980114492 keyUpdateInterval=600 min(s), tokenLifetime=10 min(s)
2020-12-03 07:22:00,897 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:22:00,897 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:18020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=1000
2020-12-03 07:22:00,908 [Thread-178] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:00,909 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x175089f2dddbc42e: Processing first storage report for DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd from datanode fcfa3dd6-e926-44de-87fa-c54cedb559cc
2020-12-03 07:22:00,910 [Thread-178] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:00,910 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x175089f2dddbc42e: from storage DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd node DatanodeRegistration(127.0.0.1:32825, datanodeUuid=fcfa3dd6-e926-44de-87fa-c54cedb559cc, infoPort=44763, infoSecurePort=0, ipcPort=43105, storageInfo=lv=-57;cid=testClusterID;nsid=1772939335;c=1606980114492), blocks: 1, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
2020-12-03 07:22:00,910 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x175089f2dddbc42e: Processing first storage report for DS-ef772597-ccee-4e3e-8242-d1e42fd95974 from datanode fcfa3dd6-e926-44de-87fa-c54cedb559cc
2020-12-03 07:22:00,910 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x175089f2dddbc42e: from storage DS-ef772597-ccee-4e3e-8242-d1e42fd95974 node DatanodeRegistration(127.0.0.1:32825, datanodeUuid=fcfa3dd6-e926-44de-87fa-c54cedb559cc, infoPort=44763, infoSecurePort=0, ipcPort=43105, storageInfo=lv=-57;cid=testClusterID;nsid=1772939335;c=1606980114492), blocks: 1, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:22:00,911 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x175089f2dddbc42e,  containing 2 storage report(s), of which we sent 2. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 4 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:22:00,912 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:00,918 [IPC Server handler 2 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:22:00,919 [Listener at localhost/46144] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2789)) - !dn.datanode.isDatanodeFullyStarted()
2020-12-03 07:22:00,919 [Listener at localhost/46144] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:22:00,976 [Thread-178] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:00,976 [Thread-178] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:01,016 [Thread-178] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=1772939335;bpid=BP-1338698873-172.17.0.5-1606980114492;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=1772939335;c=1606980114492;bpid=BP-1338698873-172.17.0.5-1606980114492;dnuuid=bb4cfced-0bdc-453a-99a9-88d3aea04105
2020-12-03 07:22:01,019 [Thread-178] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85
2020-12-03 07:22:01,020 [Thread-178] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, StorageType: DISK
2020-12-03 07:22:01,022 [Thread-178] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-5b5d6366-8003-4c96-a269-54331e87d267
2020-12-03 07:22:01,022 [Thread-178] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, StorageType: DISK
2020-12-03 07:22:01,023 [IPC Server handler 6 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:22:01,024 [Thread-178] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:22:01,024 [Listener at localhost/46144] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2789)) - !dn.datanode.isDatanodeFullyStarted()
2020-12-03 07:22:01,024 [Listener at localhost/46144] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:22:01,025 [Thread-178] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:22:01,026 [Thread-178] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:22:01,026 [Thread-178] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:22:01,026 [Thread-178] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:22:01,027 [Thread-178] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:01,028 [Thread-200] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2020-12-03 07:22:01,028 [Thread-201] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2020-12-03 07:22:01,030 [Thread-201] INFO  impl.FsDatasetImpl (BlockPoolSlice.java:loadDfsUsed(292)) - Cached dfsUsed found for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1338698873-172.17.0.5-1606980114492/current: 25611
2020-12-03 07:22:01,030 [Thread-200] INFO  impl.FsDatasetImpl (BlockPoolSlice.java:loadDfsUsed(292)) - Cached dfsUsed found for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1338698873-172.17.0.5-1606980114492/current: 25611
2020-12-03 07:22:01,039 [Thread-200] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1338698873-172.17.0.5-1606980114492 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 11ms
2020-12-03 07:22:01,039 [Thread-201] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1338698873-172.17.0.5-1606980114492 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 11ms
2020-12-03 07:22:01,039 [Thread-178] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1338698873-172.17.0.5-1606980114492: 12ms
2020-12-03 07:22:01,040 [Thread-202] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2020-12-03 07:22:01,040 [Thread-203] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2020-12-03 07:22:01,042 [Thread-202] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(925)) - Successfully read replica from cache file : /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1338698873-172.17.0.5-1606980114492/current/replicas
2020-12-03 07:22:01,042 [Thread-203] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(925)) - Successfully read replica from cache file : /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1338698873-172.17.0.5-1606980114492/current/replicas
2020-12-03 07:22:01,042 [Thread-202] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 2ms
2020-12-03 07:22:01,042 [Thread-203] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 2ms
2020-12-03 07:22:01,042 [Thread-178] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1338698873-172.17.0.5-1606980114492: 2ms
2020-12-03 07:22:01,043 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85): no suitable block pools found to scan.  Waiting 1814397592 ms.
2020-12-03 07:22:01,043 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-5b5d6366-8003-4c96-a269-54331e87d267): no suitable block pools found to scan.  Waiting 1814397592 ms.
2020-12-03 07:22:01,043 [Thread-178] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 9:50 AM with interval of 21600000ms
2020-12-03 07:22:01,045 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid bb4cfced-0bdc-453a-99a9-88d3aea04105) service to localhost/127.0.0.1:18020 beginning handshake with NN
2020-12-03 07:22:01,046 [IPC Server handler 5 on default port 18020] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:33955, datanodeUuid=bb4cfced-0bdc-453a-99a9-88d3aea04105, infoPort=42477, infoSecurePort=0, ipcPort=46144, storageInfo=lv=-57;cid=testClusterID;nsid=1772939335;c=1606980114492) storage bb4cfced-0bdc-453a-99a9-88d3aea04105
2020-12-03 07:22:01,047 [IPC Server handler 5 on default port 18020] INFO  net.NetworkTopology (NetworkTopology.java:remove(219)) - Removing a node: /default-rack/127.0.0.1:33955
2020-12-03 07:22:01,047 [IPC Server handler 5 on default port 18020] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:33955
2020-12-03 07:22:01,048 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid bb4cfced-0bdc-453a-99a9-88d3aea04105) service to localhost/127.0.0.1:18020 successfully registered with NN
2020-12-03 07:22:01,048 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (DataNode.java:registerBlockPoolWithSecretManager(1615)) - Block token params received from NN: for block pool BP-1338698873-172.17.0.5-1606980114492 keyUpdateInterval=600 min(s), tokenLifetime=10 min(s)
2020-12-03 07:22:01,048 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:22:01,048 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:18020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=1000
2020-12-03 07:22:01,052 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x58d4b3d17e7a5a23: Processing first storage report for DS-5b5d6366-8003-4c96-a269-54331e87d267 from datanode bb4cfced-0bdc-453a-99a9-88d3aea04105
2020-12-03 07:22:01,053 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x58d4b3d17e7a5a23: from storage DS-5b5d6366-8003-4c96-a269-54331e87d267 node DatanodeRegistration(127.0.0.1:33955, datanodeUuid=bb4cfced-0bdc-453a-99a9-88d3aea04105, infoPort=42477, infoSecurePort=0, ipcPort=46144, storageInfo=lv=-57;cid=testClusterID;nsid=1772939335;c=1606980114492), blocks: 1, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
2020-12-03 07:22:01,053 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x58d4b3d17e7a5a23: Processing first storage report for DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85 from datanode bb4cfced-0bdc-453a-99a9-88d3aea04105
2020-12-03 07:22:01,054 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x58d4b3d17e7a5a23: from storage DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85 node DatanodeRegistration(127.0.0.1:33955, datanodeUuid=bb4cfced-0bdc-453a-99a9-88d3aea04105, infoPort=42477, infoSecurePort=0, ipcPort=46144, storageInfo=lv=-57;cid=testClusterID;nsid=1772939335;c=1606980114492), blocks: 1, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
2020-12-03 07:22:01,055 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x58d4b3d17e7a5a23,  containing 2 storage report(s), of which we sent 2. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 4 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:22:01,055 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:01,126 [IPC Server handler 8 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:22:01,129 [Listener at localhost/46144] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2758)) - Cluster is active
2020-12-03 07:22:01,129 [Listener at localhost/46144] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:stopAndJoinNameNode(2130)) - Shutting down the namenode
2020-12-03 07:22:01,130 [Listener at localhost/46144] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1334)) - Stopping services started for active state
2020-12-03 07:22:01,130 [Listener at localhost/46144] INFO  namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1410)) - Ending log segment 1, 9
2020-12-03 07:22:01,130 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@25748410] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4198)) - LazyPersistFileScrubber was interrupted, exiting
2020-12-03 07:22:01,131 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@485a3466] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4107)) - NameNodeEditLogRoller was interrupted, exiting
2020-12-03 07:22:01,131 [Listener at localhost/46144] INFO  namenode.FSEditLog (FSEditLog.java:printStatistics(778)) - Number of transactions: 10 Total time for transactions(ms): 18 Number of transactions batched in Syncs: 1 Number of syncs: 10 SyncTimes(ms): 1 1 
2020-12-03 07:22:01,133 [Listener at localhost/46144] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(145)) - Finalizing edits file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_inprogress_0000000000000000001 -> /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000001-0000000000000000010
2020-12-03 07:22:01,134 [Listener at localhost/46144] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(145)) - Finalizing edits file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_inprogress_0000000000000000001 -> /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000001-0000000000000000010
2020-12-03 07:22:01,135 [FSEditLogAsync] INFO  namenode.FSEditLog (FSEditLogAsync.java:run(253)) - FSEditLogAsync was interrupted, exiting
2020-12-03 07:22:01,135 [CacheReplicationMonitor(1368553405)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2020-12-03 07:22:01,136 [Listener at localhost/46144] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 18020
2020-12-03 07:22:01,137 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:22:01,137 [IPC Server listener on 18020] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 18020
2020-12-03 07:22:01,137 [RedundancyMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4687)) - Stopping RedundancyMonitor.
2020-12-03 07:22:01,138 [StorageInfoMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4722)) - Stopping thread.
2020-12-03 07:22:01,174 [Listener at localhost/46144] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1334)) - Stopping services started for active state
2020-12-03 07:22:01,175 [Listener at localhost/46144] INFO  namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1434)) - Stopping services started for standby state
2020-12-03 07:22:01,176 [Listener at localhost/46144] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@28b46423{/,null,UNAVAILABLE}{/hdfs}
2020-12-03 07:22:01,178 [Listener at localhost/46144] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@4a3631f8{HTTP/1.1,[http/1.1]}{localhost:19870}
2020-12-03 07:22:01,178 [Listener at localhost/46144] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@1c9b0314{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:22:01,178 [Listener at localhost/46144] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@5efa40fe{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:22:01,192 [Listener at localhost/46144] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:32825
2020-12-03 07:22:01,194 [Listener at localhost/46144] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:01,200 [Listener at localhost/46144] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:32825
2020-12-03 07:22:01,203 [Listener at localhost/46144] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:32825
2020-12-03 07:22:01,204 [Listener at localhost/46144] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:01,214 [Listener at localhost/46144] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:33955
2020-12-03 07:22:01,215 [Listener at localhost/46144] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:01,220 [Listener at localhost/46144] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:32825
2020-12-03 07:22:01,223 [Listener at localhost/46144] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:32825
2020-12-03 07:22:01,225 [Listener at localhost/46144] INFO  namenode.NameNode (NameNode.java:createNameNode(1632)) - createNameNode []
2020-12-03 07:22:01,225 [Listener at localhost/46144] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - NameNode metrics system started (again)
2020-12-03 07:22:01,226 [Listener at localhost/46144] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://localhost:18020
2020-12-03 07:22:01,226 [Listener at localhost/46144] INFO  namenode.NameNode (NameNode.java:<init>(944)) - Clients should use localhost:18020 to access this namenode/service.
2020-12-03 07:22:01,234 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@426e505c] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:22:01,234 [Listener at localhost/46144] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1641)) - Starting Web-server for hdfs at: http://localhost:19870
2020-12-03 07:22:01,234 [Listener at localhost/46144] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:22:01,236 [Listener at localhost/46144] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:22:01,239 [Listener at localhost/46144] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-12-03 07:22:01,241 [Listener at localhost/46144] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:22:01,243 [Listener at localhost/46144] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:22:01,244 [Listener at localhost/46144] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-12-03 07:22:01,244 [Listener at localhost/46144] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:22:01,244 [Listener at localhost/46144] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:22:01,246 [Listener at localhost/46144] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(102)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-12-03 07:22:01,246 [Listener at localhost/46144] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(806)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-12-03 07:22:01,246 [Listener at localhost/46144] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 19870
2020-12-03 07:22:01,247 [Listener at localhost/46144] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:22:01,258 [Listener at localhost/46144] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@4b2a30d{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:22:01,259 [Listener at localhost/46144] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@56ba8773{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:22:01,267 [Listener at localhost/46144] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@878537d{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-12-03 07:22:01,268 [Listener at localhost/46144] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@4455f57d{HTTP/1.1,[http/1.1]}{localhost:19870}
2020-12-03 07:22:01,268 [Listener at localhost/46144] INFO  server.Server (Server.java:doStart(419)) - Started @8697ms
2020-12-03 07:22:01,272 [Listener at localhost/46144] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(229)) - Edit logging is async:true
2020-12-03 07:22:01,286 [Listener at localhost/46144] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(755)) - KeyProvider: null
2020-12-03 07:22:01,286 [Listener at localhost/46144] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(123)) - fsLock is fair: true
2020-12-03 07:22:01,286 [Listener at localhost/46144] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(141)) - Detailed lock hold time metrics enabled: false
2020-12-03 07:22:01,287 [Listener at localhost/46144] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(780)) - fsOwner             = root (auth:SIMPLE)
2020-12-03 07:22:01,287 [Listener at localhost/46144] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(781)) - supergroup          = supergroup
2020-12-03 07:22:01,287 [Listener at localhost/46144] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(782)) - isPermissionEnabled = true
2020-12-03 07:22:01,287 [Listener at localhost/46144] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(793)) - HA Enabled: false
2020-12-03 07:22:01,288 [Listener at localhost/46144] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:22:01,288 [Listener at localhost/46144] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:22:01,289 [Listener at localhost/46144] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(303)) - dfs.block.invalidate.limit: configured=1000, counted=20, effected=1000
2020-12-03 07:22:01,289 [Listener at localhost/46144] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(311)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-12-03 07:22:01,289 [Listener at localhost/46144] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:22:01,289 [Listener at localhost/46144] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-12-03 07:22:01,290 [Listener at localhost/46144] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Dec 03 07:22:01
2020-12-03 07:22:01,290 [Listener at localhost/46144] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-12-03 07:22:01,290 [Listener at localhost/46144] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:22:01,291 [Listener at localhost/46144] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.8 GB = 36.4 MB
2020-12-03 07:22:01,291 [Listener at localhost/46144] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-12-03 07:22:01,295 [Listener at localhost/46144] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:22:01,296 [Listener at localhost/46144] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:22:01,296 [Listener at localhost/46144] INFO  blockmanagement.BlockManager (BlockManager.java:createSPSManager(5183)) - Storage policy satisfier is disabled
2020-12-03 07:22:01,296 [Listener at localhost/46144] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(595)) - dfs.block.access.token.enable = true
2020-12-03 07:22:01,297 [Listener at localhost/46144] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(617)) - dfs.block.access.key.update.interval=600 min(s), dfs.block.access.token.lifetime=600 min(s), dfs.encrypt.data.transfer.algorithm=null
2020-12-03 07:22:01,297 [Listener at localhost/46144] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:22:01,298 [Listener at localhost/46144] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-12-03 07:22:01,298 [Listener at localhost/46144] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-12-03 07:22:01,298 [Listener at localhost/46144] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-12-03 07:22:01,298 [Listener at localhost/46144] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-12-03 07:22:01,299 [Listener at localhost/46144] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(581)) - defaultReplication         = 2
2020-12-03 07:22:01,299 [Listener at localhost/46144] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(582)) - maxReplication             = 512
2020-12-03 07:22:01,299 [Listener at localhost/46144] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(583)) - minReplication             = 1
2020-12-03 07:22:01,299 [Listener at localhost/46144] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(584)) - maxReplicationStreams      = 2
2020-12-03 07:22:01,299 [Listener at localhost/46144] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(585)) - redundancyRecheckInterval  = 3000ms
2020-12-03 07:22:01,300 [Listener at localhost/46144] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(586)) - encryptDataTransfer        = false
2020-12-03 07:22:01,300 [Listener at localhost/46144] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(587)) - maxNumBlocksToLog          = 1000
2020-12-03 07:22:01,300 [Listener at localhost/46144] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-12-03 07:22:01,300 [Listener at localhost/46144] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:22:01,301 [Listener at localhost/46144] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.8 GB = 18.2 MB
2020-12-03 07:22:01,301 [Listener at localhost/46144] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-12-03 07:22:01,304 [Listener at localhost/46144] INFO  namenode.FSDirectory (FSDirectory.java:<init>(290)) - ACLs enabled? false
2020-12-03 07:22:01,305 [Listener at localhost/46144] INFO  namenode.FSDirectory (FSDirectory.java:<init>(294)) - POSIX ACL inheritance enabled? true
2020-12-03 07:22:01,305 [Listener at localhost/46144] INFO  namenode.FSDirectory (FSDirectory.java:<init>(298)) - XAttrs enabled? true
2020-12-03 07:22:01,305 [Listener at localhost/46144] INFO  namenode.NameNode (FSDirectory.java:<init>(362)) - Caching file names occurring more than 10 times
2020-12-03 07:22:01,305 [Listener at localhost/46144] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-12-03 07:22:01,306 [Listener at localhost/46144] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-12-03 07:22:01,306 [Listener at localhost/46144] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-12-03 07:22:01,306 [Listener at localhost/46144] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:22:01,306 [Listener at localhost/46144] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.8 GB = 4.6 MB
2020-12-03 07:22:01,306 [Listener at localhost/46144] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-12-03 07:22:01,307 [Listener at localhost/46144] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-12-03 07:22:01,307 [Listener at localhost/46144] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-12-03 07:22:01,308 [Listener at localhost/46144] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-12-03 07:22:01,308 [Listener at localhost/46144] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(996)) - Retry cache on namenode is enabled
2020-12-03 07:22:01,308 [Listener at localhost/46144] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(1004)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-12-03 07:22:01,308 [Listener at localhost/46144] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-12-03 07:22:01,308 [Listener at localhost/46144] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:22:01,308 [Listener at localhost/46144] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.8 GB = 559.3 KB
2020-12-03 07:22:01,308 [Listener at localhost/46144] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-12-03 07:22:01,378 [Listener at localhost/46144] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/in_use.lock acquired by nodename 5792@13788a696671
2020-12-03 07:22:01,415 [Listener at localhost/46144] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/in_use.lock acquired by nodename 5792@13788a696671
2020-12-03 07:22:01,419 [Listener at localhost/46144] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(428)) - Recovering unfinalized segments in /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current
2020-12-03 07:22:01,421 [Listener at localhost/46144] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(428)) - Recovering unfinalized segments in /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current
2020-12-03 07:22:01,429 [Listener at localhost/46144] INFO  namenode.FSImage (FSImage.java:loadFSImageFile(797)) - Planning to load image: FSImageFile(file=/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2020-12-03 07:22:01,433 [Listener at localhost/46144] INFO  namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(234)) - Loading 1 INodes.
2020-12-03 07:22:01,434 [Listener at localhost/46144] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(246)) - Loaded FSImage in 0 seconds.
2020-12-03 07:22:01,435 [Listener at localhost/46144] INFO  namenode.FSImage (FSImage.java:loadFSImage(978)) - Loaded image for txid 0 from /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000000
2020-12-03 07:22:01,437 [Listener at localhost/46144] INFO  namenode.FSImage (FSImage.java:loadEdits(910)) - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@40ee0a22 expecting start txid #1
2020-12-03 07:22:01,438 [Listener at localhost/46144] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(178)) - Start loading edits file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000001-0000000000000000010, /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000001-0000000000000000010 maxTxnsToRead = 9223372036854775807
2020-12-03 07:22:01,438 [Listener at localhost/46144] INFO  namenode.RedundantEditLogInputStream (RedundantEditLogInputStream.java:nextOp(186)) - Fast-forwarding stream '/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000001-0000000000000000010' to transaction ID 1
2020-12-03 07:22:01,482 [Listener at localhost/46144] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(188)) - Loaded 1 edits file(s) (the last named /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000001-0000000000000000010, /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000001-0000000000000000010) of total size 569.0, total edits 10.0, total load time 19.0 ms
2020-12-03 07:22:01,482 [Listener at localhost/46144] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1110)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-12-03 07:22:01,483 [Listener at localhost/46144] INFO  namenode.FSEditLog (FSEditLog.java:startLogSegment(1365)) - Starting log segment at 11
2020-12-03 07:22:01,560 [Listener at localhost/46144] INFO  namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2020-12-03 07:22:01,561 [Listener at localhost/46144] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(727)) - Finished loading FSImage in 251 msecs
2020-12-03 07:22:01,561 [Listener at localhost/46144] INFO  namenode.NameNode (NameNodeRpcServer.java:<init>(448)) - RPC server is binding to localhost:18020
2020-12-03 07:22:01,562 [Listener at localhost/46144] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:22:01,563 [Socket Reader #1 for port 18020] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 18020
2020-12-03 07:22:01,571 [Listener at localhost/18020] INFO  namenode.FSNamesystem (FSNamesystem.java:registerMBean(5090)) - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2020-12-03 07:22:01,616 [Listener at localhost/18020] INFO  namenode.LeaseManager (LeaseManager.java:getNumUnderConstructionBlocks(171)) - Number of blocks under construction: 0
2020-12-03 07:22:01,620 [Listener at localhost/18020] INFO  hdfs.StateChange (BlockManagerSafeMode.java:reportStatus(617)) - STATE* Safe mode ON. 
The reported blocks 0 needs additional 1 blocks to reach the threshold 0.9990 of total blocks 2.
The minimum number of live datanodes is not required. Safe mode will be turned off automatically once the thresholds have been reached.
2020-12-03 07:22:01,621 [org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager$Monitor@25d3cfc8] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:updateKeys(263)) - Updating block keys
2020-12-03 07:22:01,626 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:22:01,626 [IPC Server listener on 18020] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 18020: starting
2020-12-03 07:22:01,631 [Listener at localhost/18020] INFO  namenode.NameNode (NameNode.java:startCommonServices(828)) - NameNode RPC up at: localhost/127.0.0.1:18020
2020-12-03 07:22:01,632 [Listener at localhost/18020] INFO  namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1222)) - Starting services required for active state
2020-12-03 07:22:01,637 [Listener at localhost/18020] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(777)) - Initializing quota with 4 thread(s)
2020-12-03 07:22:01,639 [Listener at localhost/18020] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(786)) - Quota initialization completed in 2 milliseconds
name space=2
storage space=4096
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2020-12-03 07:22:01,642 [CacheReplicationMonitor(840456706)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-12-03 07:22:01,642 [Listener at localhost/18020] WARN  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitClusterUp(1436)) - Waiting for the Mini HDFS Cluster to start...
2020-12-03 07:22:01,899 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] WARN  datanode.DataNode (BPServiceActor.java:offerService(731)) - IOException in offerService
java.io.EOFException: End of File Exception between local host is: "13788a696671/172.17.0.5"; destination host is: "localhost":18020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:833)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:791)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy22.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:168)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:517)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:648)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:849)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1850)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1183)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1079)
2020-12-03 07:22:02,060 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActor(672)) - DatanodeCommand action : DNA_REGISTER from localhost/127.0.0.1:18020 with active state
2020-12-03 07:22:02,063 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid bb4cfced-0bdc-453a-99a9-88d3aea04105) service to localhost/127.0.0.1:18020 beginning handshake with NN
2020-12-03 07:22:02,064 [IPC Server handler 5 on default port 18020] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:33955, datanodeUuid=bb4cfced-0bdc-453a-99a9-88d3aea04105, infoPort=42477, infoSecurePort=0, ipcPort=46144, storageInfo=lv=-57;cid=testClusterID;nsid=1772939335;c=1606980114492) storage bb4cfced-0bdc-453a-99a9-88d3aea04105
2020-12-03 07:22:02,065 [IPC Server handler 5 on default port 18020] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:33955
2020-12-03 07:22:02,065 [IPC Server handler 5 on default port 18020] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN bb4cfced-0bdc-453a-99a9-88d3aea04105 (127.0.0.1:33955).
2020-12-03 07:22:02,067 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid bb4cfced-0bdc-453a-99a9-88d3aea04105) service to localhost/127.0.0.1:18020 successfully registered with NN
2020-12-03 07:22:02,067 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:22:02,069 [IPC Server handler 4 on default port 18020] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85 for DN 127.0.0.1:33955
2020-12-03 07:22:02,069 [IPC Server handler 4 on default port 18020] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-5b5d6366-8003-4c96-a269-54331e87d267 for DN 127.0.0.1:33955
2020-12-03 07:22:02,072 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x58d4b3d17e7a5a24: Processing first storage report for DS-5b5d6366-8003-4c96-a269-54331e87d267 from datanode bb4cfced-0bdc-453a-99a9-88d3aea04105
2020-12-03 07:22:02,072 [Block report processor] INFO  blockmanagement.BlockManager (BlockManager.java:initializeReplQueues(4922)) - initializing replication queues
2020-12-03 07:22:02,073 [Block report processor] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(395)) - STATE* Safe mode is OFF
2020-12-03 07:22:02,073 [Block report processor] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(400)) - STATE* Leaving safe mode after 0 secs
2020-12-03 07:22:02,074 [Block report processor] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(406)) - STATE* Network topology has 1 racks and 1 datanodes
2020-12-03 07:22:02,074 [Block report processor] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(408)) - STATE* UnderReplicatedBlocks has 0 blocks
2020-12-03 07:22:02,075 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x58d4b3d17e7a5a24: from storage DS-5b5d6366-8003-4c96-a269-54331e87d267 node DatanodeRegistration(127.0.0.1:33955, datanodeUuid=bb4cfced-0bdc-453a-99a9-88d3aea04105, infoPort=42477, infoSecurePort=0, ipcPort=46144, storageInfo=lv=-57;cid=testClusterID;nsid=1772939335;c=1606980114492), blocks: 1, hasStaleStorage: true, processing time: 3 msecs, invalidatedBlocks: 0
2020-12-03 07:22:02,082 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3585)) - Total number of blocks            = 2
2020-12-03 07:22:02,082 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3586)) - Number of invalid blocks          = 0
2020-12-03 07:22:02,082 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3587)) - Number of under-replicated blocks = 2
2020-12-03 07:22:02,082 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3588)) - Number of  over-replicated blocks = 0
2020-12-03 07:22:02,082 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3590)) - Number of blocks being written    = 0
2020-12-03 07:22:02,082 [Reconstruction Queue Initializer] INFO  hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(3593)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 8 msec
2020-12-03 07:22:02,083 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x58d4b3d17e7a5a24: Processing first storage report for DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85 from datanode bb4cfced-0bdc-453a-99a9-88d3aea04105
2020-12-03 07:22:02,086 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x58d4b3d17e7a5a24: from storage DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85 node DatanodeRegistration(127.0.0.1:33955, datanodeUuid=bb4cfced-0bdc-453a-99a9-88d3aea04105, infoPort=42477, infoSecurePort=0, ipcPort=46144, storageInfo=lv=-57;cid=testClusterID;nsid=1772939335;c=1606980114492), blocks: 1, hasStaleStorage: false, processing time: 3 msecs, invalidatedBlocks: 0
2020-12-03 07:22:02,087 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x58d4b3d17e7a5a24,  containing 2 storage report(s), of which we sent 2. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 16 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:22:02,087 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:02,643 [Listener at localhost/18020] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:restartNameNode(2191)) - Restarted the namenode
2020-12-03 07:22:02,644 [Listener at localhost/18020] DEBUG hdfs.DFSClient (DFSClient.java:<init>(318)) - Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
2020-12-03 07:22:02,649 [Listener at localhost/18020] DEBUG hdfs.DFSClient (DFSClient.java:<init>(318)) - Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
2020-12-03 07:22:02,653 [IPC Server handler 0 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:22:02,654 [Listener at localhost/18020] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:22:02,654 [Listener at localhost/18020] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:22:02,756 [IPC Server handler 3 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:22:02,758 [Listener at localhost/18020] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:22:02,758 [Listener at localhost/18020] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:22:02,860 [IPC Server handler 5 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:22:02,861 [Listener at localhost/18020] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:22:02,861 [Listener at localhost/18020] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:22:02,904 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActor(672)) - DatanodeCommand action : DNA_REGISTER from localhost/127.0.0.1:18020 with active state
2020-12-03 07:22:02,906 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid fcfa3dd6-e926-44de-87fa-c54cedb559cc) service to localhost/127.0.0.1:18020 beginning handshake with NN
2020-12-03 07:22:02,907 [IPC Server handler 6 on default port 18020] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:32825, datanodeUuid=fcfa3dd6-e926-44de-87fa-c54cedb559cc, infoPort=44763, infoSecurePort=0, ipcPort=43105, storageInfo=lv=-57;cid=testClusterID;nsid=1772939335;c=1606980114492) storage fcfa3dd6-e926-44de-87fa-c54cedb559cc
2020-12-03 07:22:02,907 [IPC Server handler 6 on default port 18020] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:32825
2020-12-03 07:22:02,907 [IPC Server handler 6 on default port 18020] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN fcfa3dd6-e926-44de-87fa-c54cedb559cc (127.0.0.1:32825).
2020-12-03 07:22:02,908 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid fcfa3dd6-e926-44de-87fa-c54cedb559cc) service to localhost/127.0.0.1:18020 successfully registered with NN
2020-12-03 07:22:02,908 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:22:02,910 [IPC Server handler 8 on default port 18020] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-ef772597-ccee-4e3e-8242-d1e42fd95974 for DN 127.0.0.1:32825
2020-12-03 07:22:02,911 [IPC Server handler 8 on default port 18020] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd for DN 127.0.0.1:32825
2020-12-03 07:22:02,913 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x175089f2dddbc42f: Processing first storage report for DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd from datanode fcfa3dd6-e926-44de-87fa-c54cedb559cc
2020-12-03 07:22:02,913 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x175089f2dddbc42f: from storage DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd node DatanodeRegistration(127.0.0.1:32825, datanodeUuid=fcfa3dd6-e926-44de-87fa-c54cedb559cc, infoPort=44763, infoSecurePort=0, ipcPort=43105, storageInfo=lv=-57;cid=testClusterID;nsid=1772939335;c=1606980114492), blocks: 1, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:22:02,913 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x175089f2dddbc42f: Processing first storage report for DS-ef772597-ccee-4e3e-8242-d1e42fd95974 from datanode fcfa3dd6-e926-44de-87fa-c54cedb559cc
2020-12-03 07:22:02,913 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x175089f2dddbc42f: from storage DS-ef772597-ccee-4e3e-8242-d1e42fd95974 node DatanodeRegistration(127.0.0.1:32825, datanodeUuid=fcfa3dd6-e926-44de-87fa-c54cedb559cc, infoPort=44763, infoSecurePort=0, ipcPort=43105, storageInfo=lv=-57;cid=testClusterID;nsid=1772939335;c=1606980114492), blocks: 1, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:22:02,914 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x175089f2dddbc42f,  containing 2 storage report(s), of which we sent 2. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:22:02,914 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:02,963 [IPC Server handler 7 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:22:02,964 [Listener at localhost/18020] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2758)) - Cluster is active
2020-12-03 07:22:02,964 [Listener at localhost/18020] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:stopAndJoinNameNode(2130)) - Shutting down the namenode
2020-12-03 07:22:02,964 [Listener at localhost/18020] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1334)) - Stopping services started for active state
2020-12-03 07:22:02,964 [Listener at localhost/18020] INFO  namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1410)) - Ending log segment 11, 11
2020-12-03 07:22:02,964 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@4e4c3a38] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4107)) - NameNodeEditLogRoller was interrupted, exiting
2020-12-03 07:22:02,964 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@293cde83] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4198)) - LazyPersistFileScrubber was interrupted, exiting
2020-12-03 07:22:02,967 [Listener at localhost/18020] INFO  namenode.FSEditLog (FSEditLog.java:printStatistics(778)) - Number of transactions: 2 Total time for transactions(ms): 5 Number of transactions batched in Syncs: 10 Number of syncs: 3 SyncTimes(ms): 2 2 
2020-12-03 07:22:02,968 [Listener at localhost/18020] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(145)) - Finalizing edits file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_inprogress_0000000000000000011 -> /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000011-0000000000000000012
2020-12-03 07:22:02,969 [Listener at localhost/18020] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(145)) - Finalizing edits file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_inprogress_0000000000000000011 -> /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000011-0000000000000000012
2020-12-03 07:22:02,969 [FSEditLogAsync] INFO  namenode.FSEditLog (FSEditLogAsync.java:run(253)) - FSEditLogAsync was interrupted, exiting
2020-12-03 07:22:02,970 [CacheReplicationMonitor(840456706)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2020-12-03 07:22:02,971 [Listener at localhost/18020] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 18020
2020-12-03 07:22:02,976 [IPC Server listener on 18020] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 18020
2020-12-03 07:22:02,976 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:22:02,977 [RedundancyMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4687)) - Stopping RedundancyMonitor.
2020-12-03 07:22:02,977 [StorageInfoMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4722)) - Stopping thread.
2020-12-03 07:22:02,986 [Listener at localhost/18020] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1334)) - Stopping services started for active state
2020-12-03 07:22:02,986 [Listener at localhost/18020] INFO  namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1434)) - Stopping services started for standby state
2020-12-03 07:22:02,988 [Listener at localhost/18020] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@878537d{/,null,UNAVAILABLE}{/hdfs}
2020-12-03 07:22:02,990 [Listener at localhost/18020] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@4455f57d{HTTP/1.1,[http/1.1]}{localhost:19870}
2020-12-03 07:22:02,990 [Listener at localhost/18020] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@56ba8773{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:22:02,991 [Listener at localhost/18020] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@4b2a30d{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:22:02,997 [Listener at localhost/18020] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:32825
2020-12-03 07:22:02,999 [Listener at localhost/18020] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:32825
2020-12-03 07:22:03,002 [Listener at localhost/18020] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:32825
2020-12-03 07:22:03,004 [Listener at localhost/18020] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:33955
2020-12-03 07:22:03,006 [Listener at localhost/18020] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:32825
2020-12-03 07:22:03,008 [Listener at localhost/18020] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:32825
2020-12-03 07:22:03,010 [Listener at localhost/18020] INFO  namenode.NameNode (NameNode.java:createNameNode(1632)) - createNameNode []
2020-12-03 07:22:03,010 [Listener at localhost/18020] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - NameNode metrics system started (again)
2020-12-03 07:22:03,011 [Listener at localhost/18020] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://localhost:18020
2020-12-03 07:22:03,011 [Listener at localhost/18020] INFO  namenode.NameNode (NameNode.java:<init>(944)) - Clients should use localhost:18020 to access this namenode/service.
2020-12-03 07:22:03,016 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@4e628b52] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:22:03,016 [Listener at localhost/18020] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1641)) - Starting Web-server for hdfs at: http://localhost:19870
2020-12-03 07:22:03,017 [Listener at localhost/18020] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:22:03,018 [Listener at localhost/18020] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:22:03,019 [Listener at localhost/18020] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-12-03 07:22:03,019 [Listener at localhost/18020] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:22:03,021 [Listener at localhost/18020] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:22:03,022 [Listener at localhost/18020] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-12-03 07:22:03,023 [Listener at localhost/18020] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:22:03,023 [Listener at localhost/18020] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:22:03,025 [Listener at localhost/18020] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(102)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-12-03 07:22:03,025 [Listener at localhost/18020] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(806)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-12-03 07:22:03,026 [Listener at localhost/18020] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 19870
2020-12-03 07:22:03,026 [Listener at localhost/18020] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:22:03,028 [Listener at localhost/18020] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@a64e035{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:22:03,029 [Listener at localhost/18020] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@41c204a0{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:22:03,035 [Listener at localhost/18020] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@78de58ea{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-12-03 07:22:03,037 [Listener at localhost/18020] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@60e5272{HTTP/1.1,[http/1.1]}{localhost:19870}
2020-12-03 07:22:03,037 [Listener at localhost/18020] INFO  server.Server (Server.java:doStart(419)) - Started @10466ms
2020-12-03 07:22:03,040 [Listener at localhost/18020] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(229)) - Edit logging is async:true
2020-12-03 07:22:03,040 [Listener at localhost/18020] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(755)) - KeyProvider: null
2020-12-03 07:22:03,040 [Listener at localhost/18020] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(123)) - fsLock is fair: true
2020-12-03 07:22:03,040 [Listener at localhost/18020] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(141)) - Detailed lock hold time metrics enabled: false
2020-12-03 07:22:03,040 [Listener at localhost/18020] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(780)) - fsOwner             = root (auth:SIMPLE)
2020-12-03 07:22:03,041 [Listener at localhost/18020] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(781)) - supergroup          = supergroup
2020-12-03 07:22:03,041 [Listener at localhost/18020] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(782)) - isPermissionEnabled = true
2020-12-03 07:22:03,041 [Listener at localhost/18020] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(793)) - HA Enabled: false
2020-12-03 07:22:03,041 [Listener at localhost/18020] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:22:03,042 [Listener at localhost/18020] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:22:03,042 [Listener at localhost/18020] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(303)) - dfs.block.invalidate.limit: configured=1000, counted=20, effected=1000
2020-12-03 07:22:03,042 [Listener at localhost/18020] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(311)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-12-03 07:22:03,042 [Listener at localhost/18020] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:22:03,042 [Listener at localhost/18020] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-12-03 07:22:03,043 [Listener at localhost/18020] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Dec 03 07:22:03
2020-12-03 07:22:03,043 [Listener at localhost/18020] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-12-03 07:22:03,043 [Listener at localhost/18020] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:22:03,043 [Listener at localhost/18020] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.8 GB = 36.4 MB
2020-12-03 07:22:03,043 [Listener at localhost/18020] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-12-03 07:22:03,054 [Listener at localhost/18020] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:22:03,055 [Listener at localhost/18020] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:22:03,055 [Listener at localhost/18020] INFO  blockmanagement.BlockManager (BlockManager.java:createSPSManager(5183)) - Storage policy satisfier is disabled
2020-12-03 07:22:03,055 [Listener at localhost/18020] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(595)) - dfs.block.access.token.enable = true
2020-12-03 07:22:03,055 [Listener at localhost/18020] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(617)) - dfs.block.access.key.update.interval=600 min(s), dfs.block.access.token.lifetime=600 min(s), dfs.encrypt.data.transfer.algorithm=null
2020-12-03 07:22:03,056 [Listener at localhost/18020] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:22:03,056 [Listener at localhost/18020] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-12-03 07:22:03,057 [Listener at localhost/18020] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-12-03 07:22:03,057 [Listener at localhost/18020] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-12-03 07:22:03,057 [Listener at localhost/18020] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-12-03 07:22:03,057 [Listener at localhost/18020] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(581)) - defaultReplication         = 2
2020-12-03 07:22:03,057 [Listener at localhost/18020] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(582)) - maxReplication             = 512
2020-12-03 07:22:03,057 [Listener at localhost/18020] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(583)) - minReplication             = 1
2020-12-03 07:22:03,057 [Listener at localhost/18020] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(584)) - maxReplicationStreams      = 2
2020-12-03 07:22:03,057 [Listener at localhost/18020] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(585)) - redundancyRecheckInterval  = 3000ms
2020-12-03 07:22:03,058 [Listener at localhost/18020] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(586)) - encryptDataTransfer        = false
2020-12-03 07:22:03,058 [Listener at localhost/18020] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(587)) - maxNumBlocksToLog          = 1000
2020-12-03 07:22:03,058 [Listener at localhost/18020] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-12-03 07:22:03,059 [Listener at localhost/18020] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:22:03,059 [Listener at localhost/18020] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.8 GB = 18.2 MB
2020-12-03 07:22:03,059 [Listener at localhost/18020] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-12-03 07:22:03,065 [Listener at localhost/18020] INFO  namenode.FSDirectory (FSDirectory.java:<init>(290)) - ACLs enabled? false
2020-12-03 07:22:03,065 [Listener at localhost/18020] INFO  namenode.FSDirectory (FSDirectory.java:<init>(294)) - POSIX ACL inheritance enabled? true
2020-12-03 07:22:03,065 [Listener at localhost/18020] INFO  namenode.FSDirectory (FSDirectory.java:<init>(298)) - XAttrs enabled? true
2020-12-03 07:22:03,065 [Listener at localhost/18020] INFO  namenode.NameNode (FSDirectory.java:<init>(362)) - Caching file names occurring more than 10 times
2020-12-03 07:22:03,066 [Listener at localhost/18020] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-12-03 07:22:03,066 [Listener at localhost/18020] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-12-03 07:22:03,066 [Listener at localhost/18020] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-12-03 07:22:03,066 [Listener at localhost/18020] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:22:03,066 [Listener at localhost/18020] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.8 GB = 4.6 MB
2020-12-03 07:22:03,066 [Listener at localhost/18020] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-12-03 07:22:03,067 [Listener at localhost/18020] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-12-03 07:22:03,068 [Listener at localhost/18020] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-12-03 07:22:03,068 [Listener at localhost/18020] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-12-03 07:22:03,068 [Listener at localhost/18020] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(996)) - Retry cache on namenode is enabled
2020-12-03 07:22:03,068 [Listener at localhost/18020] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(1004)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-12-03 07:22:03,068 [Listener at localhost/18020] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-12-03 07:22:03,068 [Listener at localhost/18020] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:22:03,068 [Listener at localhost/18020] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.8 GB = 559.3 KB
2020-12-03 07:22:03,069 [Listener at localhost/18020] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-12-03 07:22:03,069 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] WARN  datanode.DataNode (BPServiceActor.java:offerService(731)) - IOException in offerService
java.io.EOFException: End of File Exception between local host is: "13788a696671/172.17.0.5"; destination host is: "localhost":18020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:833)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:791)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy22.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:168)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:517)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:648)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:849)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1850)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1183)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1079)
2020-12-03 07:22:03,122 [Listener at localhost/18020] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/in_use.lock acquired by nodename 5792@13788a696671
2020-12-03 07:22:03,155 [Listener at localhost/18020] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/in_use.lock acquired by nodename 5792@13788a696671
2020-12-03 07:22:03,158 [Listener at localhost/18020] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(428)) - Recovering unfinalized segments in /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current
2020-12-03 07:22:03,158 [Listener at localhost/18020] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(428)) - Recovering unfinalized segments in /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current
2020-12-03 07:22:03,159 [Listener at localhost/18020] INFO  namenode.FSImage (FSImage.java:loadFSImageFile(797)) - Planning to load image: FSImageFile(file=/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2020-12-03 07:22:03,162 [Listener at localhost/18020] INFO  namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(234)) - Loading 1 INodes.
2020-12-03 07:22:03,162 [Listener at localhost/18020] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(246)) - Loaded FSImage in 0 seconds.
2020-12-03 07:22:03,162 [Listener at localhost/18020] INFO  namenode.FSImage (FSImage.java:loadFSImage(978)) - Loaded image for txid 0 from /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000000
2020-12-03 07:22:03,163 [Listener at localhost/18020] INFO  namenode.FSImage (FSImage.java:loadEdits(910)) - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@5e9456ae expecting start txid #1
2020-12-03 07:22:03,163 [Listener at localhost/18020] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(178)) - Start loading edits file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000001-0000000000000000010, /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000001-0000000000000000010 maxTxnsToRead = 9223372036854775807
2020-12-03 07:22:03,163 [Listener at localhost/18020] INFO  namenode.RedundantEditLogInputStream (RedundantEditLogInputStream.java:nextOp(186)) - Fast-forwarding stream '/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000001-0000000000000000010' to transaction ID 1
2020-12-03 07:22:03,165 [Listener at localhost/18020] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(188)) - Loaded 1 edits file(s) (the last named /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000001-0000000000000000010, /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000001-0000000000000000010) of total size 569.0, total edits 10.0, total load time 2.0 ms
2020-12-03 07:22:03,166 [Listener at localhost/18020] INFO  namenode.RedundantEditLogInputStream (RedundantEditLogInputStream.java:nextOp(186)) - Fast-forwarding stream '/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000011-0000000000000000012' to transaction ID 1
2020-12-03 07:22:03,166 [Listener at localhost/18020] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1110)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-12-03 07:22:03,167 [Listener at localhost/18020] INFO  namenode.FSEditLog (FSEditLog.java:startLogSegment(1365)) - Starting log segment at 13
2020-12-03 07:22:03,240 [Listener at localhost/18020] INFO  namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2020-12-03 07:22:03,240 [Listener at localhost/18020] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(727)) - Finished loading FSImage in 170 msecs
2020-12-03 07:22:03,241 [Listener at localhost/18020] INFO  namenode.NameNode (NameNodeRpcServer.java:<init>(448)) - RPC server is binding to localhost:18020
2020-12-03 07:22:03,242 [Listener at localhost/18020] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:22:03,243 [Socket Reader #1 for port 18020] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 18020
2020-12-03 07:22:03,248 [Listener at localhost/18020] INFO  namenode.FSNamesystem (FSNamesystem.java:registerMBean(5090)) - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2020-12-03 07:22:03,273 [Listener at localhost/18020] INFO  namenode.LeaseManager (LeaseManager.java:getNumUnderConstructionBlocks(171)) - Number of blocks under construction: 0
2020-12-03 07:22:03,275 [org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager$Monitor@63da207f] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:updateKeys(263)) - Updating block keys
2020-12-03 07:22:03,276 [Listener at localhost/18020] INFO  hdfs.StateChange (BlockManagerSafeMode.java:reportStatus(617)) - STATE* Safe mode ON. 
The reported blocks 0 needs additional 1 blocks to reach the threshold 0.9990 of total blocks 2.
The minimum number of live datanodes is not required. Safe mode will be turned off automatically once the thresholds have been reached.
2020-12-03 07:22:03,282 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:22:03,282 [IPC Server listener on 18020] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 18020: starting
2020-12-03 07:22:03,284 [Listener at localhost/18020] INFO  namenode.NameNode (NameNode.java:startCommonServices(828)) - NameNode RPC up at: localhost/127.0.0.1:18020
2020-12-03 07:22:03,285 [Listener at localhost/18020] INFO  namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1222)) - Starting services required for active state
2020-12-03 07:22:03,285 [Listener at localhost/18020] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(777)) - Initializing quota with 4 thread(s)
2020-12-03 07:22:03,286 [Listener at localhost/18020] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(786)) - Quota initialization completed in 1 milliseconds
name space=2
storage space=4096
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2020-12-03 07:22:03,290 [CacheReplicationMonitor(1138941894)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-12-03 07:22:03,290 [Listener at localhost/18020] WARN  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitClusterUp(1436)) - Waiting for the Mini HDFS Cluster to start...
2020-12-03 07:22:03,916 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActor(672)) - DatanodeCommand action : DNA_REGISTER from localhost/127.0.0.1:18020 with active state
2020-12-03 07:22:03,918 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid fcfa3dd6-e926-44de-87fa-c54cedb559cc) service to localhost/127.0.0.1:18020 beginning handshake with NN
2020-12-03 07:22:03,920 [IPC Server handler 2 on default port 18020] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:32825, datanodeUuid=fcfa3dd6-e926-44de-87fa-c54cedb559cc, infoPort=44763, infoSecurePort=0, ipcPort=43105, storageInfo=lv=-57;cid=testClusterID;nsid=1772939335;c=1606980114492) storage fcfa3dd6-e926-44de-87fa-c54cedb559cc
2020-12-03 07:22:03,920 [IPC Server handler 2 on default port 18020] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:32825
2020-12-03 07:22:03,921 [IPC Server handler 2 on default port 18020] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN fcfa3dd6-e926-44de-87fa-c54cedb559cc (127.0.0.1:32825).
2020-12-03 07:22:03,922 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid fcfa3dd6-e926-44de-87fa-c54cedb559cc) service to localhost/127.0.0.1:18020 successfully registered with NN
2020-12-03 07:22:03,922 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:22:03,923 [IPC Server handler 3 on default port 18020] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-ef772597-ccee-4e3e-8242-d1e42fd95974 for DN 127.0.0.1:32825
2020-12-03 07:22:03,924 [IPC Server handler 3 on default port 18020] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd for DN 127.0.0.1:32825
2020-12-03 07:22:03,926 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x175089f2dddbc430: Processing first storage report for DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd from datanode fcfa3dd6-e926-44de-87fa-c54cedb559cc
2020-12-03 07:22:03,926 [Block report processor] INFO  blockmanagement.BlockManager (BlockManager.java:initializeReplQueues(4922)) - initializing replication queues
2020-12-03 07:22:03,927 [Block report processor] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(395)) - STATE* Safe mode is OFF
2020-12-03 07:22:03,927 [Block report processor] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(400)) - STATE* Leaving safe mode after 0 secs
2020-12-03 07:22:03,927 [Block report processor] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(406)) - STATE* Network topology has 1 racks and 1 datanodes
2020-12-03 07:22:03,927 [Block report processor] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(408)) - STATE* UnderReplicatedBlocks has 0 blocks
2020-12-03 07:22:03,928 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x175089f2dddbc430: from storage DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd node DatanodeRegistration(127.0.0.1:32825, datanodeUuid=fcfa3dd6-e926-44de-87fa-c54cedb559cc, infoPort=44763, infoSecurePort=0, ipcPort=43105, storageInfo=lv=-57;cid=testClusterID;nsid=1772939335;c=1606980114492), blocks: 1, hasStaleStorage: true, processing time: 2 msecs, invalidatedBlocks: 0
2020-12-03 07:22:03,932 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3585)) - Total number of blocks            = 2
2020-12-03 07:22:03,932 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3586)) - Number of invalid blocks          = 0
2020-12-03 07:22:03,932 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3587)) - Number of under-replicated blocks = 2
2020-12-03 07:22:03,932 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3588)) - Number of  over-replicated blocks = 0
2020-12-03 07:22:03,932 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3590)) - Number of blocks being written    = 0
2020-12-03 07:22:03,932 [Reconstruction Queue Initializer] INFO  hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(3593)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 6 msec
2020-12-03 07:22:03,933 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x175089f2dddbc430: Processing first storage report for DS-ef772597-ccee-4e3e-8242-d1e42fd95974 from datanode fcfa3dd6-e926-44de-87fa-c54cedb559cc
2020-12-03 07:22:03,934 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x175089f2dddbc430: from storage DS-ef772597-ccee-4e3e-8242-d1e42fd95974 node DatanodeRegistration(127.0.0.1:32825, datanodeUuid=fcfa3dd6-e926-44de-87fa-c54cedb559cc, infoPort=44763, infoSecurePort=0, ipcPort=43105, storageInfo=lv=-57;cid=testClusterID;nsid=1772939335;c=1606980114492), blocks: 1, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
2020-12-03 07:22:03,935 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x175089f2dddbc430,  containing 2 storage report(s), of which we sent 2. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 10 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:22:03,935 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:04,073 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActor(672)) - DatanodeCommand action : DNA_REGISTER from localhost/127.0.0.1:18020 with active state
2020-12-03 07:22:04,074 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid bb4cfced-0bdc-453a-99a9-88d3aea04105) service to localhost/127.0.0.1:18020 beginning handshake with NN
2020-12-03 07:22:04,075 [IPC Server handler 7 on default port 18020] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:33955, datanodeUuid=bb4cfced-0bdc-453a-99a9-88d3aea04105, infoPort=42477, infoSecurePort=0, ipcPort=46144, storageInfo=lv=-57;cid=testClusterID;nsid=1772939335;c=1606980114492) storage bb4cfced-0bdc-453a-99a9-88d3aea04105
2020-12-03 07:22:04,076 [IPC Server handler 7 on default port 18020] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:33955
2020-12-03 07:22:04,076 [IPC Server handler 7 on default port 18020] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN bb4cfced-0bdc-453a-99a9-88d3aea04105 (127.0.0.1:33955).
2020-12-03 07:22:04,077 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid bb4cfced-0bdc-453a-99a9-88d3aea04105) service to localhost/127.0.0.1:18020 successfully registered with NN
2020-12-03 07:22:04,077 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:22:04,079 [IPC Server handler 8 on default port 18020] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85 for DN 127.0.0.1:33955
2020-12-03 07:22:04,079 [IPC Server handler 8 on default port 18020] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-5b5d6366-8003-4c96-a269-54331e87d267 for DN 127.0.0.1:33955
2020-12-03 07:22:04,082 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x58d4b3d17e7a5a25: Processing first storage report for DS-5b5d6366-8003-4c96-a269-54331e87d267 from datanode bb4cfced-0bdc-453a-99a9-88d3aea04105
2020-12-03 07:22:04,082 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x58d4b3d17e7a5a25: from storage DS-5b5d6366-8003-4c96-a269-54331e87d267 node DatanodeRegistration(127.0.0.1:33955, datanodeUuid=bb4cfced-0bdc-453a-99a9-88d3aea04105, infoPort=42477, infoSecurePort=0, ipcPort=46144, storageInfo=lv=-57;cid=testClusterID;nsid=1772939335;c=1606980114492), blocks: 1, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:22:04,082 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x58d4b3d17e7a5a25: Processing first storage report for DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85 from datanode bb4cfced-0bdc-453a-99a9-88d3aea04105
2020-12-03 07:22:04,083 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x58d4b3d17e7a5a25: from storage DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85 node DatanodeRegistration(127.0.0.1:33955, datanodeUuid=bb4cfced-0bdc-453a-99a9-88d3aea04105, infoPort=42477, infoSecurePort=0, ipcPort=46144, storageInfo=lv=-57;cid=testClusterID;nsid=1772939335;c=1606980114492), blocks: 1, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
2020-12-03 07:22:04,083 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x58d4b3d17e7a5a25,  containing 2 storage report(s), of which we sent 2. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 3 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:22:04,084 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:04,290 [Listener at localhost/18020] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:restartNameNode(2191)) - Restarted the namenode
2020-12-03 07:22:04,291 [Listener at localhost/18020] DEBUG hdfs.DFSClient (DFSClient.java:<init>(318)) - Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
2020-12-03 07:22:04,293 [Listener at localhost/18020] DEBUG hdfs.DFSClient (DFSClient.java:<init>(318)) - Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
2020-12-03 07:22:04,297 [IPC Server handler 0 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:22:04,298 [Listener at localhost/18020] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2758)) - Cluster is active
2020-12-03 07:22:04,298 [Listener at localhost/18020] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:stopDataNode(2331)) - MiniDFSCluster Stopping DataNode 127.0.0.1:33955 from a total of 2 datanodes.
2020-12-03 07:22:04,298 [Listener at localhost/18020] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:22:04,298 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@11a7ba62] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:22:04,299 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85) exiting.
2020-12-03 07:22:04,299 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-5b5d6366-8003-4c96-a269-54331e87d267) exiting.
2020-12-03 07:22:04,324 [Listener at localhost/18020] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@2cfbeac4{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:22:04,325 [Listener at localhost/18020] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@12db3386{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:22:04,325 [Listener at localhost/18020] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@5bfc257{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:22:04,326 [Listener at localhost/18020] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@73877e19{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:22:04,329 [Listener at localhost/18020] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 46144
2020-12-03 07:22:04,333 [IPC Server listener on 46144] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 46144
2020-12-03 07:22:04,334 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:22:04,335 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:22:04,335 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid bb4cfced-0bdc-453a-99a9-88d3aea04105) service to localhost/127.0.0.1:18020
2020-12-03 07:22:04,335 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid bb4cfced-0bdc-453a-99a9-88d3aea04105)
2020-12-03 07:22:04,335 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:04,337 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1338698873-172.17.0.5-1606980114492] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:04,337 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1338698873-172.17.0.5-1606980114492] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:04,345 [Listener at localhost/18020] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:22:04,345 [Listener at localhost/18020] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:22:04,346 [Listener at localhost/18020] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:22:04,346 [Listener at localhost/18020] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:22:04,348 [Listener at localhost/18020] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:22:04,350 [Listener at localhost/18020] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:22:04,355 [Listener at localhost/18020] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:22:04,356 [Listener at localhost/18020] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:22:04,360 [Listener at localhost/18020] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:22:04,360 [Listener at localhost/18020] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:22:04,361 [Listener at localhost/18020] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:22:04,361 [Listener at localhost/18020] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:22:04,361 [Listener at localhost/18020] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:22:04,361 [Listener at localhost/18020] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:22:04,362 [Listener at localhost/18020] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:22:04,363 [Listener at localhost/18020] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:33955
2020-12-03 07:22:04,363 [Listener at localhost/18020] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:22:04,363 [Listener at localhost/18020] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:22:04,365 [Listener at localhost/18020] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:22:04,367 [Listener at localhost/18020] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:22:04,368 [Listener at localhost/18020] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:22:04,368 [Listener at localhost/18020] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:22:04,372 [Listener at localhost/18020] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:22:04,373 [Listener at localhost/18020] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:22:04,373 [Listener at localhost/18020] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:22:04,373 [Listener at localhost/18020] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:22:04,374 [Listener at localhost/18020] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 45064
2020-12-03 07:22:04,374 [Listener at localhost/18020] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:22:04,376 [Listener at localhost/18020] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@51e0301d{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:22:04,377 [Listener at localhost/18020] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@770b3be0{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:22:04,387 [Listener at localhost/18020] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@7ac9af2a{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:22:04,389 [Listener at localhost/18020] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@7bb004b8{HTTP/1.1,[http/1.1]}{localhost:45064}
2020-12-03 07:22:04,389 [Listener at localhost/18020] INFO  server.Server (Server.java:doStart(419)) - Started @11818ms
2020-12-03 07:22:04,415 [Listener at localhost/18020] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:38394
2020-12-03 07:22:04,415 [Listener at localhost/18020] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:22:04,416 [Listener at localhost/18020] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:22:04,415 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@652ce654] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:22:04,416 [Listener at localhost/18020] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:22:04,417 [Socket Reader #1 for port 46144] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 46144
2020-12-03 07:22:04,421 [Listener at localhost/46144] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:46144
2020-12-03 07:22:04,425 [Listener at localhost/46144] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:22:04,426 [Listener at localhost/46144] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:22:04,426 [Thread-301] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:18020 starting to offer service
2020-12-03 07:22:04,437 [IPC Server listener on 46144] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 46144: starting
2020-12-03 07:22:04,440 [Thread-301] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:18020
2020-12-03 07:22:04,438 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:22:04,443 [Thread-301] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:22:04,443 [Listener at localhost/46144] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:restartDataNodes(2519)) - Restarted DataNode 1
2020-12-03 07:22:04,443 [Listener at localhost/46144] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:stopDataNode(2331)) - MiniDFSCluster Stopping DataNode 127.0.0.1:32825 from a total of 2 datanodes.
2020-12-03 07:22:04,447 [Listener at localhost/46144] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:22:04,448 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@35f8a9d3] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:22:04,452 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd) exiting.
2020-12-03 07:22:04,452 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3, DS-ef772597-ccee-4e3e-8242-d1e42fd95974) exiting.
2020-12-03 07:22:04,489 [Listener at localhost/46144] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@4bdc8b5d{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:22:04,490 [Thread-301] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/in_use.lock acquired by nodename 5792@13788a696671
2020-12-03 07:22:04,490 [Listener at localhost/46144] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@3bcd426c{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:22:04,490 [Listener at localhost/46144] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@5972d253{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:22:04,491 [Listener at localhost/46144] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@5e840abf{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:22:04,503 [Listener at localhost/46144] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 43105
2020-12-03 07:22:04,506 [IPC Server listener on 43105] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 43105
2020-12-03 07:22:04,508 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:22:04,509 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:22:04,509 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid fcfa3dd6-e926-44de-87fa-c54cedb559cc) service to localhost/127.0.0.1:18020
2020-12-03 07:22:04,509 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid fcfa3dd6-e926-44de-87fa-c54cedb559cc)
2020-12-03 07:22:04,509 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:04,510 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1338698873-172.17.0.5-1606980114492] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:04,510 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1338698873-172.17.0.5-1606980114492] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:04,523 [Listener at localhost/46144] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:22:04,523 [Listener at localhost/46144] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:22:04,525 [Listener at localhost/46144] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:22:04,525 [Listener at localhost/46144] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:22:04,526 [Listener at localhost/46144] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:22:04,528 [Listener at localhost/46144] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
2020-12-03 07:22:04,529 [Listener at localhost/46144] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:22:04,530 [Listener at localhost/46144] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:22:04,532 [Listener at localhost/46144] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:22:04,533 [Listener at localhost/46144] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:22:04,533 [Listener at localhost/46144] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:22:04,533 [Listener at localhost/46144] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:22:04,534 [Listener at localhost/46144] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:22:04,534 [Listener at localhost/46144] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:22:04,534 [Listener at localhost/46144] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:22:04,535 [Listener at localhost/46144] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:32825
2020-12-03 07:22:04,535 [Listener at localhost/46144] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:22:04,536 [Listener at localhost/46144] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:22:04,538 [Listener at localhost/46144] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:22:04,540 [Listener at localhost/46144] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:22:04,541 [Listener at localhost/46144] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:22:04,541 [Listener at localhost/46144] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:22:04,544 [Listener at localhost/46144] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:22:04,545 [Listener at localhost/46144] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:22:04,545 [Listener at localhost/46144] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:22:04,545 [Listener at localhost/46144] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:22:04,546 [Listener at localhost/46144] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 34205
2020-12-03 07:22:04,546 [Listener at localhost/46144] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:22:04,548 [Listener at localhost/46144] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@6be25526{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:22:04,549 [Listener at localhost/46144] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@66ba7e45{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:22:04,559 [Listener at localhost/46144] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@57adfab0{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:22:04,560 [Listener at localhost/46144] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@1949309d{HTTP/1.1,[http/1.1]}{localhost:34205}
2020-12-03 07:22:04,560 [Listener at localhost/46144] INFO  server.Server (Server.java:doStart(419)) - Started @11989ms
2020-12-03 07:22:04,568 [Thread-301] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/in_use.lock acquired by nodename 5792@13788a696671
2020-12-03 07:22:04,582 [Listener at localhost/46144] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:39424
2020-12-03 07:22:04,583 [Listener at localhost/46144] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:22:04,583 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@99a78d7] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:22:04,583 [Listener at localhost/46144] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:22:04,584 [Listener at localhost/46144] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:22:04,584 [Socket Reader #1 for port 43105] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 43105
2020-12-03 07:22:04,590 [Listener at localhost/43105] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:43105
2020-12-03 07:22:04,600 [Listener at localhost/43105] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:22:04,600 [Listener at localhost/43105] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:22:04,601 [Thread-323] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:18020 starting to offer service
2020-12-03 07:22:04,605 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:22:04,605 [IPC Server listener on 43105] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 43105: starting
2020-12-03 07:22:04,618 [Listener at localhost/43105] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:restartDataNodes(2519)) - Restarted DataNode 0
2020-12-03 07:22:04,621 [Listener at localhost/43105] DEBUG hdfs.DFSClient (DFSClient.java:<init>(318)) - Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
2020-12-03 07:22:04,625 [IPC Server handler 3 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:22:04,626 [Thread-323] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:18020
2020-12-03 07:22:04,629 [Thread-323] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:22:04,629 [Listener at localhost/43105] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2789)) - !dn.datanode.isDatanodeFullyStarted()
2020-12-03 07:22:04,630 [Listener at localhost/43105] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:22:04,650 [Thread-301] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:04,650 [Thread-301] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:04,686 [Thread-323] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/in_use.lock acquired by nodename 5792@13788a696671
2020-12-03 07:22:04,724 [Thread-301] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:04,725 [Thread-301] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:04,742 [IPC Server handler 4 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:22:04,743 [Listener at localhost/43105] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2789)) - !dn.datanode.isDatanodeFullyStarted()
2020-12-03 07:22:04,743 [Listener at localhost/43105] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:22:04,792 [Thread-301] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=1772939335;bpid=BP-1338698873-172.17.0.5-1606980114492;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=1772939335;c=1606980114492;bpid=BP-1338698873-172.17.0.5-1606980114492;dnuuid=bb4cfced-0bdc-453a-99a9-88d3aea04105
2020-12-03 07:22:04,795 [Thread-301] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85
2020-12-03 07:22:04,797 [Thread-301] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, StorageType: DISK
2020-12-03 07:22:04,799 [Thread-301] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-5b5d6366-8003-4c96-a269-54331e87d267
2020-12-03 07:22:04,799 [Thread-301] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, StorageType: DISK
2020-12-03 07:22:04,801 [Thread-301] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:22:04,802 [Thread-301] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:22:04,803 [Thread-301] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:22:04,803 [Thread-301] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:22:04,803 [Thread-301] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:22:04,806 [Thread-301] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:04,807 [Thread-336] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2020-12-03 07:22:04,807 [Thread-337] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2020-12-03 07:22:04,809 [Thread-337] INFO  impl.FsDatasetImpl (BlockPoolSlice.java:loadDfsUsed(292)) - Cached dfsUsed found for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1338698873-172.17.0.5-1606980114492/current: 25611
2020-12-03 07:22:04,809 [Thread-336] INFO  impl.FsDatasetImpl (BlockPoolSlice.java:loadDfsUsed(292)) - Cached dfsUsed found for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1338698873-172.17.0.5-1606980114492/current: 25611
2020-12-03 07:22:04,817 [Thread-336] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1338698873-172.17.0.5-1606980114492 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 10ms
2020-12-03 07:22:04,819 [Thread-337] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1338698873-172.17.0.5-1606980114492 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 13ms
2020-12-03 07:22:04,820 [Thread-301] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1338698873-172.17.0.5-1606980114492: 14ms
2020-12-03 07:22:04,820 [Thread-338] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2020-12-03 07:22:04,820 [Thread-339] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2020-12-03 07:22:04,823 [Thread-339] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(925)) - Successfully read replica from cache file : /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1338698873-172.17.0.5-1606980114492/current/replicas
2020-12-03 07:22:04,823 [Thread-338] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(925)) - Successfully read replica from cache file : /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1338698873-172.17.0.5-1606980114492/current/replicas
2020-12-03 07:22:04,823 [Thread-339] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 2ms
2020-12-03 07:22:04,823 [Thread-338] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 3ms
2020-12-03 07:22:04,824 [Thread-301] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1338698873-172.17.0.5-1606980114492: 4ms
2020-12-03 07:22:04,825 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85): no suitable block pools found to scan.  Waiting 1814393810 ms.
2020-12-03 07:22:04,825 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-5b5d6366-8003-4c96-a269-54331e87d267): no suitable block pools found to scan.  Waiting 1814393810 ms.
2020-12-03 07:22:04,825 [Thread-301] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 9:01 AM with interval of 21600000ms
2020-12-03 07:22:04,827 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid bb4cfced-0bdc-453a-99a9-88d3aea04105) service to localhost/127.0.0.1:18020 beginning handshake with NN
2020-12-03 07:22:04,828 [IPC Server handler 5 on default port 18020] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:33955, datanodeUuid=bb4cfced-0bdc-453a-99a9-88d3aea04105, infoPort=38394, infoSecurePort=0, ipcPort=46144, storageInfo=lv=-57;cid=testClusterID;nsid=1772939335;c=1606980114492) storage bb4cfced-0bdc-453a-99a9-88d3aea04105
2020-12-03 07:22:04,829 [IPC Server handler 5 on default port 18020] INFO  net.NetworkTopology (NetworkTopology.java:remove(219)) - Removing a node: /default-rack/127.0.0.1:33955
2020-12-03 07:22:04,829 [IPC Server handler 5 on default port 18020] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:33955
2020-12-03 07:22:04,830 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid bb4cfced-0bdc-453a-99a9-88d3aea04105) service to localhost/127.0.0.1:18020 successfully registered with NN
2020-12-03 07:22:04,830 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (DataNode.java:registerBlockPoolWithSecretManager(1615)) - Block token params received from NN: for block pool BP-1338698873-172.17.0.5-1606980114492 keyUpdateInterval=600 min(s), tokenLifetime=600 min(s)
2020-12-03 07:22:04,830 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:22:04,831 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:18020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=1000
2020-12-03 07:22:04,836 [Thread-323] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/in_use.lock acquired by nodename 5792@13788a696671
2020-12-03 07:22:04,841 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x7744ebc2ae68a7ac: Processing first storage report for DS-5b5d6366-8003-4c96-a269-54331e87d267 from datanode bb4cfced-0bdc-453a-99a9-88d3aea04105
2020-12-03 07:22:04,842 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x7744ebc2ae68a7ac: from storage DS-5b5d6366-8003-4c96-a269-54331e87d267 node DatanodeRegistration(127.0.0.1:33955, datanodeUuid=bb4cfced-0bdc-453a-99a9-88d3aea04105, infoPort=38394, infoSecurePort=0, ipcPort=46144, storageInfo=lv=-57;cid=testClusterID;nsid=1772939335;c=1606980114492), blocks: 1, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
2020-12-03 07:22:04,842 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x7744ebc2ae68a7ac: Processing first storage report for DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85 from datanode bb4cfced-0bdc-453a-99a9-88d3aea04105
2020-12-03 07:22:04,842 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x7744ebc2ae68a7ac: from storage DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85 node DatanodeRegistration(127.0.0.1:33955, datanodeUuid=bb4cfced-0bdc-453a-99a9-88d3aea04105, infoPort=38394, infoSecurePort=0, ipcPort=46144, storageInfo=lv=-57;cid=testClusterID;nsid=1772939335;c=1606980114492), blocks: 1, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:22:04,843 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x7744ebc2ae68a7ac,  containing 2 storage report(s), of which we sent 2. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 3 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:22:04,843 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:04,850 [IPC Server handler 8 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:22:04,851 [Listener at localhost/43105] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2789)) - !dn.datanode.isDatanodeFullyStarted()
2020-12-03 07:22:04,851 [Listener at localhost/43105] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:22:04,901 [Thread-323] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:04,901 [Thread-323] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:04,953 [IPC Server handler 9 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:22:04,955 [Listener at localhost/43105] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2789)) - !dn.datanode.isDatanodeFullyStarted()
2020-12-03 07:22:04,955 [Listener at localhost/43105] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:22:04,972 [Thread-323] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:04,973 [Thread-323] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:05,006 [Thread-323] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=1772939335;bpid=BP-1338698873-172.17.0.5-1606980114492;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=1772939335;c=1606980114492;bpid=BP-1338698873-172.17.0.5-1606980114492;dnuuid=fcfa3dd6-e926-44de-87fa-c54cedb559cc
2020-12-03 07:22:05,016 [Thread-323] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-ef772597-ccee-4e3e-8242-d1e42fd95974
2020-12-03 07:22:05,018 [Thread-323] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3, StorageType: DISK
2020-12-03 07:22:05,021 [Thread-323] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd
2020-12-03 07:22:05,023 [Thread-323] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, StorageType: DISK
2020-12-03 07:22:05,023 [Thread-323] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:22:05,025 [Thread-323] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
2020-12-03 07:22:05,026 [Thread-323] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
2020-12-03 07:22:05,026 [Thread-323] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:22:05,027 [Thread-323] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:22:05,029 [Thread-323] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:05,029 [Thread-345] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3...
2020-12-03 07:22:05,029 [Thread-346] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4...
2020-12-03 07:22:05,031 [Thread-345] INFO  impl.FsDatasetImpl (BlockPoolSlice.java:loadDfsUsed(292)) - Cached dfsUsed found for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1338698873-172.17.0.5-1606980114492/current: 25611
2020-12-03 07:22:05,031 [Thread-346] INFO  impl.FsDatasetImpl (BlockPoolSlice.java:loadDfsUsed(292)) - Cached dfsUsed found for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1338698873-172.17.0.5-1606980114492/current: 25611
2020-12-03 07:22:05,039 [Thread-346] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1338698873-172.17.0.5-1606980114492 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4: 10ms
2020-12-03 07:22:05,040 [Thread-345] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1338698873-172.17.0.5-1606980114492 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3: 11ms
2020-12-03 07:22:05,042 [Thread-323] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1338698873-172.17.0.5-1606980114492: 13ms
2020-12-03 07:22:05,042 [Thread-347] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3...
2020-12-03 07:22:05,043 [Thread-348] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4...
2020-12-03 07:22:05,044 [Thread-348] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(925)) - Successfully read replica from cache file : /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1338698873-172.17.0.5-1606980114492/current/replicas
2020-12-03 07:22:05,044 [Thread-347] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(925)) - Successfully read replica from cache file : /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1338698873-172.17.0.5-1606980114492/current/replicas
2020-12-03 07:22:05,045 [Thread-348] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4: 2ms
2020-12-03 07:22:05,045 [Thread-347] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3: 2ms
2020-12-03 07:22:05,046 [Thread-323] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1338698873-172.17.0.5-1606980114492: 4ms
2020-12-03 07:22:05,047 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd): no suitable block pools found to scan.  Waiting 1814393588 ms.
2020-12-03 07:22:05,048 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3, DS-ef772597-ccee-4e3e-8242-d1e42fd95974): no suitable block pools found to scan.  Waiting 1814393587 ms.
2020-12-03 07:22:05,048 [Thread-323] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 10:18 AM with interval of 21600000ms
2020-12-03 07:22:05,050 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid fcfa3dd6-e926-44de-87fa-c54cedb559cc) service to localhost/127.0.0.1:18020 beginning handshake with NN
2020-12-03 07:22:05,052 [IPC Server handler 0 on default port 18020] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:32825, datanodeUuid=fcfa3dd6-e926-44de-87fa-c54cedb559cc, infoPort=39424, infoSecurePort=0, ipcPort=43105, storageInfo=lv=-57;cid=testClusterID;nsid=1772939335;c=1606980114492) storage fcfa3dd6-e926-44de-87fa-c54cedb559cc
2020-12-03 07:22:05,052 [IPC Server handler 0 on default port 18020] INFO  net.NetworkTopology (NetworkTopology.java:remove(219)) - Removing a node: /default-rack/127.0.0.1:32825
2020-12-03 07:22:05,053 [IPC Server handler 0 on default port 18020] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:32825
2020-12-03 07:22:05,054 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid fcfa3dd6-e926-44de-87fa-c54cedb559cc) service to localhost/127.0.0.1:18020 successfully registered with NN
2020-12-03 07:22:05,054 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (DataNode.java:registerBlockPoolWithSecretManager(1615)) - Block token params received from NN: for block pool BP-1338698873-172.17.0.5-1606980114492 keyUpdateInterval=600 min(s), tokenLifetime=600 min(s)
2020-12-03 07:22:05,055 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:22:05,055 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:18020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=1000
2020-12-03 07:22:05,068 [IPC Server handler 1 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:22:05,072 [Listener at localhost/43105] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2758)) - Cluster is active
2020-12-03 07:22:05,072 [Listener at localhost/43105] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:stopAndJoinNameNode(2130)) - Shutting down the namenode
2020-12-03 07:22:05,072 [Listener at localhost/43105] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1334)) - Stopping services started for active state
2020-12-03 07:22:05,073 [Listener at localhost/43105] INFO  namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1410)) - Ending log segment 13, 13
2020-12-03 07:22:05,073 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@5631962] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4198)) - LazyPersistFileScrubber was interrupted, exiting
2020-12-03 07:22:05,077 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@1dcca8d3] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4107)) - NameNodeEditLogRoller was interrupted, exiting
2020-12-03 07:22:05,078 [Listener at localhost/43105] INFO  namenode.FSEditLog (FSEditLog.java:printStatistics(778)) - Number of transactions: 2 Total time for transactions(ms): 5 Number of transactions batched in Syncs: 12 Number of syncs: 3 SyncTimes(ms): 2 1 
2020-12-03 07:22:05,079 [Listener at localhost/43105] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(145)) - Finalizing edits file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_inprogress_0000000000000000013 -> /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000013-0000000000000000014
2020-12-03 07:22:05,079 [Listener at localhost/43105] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(145)) - Finalizing edits file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_inprogress_0000000000000000013 -> /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000013-0000000000000000014
2020-12-03 07:22:05,080 [FSEditLogAsync] INFO  namenode.FSEditLog (FSEditLogAsync.java:run(253)) - FSEditLogAsync was interrupted, exiting
2020-12-03 07:22:05,080 [CacheReplicationMonitor(1138941894)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2020-12-03 07:22:05,080 [Listener at localhost/43105] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 18020
2020-12-03 07:22:05,086 [IPC Server listener on 18020] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 18020
2020-12-03 07:22:05,087 [StorageInfoMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4722)) - Stopping thread.
2020-12-03 07:22:05,088 [IPC Server handler 2 on default port 18020] WARN  ipc.Server (Server.java:processResponse(1670)) - IPC Server handler 2 on default port 18020, call Call#92 Retry#0 org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.sendHeartbeat from 127.0.0.1:46774: output error
2020-12-03 07:22:05,086 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:22:05,089 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] WARN  datanode.DataNode (BPServiceActor.java:offerService(731)) - IOException in offerService
java.io.EOFException: End of File Exception between local host is: "13788a696671/172.17.0.5"; destination host is: "localhost":18020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:833)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:791)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy22.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:168)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:517)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:648)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:849)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1850)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1183)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1079)
2020-12-03 07:22:05,087 [RedundancyMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4687)) - Stopping RedundancyMonitor.
2020-12-03 07:22:05,090 [IPC Server handler 2 on default port 18020] INFO  ipc.Server (Server.java:run(2928)) - IPC Server handler 2 on default port 18020 caught an exception
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:475)
	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3550)
	at org.apache.hadoop.ipc.Server.access$1700(Server.java:139)
	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1620)
	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1690)
	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2785)
	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1762)
	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1081)
	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:873)
	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:859)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1016)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)
2020-12-03 07:22:05,101 [Listener at localhost/43105] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1334)) - Stopping services started for active state
2020-12-03 07:22:05,102 [Listener at localhost/43105] INFO  namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1434)) - Stopping services started for standby state
2020-12-03 07:22:05,103 [Listener at localhost/43105] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@78de58ea{/,null,UNAVAILABLE}{/hdfs}
2020-12-03 07:22:05,105 [Listener at localhost/43105] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@60e5272{HTTP/1.1,[http/1.1]}{localhost:19870}
2020-12-03 07:22:05,105 [Listener at localhost/43105] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@41c204a0{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:22:05,106 [Listener at localhost/43105] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@a64e035{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:22:05,112 [Listener at localhost/43105] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:32825
2020-12-03 07:22:05,114 [Listener at localhost/43105] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:05,118 [DataXceiver for client DFSClient_NONMAPREDUCE_-2074459051_1 at /127.0.0.1:50384 [Sending block BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001]] WARN  datanode.DataNode (DataXceiver.java:checkAccess(1449)) - Block token verification failed: op=READ_BLOCK, remoteAddress=/127.0.0.1:50384, message=Can't re-compute password for block_token_identifier (expiryDate=1606980720165, keyId=38854922, userId=root, blockPoolId=BP-1338698873-172.17.0.5-1606980114492, blockId=1073741825, access modes=[READ], storageTypes= [DISK, DISK], storageIds= [DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85, DS-ef772597-ccee-4e3e-8242-d1e42fd95974]), since the required block key (keyID=38854922) doesn't exist.
2020-12-03 07:22:05,118 [Listener at localhost/43105] DEBUG hdfs.DFSClient (DFSInputStream.java:tokenRefetchNeeded(1298)) - Access token was invalid when connecting to /127.0.0.1:32825: {}
org.apache.hadoop.hdfs.security.token.block.InvalidBlockTokenException: Got access token error, status message , for OP_READ_BLOCK, self=/127.0.0.1:50384, remote=/127.0.0.1:32825, for file /fileToRead.dat, for pool BP-1338698873-172.17.0.5-1606980114492 block 1073741825_1001
	at org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:119)
	at org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:110)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.checkSuccess(BlockReaderRemote.java:440)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.newBlockReader(BlockReaderRemote.java:408)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReader(BlockReaderFactory.java:854)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:750)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:380)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:644)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:575)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:757)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:829)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.checkFile1(TestBlockTokenWithDFS.java:102)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.doTestRead(TestBlockTokenWithDFS.java:579)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.testRead(TestBlockTokenWithDFS.java:360)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-12-03 07:22:05,120 [Listener at localhost/43105] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:33955
2020-12-03 07:22:05,121 [Listener at localhost/43105] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:05,125 [DataXceiver for client DFSClient_NONMAPREDUCE_-2074459051_1 at /127.0.0.1:56752 [Sending block BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001]] WARN  datanode.DataNode (DataXceiver.java:checkAccess(1449)) - Block token verification failed: op=READ_BLOCK, remoteAddress=/127.0.0.1:56752, message=Can't re-compute password for block_token_identifier (expiryDate=1606980720165, keyId=38854922, userId=root, blockPoolId=BP-1338698873-172.17.0.5-1606980114492, blockId=1073741825, access modes=[READ], storageTypes= [DISK, DISK], storageIds= [DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85, DS-ef772597-ccee-4e3e-8242-d1e42fd95974]), since the required block key (keyID=38854922) doesn't exist.
2020-12-03 07:22:05,126 [Listener at localhost/43105] DEBUG hdfs.DFSClient (DFSInputStream.java:tokenRefetchNeeded(1298)) - Access token was invalid when connecting to /127.0.0.1:33955: {}
org.apache.hadoop.hdfs.security.token.block.InvalidBlockTokenException: Got access token error, status message , for OP_READ_BLOCK, self=/127.0.0.1:56752, remote=/127.0.0.1:33955, for file /fileToRead.dat, for pool BP-1338698873-172.17.0.5-1606980114492 block 1073741825_1001
	at org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:119)
	at org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:110)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.checkSuccess(BlockReaderRemote.java:440)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.newBlockReader(BlockReaderRemote.java:408)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReader(BlockReaderFactory.java:854)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:750)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:380)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:644)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:575)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:757)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:829)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.checkFile1(TestBlockTokenWithDFS.java:102)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.doTestRead(TestBlockTokenWithDFS.java:579)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.testRead(TestBlockTokenWithDFS.java:360)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-12-03 07:22:05,131 [Listener at localhost/43105] WARN  hdfs.DFSClient (DFSInputStream.java:readWithStrategy(786)) - DFS Read
java.net.ConnectException: Call From 13788a696671/172.17.0.5 to localhost:18020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:833)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy23.getBlockLocations(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:324)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy24.getBlockLocations(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:864)
	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:853)
	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:842)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchBlockAt(DFSInputStream.java:455)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchBlockAt(DFSInputStream.java:441)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:594)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:757)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:829)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.checkFile1(TestBlockTokenWithDFS.java:102)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.doTestRead(TestBlockTokenWithDFS.java:579)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.testRead(TestBlockTokenWithDFS.java:360)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:533)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 52 more
2020-12-03 07:22:05,132 [Listener at localhost/43105] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:32825
2020-12-03 07:22:05,133 [Listener at localhost/43105] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:05,134 [DataXceiver for client DFSClient_NONMAPREDUCE_-2074459051_1 at /127.0.0.1:50390 [Sending block BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001]] WARN  datanode.DataNode (DataXceiver.java:checkAccess(1449)) - Block token verification failed: op=READ_BLOCK, remoteAddress=/127.0.0.1:50390, message=Can't re-compute password for block_token_identifier (expiryDate=1606980720194, keyId=38854922, userId=root, blockPoolId=BP-1338698873-172.17.0.5-1606980114492, blockId=1073741825, access modes=[READ], storageTypes= [DISK, DISK], storageIds= [DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85, DS-ef772597-ccee-4e3e-8242-d1e42fd95974]), since the required block key (keyID=38854922) doesn't exist.
2020-12-03 07:22:05,136 [Listener at localhost/43105] DEBUG hdfs.DFSClient (DFSInputStream.java:tokenRefetchNeeded(1298)) - Access token was invalid when connecting to /127.0.0.1:32825: {}
org.apache.hadoop.hdfs.security.token.block.InvalidBlockTokenException: Got access token error, status message , for OP_READ_BLOCK, self=/127.0.0.1:50390, remote=/127.0.0.1:32825, for file /fileToRead.dat, for pool BP-1338698873-172.17.0.5-1606980114492 block 1073741825_1001
	at org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:119)
	at org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:110)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.checkSuccess(BlockReaderRemote.java:440)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.newBlockReader(BlockReaderRemote.java:408)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReader(BlockReaderFactory.java:854)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:750)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:380)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:644)
	at org.apache.hadoop.hdfs.DFSInputStream.actualGetFromOneDataNode(DFSInputStream.java:1049)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchBlockByteRange(DFSInputStream.java:1001)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:92)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.checkFile2(TestBlockTokenWithDFS.java:116)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.doTestRead(TestBlockTokenWithDFS.java:581)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.testRead(TestBlockTokenWithDFS.java:360)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-12-03 07:22:05,138 [Listener at localhost/43105] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:05,139 [DataXceiver for client DFSClient_NONMAPREDUCE_-2074459051_1 at /127.0.0.1:50394 [Sending block BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001]] WARN  datanode.DataNode (DataXceiver.java:checkAccess(1449)) - Block token verification failed: op=READ_BLOCK, remoteAddress=/127.0.0.1:50394, message=Can't re-compute password for block_token_identifier (expiryDate=1606980720194, keyId=38854922, userId=root, blockPoolId=BP-1338698873-172.17.0.5-1606980114492, blockId=1073741825, access modes=[READ], storageTypes= [DISK, DISK], storageIds= [DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85, DS-ef772597-ccee-4e3e-8242-d1e42fd95974]), since the required block key (keyID=38854922) doesn't exist.
2020-12-03 07:22:05,141 [Listener at localhost/43105] WARN  hdfs.DFSClient (DFSInputStream.java:actualGetFromOneDataNode(1107)) - Connection failure: Failed to connect to /127.0.0.1:32825 for file /fileToRead.dat for block BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001:org.apache.hadoop.hdfs.security.token.block.InvalidBlockTokenException: Got access token error, status message , for OP_READ_BLOCK, self=/127.0.0.1:50394, remote=/127.0.0.1:32825, for file /fileToRead.dat, for pool BP-1338698873-172.17.0.5-1606980114492 block 1073741825_1001
org.apache.hadoop.hdfs.security.token.block.InvalidBlockTokenException: Got access token error, status message , for OP_READ_BLOCK, self=/127.0.0.1:50394, remote=/127.0.0.1:32825, for file /fileToRead.dat, for pool BP-1338698873-172.17.0.5-1606980114492 block 1073741825_1001
	at org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:119)
	at org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:110)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.checkSuccess(BlockReaderRemote.java:440)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.newBlockReader(BlockReaderRemote.java:408)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReader(BlockReaderFactory.java:854)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:750)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:380)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:644)
	at org.apache.hadoop.hdfs.DFSInputStream.actualGetFromOneDataNode(DFSInputStream.java:1049)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchBlockByteRange(DFSInputStream.java:1001)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:92)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.checkFile2(TestBlockTokenWithDFS.java:116)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.doTestRead(TestBlockTokenWithDFS.java:581)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.testRead(TestBlockTokenWithDFS.java:360)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-12-03 07:22:05,142 [Listener at localhost/43105] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:33955
2020-12-03 07:22:05,143 [Listener at localhost/43105] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:05,144 [DataXceiver for client DFSClient_NONMAPREDUCE_-2074459051_1 at /127.0.0.1:56762 [Sending block BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001]] WARN  datanode.DataNode (DataXceiver.java:checkAccess(1449)) - Block token verification failed: op=READ_BLOCK, remoteAddress=/127.0.0.1:56762, message=Can't re-compute password for block_token_identifier (expiryDate=1606980720194, keyId=38854922, userId=root, blockPoolId=BP-1338698873-172.17.0.5-1606980114492, blockId=1073741825, access modes=[READ], storageTypes= [DISK, DISK], storageIds= [DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85, DS-ef772597-ccee-4e3e-8242-d1e42fd95974]), since the required block key (keyID=38854922) doesn't exist.
2020-12-03 07:22:05,145 [Listener at localhost/43105] DEBUG hdfs.DFSClient (DFSInputStream.java:tokenRefetchNeeded(1298)) - Access token was invalid when connecting to /127.0.0.1:33955: {}
org.apache.hadoop.hdfs.security.token.block.InvalidBlockTokenException: Got access token error, status message , for OP_READ_BLOCK, self=/127.0.0.1:56762, remote=/127.0.0.1:33955, for file /fileToRead.dat, for pool BP-1338698873-172.17.0.5-1606980114492 block 1073741825_1001
	at org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:119)
	at org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:110)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.checkSuccess(BlockReaderRemote.java:440)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.newBlockReader(BlockReaderRemote.java:408)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReader(BlockReaderFactory.java:854)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:750)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:380)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:644)
	at org.apache.hadoop.hdfs.DFSInputStream.actualGetFromOneDataNode(DFSInputStream.java:1049)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchBlockByteRange(DFSInputStream.java:1001)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:92)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.checkFile2(TestBlockTokenWithDFS.java:116)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.doTestRead(TestBlockTokenWithDFS.java:581)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.testRead(TestBlockTokenWithDFS.java:360)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-12-03 07:22:05,147 [Listener at localhost/43105] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:05,148 [DataXceiver for client DFSClient_NONMAPREDUCE_-2074459051_1 at /127.0.0.1:56766 [Sending block BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001]] WARN  datanode.DataNode (DataXceiver.java:checkAccess(1449)) - Block token verification failed: op=READ_BLOCK, remoteAddress=/127.0.0.1:56766, message=Can't re-compute password for block_token_identifier (expiryDate=1606980720194, keyId=38854922, userId=root, blockPoolId=BP-1338698873-172.17.0.5-1606980114492, blockId=1073741825, access modes=[READ], storageTypes= [DISK, DISK], storageIds= [DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85, DS-ef772597-ccee-4e3e-8242-d1e42fd95974]), since the required block key (keyID=38854922) doesn't exist.
2020-12-03 07:22:05,150 [Listener at localhost/43105] WARN  hdfs.DFSClient (DFSInputStream.java:actualGetFromOneDataNode(1107)) - Connection failure: Failed to connect to /127.0.0.1:33955 for file /fileToRead.dat for block BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001:org.apache.hadoop.hdfs.security.token.block.InvalidBlockTokenException: Got access token error, status message , for OP_READ_BLOCK, self=/127.0.0.1:56766, remote=/127.0.0.1:33955, for file /fileToRead.dat, for pool BP-1338698873-172.17.0.5-1606980114492 block 1073741825_1001
org.apache.hadoop.hdfs.security.token.block.InvalidBlockTokenException: Got access token error, status message , for OP_READ_BLOCK, self=/127.0.0.1:56766, remote=/127.0.0.1:33955, for file /fileToRead.dat, for pool BP-1338698873-172.17.0.5-1606980114492 block 1073741825_1001
	at org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:119)
	at org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:110)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.checkSuccess(BlockReaderRemote.java:440)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.newBlockReader(BlockReaderRemote.java:408)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReader(BlockReaderFactory.java:854)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:750)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:380)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:644)
	at org.apache.hadoop.hdfs.DFSInputStream.actualGetFromOneDataNode(DFSInputStream.java:1049)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchBlockByteRange(DFSInputStream.java:1001)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:92)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.checkFile2(TestBlockTokenWithDFS.java:116)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.doTestRead(TestBlockTokenWithDFS.java:581)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.testRead(TestBlockTokenWithDFS.java:360)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-12-03 07:22:05,150 [Listener at localhost/43105] WARN  hdfs.DFSClient (DFSInputStream.java:reportLostBlock(963)) - No live nodes contain block BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001 after checking nodes = [DatanodeInfoWithStorage[127.0.0.1:32825,DS-ef772597-ccee-4e3e-8242-d1e42fd95974,DISK], DatanodeInfoWithStorage[127.0.0.1:33955,DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85,DISK]], ignoredNodes = null
2020-12-03 07:22:05,151 [Listener at localhost/43105] INFO  hdfs.DFSClient (DFSInputStream.java:refetchLocations(886)) - Could not obtain BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[127.0.0.1:32825,DS-ef772597-ccee-4e3e-8242-d1e42fd95974,DISK] DatanodeInfoWithStorage[127.0.0.1:33955,DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85,DISK] Dead nodes:  DatanodeInfoWithStorage[127.0.0.1:33955,DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85,DISK] DatanodeInfoWithStorage[127.0.0.1:32825,DS-ef772597-ccee-4e3e-8242-d1e42fd95974,DISK]. Will get new block locations from namenode and retry...
2020-12-03 07:22:05,151 [Listener at localhost/43105] WARN  hdfs.DFSClient (DFSInputStream.java:refetchLocations(905)) - DFS chooseDataNode: got # 1 IOException, will wait for 0.8370431983787818 msec.
2020-12-03 07:22:05,152 [Listener at localhost/43105] INFO  namenode.NameNode (NameNode.java:createNameNode(1632)) - createNameNode []
2020-12-03 07:22:05,152 [Listener at localhost/43105] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - NameNode metrics system started (again)
2020-12-03 07:22:05,153 [Listener at localhost/43105] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://localhost:18020
2020-12-03 07:22:05,153 [Listener at localhost/43105] INFO  namenode.NameNode (NameNode.java:<init>(944)) - Clients should use localhost:18020 to access this namenode/service.
2020-12-03 07:22:05,161 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@4567e53d] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:22:05,163 [Listener at localhost/43105] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1641)) - Starting Web-server for hdfs at: http://localhost:19870
2020-12-03 07:22:05,163 [Listener at localhost/43105] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:22:05,165 [Listener at localhost/43105] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:22:05,165 [Listener at localhost/43105] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-12-03 07:22:05,166 [Listener at localhost/43105] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:22:05,168 [Listener at localhost/43105] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:22:05,168 [Listener at localhost/43105] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-12-03 07:22:05,168 [Listener at localhost/43105] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:22:05,169 [Listener at localhost/43105] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:22:05,170 [Listener at localhost/43105] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(102)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-12-03 07:22:05,170 [Listener at localhost/43105] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(806)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-12-03 07:22:05,171 [Listener at localhost/43105] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 19870
2020-12-03 07:22:05,171 [Listener at localhost/43105] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:22:05,173 [Listener at localhost/43105] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@5e268ce6{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:22:05,174 [Listener at localhost/43105] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@6e91893{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:22:05,180 [Listener at localhost/43105] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@7207cb51{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-12-03 07:22:05,182 [Listener at localhost/43105] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@2a27cb34{HTTP/1.1,[http/1.1]}{localhost:19870}
2020-12-03 07:22:05,183 [Listener at localhost/43105] INFO  server.Server (Server.java:doStart(419)) - Started @12611ms
2020-12-03 07:22:05,185 [Listener at localhost/43105] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(229)) - Edit logging is async:true
2020-12-03 07:22:05,185 [Listener at localhost/43105] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(755)) - KeyProvider: null
2020-12-03 07:22:05,185 [Listener at localhost/43105] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(123)) - fsLock is fair: true
2020-12-03 07:22:05,185 [Listener at localhost/43105] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(141)) - Detailed lock hold time metrics enabled: false
2020-12-03 07:22:05,185 [Listener at localhost/43105] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(780)) - fsOwner             = root (auth:SIMPLE)
2020-12-03 07:22:05,186 [Listener at localhost/43105] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(781)) - supergroup          = supergroup
2020-12-03 07:22:05,186 [Listener at localhost/43105] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(782)) - isPermissionEnabled = true
2020-12-03 07:22:05,186 [Listener at localhost/43105] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(793)) - HA Enabled: false
2020-12-03 07:22:05,186 [Listener at localhost/43105] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:22:05,187 [Listener at localhost/43105] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:22:05,187 [Listener at localhost/43105] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(303)) - dfs.block.invalidate.limit: configured=1000, counted=20, effected=1000
2020-12-03 07:22:05,187 [Listener at localhost/43105] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(311)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-12-03 07:22:05,187 [Listener at localhost/43105] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:22:05,187 [Listener at localhost/43105] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-12-03 07:22:05,188 [Listener at localhost/43105] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Dec 03 07:22:05
2020-12-03 07:22:05,188 [Listener at localhost/43105] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-12-03 07:22:05,188 [Listener at localhost/43105] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:22:05,188 [Listener at localhost/43105] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.8 GB = 36.4 MB
2020-12-03 07:22:05,188 [Listener at localhost/43105] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-12-03 07:22:05,196 [Listener at localhost/43105] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:22:05,197 [Listener at localhost/43105] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:22:05,197 [Listener at localhost/43105] INFO  blockmanagement.BlockManager (BlockManager.java:createSPSManager(5183)) - Storage policy satisfier is disabled
2020-12-03 07:22:05,197 [Listener at localhost/43105] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(595)) - dfs.block.access.token.enable = true
2020-12-03 07:22:05,197 [Listener at localhost/43105] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(617)) - dfs.block.access.key.update.interval=600 min(s), dfs.block.access.token.lifetime=600 min(s), dfs.encrypt.data.transfer.algorithm=null
2020-12-03 07:22:05,198 [Listener at localhost/43105] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:22:05,198 [Listener at localhost/43105] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-12-03 07:22:05,198 [Listener at localhost/43105] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-12-03 07:22:05,198 [Listener at localhost/43105] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-12-03 07:22:05,198 [Listener at localhost/43105] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-12-03 07:22:05,199 [Listener at localhost/43105] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(581)) - defaultReplication         = 2
2020-12-03 07:22:05,199 [Listener at localhost/43105] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(582)) - maxReplication             = 512
2020-12-03 07:22:05,199 [Listener at localhost/43105] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(583)) - minReplication             = 1
2020-12-03 07:22:05,199 [Listener at localhost/43105] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(584)) - maxReplicationStreams      = 2
2020-12-03 07:22:05,199 [Listener at localhost/43105] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(585)) - redundancyRecheckInterval  = 3000ms
2020-12-03 07:22:05,199 [Listener at localhost/43105] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(586)) - encryptDataTransfer        = false
2020-12-03 07:22:05,199 [Listener at localhost/43105] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(587)) - maxNumBlocksToLog          = 1000
2020-12-03 07:22:05,200 [Listener at localhost/43105] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-12-03 07:22:05,200 [Listener at localhost/43105] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:22:05,200 [Listener at localhost/43105] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.8 GB = 18.2 MB
2020-12-03 07:22:05,200 [Listener at localhost/43105] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-12-03 07:22:05,206 [Listener at localhost/43105] INFO  namenode.FSDirectory (FSDirectory.java:<init>(290)) - ACLs enabled? false
2020-12-03 07:22:05,206 [Listener at localhost/43105] INFO  namenode.FSDirectory (FSDirectory.java:<init>(294)) - POSIX ACL inheritance enabled? true
2020-12-03 07:22:05,207 [Listener at localhost/43105] INFO  namenode.FSDirectory (FSDirectory.java:<init>(298)) - XAttrs enabled? true
2020-12-03 07:22:05,207 [Listener at localhost/43105] INFO  namenode.NameNode (FSDirectory.java:<init>(362)) - Caching file names occurring more than 10 times
2020-12-03 07:22:05,207 [Listener at localhost/43105] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-12-03 07:22:05,207 [Listener at localhost/43105] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-12-03 07:22:05,207 [Listener at localhost/43105] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-12-03 07:22:05,207 [Listener at localhost/43105] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:22:05,208 [Listener at localhost/43105] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.8 GB = 4.6 MB
2020-12-03 07:22:05,208 [Listener at localhost/43105] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-12-03 07:22:05,210 [Listener at localhost/43105] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-12-03 07:22:05,210 [Listener at localhost/43105] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-12-03 07:22:05,210 [Listener at localhost/43105] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-12-03 07:22:05,210 [Listener at localhost/43105] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(996)) - Retry cache on namenode is enabled
2020-12-03 07:22:05,210 [Listener at localhost/43105] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(1004)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-12-03 07:22:05,210 [Listener at localhost/43105] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-12-03 07:22:05,211 [Listener at localhost/43105] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:22:05,211 [Listener at localhost/43105] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.8 GB = 559.3 KB
2020-12-03 07:22:05,211 [Listener at localhost/43105] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-12-03 07:22:05,249 [Listener at localhost/43105] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/in_use.lock acquired by nodename 5792@13788a696671
2020-12-03 07:22:05,282 [Listener at localhost/43105] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/in_use.lock acquired by nodename 5792@13788a696671
2020-12-03 07:22:05,285 [Listener at localhost/43105] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(428)) - Recovering unfinalized segments in /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current
2020-12-03 07:22:05,285 [Listener at localhost/43105] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(428)) - Recovering unfinalized segments in /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current
2020-12-03 07:22:05,286 [Listener at localhost/43105] INFO  namenode.FSImage (FSImage.java:loadFSImageFile(797)) - Planning to load image: FSImageFile(file=/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2020-12-03 07:22:05,290 [Listener at localhost/43105] INFO  namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(234)) - Loading 1 INodes.
2020-12-03 07:22:05,291 [Listener at localhost/43105] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(246)) - Loaded FSImage in 0 seconds.
2020-12-03 07:22:05,291 [Listener at localhost/43105] INFO  namenode.FSImage (FSImage.java:loadFSImage(978)) - Loaded image for txid 0 from /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000000
2020-12-03 07:22:05,292 [Listener at localhost/43105] INFO  namenode.FSImage (FSImage.java:loadEdits(910)) - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@64aad6db expecting start txid #1
2020-12-03 07:22:05,292 [Listener at localhost/43105] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(178)) - Start loading edits file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000001-0000000000000000010, /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000001-0000000000000000010 maxTxnsToRead = 9223372036854775807
2020-12-03 07:22:05,292 [Listener at localhost/43105] INFO  namenode.RedundantEditLogInputStream (RedundantEditLogInputStream.java:nextOp(186)) - Fast-forwarding stream '/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000001-0000000000000000010' to transaction ID 1
2020-12-03 07:22:05,294 [Listener at localhost/43105] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(188)) - Loaded 1 edits file(s) (the last named /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000001-0000000000000000010, /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000001-0000000000000000010) of total size 569.0, total edits 10.0, total load time 2.0 ms
2020-12-03 07:22:05,295 [Listener at localhost/43105] INFO  namenode.RedundantEditLogInputStream (RedundantEditLogInputStream.java:nextOp(186)) - Fast-forwarding stream '/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000011-0000000000000000012' to transaction ID 1
2020-12-03 07:22:05,295 [Listener at localhost/43105] INFO  namenode.RedundantEditLogInputStream (RedundantEditLogInputStream.java:nextOp(186)) - Fast-forwarding stream '/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000013-0000000000000000014' to transaction ID 1
2020-12-03 07:22:05,296 [Listener at localhost/43105] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1110)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-12-03 07:22:05,296 [Listener at localhost/43105] INFO  namenode.FSEditLog (FSEditLog.java:startLogSegment(1365)) - Starting log segment at 15
2020-12-03 07:22:05,375 [Listener at localhost/43105] INFO  namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2020-12-03 07:22:05,376 [Listener at localhost/43105] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(727)) - Finished loading FSImage in 164 msecs
2020-12-03 07:22:05,376 [Listener at localhost/43105] INFO  namenode.NameNode (NameNodeRpcServer.java:<init>(448)) - RPC server is binding to localhost:18020
2020-12-03 07:22:05,377 [Listener at localhost/43105] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:22:05,378 [Socket Reader #1 for port 18020] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 18020
2020-12-03 07:22:05,386 [Listener at localhost/18020] INFO  namenode.FSNamesystem (FSNamesystem.java:registerMBean(5090)) - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2020-12-03 07:22:05,419 [Listener at localhost/18020] INFO  namenode.LeaseManager (LeaseManager.java:getNumUnderConstructionBlocks(171)) - Number of blocks under construction: 0
2020-12-03 07:22:05,424 [org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager$Monitor@4a6c18ad] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:updateKeys(263)) - Updating block keys
2020-12-03 07:22:05,427 [Listener at localhost/18020] INFO  hdfs.StateChange (BlockManagerSafeMode.java:reportStatus(617)) - STATE* Safe mode ON. 
The reported blocks 0 needs additional 1 blocks to reach the threshold 0.9990 of total blocks 2.
The minimum number of live datanodes is not required. Safe mode will be turned off automatically once the thresholds have been reached.
2020-12-03 07:22:05,450 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:22:05,450 [IPC Server listener on 18020] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 18020: starting
2020-12-03 07:22:05,453 [Listener at localhost/18020] INFO  namenode.NameNode (NameNode.java:startCommonServices(828)) - NameNode RPC up at: localhost/127.0.0.1:18020
2020-12-03 07:22:05,455 [Listener at localhost/18020] INFO  namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1222)) - Starting services required for active state
2020-12-03 07:22:05,455 [Listener at localhost/18020] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(777)) - Initializing quota with 4 thread(s)
2020-12-03 07:22:05,464 [Listener at localhost/18020] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(786)) - Quota initialization completed in 2 milliseconds
name space=2
storage space=4096
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2020-12-03 07:22:05,472 [Listener at localhost/18020] WARN  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitClusterUp(1436)) - Waiting for the Mini HDFS Cluster to start...
2020-12-03 07:22:05,473 [CacheReplicationMonitor(658625640)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-12-03 07:22:05,845 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActor(672)) - DatanodeCommand action : DNA_REGISTER from localhost/127.0.0.1:18020 with active state
2020-12-03 07:22:05,849 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid bb4cfced-0bdc-453a-99a9-88d3aea04105) service to localhost/127.0.0.1:18020 beginning handshake with NN
2020-12-03 07:22:05,853 [IPC Server handler 3 on default port 18020] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:33955, datanodeUuid=bb4cfced-0bdc-453a-99a9-88d3aea04105, infoPort=38394, infoSecurePort=0, ipcPort=46144, storageInfo=lv=-57;cid=testClusterID;nsid=1772939335;c=1606980114492) storage bb4cfced-0bdc-453a-99a9-88d3aea04105
2020-12-03 07:22:05,854 [IPC Server handler 3 on default port 18020] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:33955
2020-12-03 07:22:05,854 [IPC Server handler 3 on default port 18020] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN bb4cfced-0bdc-453a-99a9-88d3aea04105 (127.0.0.1:33955).
2020-12-03 07:22:05,857 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid bb4cfced-0bdc-453a-99a9-88d3aea04105) service to localhost/127.0.0.1:18020 successfully registered with NN
2020-12-03 07:22:05,857 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:22:05,860 [IPC Server handler 4 on default port 18020] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85 for DN 127.0.0.1:33955
2020-12-03 07:22:05,860 [IPC Server handler 4 on default port 18020] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-5b5d6366-8003-4c96-a269-54331e87d267 for DN 127.0.0.1:33955
2020-12-03 07:22:05,865 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x7744ebc2ae68a7ad: Processing first storage report for DS-5b5d6366-8003-4c96-a269-54331e87d267 from datanode bb4cfced-0bdc-453a-99a9-88d3aea04105
2020-12-03 07:22:05,865 [Block report processor] INFO  blockmanagement.BlockManager (BlockManager.java:initializeReplQueues(4922)) - initializing replication queues
2020-12-03 07:22:05,867 [Block report processor] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(395)) - STATE* Safe mode is OFF
2020-12-03 07:22:05,868 [Block report processor] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(400)) - STATE* Leaving safe mode after 0 secs
2020-12-03 07:22:05,868 [Block report processor] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(406)) - STATE* Network topology has 1 racks and 1 datanodes
2020-12-03 07:22:05,868 [Block report processor] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(408)) - STATE* UnderReplicatedBlocks has 0 blocks
2020-12-03 07:22:05,869 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x7744ebc2ae68a7ad: from storage DS-5b5d6366-8003-4c96-a269-54331e87d267 node DatanodeRegistration(127.0.0.1:33955, datanodeUuid=bb4cfced-0bdc-453a-99a9-88d3aea04105, infoPort=38394, infoSecurePort=0, ipcPort=46144, storageInfo=lv=-57;cid=testClusterID;nsid=1772939335;c=1606980114492), blocks: 1, hasStaleStorage: true, processing time: 4 msecs, invalidatedBlocks: 0
2020-12-03 07:22:05,873 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3585)) - Total number of blocks            = 2
2020-12-03 07:22:05,873 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3586)) - Number of invalid blocks          = 0
2020-12-03 07:22:05,873 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3587)) - Number of under-replicated blocks = 2
2020-12-03 07:22:05,874 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3588)) - Number of  over-replicated blocks = 0
2020-12-03 07:22:05,874 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3590)) - Number of blocks being written    = 0
2020-12-03 07:22:05,874 [Reconstruction Queue Initializer] INFO  hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(3593)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 6 msec
2020-12-03 07:22:05,875 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x7744ebc2ae68a7ad: Processing first storage report for DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85 from datanode bb4cfced-0bdc-453a-99a9-88d3aea04105
2020-12-03 07:22:05,877 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x7744ebc2ae68a7ad: from storage DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85 node DatanodeRegistration(127.0.0.1:33955, datanodeUuid=bb4cfced-0bdc-453a-99a9-88d3aea04105, infoPort=38394, infoSecurePort=0, ipcPort=46144, storageInfo=lv=-57;cid=testClusterID;nsid=1772939335;c=1606980114492), blocks: 1, hasStaleStorage: false, processing time: 2 msecs, invalidatedBlocks: 0
2020-12-03 07:22:05,880 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x7744ebc2ae68a7ad,  containing 2 storage report(s), of which we sent 2. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 18 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:22:05,880 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:06,093 [IPC Server handler 6 on default port 18020] WARN  blockmanagement.BlockManager (BlockManager.java:requestBlockReportLeaseId(2438)) - Failed to find datanode DatanodeRegistration(127.0.0.1:32825, datanodeUuid=fcfa3dd6-e926-44de-87fa-c54cedb559cc, infoPort=39424, infoSecurePort=0, ipcPort=43105, storageInfo=lv=-57;cid=testClusterID;nsid=1772939335;c=1606980114492)
2020-12-03 07:22:06,095 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActor(672)) - DatanodeCommand action : DNA_REGISTER from localhost/127.0.0.1:18020 with active state
2020-12-03 07:22:06,096 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid fcfa3dd6-e926-44de-87fa-c54cedb559cc) service to localhost/127.0.0.1:18020 beginning handshake with NN
2020-12-03 07:22:06,097 [IPC Server handler 8 on default port 18020] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:32825, datanodeUuid=fcfa3dd6-e926-44de-87fa-c54cedb559cc, infoPort=39424, infoSecurePort=0, ipcPort=43105, storageInfo=lv=-57;cid=testClusterID;nsid=1772939335;c=1606980114492) storage fcfa3dd6-e926-44de-87fa-c54cedb559cc
2020-12-03 07:22:06,097 [IPC Server handler 8 on default port 18020] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:32825
2020-12-03 07:22:06,098 [IPC Server handler 8 on default port 18020] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN fcfa3dd6-e926-44de-87fa-c54cedb559cc (127.0.0.1:32825).
2020-12-03 07:22:06,098 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid fcfa3dd6-e926-44de-87fa-c54cedb559cc) service to localhost/127.0.0.1:18020 successfully registered with NN
2020-12-03 07:22:06,099 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:22:06,103 [IPC Server handler 9 on default port 18020] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-ef772597-ccee-4e3e-8242-d1e42fd95974 for DN 127.0.0.1:32825
2020-12-03 07:22:06,107 [IPC Server handler 9 on default port 18020] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd for DN 127.0.0.1:32825
2020-12-03 07:22:06,109 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x90412d32bf4bfdd4: Processing first storage report for DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd from datanode fcfa3dd6-e926-44de-87fa-c54cedb559cc
2020-12-03 07:22:06,110 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x90412d32bf4bfdd4: from storage DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd node DatanodeRegistration(127.0.0.1:32825, datanodeUuid=fcfa3dd6-e926-44de-87fa-c54cedb559cc, infoPort=39424, infoSecurePort=0, ipcPort=43105, storageInfo=lv=-57;cid=testClusterID;nsid=1772939335;c=1606980114492), blocks: 1, hasStaleStorage: true, processing time: 1 msecs, invalidatedBlocks: 0
2020-12-03 07:22:06,110 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x90412d32bf4bfdd4: Processing first storage report for DS-ef772597-ccee-4e3e-8242-d1e42fd95974 from datanode fcfa3dd6-e926-44de-87fa-c54cedb559cc
2020-12-03 07:22:06,110 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x90412d32bf4bfdd4: from storage DS-ef772597-ccee-4e3e-8242-d1e42fd95974 node DatanodeRegistration(127.0.0.1:32825, datanodeUuid=fcfa3dd6-e926-44de-87fa-c54cedb559cc, infoPort=39424, infoSecurePort=0, ipcPort=43105, storageInfo=lv=-57;cid=testClusterID;nsid=1772939335;c=1606980114492), blocks: 1, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
2020-12-03 07:22:06,111 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x90412d32bf4bfdd4,  containing 2 storage report(s), of which we sent 2. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 3 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:22:06,111 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:06,473 [Listener at localhost/18020] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:restartNameNode(2191)) - Restarted the namenode
2020-12-03 07:22:06,474 [Listener at localhost/18020] DEBUG hdfs.DFSClient (DFSClient.java:<init>(318)) - Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
2020-12-03 07:22:06,482 [IPC Server handler 0 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:22:06,484 [Listener at localhost/18020] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2758)) - Cluster is active
2020-12-03 07:22:06,484 [Listener at localhost/18020] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:33955
2020-12-03 07:22:06,485 [Listener at localhost/18020] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:06,487 [DataXceiver for client DFSClient_NONMAPREDUCE_-2074459051_1 at /127.0.0.1:56774 [Sending block BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001]] WARN  datanode.DataNode (DataXceiver.java:checkAccess(1449)) - Block token verification failed: op=READ_BLOCK, remoteAddress=/127.0.0.1:56774, message=Can't re-compute password for block_token_identifier (expiryDate=1606980720165, keyId=38854922, userId=root, blockPoolId=BP-1338698873-172.17.0.5-1606980114492, blockId=1073741825, access modes=[READ], storageTypes= [DISK, DISK], storageIds= [DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85, DS-ef772597-ccee-4e3e-8242-d1e42fd95974]), since the required block key (keyID=38854922) doesn't exist.
2020-12-03 07:22:06,487 [Listener at localhost/18020] DEBUG hdfs.DFSClient (DFSInputStream.java:tokenRefetchNeeded(1298)) - Access token was invalid when connecting to /127.0.0.1:33955: {}
org.apache.hadoop.hdfs.security.token.block.InvalidBlockTokenException: Got access token error, status message , for OP_READ_BLOCK, self=/127.0.0.1:56774, remote=/127.0.0.1:33955, for file /fileToRead.dat, for pool BP-1338698873-172.17.0.5-1606980114492 block 1073741825_1001
	at org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:119)
	at org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:110)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.checkSuccess(BlockReaderRemote.java:440)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.newBlockReader(BlockReaderRemote.java:408)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReader(BlockReaderFactory.java:854)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:750)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:380)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:644)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:575)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:757)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:829)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.checkFile1(TestBlockTokenWithDFS.java:102)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.doTestRead(TestBlockTokenWithDFS.java:588)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.testRead(TestBlockTokenWithDFS.java:360)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-12-03 07:22:06,499 [IPC Server handler 1 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/fileToRead.dat	dst=null	perm=null	proto=rpc
2020-12-03 07:22:06,526 [Listener at localhost/18020] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:33955
2020-12-03 07:22:06,540 [Listener at localhost/18020] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:06,549 [Listener at localhost/18020] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:33955
2020-12-03 07:22:06,557 [Listener at localhost/18020] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:32825
2020-12-03 07:22:06,558 [Listener at localhost/18020] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:06,560 [DataXceiver for client DFSClient_NONMAPREDUCE_-2074459051_1 at /127.0.0.1:50412 [Sending block BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001]] WARN  datanode.DataNode (DataXceiver.java:checkAccess(1449)) - Block token verification failed: op=READ_BLOCK, remoteAddress=/127.0.0.1:50412, message=Can't re-compute password for block_token_identifier (expiryDate=1606980720178, keyId=38854922, userId=root, blockPoolId=BP-1338698873-172.17.0.5-1606980114492, blockId=1073741825, access modes=[READ], storageTypes= [DISK, DISK], storageIds= [DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85, DS-ef772597-ccee-4e3e-8242-d1e42fd95974]), since the required block key (keyID=38854922) doesn't exist.
2020-12-03 07:22:06,560 [Listener at localhost/18020] DEBUG hdfs.DFSClient (DFSInputStream.java:tokenRefetchNeeded(1298)) - Access token was invalid when connecting to /127.0.0.1:32825: {}
org.apache.hadoop.hdfs.security.token.block.InvalidBlockTokenException: Got access token error, status message , for OP_READ_BLOCK, self=/127.0.0.1:50412, remote=/127.0.0.1:32825, for file /fileToRead.dat, for pool BP-1338698873-172.17.0.5-1606980114492 block 1073741825_1001
	at org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:119)
	at org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:110)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.checkSuccess(BlockReaderRemote.java:440)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.newBlockReader(BlockReaderRemote.java:408)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReader(BlockReaderFactory.java:854)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:750)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:380)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:644)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:575)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToNewSource(DFSInputStream.java:1514)
	at org.apache.hadoop.fs.FSDataInputStream.seekToNewSource(FSDataInputStream.java:131)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.doTestRead(TestBlockTokenWithDFS.java:592)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.testRead(TestBlockTokenWithDFS.java:360)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-12-03 07:22:06,564 [IPC Server handler 3 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/fileToRead.dat	dst=null	perm=null	proto=rpc
2020-12-03 07:22:06,568 [Listener at localhost/18020] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:32825
2020-12-03 07:22:06,568 [Listener at localhost/18020] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:06,571 [Listener at localhost/18020] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:32825
2020-12-03 07:22:06,574 [Listener at localhost/18020] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:32825
2020-12-03 07:22:06,575 [Listener at localhost/18020] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:06,576 [DataXceiver for client DFSClient_NONMAPREDUCE_-2074459051_1 at /127.0.0.1:50416 [Sending block BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001]] WARN  datanode.DataNode (DataXceiver.java:checkAccess(1449)) - Block token verification failed: op=READ_BLOCK, remoteAddress=/127.0.0.1:50416, message=Can't re-compute password for block_token_identifier (expiryDate=1606980720194, keyId=38854922, userId=root, blockPoolId=BP-1338698873-172.17.0.5-1606980114492, blockId=1073741825, access modes=[READ], storageTypes= [DISK, DISK], storageIds= [DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85, DS-ef772597-ccee-4e3e-8242-d1e42fd95974]), since the required block key (keyID=38854922) doesn't exist.
2020-12-03 07:22:06,576 [Listener at localhost/18020] DEBUG hdfs.DFSClient (DFSInputStream.java:tokenRefetchNeeded(1298)) - Access token was invalid when connecting to /127.0.0.1:32825: {}
org.apache.hadoop.hdfs.security.token.block.InvalidBlockTokenException: Got access token error, status message , for OP_READ_BLOCK, self=/127.0.0.1:50416, remote=/127.0.0.1:32825, for file /fileToRead.dat, for pool BP-1338698873-172.17.0.5-1606980114492 block 1073741825_1001
	at org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:119)
	at org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:110)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.checkSuccess(BlockReaderRemote.java:440)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.newBlockReader(BlockReaderRemote.java:408)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReader(BlockReaderFactory.java:854)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:750)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:380)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:644)
	at org.apache.hadoop.hdfs.DFSInputStream.actualGetFromOneDataNode(DFSInputStream.java:1049)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchBlockByteRange(DFSInputStream.java:1001)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:92)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.checkFile2(TestBlockTokenWithDFS.java:116)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.doTestRead(TestBlockTokenWithDFS.java:597)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.testRead(TestBlockTokenWithDFS.java:360)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-12-03 07:22:06,581 [IPC Server handler 4 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/fileToRead.dat	dst=null	perm=null	proto=rpc
2020-12-03 07:22:06,585 [Listener at localhost/18020] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:06,598 [Listener at localhost/18020] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:32825
2020-12-03 07:22:06,600 [DataXceiver for client DFSClient_NONMAPREDUCE_-2074459051_1 at /127.0.0.1:50418 [Sending block BP-1338698873-172.17.0.5-1606980114492:blk_1073741826_1002]] WARN  datanode.DataNode (DataXceiver.java:checkAccess(1449)) - Block token verification failed: op=READ_BLOCK, remoteAddress=/127.0.0.1:50418, message=Can't re-compute password for block_token_identifier (expiryDate=1606980720205, keyId=38854922, userId=root, blockPoolId=BP-1338698873-172.17.0.5-1606980114492, blockId=1073741826, access modes=[READ], storageTypes= [DISK, DISK], storageIds= [DS-5b5d6366-8003-4c96-a269-54331e87d267, DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd]), since the required block key (keyID=38854922) doesn't exist.
2020-12-03 07:22:06,600 [Listener at localhost/18020] DEBUG hdfs.DFSClient (DFSInputStream.java:tokenRefetchNeeded(1298)) - Access token was invalid when connecting to /127.0.0.1:32825: {}
org.apache.hadoop.hdfs.security.token.block.InvalidBlockTokenException: Got access token error, status message , for OP_READ_BLOCK, self=/127.0.0.1:50418, remote=/127.0.0.1:32825, for file /fileToRead.dat, for pool BP-1338698873-172.17.0.5-1606980114492 block 1073741826_1002
	at org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:119)
	at org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:110)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.checkSuccess(BlockReaderRemote.java:440)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.newBlockReader(BlockReaderRemote.java:408)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReader(BlockReaderFactory.java:854)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:750)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:380)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:644)
	at org.apache.hadoop.hdfs.DFSInputStream.actualGetFromOneDataNode(DFSInputStream.java:1049)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchBlockByteRange(DFSInputStream.java:1001)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:92)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.checkFile2(TestBlockTokenWithDFS.java:116)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.doTestRead(TestBlockTokenWithDFS.java:597)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.testRead(TestBlockTokenWithDFS.java:360)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-12-03 07:22:06,605 [IPC Server handler 5 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/fileToRead.dat	dst=null	perm=null	proto=rpc
2020-12-03 07:22:06,609 [Listener at localhost/18020] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:06,612 [Listener at localhost/18020] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:stopDataNode(2331)) - MiniDFSCluster Stopping DataNode 127.0.0.1:32825 from a total of 2 datanodes.
2020-12-03 07:22:06,612 [Listener at localhost/18020] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:22:06,612 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@19ccca5] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:22:06,614 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3, DS-ef772597-ccee-4e3e-8242-d1e42fd95974) exiting.
2020-12-03 07:22:06,614 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd) exiting.
2020-12-03 07:22:06,630 [Listener at localhost/18020] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@57adfab0{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:22:06,631 [Listener at localhost/18020] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@1949309d{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:22:06,631 [Listener at localhost/18020] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@66ba7e45{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:22:06,631 [Listener at localhost/18020] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@6be25526{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:22:06,632 [Listener at localhost/18020] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 43105
2020-12-03 07:22:06,634 [IPC Server listener on 43105] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 43105
2020-12-03 07:22:06,639 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:22:06,642 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:22:06,642 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid fcfa3dd6-e926-44de-87fa-c54cedb559cc) service to localhost/127.0.0.1:18020
2020-12-03 07:22:06,642 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid fcfa3dd6-e926-44de-87fa-c54cedb559cc)
2020-12-03 07:22:06,642 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:06,643 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1338698873-172.17.0.5-1606980114492] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:06,645 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1338698873-172.17.0.5-1606980114492] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:06,649 [Listener at localhost/18020] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:22:06,649 [Listener at localhost/18020] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:22:06,650 [Listener at localhost/18020] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:22:06,650 [Listener at localhost/18020] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:22:06,653 [Listener at localhost/18020] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:22:06,654 [Listener at localhost/18020] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
2020-12-03 07:22:06,655 [Listener at localhost/18020] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:22:06,660 [Listener at localhost/18020] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:22:06,661 [Listener at localhost/18020] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:22:06,661 [Listener at localhost/18020] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:22:06,662 [Listener at localhost/18020] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:22:06,662 [Listener at localhost/18020] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:22:06,662 [Listener at localhost/18020] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:22:06,662 [Listener at localhost/18020] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:22:06,663 [Listener at localhost/18020] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:22:06,664 [Listener at localhost/18020] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:39472
2020-12-03 07:22:06,664 [Listener at localhost/18020] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:22:06,664 [Listener at localhost/18020] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:22:06,666 [Listener at localhost/18020] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:22:06,668 [Listener at localhost/18020] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:22:06,669 [Listener at localhost/18020] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:22:06,669 [Listener at localhost/18020] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:22:06,673 [Listener at localhost/18020] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:22:06,674 [Listener at localhost/18020] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:22:06,674 [Listener at localhost/18020] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:22:06,675 [Listener at localhost/18020] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:22:06,677 [Listener at localhost/18020] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 34984
2020-12-03 07:22:06,677 [Listener at localhost/18020] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:22:06,699 [Listener at localhost/18020] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@2679311f{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:22:06,701 [Listener at localhost/18020] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@451f87af{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:22:06,711 [Listener at localhost/18020] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@540dbda9{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:22:06,714 [Listener at localhost/18020] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@22bb5646{HTTP/1.1,[http/1.1]}{localhost:34984}
2020-12-03 07:22:06,714 [Listener at localhost/18020] INFO  server.Server (Server.java:doStart(419)) - Started @14143ms
2020-12-03 07:22:06,747 [Listener at localhost/18020] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:41581
2020-12-03 07:22:06,747 [Listener at localhost/18020] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:22:06,748 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@2ce45a7b] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:22:06,748 [Listener at localhost/18020] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:22:06,755 [Listener at localhost/18020] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:22:06,756 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:22:06,763 [Listener at localhost/44628] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:44628
2020-12-03 07:22:06,770 [Listener at localhost/44628] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:22:06,771 [Listener at localhost/44628] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:22:06,772 [Thread-420] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:18020 starting to offer service
2020-12-03 07:22:06,772 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:22:06,774 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:22:06,780 [Thread-420] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:18020
2020-12-03 07:22:06,781 [Listener at localhost/44628] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:restartDataNodes(2519)) - Restarted DataNode 1
2020-12-03 07:22:06,781 [Thread-420] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:22:06,781 [Listener at localhost/44628] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:stopDataNode(2331)) - MiniDFSCluster Stopping DataNode 127.0.0.1:33955 from a total of 2 datanodes.
2020-12-03 07:22:06,782 [Listener at localhost/44628] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:22:06,782 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@c81fd12] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:22:06,783 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85) exiting.
2020-12-03 07:22:06,783 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-5b5d6366-8003-4c96-a269-54331e87d267) exiting.
2020-12-03 07:22:06,804 [Listener at localhost/44628] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@7ac9af2a{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:22:06,805 [Listener at localhost/44628] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@7bb004b8{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:22:06,806 [Listener at localhost/44628] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@770b3be0{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:22:06,807 [Listener at localhost/44628] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@51e0301d{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:22:06,808 [Listener at localhost/44628] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 46144
2020-12-03 07:22:06,814 [IPC Server listener on 46144] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 46144
2020-12-03 07:22:06,825 [Thread-420] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/in_use.lock acquired by nodename 5792@13788a696671
2020-12-03 07:22:06,826 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:22:06,826 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:22:06,826 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid bb4cfced-0bdc-453a-99a9-88d3aea04105) service to localhost/127.0.0.1:18020
2020-12-03 07:22:06,827 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid bb4cfced-0bdc-453a-99a9-88d3aea04105)
2020-12-03 07:22:06,827 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:06,828 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1338698873-172.17.0.5-1606980114492] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:06,828 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1338698873-172.17.0.5-1606980114492] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:06,834 [Listener at localhost/44628] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:22:06,834 [Listener at localhost/44628] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:22:06,835 [Listener at localhost/44628] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:22:06,835 [Listener at localhost/44628] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:22:06,837 [Listener at localhost/44628] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:22:06,839 [Listener at localhost/44628] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:22:06,840 [Listener at localhost/44628] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:22:06,841 [Listener at localhost/44628] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:22:06,842 [Listener at localhost/44628] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:22:06,842 [Listener at localhost/44628] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:22:06,843 [Listener at localhost/44628] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:22:06,843 [Listener at localhost/44628] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:22:06,843 [Listener at localhost/44628] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:22:06,843 [Listener at localhost/44628] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:22:06,846 [Listener at localhost/44628] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:22:06,848 [Listener at localhost/44628] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:43584
2020-12-03 07:22:06,848 [Listener at localhost/44628] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:22:06,848 [Listener at localhost/44628] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:22:06,850 [Listener at localhost/44628] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:22:06,852 [Listener at localhost/44628] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:22:06,855 [Listener at localhost/44628] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:22:06,855 [Listener at localhost/44628] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:22:06,858 [Listener at localhost/44628] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:22:06,859 [Listener at localhost/44628] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:22:06,859 [Listener at localhost/44628] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:22:06,859 [Listener at localhost/44628] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:22:06,861 [Listener at localhost/44628] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 45335
2020-12-03 07:22:06,861 [Listener at localhost/44628] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:22:06,863 [Listener at localhost/44628] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@6293e39e{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:22:06,863 [Listener at localhost/44628] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@34a0ef00{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:22:06,871 [Listener at localhost/44628] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@58a4a74d{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:22:06,914 [Thread-420] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/in_use.lock acquired by nodename 5792@13788a696671
2020-12-03 07:22:06,919 [Listener at localhost/44628] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@54aca26f{HTTP/1.1,[http/1.1]}{localhost:45335}
2020-12-03 07:22:06,919 [Listener at localhost/44628] INFO  server.Server (Server.java:doStart(419)) - Started @14348ms
2020-12-03 07:22:06,950 [Listener at localhost/44628] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:36907
2020-12-03 07:22:06,951 [Listener at localhost/44628] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:22:06,951 [Listener at localhost/44628] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:22:06,951 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@35088e87] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:22:06,952 [Listener at localhost/44628] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:22:06,952 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:22:06,961 [Listener at localhost/33243] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:33243
2020-12-03 07:22:06,963 [Thread-420] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:06,963 [Thread-420] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:06,971 [Listener at localhost/33243] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:22:06,971 [Listener at localhost/33243] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:22:06,972 [Thread-442] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:18020 starting to offer service
2020-12-03 07:22:06,988 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:22:07,014 [Thread-442] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:18020
2020-12-03 07:22:07,016 [Thread-442] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:22:07,016 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:22:07,020 [Listener at localhost/33243] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:restartDataNodes(2519)) - Restarted DataNode 0
2020-12-03 07:22:07,026 [Listener at localhost/33243] DEBUG hdfs.DFSClient (DFSClient.java:<init>(318)) - Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
2020-12-03 07:22:07,028 [IPC Server handler 8 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:22:07,031 [Listener at localhost/33243] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2789)) - !dn.datanode.isDatanodeFullyStarted()
2020-12-03 07:22:07,032 [Listener at localhost/33243] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:22:07,033 [Thread-420] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:07,034 [Thread-420] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:07,057 [Thread-442] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/in_use.lock acquired by nodename 5792@13788a696671
2020-12-03 07:22:07,100 [Thread-420] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=1772939335;bpid=BP-1338698873-172.17.0.5-1606980114492;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=1772939335;c=1606980114492;bpid=BP-1338698873-172.17.0.5-1606980114492;dnuuid=fcfa3dd6-e926-44de-87fa-c54cedb559cc
2020-12-03 07:22:07,103 [Thread-420] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-ef772597-ccee-4e3e-8242-d1e42fd95974
2020-12-03 07:22:07,104 [Thread-420] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3, StorageType: DISK
2020-12-03 07:22:07,110 [Thread-420] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd
2020-12-03 07:22:07,110 [Thread-420] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, StorageType: DISK
2020-12-03 07:22:07,112 [Thread-420] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:22:07,115 [Thread-420] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
2020-12-03 07:22:07,115 [Thread-420] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
2020-12-03 07:22:07,116 [Thread-420] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:22:07,116 [Thread-420] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:22:07,116 [Thread-420] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:07,116 [Thread-455] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3...
2020-12-03 07:22:07,117 [Thread-456] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4...
2020-12-03 07:22:07,121 [Thread-456] INFO  impl.FsDatasetImpl (BlockPoolSlice.java:loadDfsUsed(292)) - Cached dfsUsed found for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1338698873-172.17.0.5-1606980114492/current: 25611
2020-12-03 07:22:07,121 [Thread-455] INFO  impl.FsDatasetImpl (BlockPoolSlice.java:loadDfsUsed(292)) - Cached dfsUsed found for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1338698873-172.17.0.5-1606980114492/current: 25611
2020-12-03 07:22:07,133 [IPC Server handler 0 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:22:07,137 [Thread-455] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1338698873-172.17.0.5-1606980114492 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3: 20ms
2020-12-03 07:22:07,137 [Listener at localhost/33243] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2789)) - !dn.datanode.isDatanodeFullyStarted()
2020-12-03 07:22:07,137 [Listener at localhost/33243] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:22:07,138 [Thread-456] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1338698873-172.17.0.5-1606980114492 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4: 21ms
2020-12-03 07:22:07,138 [Thread-420] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1338698873-172.17.0.5-1606980114492: 23ms
2020-12-03 07:22:07,139 [Thread-457] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3...
2020-12-03 07:22:07,139 [Thread-458] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4...
2020-12-03 07:22:07,142 [Thread-458] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(925)) - Successfully read replica from cache file : /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1338698873-172.17.0.5-1606980114492/current/replicas
2020-12-03 07:22:07,142 [Thread-457] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(925)) - Successfully read replica from cache file : /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1338698873-172.17.0.5-1606980114492/current/replicas
2020-12-03 07:22:07,142 [Thread-458] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4: 3ms
2020-12-03 07:22:07,142 [Thread-457] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3: 3ms
2020-12-03 07:22:07,144 [Thread-420] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1338698873-172.17.0.5-1606980114492: 5ms
2020-12-03 07:22:07,145 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd): no suitable block pools found to scan.  Waiting 1814391490 ms.
2020-12-03 07:22:07,145 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3, DS-ef772597-ccee-4e3e-8242-d1e42fd95974): no suitable block pools found to scan.  Waiting 1814391490 ms.
2020-12-03 07:22:07,146 [Thread-420] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 12:39 PM with interval of 21600000ms
2020-12-03 07:22:07,148 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid fcfa3dd6-e926-44de-87fa-c54cedb559cc) service to localhost/127.0.0.1:18020 beginning handshake with NN
2020-12-03 07:22:07,149 [IPC Server handler 1 on default port 18020] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:39472, datanodeUuid=fcfa3dd6-e926-44de-87fa-c54cedb559cc, infoPort=41581, infoSecurePort=0, ipcPort=44628, storageInfo=lv=-57;cid=testClusterID;nsid=1772939335;c=1606980114492) storage fcfa3dd6-e926-44de-87fa-c54cedb559cc
2020-12-03 07:22:07,150 [IPC Server handler 1 on default port 18020] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1078)) - BLOCK* registerDatanode: 127.0.0.1:32825 is replaced by DatanodeRegistration(127.0.0.1:39472, datanodeUuid=fcfa3dd6-e926-44de-87fa-c54cedb559cc, infoPort=41581, infoSecurePort=0, ipcPort=44628, storageInfo=lv=-57;cid=testClusterID;nsid=1772939335;c=1606980114492) with the same storageID fcfa3dd6-e926-44de-87fa-c54cedb559cc
2020-12-03 07:22:07,150 [IPC Server handler 1 on default port 18020] INFO  net.NetworkTopology (NetworkTopology.java:remove(219)) - Removing a node: /default-rack/127.0.0.1:32825
2020-12-03 07:22:07,152 [IPC Server handler 1 on default port 18020] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:39472
2020-12-03 07:22:07,153 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid fcfa3dd6-e926-44de-87fa-c54cedb559cc) service to localhost/127.0.0.1:18020 successfully registered with NN
2020-12-03 07:22:07,153 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (DataNode.java:registerBlockPoolWithSecretManager(1615)) - Block token params received from NN: for block pool BP-1338698873-172.17.0.5-1606980114492 keyUpdateInterval=600 min(s), tokenLifetime=600 min(s)
2020-12-03 07:22:07,153 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:22:07,153 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:18020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=1000
2020-12-03 07:22:07,178 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x12ad00cc958a2af4: Processing first storage report for DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd from datanode fcfa3dd6-e926-44de-87fa-c54cedb559cc
2020-12-03 07:22:07,179 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x12ad00cc958a2af4: from storage DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd node DatanodeRegistration(127.0.0.1:39472, datanodeUuid=fcfa3dd6-e926-44de-87fa-c54cedb559cc, infoPort=41581, infoSecurePort=0, ipcPort=44628, storageInfo=lv=-57;cid=testClusterID;nsid=1772939335;c=1606980114492), blocks: 1, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
2020-12-03 07:22:07,179 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x12ad00cc958a2af4: Processing first storage report for DS-ef772597-ccee-4e3e-8242-d1e42fd95974 from datanode fcfa3dd6-e926-44de-87fa-c54cedb559cc
2020-12-03 07:22:07,179 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x12ad00cc958a2af4: from storage DS-ef772597-ccee-4e3e-8242-d1e42fd95974 node DatanodeRegistration(127.0.0.1:39472, datanodeUuid=fcfa3dd6-e926-44de-87fa-c54cedb559cc, infoPort=41581, infoSecurePort=0, ipcPort=44628, storageInfo=lv=-57;cid=testClusterID;nsid=1772939335;c=1606980114492), blocks: 1, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:22:07,182 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x12ad00cc958a2af4,  containing 2 storage report(s), of which we sent 2. The reports had 2 total blocks and used 1 RPC(s). This took 1 msec to generate and 5 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:22:07,182 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:07,184 [Thread-442] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/in_use.lock acquired by nodename 5792@13788a696671
2020-12-03 07:22:07,231 [Thread-442] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:07,232 [Thread-442] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:07,241 [IPC Server handler 5 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:22:07,242 [Listener at localhost/33243] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2789)) - !dn.datanode.isDatanodeFullyStarted()
2020-12-03 07:22:07,242 [Listener at localhost/33243] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:22:07,288 [Thread-442] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:07,289 [Thread-442] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:07,321 [Thread-442] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=1772939335;bpid=BP-1338698873-172.17.0.5-1606980114492;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=1772939335;c=1606980114492;bpid=BP-1338698873-172.17.0.5-1606980114492;dnuuid=bb4cfced-0bdc-453a-99a9-88d3aea04105
2020-12-03 07:22:07,325 [Thread-442] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85
2020-12-03 07:22:07,325 [Thread-442] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, StorageType: DISK
2020-12-03 07:22:07,330 [Thread-442] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-5b5d6366-8003-4c96-a269-54331e87d267
2020-12-03 07:22:07,332 [Thread-442] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, StorageType: DISK
2020-12-03 07:22:07,333 [Thread-442] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:22:07,335 [Thread-442] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:22:07,335 [Thread-442] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:22:07,335 [Thread-442] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:22:07,336 [Thread-442] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:22:07,339 [Thread-442] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:07,340 [Thread-464] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2020-12-03 07:22:07,340 [Thread-465] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2020-12-03 07:22:07,341 [Thread-465] INFO  impl.FsDatasetImpl (BlockPoolSlice.java:loadDfsUsed(292)) - Cached dfsUsed found for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1338698873-172.17.0.5-1606980114492/current: 25611
2020-12-03 07:22:07,341 [Thread-464] INFO  impl.FsDatasetImpl (BlockPoolSlice.java:loadDfsUsed(292)) - Cached dfsUsed found for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1338698873-172.17.0.5-1606980114492/current: 25611
2020-12-03 07:22:07,344 [IPC Server handler 6 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:22:07,346 [Listener at localhost/33243] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2789)) - !dn.datanode.isDatanodeFullyStarted()
2020-12-03 07:22:07,346 [Listener at localhost/33243] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:22:07,353 [Thread-465] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1338698873-172.17.0.5-1606980114492 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 13ms
2020-12-03 07:22:07,356 [Thread-464] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1338698873-172.17.0.5-1606980114492 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 17ms
2020-12-03 07:22:07,357 [Thread-442] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1338698873-172.17.0.5-1606980114492: 17ms
2020-12-03 07:22:07,357 [Thread-466] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2020-12-03 07:22:07,358 [Thread-467] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2020-12-03 07:22:07,360 [Thread-466] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(925)) - Successfully read replica from cache file : /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1338698873-172.17.0.5-1606980114492/current/replicas
2020-12-03 07:22:07,360 [Thread-466] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 2ms
2020-12-03 07:22:07,360 [Thread-467] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(925)) - Successfully read replica from cache file : /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1338698873-172.17.0.5-1606980114492/current/replicas
2020-12-03 07:22:07,360 [Thread-467] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1338698873-172.17.0.5-1606980114492 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 0ms
2020-12-03 07:22:07,360 [Thread-442] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1338698873-172.17.0.5-1606980114492: 4ms
2020-12-03 07:22:07,361 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85): no suitable block pools found to scan.  Waiting 1814391274 ms.
2020-12-03 07:22:07,361 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-5b5d6366-8003-4c96-a269-54331e87d267): no suitable block pools found to scan.  Waiting 1814391274 ms.
2020-12-03 07:22:07,362 [Thread-442] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 1:20 PM with interval of 21600000ms
2020-12-03 07:22:07,363 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid bb4cfced-0bdc-453a-99a9-88d3aea04105) service to localhost/127.0.0.1:18020 beginning handshake with NN
2020-12-03 07:22:07,365 [IPC Server handler 7 on default port 18020] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:43584, datanodeUuid=bb4cfced-0bdc-453a-99a9-88d3aea04105, infoPort=36907, infoSecurePort=0, ipcPort=33243, storageInfo=lv=-57;cid=testClusterID;nsid=1772939335;c=1606980114492) storage bb4cfced-0bdc-453a-99a9-88d3aea04105
2020-12-03 07:22:07,365 [IPC Server handler 7 on default port 18020] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1078)) - BLOCK* registerDatanode: 127.0.0.1:33955 is replaced by DatanodeRegistration(127.0.0.1:43584, datanodeUuid=bb4cfced-0bdc-453a-99a9-88d3aea04105, infoPort=36907, infoSecurePort=0, ipcPort=33243, storageInfo=lv=-57;cid=testClusterID;nsid=1772939335;c=1606980114492) with the same storageID bb4cfced-0bdc-453a-99a9-88d3aea04105
2020-12-03 07:22:07,365 [IPC Server handler 7 on default port 18020] INFO  net.NetworkTopology (NetworkTopology.java:remove(219)) - Removing a node: /default-rack/127.0.0.1:33955
2020-12-03 07:22:07,365 [IPC Server handler 7 on default port 18020] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:43584
2020-12-03 07:22:07,366 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid bb4cfced-0bdc-453a-99a9-88d3aea04105) service to localhost/127.0.0.1:18020 successfully registered with NN
2020-12-03 07:22:07,367 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (DataNode.java:registerBlockPoolWithSecretManager(1615)) - Block token params received from NN: for block pool BP-1338698873-172.17.0.5-1606980114492 keyUpdateInterval=600 min(s), tokenLifetime=600 min(s)
2020-12-03 07:22:07,367 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:22:07,367 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:18020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=1000
2020-12-03 07:22:07,374 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x539b5d50ff03c7ab: Processing first storage report for DS-5b5d6366-8003-4c96-a269-54331e87d267 from datanode bb4cfced-0bdc-453a-99a9-88d3aea04105
2020-12-03 07:22:07,374 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x539b5d50ff03c7ab: from storage DS-5b5d6366-8003-4c96-a269-54331e87d267 node DatanodeRegistration(127.0.0.1:43584, datanodeUuid=bb4cfced-0bdc-453a-99a9-88d3aea04105, infoPort=36907, infoSecurePort=0, ipcPort=33243, storageInfo=lv=-57;cid=testClusterID;nsid=1772939335;c=1606980114492), blocks: 1, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
2020-12-03 07:22:07,375 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x539b5d50ff03c7ab: Processing first storage report for DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85 from datanode bb4cfced-0bdc-453a-99a9-88d3aea04105
2020-12-03 07:22:07,375 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x539b5d50ff03c7ab: from storage DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85 node DatanodeRegistration(127.0.0.1:43584, datanodeUuid=bb4cfced-0bdc-453a-99a9-88d3aea04105, infoPort=36907, infoSecurePort=0, ipcPort=33243, storageInfo=lv=-57;cid=testClusterID;nsid=1772939335;c=1606980114492), blocks: 1, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:22:07,376 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x539b5d50ff03c7ab,  containing 2 storage report(s), of which we sent 2. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 3 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:22:07,376 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:07,448 [IPC Server handler 2 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:22:07,452 [Listener at localhost/33243] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2758)) - Cluster is active
2020-12-03 07:22:07,452 [Listener at localhost/33243] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:33955
2020-12-03 07:22:07,454 [Listener at localhost/33243] WARN  impl.BlockReaderFactory (BlockReaderFactory.java:getRemoteBlockReaderFromTcp(765)) - I/O error constructing remote block reader.
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:533)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2940)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:822)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:747)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:380)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:644)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:575)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:757)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:829)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.checkFile1(TestBlockTokenWithDFS.java:102)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.doTestRead(TestBlockTokenWithDFS.java:610)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.testRead(TestBlockTokenWithDFS.java:360)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-12-03 07:22:07,455 [Listener at localhost/33243] WARN  hdfs.DFSClient (DFSInputStream.java:blockSeekTo(597)) - Failed to connect to /127.0.0.1:33955 for file /fileToRead.dat for block BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001, add to deadNodes and continue. 
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:533)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2940)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:822)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:747)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:380)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:644)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:575)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:757)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:829)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.checkFile1(TestBlockTokenWithDFS.java:102)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.doTestRead(TestBlockTokenWithDFS.java:610)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.testRead(TestBlockTokenWithDFS.java:360)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-12-03 07:22:07,455 [Listener at localhost/33243] WARN  hdfs.DFSClient (DFSInputStream.java:reportLostBlock(963)) - No live nodes contain block BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001 after checking nodes = [DatanodeInfoWithStorage[127.0.0.1:32825,DS-ef772597-ccee-4e3e-8242-d1e42fd95974,DISK], DatanodeInfoWithStorage[127.0.0.1:33955,DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85,DISK]], ignoredNodes = null
2020-12-03 07:22:07,456 [Listener at localhost/33243] INFO  hdfs.DFSClient (DFSInputStream.java:refetchLocations(886)) - Could not obtain BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[127.0.0.1:32825,DS-ef772597-ccee-4e3e-8242-d1e42fd95974,DISK] DatanodeInfoWithStorage[127.0.0.1:33955,DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85,DISK] Dead nodes:  DatanodeInfoWithStorage[127.0.0.1:33955,DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85,DISK] DatanodeInfoWithStorage[127.0.0.1:32825,DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd,DISK]. Will get new block locations from namenode and retry...
2020-12-03 07:22:07,456 [Listener at localhost/33243] WARN  hdfs.DFSClient (DFSInputStream.java:refetchLocations(905)) - DFS chooseDataNode: got # 1 IOException, will wait for 0.7537202562126799 msec.
2020-12-03 07:22:07,462 [IPC Server handler 0 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/fileToRead.dat	dst=null	perm=null	proto=rpc
2020-12-03 07:22:07,476 [Listener at localhost/33243] DEBUG hdfs.DFSClient (DFSInputStream.java:fetchLocatedBlocksAndGetLastBlockLength(243)) - newInfo = LocatedBlocks{;  fileLength=2048;  underConstruction=false;  blocks=[LocatedBlock{BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001; getBlockSize()=1024; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43584,DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85,DISK], DatanodeInfoWithStorage[127.0.0.1:39472,DS-ef772597-ccee-4e3e-8242-d1e42fd95974,DISK]]}, LocatedBlock{BP-1338698873-172.17.0.5-1606980114492:blk_1073741826_1002; getBlockSize()=1024; corrupt=false; offset=1024; locs=[DatanodeInfoWithStorage[127.0.0.1:43584,DS-5b5d6366-8003-4c96-a269-54331e87d267,DISK], DatanodeInfoWithStorage[127.0.0.1:39472,DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd,DISK]]}];  lastLocatedBlock=LocatedBlock{BP-1338698873-172.17.0.5-1606980114492:blk_1073741826_1002; getBlockSize()=1024; corrupt=false; offset=1024; locs=[DatanodeInfoWithStorage[127.0.0.1:39472,DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd,DISK], DatanodeInfoWithStorage[127.0.0.1:43584,DS-5b5d6366-8003-4c96-a269-54331e87d267,DISK]]};  isLastBlockComplete=true;  ecPolicy=null}
2020-12-03 07:22:07,480 [Listener at localhost/33243] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:43584
2020-12-03 07:22:07,481 [Listener at localhost/33243] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:07,489 [Listener at localhost/33243] INFO  hdfs.DFSClient (DFSInputStream.java:blockSeekTo(579)) - Successfully connected to /127.0.0.1:43584 for BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001
2020-12-03 07:22:07,490 [Listener at localhost/33243] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:43584
2020-12-03 07:22:07,493 [Listener at localhost/33243] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:33955
2020-12-03 07:22:07,498 [Listener at localhost/33243] WARN  impl.BlockReaderFactory (BlockReaderFactory.java:getRemoteBlockReaderFromTcp(765)) - I/O error constructing remote block reader.
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:533)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2940)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:822)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:747)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:380)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:644)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:575)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToNewSource(DFSInputStream.java:1514)
	at org.apache.hadoop.fs.FSDataInputStream.seekToNewSource(FSDataInputStream.java:131)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.doTestRead(TestBlockTokenWithDFS.java:615)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.testRead(TestBlockTokenWithDFS.java:360)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-12-03 07:22:07,499 [Listener at localhost/33243] WARN  hdfs.DFSClient (DFSInputStream.java:blockSeekTo(597)) - Failed to connect to /127.0.0.1:33955 for file /fileToRead.dat for block BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001, add to deadNodes and continue. 
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:533)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2940)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:822)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:747)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:380)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:644)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:575)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToNewSource(DFSInputStream.java:1514)
	at org.apache.hadoop.fs.FSDataInputStream.seekToNewSource(FSDataInputStream.java:131)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.doTestRead(TestBlockTokenWithDFS.java:615)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.testRead(TestBlockTokenWithDFS.java:360)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-12-03 07:22:07,499 [Listener at localhost/33243] WARN  hdfs.DFSClient (DFSInputStream.java:reportLostBlock(963)) - No live nodes contain block BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001 after checking nodes = [DatanodeInfoWithStorage[127.0.0.1:33955,DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85,DISK], DatanodeInfoWithStorage[127.0.0.1:32825,DS-ef772597-ccee-4e3e-8242-d1e42fd95974,DISK]], ignoredNodes = null
2020-12-03 07:22:07,500 [Listener at localhost/33243] INFO  hdfs.DFSClient (DFSInputStream.java:refetchLocations(886)) - Could not obtain BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[127.0.0.1:33955,DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85,DISK] DatanodeInfoWithStorage[127.0.0.1:32825,DS-ef772597-ccee-4e3e-8242-d1e42fd95974,DISK] Dead nodes:  DatanodeInfoWithStorage[127.0.0.1:33955,DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85,DISK] DatanodeInfoWithStorage[127.0.0.1:32825,DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd,DISK]. Will get new block locations from namenode and retry...
2020-12-03 07:22:07,500 [Listener at localhost/33243] WARN  hdfs.DFSClient (DFSInputStream.java:refetchLocations(905)) - DFS chooseDataNode: got # 1 IOException, will wait for 5.801199267742004 msec.
2020-12-03 07:22:07,509 [IPC Server handler 1 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/fileToRead.dat	dst=null	perm=null	proto=rpc
2020-12-03 07:22:07,526 [Listener at localhost/33243] DEBUG hdfs.DFSClient (DFSInputStream.java:fetchLocatedBlocksAndGetLastBlockLength(243)) - newInfo = LocatedBlocks{;  fileLength=2048;  underConstruction=false;  blocks=[LocatedBlock{BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001; getBlockSize()=1024; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39472,DS-ef772597-ccee-4e3e-8242-d1e42fd95974,DISK], DatanodeInfoWithStorage[127.0.0.1:43584,DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85,DISK]]}, LocatedBlock{BP-1338698873-172.17.0.5-1606980114492:blk_1073741826_1002; getBlockSize()=1024; corrupt=false; offset=1024; locs=[DatanodeInfoWithStorage[127.0.0.1:43584,DS-5b5d6366-8003-4c96-a269-54331e87d267,DISK], DatanodeInfoWithStorage[127.0.0.1:39472,DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd,DISK]]}];  lastLocatedBlock=LocatedBlock{BP-1338698873-172.17.0.5-1606980114492:blk_1073741826_1002; getBlockSize()=1024; corrupt=false; offset=1024; locs=[DatanodeInfoWithStorage[127.0.0.1:39472,DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd,DISK], DatanodeInfoWithStorage[127.0.0.1:43584,DS-5b5d6366-8003-4c96-a269-54331e87d267,DISK]]};  isLastBlockComplete=true;  ecPolicy=null}
2020-12-03 07:22:07,527 [Listener at localhost/33243] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:39472
2020-12-03 07:22:07,528 [Listener at localhost/33243] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:07,536 [Listener at localhost/33243] INFO  hdfs.DFSClient (DFSInputStream.java:blockSeekTo(579)) - Successfully connected to /127.0.0.1:39472 for BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001
2020-12-03 07:22:07,536 [Listener at localhost/33243] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:43584
2020-12-03 07:22:07,537 [Listener at localhost/33243] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:07,540 [Listener at localhost/33243] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:33955
2020-12-03 07:22:07,541 [Listener at localhost/33243] WARN  impl.BlockReaderFactory (BlockReaderFactory.java:getRemoteBlockReaderFromTcp(765)) - I/O error constructing remote block reader.
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:533)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2940)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:822)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:747)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:380)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:644)
	at org.apache.hadoop.hdfs.DFSInputStream.actualGetFromOneDataNode(DFSInputStream.java:1049)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchBlockByteRange(DFSInputStream.java:1001)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:92)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.checkFile2(TestBlockTokenWithDFS.java:116)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.doTestRead(TestBlockTokenWithDFS.java:619)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.testRead(TestBlockTokenWithDFS.java:360)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-12-03 07:22:07,541 [Listener at localhost/33243] WARN  hdfs.DFSClient (DFSInputStream.java:actualGetFromOneDataNode(1107)) - Connection failure: Failed to connect to /127.0.0.1:33955 for file /fileToRead.dat for block BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001:java.net.ConnectException: Connection refused
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:533)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2940)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:822)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:747)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:380)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:644)
	at org.apache.hadoop.hdfs.DFSInputStream.actualGetFromOneDataNode(DFSInputStream.java:1049)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchBlockByteRange(DFSInputStream.java:1001)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:92)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.checkFile2(TestBlockTokenWithDFS.java:116)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.doTestRead(TestBlockTokenWithDFS.java:619)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.testRead(TestBlockTokenWithDFS.java:360)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-12-03 07:22:07,542 [Listener at localhost/33243] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:32825
2020-12-03 07:22:07,544 [Listener at localhost/33243] WARN  impl.BlockReaderFactory (BlockReaderFactory.java:getRemoteBlockReaderFromTcp(765)) - I/O error constructing remote block reader.
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:533)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2940)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:822)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:747)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:380)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:644)
	at org.apache.hadoop.hdfs.DFSInputStream.actualGetFromOneDataNode(DFSInputStream.java:1049)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchBlockByteRange(DFSInputStream.java:1001)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:92)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.checkFile2(TestBlockTokenWithDFS.java:116)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.doTestRead(TestBlockTokenWithDFS.java:619)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.testRead(TestBlockTokenWithDFS.java:360)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-12-03 07:22:07,545 [Listener at localhost/33243] WARN  hdfs.DFSClient (DFSInputStream.java:actualGetFromOneDataNode(1107)) - Connection failure: Failed to connect to /127.0.0.1:32825 for file /fileToRead.dat for block BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001:java.net.ConnectException: Connection refused
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:533)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2940)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:822)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:747)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:380)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:644)
	at org.apache.hadoop.hdfs.DFSInputStream.actualGetFromOneDataNode(DFSInputStream.java:1049)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchBlockByteRange(DFSInputStream.java:1001)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:92)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.checkFile2(TestBlockTokenWithDFS.java:116)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.doTestRead(TestBlockTokenWithDFS.java:619)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.testRead(TestBlockTokenWithDFS.java:360)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-12-03 07:22:07,545 [Listener at localhost/33243] WARN  hdfs.DFSClient (DFSInputStream.java:reportLostBlock(963)) - No live nodes contain block BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001 after checking nodes = [DatanodeInfoWithStorage[127.0.0.1:33955,DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85,DISK], DatanodeInfoWithStorage[127.0.0.1:32825,DS-ef772597-ccee-4e3e-8242-d1e42fd95974,DISK]], ignoredNodes = null
2020-12-03 07:22:07,546 [Listener at localhost/33243] INFO  hdfs.DFSClient (DFSInputStream.java:refetchLocations(886)) - Could not obtain BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[127.0.0.1:33955,DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85,DISK] DatanodeInfoWithStorage[127.0.0.1:32825,DS-ef772597-ccee-4e3e-8242-d1e42fd95974,DISK] Dead nodes:  DatanodeInfoWithStorage[127.0.0.1:33955,DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85,DISK] DatanodeInfoWithStorage[127.0.0.1:32825,DS-ef772597-ccee-4e3e-8242-d1e42fd95974,DISK]. Will get new block locations from namenode and retry...
2020-12-03 07:22:07,546 [Listener at localhost/33243] WARN  hdfs.DFSClient (DFSInputStream.java:refetchLocations(905)) - DFS chooseDataNode: got # 1 IOException, will wait for 0.6499341220082777 msec.
2020-12-03 07:22:07,551 [IPC Server handler 3 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/fileToRead.dat	dst=null	perm=null	proto=rpc
2020-12-03 07:22:07,566 [Listener at localhost/33243] DEBUG hdfs.DFSClient (DFSInputStream.java:fetchLocatedBlocksAndGetLastBlockLength(243)) - newInfo = LocatedBlocks{;  fileLength=2048;  underConstruction=false;  blocks=[LocatedBlock{BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001; getBlockSize()=1024; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43584,DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85,DISK], DatanodeInfoWithStorage[127.0.0.1:39472,DS-ef772597-ccee-4e3e-8242-d1e42fd95974,DISK]]}, LocatedBlock{BP-1338698873-172.17.0.5-1606980114492:blk_1073741826_1002; getBlockSize()=1024; corrupt=false; offset=1024; locs=[DatanodeInfoWithStorage[127.0.0.1:39472,DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd,DISK], DatanodeInfoWithStorage[127.0.0.1:43584,DS-5b5d6366-8003-4c96-a269-54331e87d267,DISK]]}];  lastLocatedBlock=LocatedBlock{BP-1338698873-172.17.0.5-1606980114492:blk_1073741826_1002; getBlockSize()=1024; corrupt=false; offset=1024; locs=[DatanodeInfoWithStorage[127.0.0.1:39472,DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd,DISK], DatanodeInfoWithStorage[127.0.0.1:43584,DS-5b5d6366-8003-4c96-a269-54331e87d267,DISK]]};  isLastBlockComplete=true;  ecPolicy=null}
2020-12-03 07:22:07,567 [Listener at localhost/33243] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:43584
2020-12-03 07:22:07,569 [Listener at localhost/33243] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:07,581 [Listener at localhost/33243] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:33955
2020-12-03 07:22:07,581 [Listener at localhost/33243] WARN  impl.BlockReaderFactory (BlockReaderFactory.java:getRemoteBlockReaderFromTcp(765)) - I/O error constructing remote block reader.
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:533)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2940)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:822)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:747)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:380)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:644)
	at org.apache.hadoop.hdfs.DFSInputStream.actualGetFromOneDataNode(DFSInputStream.java:1049)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchBlockByteRange(DFSInputStream.java:1001)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:92)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.checkFile2(TestBlockTokenWithDFS.java:116)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.doTestRead(TestBlockTokenWithDFS.java:619)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.testRead(TestBlockTokenWithDFS.java:360)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-12-03 07:22:07,582 [Listener at localhost/33243] WARN  hdfs.DFSClient (DFSInputStream.java:actualGetFromOneDataNode(1107)) - Connection failure: Failed to connect to /127.0.0.1:33955 for file /fileToRead.dat for block BP-1338698873-172.17.0.5-1606980114492:blk_1073741826_1002:java.net.ConnectException: Connection refused
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:533)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2940)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:822)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:747)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:380)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:644)
	at org.apache.hadoop.hdfs.DFSInputStream.actualGetFromOneDataNode(DFSInputStream.java:1049)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchBlockByteRange(DFSInputStream.java:1001)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:92)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.checkFile2(TestBlockTokenWithDFS.java:116)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.doTestRead(TestBlockTokenWithDFS.java:619)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.testRead(TestBlockTokenWithDFS.java:360)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-12-03 07:22:07,583 [Listener at localhost/33243] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:32825
2020-12-03 07:22:07,585 [Listener at localhost/33243] WARN  impl.BlockReaderFactory (BlockReaderFactory.java:getRemoteBlockReaderFromTcp(765)) - I/O error constructing remote block reader.
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:533)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2940)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:822)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:747)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:380)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:644)
	at org.apache.hadoop.hdfs.DFSInputStream.actualGetFromOneDataNode(DFSInputStream.java:1049)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchBlockByteRange(DFSInputStream.java:1001)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:92)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.checkFile2(TestBlockTokenWithDFS.java:116)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.doTestRead(TestBlockTokenWithDFS.java:619)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.testRead(TestBlockTokenWithDFS.java:360)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-12-03 07:22:07,585 [Listener at localhost/33243] WARN  hdfs.DFSClient (DFSInputStream.java:actualGetFromOneDataNode(1107)) - Connection failure: Failed to connect to /127.0.0.1:32825 for file /fileToRead.dat for block BP-1338698873-172.17.0.5-1606980114492:blk_1073741826_1002:java.net.ConnectException: Connection refused
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:533)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2940)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:822)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:747)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:380)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:644)
	at org.apache.hadoop.hdfs.DFSInputStream.actualGetFromOneDataNode(DFSInputStream.java:1049)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchBlockByteRange(DFSInputStream.java:1001)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:92)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.checkFile2(TestBlockTokenWithDFS.java:116)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.doTestRead(TestBlockTokenWithDFS.java:619)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS.testRead(TestBlockTokenWithDFS.java:360)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-12-03 07:22:07,587 [Listener at localhost/33243] WARN  hdfs.DFSClient (DFSInputStream.java:reportLostBlock(963)) - No live nodes contain block BP-1338698873-172.17.0.5-1606980114492:blk_1073741826_1002 after checking nodes = [DatanodeInfoWithStorage[127.0.0.1:33955,DS-5b5d6366-8003-4c96-a269-54331e87d267,DISK], DatanodeInfoWithStorage[127.0.0.1:32825,DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd,DISK]], ignoredNodes = null
2020-12-03 07:22:07,588 [Listener at localhost/33243] INFO  hdfs.DFSClient (DFSInputStream.java:refetchLocations(886)) - Could not obtain BP-1338698873-172.17.0.5-1606980114492:blk_1073741826_1002 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[127.0.0.1:33955,DS-5b5d6366-8003-4c96-a269-54331e87d267,DISK] DatanodeInfoWithStorage[127.0.0.1:32825,DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd,DISK] Dead nodes:  DatanodeInfoWithStorage[127.0.0.1:33955,DS-5b5d6366-8003-4c96-a269-54331e87d267,DISK] DatanodeInfoWithStorage[127.0.0.1:32825,DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd,DISK]. Will get new block locations from namenode and retry...
2020-12-03 07:22:07,588 [Listener at localhost/33243] WARN  hdfs.DFSClient (DFSInputStream.java:refetchLocations(905)) - DFS chooseDataNode: got # 2 IOException, will wait for 24.733205984592892 msec.
2020-12-03 07:22:07,617 [IPC Server handler 4 on default port 18020] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/fileToRead.dat	dst=null	perm=null	proto=rpc
2020-12-03 07:22:07,622 [Listener at localhost/33243] DEBUG hdfs.DFSClient (DFSInputStream.java:fetchLocatedBlocksAndGetLastBlockLength(243)) - newInfo = LocatedBlocks{;  fileLength=2048;  underConstruction=false;  blocks=[LocatedBlock{BP-1338698873-172.17.0.5-1606980114492:blk_1073741825_1001; getBlockSize()=1024; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43584,DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85,DISK], DatanodeInfoWithStorage[127.0.0.1:39472,DS-ef772597-ccee-4e3e-8242-d1e42fd95974,DISK]]}, LocatedBlock{BP-1338698873-172.17.0.5-1606980114492:blk_1073741826_1002; getBlockSize()=1024; corrupt=false; offset=1024; locs=[DatanodeInfoWithStorage[127.0.0.1:43584,DS-5b5d6366-8003-4c96-a269-54331e87d267,DISK], DatanodeInfoWithStorage[127.0.0.1:39472,DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd,DISK]]}];  lastLocatedBlock=LocatedBlock{BP-1338698873-172.17.0.5-1606980114492:blk_1073741826_1002; getBlockSize()=1024; corrupt=false; offset=1024; locs=[DatanodeInfoWithStorage[127.0.0.1:39472,DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd,DISK], DatanodeInfoWithStorage[127.0.0.1:43584,DS-5b5d6366-8003-4c96-a269-54331e87d267,DISK]]};  isLastBlockComplete=true;  ecPolicy=null}
2020-12-03 07:22:07,623 [Listener at localhost/33243] DEBUG hdfs.DFSClient (DFSInputStream.java:getBestNodeDNAddrPair(952)) - Connecting to datanode 127.0.0.1:43584
2020-12-03 07:22:07,627 [Listener at localhost/33243] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(2049)) - Shutting down the Mini HDFS Cluster
2020-12-03 07:22:07,627 [Listener at localhost/33243] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 1
2020-12-03 07:22:07,628 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@1931d99] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:22:07,628 [Listener at localhost/33243] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:22:07,633 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-5b5d6366-8003-4c96-a269-54331e87d267) exiting.
2020-12-03 07:22:07,633 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-24d6c9aa-27ec-4b16-82f3-5b5a7a750b85) exiting.
2020-12-03 07:22:07,660 [Listener at localhost/33243] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@58a4a74d{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:22:07,664 [Listener at localhost/33243] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@54aca26f{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:22:07,665 [Listener at localhost/33243] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@34a0ef00{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:22:07,667 [Listener at localhost/33243] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@6293e39e{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:22:07,685 [Listener at localhost/33243] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 33243
2020-12-03 07:22:07,703 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:22:07,703 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid bb4cfced-0bdc-453a-99a9-88d3aea04105) service to localhost/127.0.0.1:18020
2020-12-03 07:22:07,704 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid bb4cfced-0bdc-453a-99a9-88d3aea04105)
2020-12-03 07:22:07,704 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:07,704 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:22:07,704 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:22:07,717 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1338698873-172.17.0.5-1606980114492] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:07,718 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1338698873-172.17.0.5-1606980114492] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:07,756 [Listener at localhost/33243] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:22:07,756 [Listener at localhost/33243] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:22:07,758 [Listener at localhost/33243] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:22:07,758 [Listener at localhost/33243] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:22:07,761 [Listener at localhost/33243] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:22:07,762 [Listener at localhost/33243] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 0
2020-12-03 07:22:07,762 [Listener at localhost/33243] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:22:07,762 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@705f83a7] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:22:07,764 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, DS-3c6e7ba6-c19d-4e0d-bbaf-02184b7648fd) exiting.
2020-12-03 07:22:07,765 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3, DS-ef772597-ccee-4e3e-8242-d1e42fd95974) exiting.
2020-12-03 07:22:07,796 [Listener at localhost/33243] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@540dbda9{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:22:07,797 [Listener at localhost/33243] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@22bb5646{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:22:07,798 [Listener at localhost/33243] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@451f87af{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:22:07,798 [Listener at localhost/33243] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@2679311f{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:22:07,803 [Listener at localhost/33243] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 44628
2020-12-03 07:22:07,812 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:22:07,812 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:22:07,814 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:22:07,814 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid fcfa3dd6-e926-44de-87fa-c54cedb559cc) service to localhost/127.0.0.1:18020
2020-12-03 07:22:07,815 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1338698873-172.17.0.5-1606980114492 (Datanode Uuid fcfa3dd6-e926-44de-87fa-c54cedb559cc)
2020-12-03 07:22:07,815 [BP-1338698873-172.17.0.5-1606980114492 heartbeating to localhost/127.0.0.1:18020] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1338698873-172.17.0.5-1606980114492
2020-12-03 07:22:07,816 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1338698873-172.17.0.5-1606980114492] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:07,816 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1338698873-172.17.0.5-1606980114492] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:07,822 [Listener at localhost/33243] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:22:07,823 [Listener at localhost/33243] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:22:07,877 [Listener at localhost/33243] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:22:07,877 [Listener at localhost/33243] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:22:07,884 [Listener at localhost/33243] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:22:07,884 [Listener at localhost/33243] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:stopAndJoinNameNode(2130)) - Shutting down the namenode
2020-12-03 07:22:07,884 [Listener at localhost/33243] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1334)) - Stopping services started for active state
2020-12-03 07:22:07,885 [Listener at localhost/33243] INFO  namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1410)) - Ending log segment 15, 15
2020-12-03 07:22:07,885 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@16c8b7bd] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4107)) - NameNodeEditLogRoller was interrupted, exiting
2020-12-03 07:22:07,886 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@7159139f] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4198)) - LazyPersistFileScrubber was interrupted, exiting
2020-12-03 07:22:07,889 [Listener at localhost/33243] INFO  namenode.FSEditLog (FSEditLog.java:printStatistics(778)) - Number of transactions: 2 Total time for transactions(ms): 5 Number of transactions batched in Syncs: 14 Number of syncs: 3 SyncTimes(ms): 2 2 
2020-12-03 07:22:07,890 [Listener at localhost/33243] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(145)) - Finalizing edits file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_inprogress_0000000000000000015 -> /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000015-0000000000000000016
2020-12-03 07:22:07,891 [Listener at localhost/33243] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(145)) - Finalizing edits file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_inprogress_0000000000000000015 -> /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000015-0000000000000000016
2020-12-03 07:22:07,891 [FSEditLogAsync] INFO  namenode.FSEditLog (FSEditLogAsync.java:run(253)) - FSEditLogAsync was interrupted, exiting
2020-12-03 07:22:07,891 [CacheReplicationMonitor(658625640)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2020-12-03 07:22:07,906 [Listener at localhost/33243] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 18020
2020-12-03 07:22:07,914 [IPC Server listener on 18020] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 18020
2020-12-03 07:22:07,914 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:22:07,915 [StorageInfoMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4722)) - Stopping thread.
2020-12-03 07:22:07,915 [RedundancyMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4687)) - Stopping RedundancyMonitor.
2020-12-03 07:22:07,931 [Listener at localhost/33243] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1334)) - Stopping services started for active state
2020-12-03 07:22:07,931 [Listener at localhost/33243] INFO  namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1434)) - Stopping services started for standby state
2020-12-03 07:22:07,933 [Listener at localhost/33243] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@7207cb51{/,null,UNAVAILABLE}{/hdfs}
2020-12-03 07:22:07,935 [Listener at localhost/33243] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@2a27cb34{HTTP/1.1,[http/1.1]}{localhost:19870}
2020-12-03 07:22:07,935 [Listener at localhost/33243] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@6e91893{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:22:07,936 [Listener at localhost/33243] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@5e268ce6{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:22:07,938 [Listener at localhost/33243] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2020-12-03 07:22:07,951 [Listener at localhost/33243] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2020-12-03 07:22:07,952 [Listener at localhost/33243] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - DataNode metrics system shutdown complete.
msx-rc 0
