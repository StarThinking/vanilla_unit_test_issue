2020-12-03 07:21:43,777 [Thread-0] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(493)) - starting cluster: numNameNodes=1, numDataNodes=6
Formatting using clusterid: testClusterID
2020-12-03 07:21:44,779 [Thread-0] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(229)) - Edit logging is async:true
2020-12-03 07:21:44,794 [Thread-0] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(755)) - KeyProvider: null
2020-12-03 07:21:44,796 [Thread-0] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(123)) - fsLock is fair: true
2020-12-03 07:21:44,797 [Thread-0] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(141)) - Detailed lock hold time metrics enabled: false
2020-12-03 07:21:44,806 [Thread-0] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(780)) - fsOwner             = root (auth:SIMPLE)
2020-12-03 07:21:44,806 [Thread-0] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(781)) - supergroup          = supergroup
2020-12-03 07:21:44,807 [Thread-0] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(782)) - isPermissionEnabled = true
2020-12-03 07:21:44,808 [Thread-0] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(793)) - HA Enabled: false
2020-12-03 07:21:44,866 [Thread-0] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:21:44,872 [Thread-0] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - hadoop.configured.node.mapping is deprecated. Instead, use net.topology.configured.node.mapping
2020-12-03 07:21:44,873 [Thread-0] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(303)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-12-03 07:21:44,873 [Thread-0] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(311)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-12-03 07:21:44,880 [Thread-0] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-12-03 07:21:44,881 [Thread-0] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Dec 03 07:21:44
2020-12-03 07:21:44,884 [Thread-0] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-12-03 07:21:44,886 [Thread-0] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:21:44,889 [Thread-0] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.9 GB = 39.3 MB
2020-12-03 07:21:44,889 [Thread-0] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-12-03 07:21:44,913 [Thread-0] INFO  blockmanagement.BlockManager (BlockManager.java:createSPSManager(5183)) - Storage policy satisfier is disabled
2020-12-03 07:21:44,913 [Thread-0] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(595)) - dfs.block.access.token.enable = false
2020-12-03 07:21:44,921 [Thread-0] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-12-03 07:21:44,922 [Thread-0] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-12-03 07:21:44,922 [Thread-0] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-12-03 07:21:44,922 [Thread-0] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-12-03 07:21:44,923 [Thread-0] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(581)) - defaultReplication         = 3
2020-12-03 07:21:44,923 [Thread-0] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(582)) - maxReplication             = 512
2020-12-03 07:21:44,924 [Thread-0] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(583)) - minReplication             = 1
2020-12-03 07:21:44,924 [Thread-0] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(584)) - maxReplicationStreams      = 2
2020-12-03 07:21:44,924 [Thread-0] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(585)) - redundancyRecheckInterval  = 3000ms
2020-12-03 07:21:44,924 [Thread-0] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(586)) - encryptDataTransfer        = false
2020-12-03 07:21:44,924 [Thread-0] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(587)) - maxNumBlocksToLog          = 1000
2020-12-03 07:21:44,962 [Thread-0] INFO  namenode.FSDirectory (SerialNumberManager.java:<clinit>(51)) - GLOBAL serial map: bits=29 maxEntries=536870911
2020-12-03 07:21:44,963 [Thread-0] INFO  namenode.FSDirectory (SerialNumberManager.java:<clinit>(51)) - USER serial map: bits=24 maxEntries=16777215
2020-12-03 07:21:44,963 [Thread-0] INFO  namenode.FSDirectory (SerialNumberManager.java:<clinit>(51)) - GROUP serial map: bits=24 maxEntries=16777215
2020-12-03 07:21:44,963 [Thread-0] INFO  namenode.FSDirectory (SerialNumberManager.java:<clinit>(51)) - XATTR serial map: bits=24 maxEntries=16777215
2020-12-03 07:21:44,990 [Thread-0] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-12-03 07:21:44,991 [Thread-0] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:21:44,992 [Thread-0] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.9 GB = 19.6 MB
2020-12-03 07:21:44,992 [Thread-0] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-12-03 07:21:44,998 [Thread-0] INFO  namenode.FSDirectory (FSDirectory.java:<init>(290)) - ACLs enabled? false
2020-12-03 07:21:44,999 [Thread-0] INFO  namenode.FSDirectory (FSDirectory.java:<init>(294)) - POSIX ACL inheritance enabled? true
2020-12-03 07:21:44,999 [Thread-0] INFO  namenode.FSDirectory (FSDirectory.java:<init>(298)) - XAttrs enabled? true
2020-12-03 07:21:45,000 [Thread-0] INFO  namenode.NameNode (FSDirectory.java:<init>(362)) - Caching file names occurring more than 10 times
2020-12-03 07:21:45,006 [Thread-0] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-12-03 07:21:45,009 [Thread-0] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-12-03 07:21:45,015 [Thread-0] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-12-03 07:21:45,015 [Thread-0] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:21:45,018 [Thread-0] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.9 GB = 4.9 MB
2020-12-03 07:21:45,018 [Thread-0] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-12-03 07:21:45,031 [Thread-0] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-12-03 07:21:45,032 [Thread-0] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-12-03 07:21:45,032 [Thread-0] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-12-03 07:21:45,037 [Thread-0] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(996)) - Retry cache on namenode is enabled
2020-12-03 07:21:45,038 [Thread-0] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(1004)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-12-03 07:21:45,042 [Thread-0] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-12-03 07:21:45,042 [Thread-0] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:21:45,043 [Thread-0] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.9 GB = 603.0 KB
2020-12-03 07:21:45,043 [Thread-0] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-12-03 07:21:45,094 [Thread-0] INFO  namenode.FSImage (FSImage.java:format(185)) - Allocated new BlockPoolId: BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:45,301 [Thread-0] INFO  common.Storage (NNStorage.java:format(595)) - Storage directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1 has been successfully formatted.
2020-12-03 07:21:45,485 [Thread-0] INFO  common.Storage (NNStorage.java:format(595)) - Storage directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2 has been successfully formatted.
2020-12-03 07:21:45,534 [FSImageSaver for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(512)) - Saving image file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/fsimage.ckpt_0000000000000000000 using no compression
2020-12-03 07:21:45,534 [FSImageSaver for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(512)) - Saving image file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage.ckpt_0000000000000000000 using no compression
2020-12-03 07:21:45,701 [FSImageSaver for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(516)) - Image file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .
2020-12-03 07:21:45,701 [FSImageSaver for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(516)) - Image file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .
2020-12-03 07:21:45,859 [Thread-0] INFO  namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:getImageTxIdToRetain(203)) - Going to retain 1 images with txid >= 0
2020-12-03 07:21:45,864 [Thread-0] INFO  namenode.NameNode (NameNode.java:createNameNode(1632)) - createNameNode []
2020-12-03 07:21:45,989 [Thread-0] INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(118)) - Loaded properties from hadoop-metrics2.properties
2020-12-03 07:21:46,396 [Thread-0] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 0 second(s).
2020-12-03 07:21:46,396 [Thread-0] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2020-12-03 07:21:46,453 [Thread-0] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://127.0.0.1:0
2020-12-03 07:21:46,507 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@1c8b31bf] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:21:46,528 [Thread-0] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1641)) - Starting Web-server for hdfs at: http://localhost:0
2020-12-03 07:21:46,535 [Thread-0] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:21:46,556 [Thread-0] INFO  util.log (Log.java:initialized(192)) - Logging initialized @4087ms
2020-12-03 07:21:46,697 [Thread-0] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:21:46,702 [Thread-0] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-12-03 07:21:46,703 [Thread-0] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:21:46,714 [Thread-0] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:21:46,718 [Thread-0] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-12-03 07:21:46,719 [Thread-0] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:21:46,719 [Thread-0] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:21:46,762 [Thread-0] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(102)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-12-03 07:21:46,763 [Thread-0] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(806)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-12-03 07:21:46,775 [Thread-0] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 39633
2020-12-03 07:21:46,777 [Thread-0] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:21:46,851 [Thread-0] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@6ec984c3{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:21:46,853 [Thread-0] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@53aea7af{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:21:46,917 [Thread-0] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@40e12390{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-12-03 07:21:46,926 [Thread-0] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@23a07e07{HTTP/1.1,[http/1.1]}{localhost:39633}
2020-12-03 07:21:46,927 [Thread-0] INFO  server.Server (Server.java:doStart(419)) - Started @4458ms
2020-12-03 07:21:46,943 [Thread-0] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(229)) - Edit logging is async:true
2020-12-03 07:21:46,944 [Thread-0] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(755)) - KeyProvider: null
2020-12-03 07:21:46,944 [Thread-0] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(123)) - fsLock is fair: true
2020-12-03 07:21:46,945 [Thread-0] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(141)) - Detailed lock hold time metrics enabled: false
2020-12-03 07:21:46,945 [Thread-0] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(780)) - fsOwner             = root (auth:SIMPLE)
2020-12-03 07:21:46,945 [Thread-0] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(781)) - supergroup          = supergroup
2020-12-03 07:21:46,946 [Thread-0] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(782)) - isPermissionEnabled = true
2020-12-03 07:21:46,946 [Thread-0] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(793)) - HA Enabled: false
2020-12-03 07:21:46,947 [Thread-0] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:21:46,948 [Thread-0] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(303)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-12-03 07:21:46,948 [Thread-0] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(311)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-12-03 07:21:46,949 [Thread-0] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-12-03 07:21:46,949 [Thread-0] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Dec 03 07:21:46
2020-12-03 07:21:46,949 [Thread-0] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-12-03 07:21:46,950 [Thread-0] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:21:46,950 [Thread-0] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.8 GB = 36.4 MB
2020-12-03 07:21:46,950 [Thread-0] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-12-03 07:21:46,957 [Thread-0] INFO  blockmanagement.BlockManager (BlockManager.java:createSPSManager(5183)) - Storage policy satisfier is disabled
2020-12-03 07:21:46,958 [Thread-0] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(595)) - dfs.block.access.token.enable = false
2020-12-03 07:21:46,958 [Thread-0] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-12-03 07:21:46,959 [Thread-0] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-12-03 07:21:46,959 [Thread-0] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-12-03 07:21:46,959 [Thread-0] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-12-03 07:21:46,960 [Thread-0] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(581)) - defaultReplication         = 3
2020-12-03 07:21:46,960 [Thread-0] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(582)) - maxReplication             = 512
2020-12-03 07:21:46,960 [Thread-0] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(583)) - minReplication             = 1
2020-12-03 07:21:46,960 [Thread-0] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(584)) - maxReplicationStreams      = 2
2020-12-03 07:21:46,961 [Thread-0] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(585)) - redundancyRecheckInterval  = 3000ms
2020-12-03 07:21:46,961 [Thread-0] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(586)) - encryptDataTransfer        = false
2020-12-03 07:21:46,961 [Thread-0] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(587)) - maxNumBlocksToLog          = 1000
2020-12-03 07:21:46,962 [Thread-0] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-12-03 07:21:46,962 [Thread-0] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:21:46,962 [Thread-0] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.8 GB = 18.2 MB
2020-12-03 07:21:46,963 [Thread-0] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-12-03 07:21:46,965 [Thread-0] INFO  namenode.FSDirectory (FSDirectory.java:<init>(290)) - ACLs enabled? false
2020-12-03 07:21:46,966 [Thread-0] INFO  namenode.FSDirectory (FSDirectory.java:<init>(294)) - POSIX ACL inheritance enabled? true
2020-12-03 07:21:46,966 [Thread-0] INFO  namenode.FSDirectory (FSDirectory.java:<init>(298)) - XAttrs enabled? true
2020-12-03 07:21:46,966 [Thread-0] INFO  namenode.NameNode (FSDirectory.java:<init>(362)) - Caching file names occurring more than 10 times
2020-12-03 07:21:46,966 [Thread-0] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-12-03 07:21:46,967 [Thread-0] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-12-03 07:21:46,967 [Thread-0] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-12-03 07:21:46,967 [Thread-0] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:21:46,968 [Thread-0] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.8 GB = 4.6 MB
2020-12-03 07:21:46,968 [Thread-0] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-12-03 07:21:46,969 [Thread-0] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-12-03 07:21:46,969 [Thread-0] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-12-03 07:21:46,970 [Thread-0] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-12-03 07:21:46,970 [Thread-0] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(996)) - Retry cache on namenode is enabled
2020-12-03 07:21:46,971 [Thread-0] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(1004)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-12-03 07:21:46,971 [Thread-0] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-12-03 07:21:46,971 [Thread-0] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:21:46,972 [Thread-0] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.8 GB = 559.3 KB
2020-12-03 07:21:46,972 [Thread-0] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-12-03 07:21:47,048 [Thread-0] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/in_use.lock acquired by nodename 2751@a3be22c8fbc1
2020-12-03 07:21:47,191 [Thread-0] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/in_use.lock acquired by nodename 2751@a3be22c8fbc1
2020-12-03 07:21:47,196 [Thread-0] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(428)) - Recovering unfinalized segments in /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current
2020-12-03 07:21:47,196 [Thread-0] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(428)) - Recovering unfinalized segments in /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current
2020-12-03 07:21:47,197 [Thread-0] INFO  namenode.FSImage (FSImage.java:loadFSImage(733)) - No edit log streams selected.
2020-12-03 07:21:47,197 [Thread-0] INFO  namenode.FSImage (FSImage.java:loadFSImageFile(797)) - Planning to load image: FSImageFile(file=/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2020-12-03 07:21:47,230 [Thread-0] INFO  namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(234)) - Loading 1 INodes.
2020-12-03 07:21:47,241 [Thread-0] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(246)) - Loaded FSImage in 0 seconds.
2020-12-03 07:21:47,241 [Thread-0] INFO  namenode.FSImage (FSImage.java:loadFSImage(978)) - Loaded image for txid 0 from /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000000
2020-12-03 07:21:47,248 [Thread-0] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1110)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-12-03 07:21:47,250 [Thread-0] INFO  namenode.FSEditLog (FSEditLog.java:startLogSegment(1365)) - Starting log segment at 1
2020-12-03 07:21:47,860 [Thread-0] INFO  namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2020-12-03 07:21:47,861 [Thread-0] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(727)) - Finished loading FSImage in 886 msecs
2020-12-03 07:21:48,089 [Thread-0] INFO  namenode.NameNode (NameNodeRpcServer.java:<init>(448)) - RPC server is binding to localhost:0
2020-12-03 07:21:48,152 [Thread-0] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:21:48,169 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:21:48,561 [Listener at localhost/40508] INFO  namenode.NameNode (NameNode.java:initialize(722)) - Clients are to use localhost:40508 to access this namenode/service.
2020-12-03 07:21:48,566 [Listener at localhost/40508] INFO  namenode.FSNamesystem (FSNamesystem.java:registerMBean(5090)) - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2020-12-03 07:21:48,594 [Listener at localhost/40508] INFO  namenode.LeaseManager (LeaseManager.java:getNumUnderConstructionBlocks(171)) - Number of blocks under construction: 0
2020-12-03 07:21:48,617 [Listener at localhost/40508] INFO  blockmanagement.BlockManager (BlockManager.java:initializeReplQueues(4922)) - initializing replication queues
2020-12-03 07:21:48,620 [Listener at localhost/40508] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(400)) - STATE* Leaving safe mode after 0 secs
2020-12-03 07:21:48,625 [Listener at localhost/40508] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(406)) - STATE* Network topology has 0 racks and 0 datanodes
2020-12-03 07:21:48,626 [Listener at localhost/40508] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(408)) - STATE* UnderReplicatedBlocks has 0 blocks
2020-12-03 07:21:48,656 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3585)) - Total number of blocks            = 0
2020-12-03 07:21:48,656 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3586)) - Number of invalid blocks          = 0
2020-12-03 07:21:48,657 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3587)) - Number of under-replicated blocks = 0
2020-12-03 07:21:48,657 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3588)) - Number of  over-replicated blocks = 0
2020-12-03 07:21:48,660 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3590)) - Number of blocks being written    = 0
2020-12-03 07:21:48,660 [Reconstruction Queue Initializer] INFO  hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(3593)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 40 msec
2020-12-03 07:21:48,708 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:21:48,708 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:21:48,713 [Listener at localhost/40508] INFO  namenode.NameNode (NameNode.java:startCommonServices(828)) - NameNode RPC up at: localhost/127.0.0.1:40508
2020-12-03 07:21:48,717 [Listener at localhost/40508] INFO  namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1222)) - Starting services required for active state
2020-12-03 07:21:48,717 [Listener at localhost/40508] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(777)) - Initializing quota with 4 thread(s)
2020-12-03 07:21:48,726 [Listener at localhost/40508] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(786)) - Quota initialization completed in 9 milliseconds
name space=1
storage space=0
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2020-12-03 07:21:48,740 [CacheReplicationMonitor(1884595011)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-12-03 07:21:48,741 [Listener at localhost/40508] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 0 with dfs.datanode.data.dir: [RAM_DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1,[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:21:48,845 [Listener at localhost/40508] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [RAM_DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:21:48,866 [Listener at localhost/40508] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:21:48,901 [Listener at localhost/40508] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:21:48,908 [Listener at localhost/40508] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:21:48,912 [Listener at localhost/40508] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:21:48,918 [Listener at localhost/40508] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:21:48,919 [Listener at localhost/40508] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:21:48,926 [Listener at localhost/40508] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:21:48,941 [Listener at localhost/40508] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:43386
2020-12-03 07:21:48,944 [Listener at localhost/40508] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:21:48,945 [Listener at localhost/40508] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:21:48,976 [Listener at localhost/40508] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:21:48,980 [Listener at localhost/40508] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:21:48,981 [Listener at localhost/40508] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:21:48,981 [Listener at localhost/40508] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:21:48,985 [Listener at localhost/40508] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:21:48,987 [Listener at localhost/40508] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:21:48,987 [Listener at localhost/40508] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:21:48,988 [Listener at localhost/40508] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:21:48,993 [Listener at localhost/40508] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 45680
2020-12-03 07:21:48,993 [Listener at localhost/40508] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:21:49,004 [Listener at localhost/40508] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@8c58693{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:21:49,007 [Listener at localhost/40508] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@2d970ddc{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:21:49,020 [Listener at localhost/40508] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@70b01ef0{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:21:49,032 [Listener at localhost/40508] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@2c5ac465{HTTP/1.1,[http/1.1]}{localhost:45680}
2020-12-03 07:21:49,033 [Listener at localhost/40508] INFO  server.Server (Server.java:doStart(419)) - Started @6564ms
2020-12-03 07:21:49,495 [Listener at localhost/40508] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:36937
2020-12-03 07:21:49,496 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@5b19ca5f] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:21:49,498 [Listener at localhost/40508] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:21:49,498 [Listener at localhost/40508] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:21:49,519 [Listener at localhost/40508] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:21:49,520 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:21:49,529 [Listener at localhost/41623] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:41623
2020-12-03 07:21:50,020 [Listener at localhost/41623] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:21:50,022 [Listener at localhost/41623] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:21:50,049 [Thread-60] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:40508 starting to offer service
2020-12-03 07:21:50,060 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:21:50,062 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:21:50,067 [Listener at localhost/41623] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 1 with dfs.datanode.data.dir: [RAM_DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4,[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5
2020-12-03 07:21:50,072 [Listener at localhost/41623] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [RAM_DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:21:50,075 [Listener at localhost/41623] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5
2020-12-03 07:21:50,078 [Listener at localhost/41623] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:21:50,078 [Listener at localhost/41623] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:21:50,078 [Listener at localhost/41623] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:21:50,079 [Listener at localhost/41623] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:21:50,080 [Listener at localhost/41623] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:21:50,080 [Listener at localhost/41623] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:21:50,081 [Listener at localhost/41623] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:45911
2020-12-03 07:21:50,081 [Listener at localhost/41623] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:21:50,082 [Listener at localhost/41623] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:21:50,084 [Listener at localhost/41623] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:21:50,087 [Listener at localhost/41623] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:21:50,088 [Listener at localhost/41623] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:21:50,088 [Listener at localhost/41623] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:21:50,093 [Listener at localhost/41623] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:21:50,095 [Listener at localhost/41623] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:21:50,095 [Listener at localhost/41623] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:21:50,095 [Listener at localhost/41623] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:21:50,097 [Listener at localhost/41623] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 42726
2020-12-03 07:21:50,097 [Listener at localhost/41623] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:21:50,100 [Listener at localhost/41623] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@4daa429a{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:21:50,100 [Listener at localhost/41623] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@340663de{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:21:50,110 [Listener at localhost/41623] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@f20ae76{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:21:50,112 [Listener at localhost/41623] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@29a3c24c{HTTP/1.1,[http/1.1]}{localhost:42726}
2020-12-03 07:21:50,113 [Listener at localhost/41623] INFO  server.Server (Server.java:doStart(419)) - Started @7644ms
2020-12-03 07:21:50,230 [Listener at localhost/41623] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:42011
2020-12-03 07:21:50,230 [Listener at localhost/41623] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:21:50,230 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@26ffb271] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:21:50,231 [Listener at localhost/41623] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:21:50,231 [Listener at localhost/41623] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:21:50,233 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:21:50,240 [Listener at localhost/40280] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:40280
2020-12-03 07:21:50,250 [Listener at localhost/40280] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:21:50,251 [Listener at localhost/40280] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:21:50,252 [Thread-84] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:40508 starting to offer service
2020-12-03 07:21:50,254 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:21:50,255 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:21:50,262 [Listener at localhost/40280] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 2 with dfs.datanode.data.dir: [RAM_DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7,[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8
2020-12-03 07:21:50,264 [Listener at localhost/40280] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [RAM_DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7
2020-12-03 07:21:50,265 [Listener at localhost/40280] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8
2020-12-03 07:21:50,266 [Listener at localhost/40280] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:21:50,267 [Listener at localhost/40280] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:21:50,267 [Listener at localhost/40280] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:21:50,268 [Listener at localhost/40280] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:21:50,268 [Listener at localhost/40280] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:21:50,269 [Listener at localhost/40280] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:21:50,269 [Listener at localhost/40280] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:36081
2020-12-03 07:21:50,270 [Listener at localhost/40280] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:21:50,270 [Listener at localhost/40280] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:21:50,271 [Listener at localhost/40280] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:21:50,273 [Listener at localhost/40280] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:21:50,274 [Listener at localhost/40280] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:21:50,274 [Listener at localhost/40280] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:21:50,277 [Listener at localhost/40280] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:21:50,278 [Listener at localhost/40280] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:21:50,279 [Listener at localhost/40280] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:21:50,279 [Listener at localhost/40280] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:21:50,280 [Listener at localhost/40280] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 45410
2020-12-03 07:21:50,280 [Listener at localhost/40280] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:21:50,283 [Listener at localhost/40280] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@2597c40{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:21:50,284 [Listener at localhost/40280] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@e027b0a{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:21:50,291 [Listener at localhost/40280] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@6868caaa{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:21:50,292 [Listener at localhost/40280] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@13ec55c4{HTTP/1.1,[http/1.1]}{localhost:45410}
2020-12-03 07:21:50,292 [Listener at localhost/40280] INFO  server.Server (Server.java:doStart(419)) - Started @7824ms
2020-12-03 07:21:50,314 [Listener at localhost/40280] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:45122
2020-12-03 07:21:50,315 [Listener at localhost/40280] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:21:50,315 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@5388e452] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:21:50,315 [Listener at localhost/40280] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:21:50,316 [Listener at localhost/40280] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:21:50,316 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:21:50,321 [Listener at localhost/33017] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:33017
2020-12-03 07:21:50,326 [Listener at localhost/33017] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:21:50,326 [Listener at localhost/33017] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:21:50,327 [Thread-106] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:40508 starting to offer service
2020-12-03 07:21:50,328 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:21:50,331 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:21:50,337 [Listener at localhost/33017] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 3 with dfs.datanode.data.dir: [RAM_DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10,[ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11
2020-12-03 07:21:50,339 [Listener at localhost/33017] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [RAM_DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10
2020-12-03 07:21:50,340 [Listener at localhost/33017] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11
2020-12-03 07:21:50,341 [Listener at localhost/33017] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:21:50,342 [Listener at localhost/33017] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:21:50,342 [Listener at localhost/33017] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:21:50,342 [Listener at localhost/33017] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:21:50,343 [Listener at localhost/33017] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:21:50,343 [Listener at localhost/33017] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:21:50,344 [Listener at localhost/33017] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:44964
2020-12-03 07:21:50,344 [Listener at localhost/33017] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:21:50,344 [Listener at localhost/33017] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:21:50,345 [Listener at localhost/33017] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:21:50,347 [Listener at localhost/33017] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:21:50,347 [Listener at localhost/33017] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:21:50,348 [Listener at localhost/33017] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:21:50,350 [Listener at localhost/33017] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:21:50,350 [Listener at localhost/33017] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:21:50,351 [Listener at localhost/33017] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:21:50,351 [Listener at localhost/33017] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:21:50,352 [Listener at localhost/33017] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 35101
2020-12-03 07:21:50,352 [Listener at localhost/33017] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:21:50,356 [Listener at localhost/33017] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@1ca63fcf{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:21:50,357 [Listener at localhost/33017] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@644d2cae{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:21:50,363 [Listener at localhost/33017] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@70b5d021{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:21:50,365 [Listener at localhost/33017] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@239db63d{HTTP/1.1,[http/1.1]}{localhost:35101}
2020-12-03 07:21:50,365 [Listener at localhost/33017] INFO  server.Server (Server.java:doStart(419)) - Started @7896ms
2020-12-03 07:21:50,458 [Listener at localhost/33017] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:40204
2020-12-03 07:21:50,459 [Listener at localhost/33017] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:21:50,460 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@14b41f7a] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:21:50,460 [Listener at localhost/33017] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:21:50,461 [Listener at localhost/33017] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:21:50,462 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:21:50,468 [Listener at localhost/43008] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:43008
2020-12-03 07:21:50,475 [Listener at localhost/43008] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:21:50,475 [Listener at localhost/43008] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:21:50,476 [Thread-128] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:40508 starting to offer service
2020-12-03 07:21:50,478 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:21:50,478 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:21:50,482 [Listener at localhost/43008] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 4 with dfs.datanode.data.dir: [RAM_DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13,[ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14
2020-12-03 07:21:50,484 [Listener at localhost/43008] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [RAM_DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13
2020-12-03 07:21:50,493 [Listener at localhost/43008] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14
2020-12-03 07:21:50,495 [Listener at localhost/43008] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:21:50,496 [Listener at localhost/43008] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:21:50,498 [Listener at localhost/43008] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:21:50,499 [Listener at localhost/43008] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:21:50,500 [Listener at localhost/43008] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:21:50,501 [Listener at localhost/43008] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:21:50,502 [Listener at localhost/43008] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:46335
2020-12-03 07:21:50,503 [Listener at localhost/43008] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:21:50,503 [Listener at localhost/43008] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:21:50,508 [Listener at localhost/43008] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:21:50,509 [Thread-84] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:40508
2020-12-03 07:21:50,509 [Thread-128] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:40508
2020-12-03 07:21:50,509 [Thread-60] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:40508
2020-12-03 07:21:50,509 [Thread-106] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:40508
2020-12-03 07:21:50,511 [Listener at localhost/43008] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:21:50,513 [Listener at localhost/43008] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:21:50,513 [Listener at localhost/43008] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:21:50,513 [Thread-60] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:21:50,513 [Thread-106] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:21:50,525 [Thread-84] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:21:50,525 [Thread-128] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:21:50,528 [Listener at localhost/43008] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:21:50,529 [Listener at localhost/43008] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:21:50,529 [Listener at localhost/43008] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:21:50,529 [Listener at localhost/43008] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:21:50,531 [Listener at localhost/43008] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 43949
2020-12-03 07:21:50,531 [Listener at localhost/43008] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:21:50,533 [Listener at localhost/43008] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@40570e0a{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:21:50,534 [Listener at localhost/43008] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@7749d006{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:21:50,554 [Listener at localhost/43008] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@4f9283ac{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:21:50,555 [Listener at localhost/43008] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@245ed753{HTTP/1.1,[http/1.1]}{localhost:43949}
2020-12-03 07:21:50,556 [Listener at localhost/43008] INFO  server.Server (Server.java:doStart(419)) - Started @8087ms
2020-12-03 07:21:50,577 [Thread-84] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/in_use.lock acquired by nodename 2751@a3be22c8fbc1
2020-12-03 07:21:50,578 [Thread-84] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [RAM_DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4 is not formatted for namespace 629858452. Formatting...
2020-12-03 07:21:50,579 [Thread-84] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-03c4e4df-d4c4-413d-a247-bf506f40e039 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4 
2020-12-03 07:21:50,577 [Thread-106] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7/in_use.lock acquired by nodename 2751@a3be22c8fbc1
2020-12-03 07:21:50,580 [Thread-106] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [RAM_DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7 is not formatted for namespace 629858452. Formatting...
2020-12-03 07:21:50,577 [Thread-60] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/in_use.lock acquired by nodename 2751@a3be22c8fbc1
2020-12-03 07:21:50,580 [Thread-60] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [RAM_DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1 is not formatted for namespace 629858452. Formatting...
2020-12-03 07:21:50,581 [Thread-60] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-16ad47b7-6740-4c6f-9eb7-62aad3546c7d for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1 
2020-12-03 07:21:50,577 [Thread-128] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10/in_use.lock acquired by nodename 2751@a3be22c8fbc1
2020-12-03 07:21:50,583 [Thread-128] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [RAM_DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10 is not formatted for namespace 629858452. Formatting...
2020-12-03 07:21:50,584 [Thread-128] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-a4774b34-f21c-4814-aefc-8f3ebb297608 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10 
2020-12-03 07:21:50,584 [Listener at localhost/43008] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:34112
2020-12-03 07:21:50,585 [Listener at localhost/43008] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:21:50,585 [Listener at localhost/43008] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:21:50,585 [Listener at localhost/43008] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:21:50,586 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:21:50,580 [Thread-106] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-65c144b2-b711-4cc2-94b7-e2af5974caa9 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7 
2020-12-03 07:21:50,585 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@54c2cfb0] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:21:50,590 [Listener at localhost/35345] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:35345
2020-12-03 07:21:50,594 [Listener at localhost/35345] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:21:50,594 [Listener at localhost/35345] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:21:50,595 [Thread-150] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:40508 starting to offer service
2020-12-03 07:21:50,609 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:21:50,610 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:21:50,615 [Listener at localhost/35345] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 5 with dfs.datanode.data.dir: [RAM_DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16,[ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17,[ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18
2020-12-03 07:21:50,617 [Listener at localhost/35345] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [RAM_DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16
2020-12-03 07:21:50,617 [Listener at localhost/35345] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17
2020-12-03 07:21:50,617 [Listener at localhost/35345] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18
2020-12-03 07:21:50,624 [Listener at localhost/35345] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:21:50,624 [Thread-150] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:40508
2020-12-03 07:21:50,626 [Listener at localhost/35345] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:21:50,626 [Thread-150] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:21:50,626 [Listener at localhost/35345] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:21:50,627 [Listener at localhost/35345] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:21:50,627 [Listener at localhost/35345] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:21:50,627 [Listener at localhost/35345] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:21:50,628 [Listener at localhost/35345] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:32804
2020-12-03 07:21:50,628 [Listener at localhost/35345] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:21:50,628 [Listener at localhost/35345] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:21:50,629 [Listener at localhost/35345] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:21:50,631 [Listener at localhost/35345] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:21:50,631 [Listener at localhost/35345] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:21:50,632 [Listener at localhost/35345] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:21:50,634 [Listener at localhost/35345] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:21:50,635 [Listener at localhost/35345] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:21:50,635 [Listener at localhost/35345] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:21:50,635 [Listener at localhost/35345] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:21:50,636 [Listener at localhost/35345] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 38208
2020-12-03 07:21:50,636 [Listener at localhost/35345] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:21:50,638 [Listener at localhost/35345] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@333a174{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:21:50,639 [Listener at localhost/35345] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@58323171{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:21:50,647 [Listener at localhost/35345] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@354a33d{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:21:50,649 [Listener at localhost/35345] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@7633055f{HTTP/1.1,[http/1.1]}{localhost:38208}
2020-12-03 07:21:50,649 [Listener at localhost/35345] INFO  server.Server (Server.java:doStart(419)) - Started @8181ms
2020-12-03 07:21:50,666 [Listener at localhost/35345] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:42175
2020-12-03 07:21:50,667 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@528dd6ee] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:21:50,667 [Listener at localhost/35345] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:21:50,667 [Listener at localhost/35345] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:21:50,668 [Listener at localhost/35345] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:21:50,669 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:21:50,692 [Thread-150] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13/in_use.lock acquired by nodename 2751@a3be22c8fbc1
2020-12-03 07:21:50,692 [Thread-150] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [RAM_DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13 is not formatted for namespace 629858452. Formatting...
2020-12-03 07:21:50,692 [Thread-150] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-fb09e1ea-5c92-4d14-a724-2616a3983122 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13 
2020-12-03 07:21:50,704 [Listener at localhost/41296] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:41296
2020-12-03 07:21:50,726 [Listener at localhost/41296] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:21:50,726 [Listener at localhost/41296] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:21:50,727 [Thread-172] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:40508 starting to offer service
2020-12-03 07:21:50,730 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:21:50,730 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:21:50,745 [Thread-172] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:40508
2020-12-03 07:21:50,746 [Thread-172] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 3 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=3, dataDirs=3)
2020-12-03 07:21:50,857 [Thread-60] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/in_use.lock acquired by nodename 2751@a3be22c8fbc1
2020-12-03 07:21:50,857 [Thread-172] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16/in_use.lock acquired by nodename 2751@a3be22c8fbc1
2020-12-03 07:21:50,857 [Thread-60] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2 is not formatted for namespace 629858452. Formatting...
2020-12-03 07:21:50,857 [Thread-84] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/in_use.lock acquired by nodename 2751@a3be22c8fbc1
2020-12-03 07:21:50,858 [Thread-84] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5 is not formatted for namespace 629858452. Formatting...
2020-12-03 07:21:50,858 [Thread-172] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [RAM_DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16 is not formatted for namespace 629858452. Formatting...
2020-12-03 07:21:50,859 [Thread-84] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-167fc370-7419-4f5b-a77d-5974db53e53b for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5 
2020-12-03 07:21:50,859 [Thread-60] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-a70eae85-caf8-4810-9307-7fdcff1de75a for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2 
2020-12-03 07:21:50,859 [Thread-172] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-650414f7-31fe-484d-8973-85ea58ac630c for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16 
2020-12-03 07:21:50,910 [Thread-128] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11/in_use.lock acquired by nodename 2751@a3be22c8fbc1
2020-12-03 07:21:50,911 [Thread-128] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11 is not formatted for namespace 629858452. Formatting...
2020-12-03 07:21:50,910 [Thread-106] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/in_use.lock acquired by nodename 2751@a3be22c8fbc1
2020-12-03 07:21:50,912 [Thread-106] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8 is not formatted for namespace 629858452. Formatting...
2020-12-03 07:21:50,912 [Thread-128] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-bd99d0d3-dfed-4a67-b8cf-af2132dc7382 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11 
2020-12-03 07:21:50,913 [Thread-106] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-e617a7b2-64f7-42ba-a4d8-2991b5de0697 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8 
2020-12-03 07:21:50,981 [Thread-150] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14/in_use.lock acquired by nodename 2751@a3be22c8fbc1
2020-12-03 07:21:50,982 [Thread-150] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14 is not formatted for namespace 629858452. Formatting...
2020-12-03 07:21:50,982 [Thread-150] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-d2b02d71-811b-4a1b-beec-e7557cf65bae for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14 
2020-12-03 07:21:51,033 [Thread-84] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:51,034 [Thread-84] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:51,035 [Thread-84] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [RAM_DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4 and block pool id BP-1681972789-172.17.0.7-1606980105077 is not formatted. Formatting ...
2020-12-03 07:21:51,036 [Thread-84] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1681972789-172.17.0.7-1606980105077 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1681972789-172.17.0.7-1606980105077/current
2020-12-03 07:21:51,040 [Thread-60] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:51,041 [Thread-60] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:51,041 [Thread-60] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [RAM_DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1 and block pool id BP-1681972789-172.17.0.7-1606980105077 is not formatted. Formatting ...
2020-12-03 07:21:51,041 [Thread-60] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1681972789-172.17.0.7-1606980105077 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1681972789-172.17.0.7-1606980105077/current
2020-12-03 07:21:51,083 [Thread-128] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:51,084 [Thread-128] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10/current/BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:51,084 [Thread-128] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [RAM_DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10 and block pool id BP-1681972789-172.17.0.7-1606980105077 is not formatted. Formatting ...
2020-12-03 07:21:51,084 [Thread-128] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1681972789-172.17.0.7-1606980105077 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10/current/BP-1681972789-172.17.0.7-1606980105077/current
2020-12-03 07:21:51,087 [Thread-106] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:51,087 [Thread-106] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7/current/BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:51,087 [Thread-106] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [RAM_DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7 and block pool id BP-1681972789-172.17.0.7-1606980105077 is not formatted. Formatting ...
2020-12-03 07:21:51,088 [Thread-106] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1681972789-172.17.0.7-1606980105077 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7/current/BP-1681972789-172.17.0.7-1606980105077/current
2020-12-03 07:21:51,125 [Thread-172] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17/in_use.lock acquired by nodename 2751@a3be22c8fbc1
2020-12-03 07:21:51,125 [Thread-172] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17 is not formatted for namespace 629858452. Formatting...
2020-12-03 07:21:51,126 [Thread-172] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-fcefd356-7d44-40b3-85cf-64c3f285c82c for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17 
2020-12-03 07:21:51,191 [Thread-150] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:51,192 [Thread-150] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13/current/BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:51,192 [Thread-150] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [RAM_DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13 and block pool id BP-1681972789-172.17.0.7-1606980105077 is not formatted. Formatting ...
2020-12-03 07:21:51,192 [Thread-150] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1681972789-172.17.0.7-1606980105077 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13/current/BP-1681972789-172.17.0.7-1606980105077/current
2020-12-03 07:21:51,255 [Thread-60] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:51,255 [Thread-60] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:51,255 [Thread-60] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2 and block pool id BP-1681972789-172.17.0.7-1606980105077 is not formatted. Formatting ...
2020-12-03 07:21:51,256 [Thread-60] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1681972789-172.17.0.7-1606980105077 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1681972789-172.17.0.7-1606980105077/current
2020-12-03 07:21:51,276 [IPC Server handler 6 on default port 40508] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:51,285 [Listener at localhost/41296] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:21:51,286 [Listener at localhost/41296] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:21:51,332 [Thread-84] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:51,332 [Thread-84] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:51,333 [Thread-128] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:51,333 [Thread-84] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5 and block pool id BP-1681972789-172.17.0.7-1606980105077 is not formatted. Formatting ...
2020-12-03 07:21:51,333 [Thread-84] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1681972789-172.17.0.7-1606980105077 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-1681972789-172.17.0.7-1606980105077/current
2020-12-03 07:21:51,333 [Thread-128] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11/current/BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:51,333 [Thread-128] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11 and block pool id BP-1681972789-172.17.0.7-1606980105077 is not formatted. Formatting ...
2020-12-03 07:21:51,333 [Thread-128] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1681972789-172.17.0.7-1606980105077 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11/current/BP-1681972789-172.17.0.7-1606980105077/current
2020-12-03 07:21:51,338 [Thread-106] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:51,338 [Thread-106] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/current/BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:51,339 [Thread-106] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8 and block pool id BP-1681972789-172.17.0.7-1606980105077 is not formatted. Formatting ...
2020-12-03 07:21:51,339 [Thread-106] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1681972789-172.17.0.7-1606980105077 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/current/BP-1681972789-172.17.0.7-1606980105077/current
2020-12-03 07:21:51,389 [IPC Server handler 4 on default port 40508] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:51,391 [Listener at localhost/41296] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:21:51,391 [Listener at localhost/41296] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:21:51,459 [Thread-150] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:51,460 [Thread-150] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14/current/BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:51,460 [Thread-150] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14 and block pool id BP-1681972789-172.17.0.7-1606980105077 is not formatted. Formatting ...
2020-12-03 07:21:51,461 [Thread-150] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1681972789-172.17.0.7-1606980105077 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14/current/BP-1681972789-172.17.0.7-1606980105077/current
2020-12-03 07:21:51,491 [Thread-172] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18/in_use.lock acquired by nodename 2751@a3be22c8fbc1
2020-12-03 07:21:51,491 [Thread-172] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18 is not formatted for namespace 629858452. Formatting...
2020-12-03 07:21:51,492 [Thread-172] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-ffc14c33-e3bb-433d-a880-69e3de925b44 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18 
2020-12-03 07:21:51,493 [Thread-60] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=629858452;bpid=BP-1681972789-172.17.0.7-1606980105077;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=629858452;c=1606980105077;bpid=BP-1681972789-172.17.0.7-1606980105077;dnuuid=null
2020-12-03 07:21:51,494 [IPC Server handler 3 on default port 40508] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:51,500 [Listener at localhost/41296] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:21:51,500 [Listener at localhost/41296] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:21:51,543 [Thread-84] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=629858452;bpid=BP-1681972789-172.17.0.7-1606980105077;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=629858452;c=1606980105077;bpid=BP-1681972789-172.17.0.7-1606980105077;dnuuid=null
2020-12-03 07:21:51,544 [Thread-106] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=629858452;bpid=BP-1681972789-172.17.0.7-1606980105077;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=629858452;c=1606980105077;bpid=BP-1681972789-172.17.0.7-1606980105077;dnuuid=null
2020-12-03 07:21:51,544 [Thread-128] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=629858452;bpid=BP-1681972789-172.17.0.7-1606980105077;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=629858452;c=1606980105077;bpid=BP-1681972789-172.17.0.7-1606980105077;dnuuid=null
2020-12-03 07:21:51,603 [IPC Server handler 5 on default port 40508] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:51,603 [Listener at localhost/41296] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:21:51,604 [Listener at localhost/41296] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:21:51,700 [Thread-150] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=629858452;bpid=BP-1681972789-172.17.0.7-1606980105077;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=629858452;c=1606980105077;bpid=BP-1681972789-172.17.0.7-1606980105077;dnuuid=null
2020-12-03 07:21:51,706 [IPC Server handler 7 on default port 40508] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:51,708 [Listener at localhost/41296] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:21:51,709 [Listener at localhost/41296] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:21:51,771 [Thread-60] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID f693c68b-acb5-4ad0-8980-23d838f7fba7
2020-12-03 07:21:51,774 [Thread-172] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:51,775 [Thread-172] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16/current/BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:51,775 [Thread-172] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [RAM_DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16 and block pool id BP-1681972789-172.17.0.7-1606980105077 is not formatted. Formatting ...
2020-12-03 07:21:51,775 [Thread-172] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1681972789-172.17.0.7-1606980105077 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16/current/BP-1681972789-172.17.0.7-1606980105077/current
2020-12-03 07:21:51,817 [IPC Server handler 8 on default port 40508] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:51,821 [Listener at localhost/41296] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:21:51,821 [Listener at localhost/41296] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:21:51,837 [Thread-84] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID 6af00937-a528-462e-97d0-7a889bd94f87
2020-12-03 07:21:51,837 [Thread-128] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID ac27962c-fa6a-4f23-86c8-18a7f92f9f56
2020-12-03 07:21:51,838 [Thread-106] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID a36cabce-9acb-4413-8be0-398b0630ef41
2020-12-03 07:21:51,924 [IPC Server handler 2 on default port 40508] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:51,924 [Listener at localhost/41296] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:21:51,924 [Listener at localhost/41296] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:21:51,933 [Thread-60] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-16ad47b7-6740-4c6f-9eb7-62aad3546c7d
2020-12-03 07:21:51,933 [Thread-60] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [RAM_DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, StorageType: RAM_DISK
2020-12-03 07:21:51,933 [Thread-128] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-a4774b34-f21c-4814-aefc-8f3ebb297608
2020-12-03 07:21:51,933 [Thread-106] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-65c144b2-b711-4cc2-94b7-e2af5974caa9
2020-12-03 07:21:51,933 [Thread-84] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-03c4e4df-d4c4-413d-a247-bf506f40e039
2020-12-03 07:21:51,935 [Thread-106] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [RAM_DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7, StorageType: RAM_DISK
2020-12-03 07:21:51,934 [Thread-128] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [RAM_DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10, StorageType: RAM_DISK
2020-12-03 07:21:51,935 [Thread-84] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [RAM_DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, StorageType: RAM_DISK
2020-12-03 07:21:51,936 [Thread-60] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-a70eae85-caf8-4810-9307-7fdcff1de75a
2020-12-03 07:21:51,936 [Thread-60] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, StorageType: DISK
2020-12-03 07:21:51,938 [Thread-84] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-167fc370-7419-4f5b-a77d-5974db53e53b
2020-12-03 07:21:51,942 [Thread-84] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5, StorageType: DISK
2020-12-03 07:21:51,947 [Thread-128] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-bd99d0d3-dfed-4a67-b8cf-af2132dc7382
2020-12-03 07:21:51,954 [Thread-128] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11, StorageType: ARCHIVE
2020-12-03 07:21:51,954 [Thread-106] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-e617a7b2-64f7-42ba-a4d8-2991b5de0697
2020-12-03 07:21:51,955 [Thread-84] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:21:51,955 [Thread-60] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:21:51,955 [Thread-106] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8, StorageType: DISK
2020-12-03 07:21:51,955 [Thread-128] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:21:51,956 [Thread-106] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:21:51,961 [Thread-84] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:21:51,963 [Thread-106] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7
2020-12-03 07:21:51,964 [Thread-60] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:21:51,964 [Thread-150] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID 67671ade-3465-42ec-9f5f-9ccb0a3921b0
2020-12-03 07:21:51,964 [Thread-128] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10
2020-12-03 07:21:51,972 [Thread-60] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:21:51,972 [Thread-150] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-fb09e1ea-5c92-4d14-a724-2616a3983122
2020-12-03 07:21:51,972 [Thread-128] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10
2020-12-03 07:21:51,972 [Thread-106] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7
2020-12-03 07:21:51,972 [Thread-84] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:21:51,978 [Thread-150] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [RAM_DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13, StorageType: RAM_DISK
2020-12-03 07:21:51,980 [Thread-60] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:21:51,980 [Thread-84] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5
2020-12-03 07:21:51,980 [Thread-106] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8
2020-12-03 07:21:51,980 [Thread-128] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11
2020-12-03 07:21:51,981 [Thread-106] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8
2020-12-03 07:21:51,981 [Thread-84] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5
2020-12-03 07:21:51,980 [Thread-60] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:21:51,982 [Thread-150] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-d2b02d71-811b-4a1b-beec-e7557cf65bae
2020-12-03 07:21:51,982 [Thread-60] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:51,982 [Thread-84] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:51,982 [Thread-106] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:51,981 [Thread-128] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11
2020-12-03 07:21:51,983 [Thread-128] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:51,982 [Thread-150] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14, StorageType: ARCHIVE
2020-12-03 07:21:51,984 [Thread-196] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2020-12-03 07:21:51,984 [Thread-199] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2020-12-03 07:21:51,985 [Thread-150] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:21:51,986 [Thread-150] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13
2020-12-03 07:21:51,988 [Thread-198] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10...
2020-12-03 07:21:51,988 [Thread-201] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11...
2020-12-03 07:21:51,989 [Thread-197] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7...
2020-12-03 07:21:51,989 [Thread-150] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13
2020-12-03 07:21:51,991 [Thread-203] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5...
2020-12-03 07:21:51,991 [Thread-202] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8...
2020-12-03 07:21:51,991 [Thread-200] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4...
2020-12-03 07:21:51,993 [Thread-150] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14
2020-12-03 07:21:51,995 [Thread-150] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14
2020-12-03 07:21:52,001 [Thread-150] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:52,002 [Thread-204] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13...
2020-12-03 07:21:52,002 [Thread-205] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14...
2020-12-03 07:21:52,037 [IPC Server handler 6 on default port 40508] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:52,040 [Listener at localhost/41296] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:21:52,040 [Listener at localhost/41296] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:21:52,062 [Thread-198] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1681972789-172.17.0.7-1606980105077 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10: 74ms
2020-12-03 07:21:52,062 [Thread-172] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:52,062 [Thread-172] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17/current/BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:52,062 [Thread-197] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1681972789-172.17.0.7-1606980105077 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7: 73ms
2020-12-03 07:21:52,062 [Thread-201] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1681972789-172.17.0.7-1606980105077 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11: 74ms
2020-12-03 07:21:52,066 [Thread-204] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1681972789-172.17.0.7-1606980105077 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13: 64ms
2020-12-03 07:21:52,065 [Thread-205] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1681972789-172.17.0.7-1606980105077 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14: 62ms
2020-12-03 07:21:52,065 [Thread-203] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1681972789-172.17.0.7-1606980105077 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5: 74ms
2020-12-03 07:21:52,065 [Thread-196] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1681972789-172.17.0.7-1606980105077 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 81ms
2020-12-03 07:21:52,064 [Thread-199] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1681972789-172.17.0.7-1606980105077 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 80ms
2020-12-03 07:21:52,064 [Thread-172] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17 and block pool id BP-1681972789-172.17.0.7-1606980105077 is not formatted. Formatting ...
2020-12-03 07:21:52,069 [Thread-172] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1681972789-172.17.0.7-1606980105077 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17/current/BP-1681972789-172.17.0.7-1606980105077/current
2020-12-03 07:21:52,063 [Thread-200] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1681972789-172.17.0.7-1606980105077 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4: 69ms
2020-12-03 07:21:52,069 [Thread-60] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1681972789-172.17.0.7-1606980105077: 86ms
2020-12-03 07:21:52,068 [Thread-150] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1681972789-172.17.0.7-1606980105077: 67ms
2020-12-03 07:21:52,068 [Thread-128] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1681972789-172.17.0.7-1606980105077: 84ms
2020-12-03 07:21:52,071 [Thread-202] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1681972789-172.17.0.7-1606980105077 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8: 80ms
2020-12-03 07:21:52,071 [Thread-84] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1681972789-172.17.0.7-1606980105077: 88ms
2020-12-03 07:21:52,071 [Thread-106] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1681972789-172.17.0.7-1606980105077: 88ms
2020-12-03 07:21:52,073 [Thread-216] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2020-12-03 07:21:52,073 [Thread-218] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13...
2020-12-03 07:21:52,073 [Thread-219] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10...
2020-12-03 07:21:52,073 [Thread-220] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7...
2020-12-03 07:21:52,074 [Thread-218] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13/current/BP-1681972789-172.17.0.7-1606980105077/current/replicas doesn't exist 
2020-12-03 07:21:52,075 [Thread-225] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5...
2020-12-03 07:21:52,073 [Thread-217] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4...
2020-12-03 07:21:52,075 [Thread-223] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11...
2020-12-03 07:21:52,075 [Thread-224] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8...
2020-12-03 07:21:52,075 [Thread-225] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-1681972789-172.17.0.7-1606980105077/current/replicas doesn't exist 
2020-12-03 07:21:52,075 [Thread-220] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7/current/BP-1681972789-172.17.0.7-1606980105077/current/replicas doesn't exist 
2020-12-03 07:21:52,075 [Thread-222] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14...
2020-12-03 07:21:52,075 [Thread-221] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2020-12-03 07:21:52,076 [Thread-222] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14/current/BP-1681972789-172.17.0.7-1606980105077/current/replicas doesn't exist 
2020-12-03 07:21:52,074 [Thread-219] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10/current/BP-1681972789-172.17.0.7-1606980105077/current/replicas doesn't exist 
2020-12-03 07:21:52,074 [Thread-216] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1681972789-172.17.0.7-1606980105077/current/replicas doesn't exist 
2020-12-03 07:21:52,076 [Thread-218] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13: 3ms
2020-12-03 07:21:52,076 [Thread-221] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1681972789-172.17.0.7-1606980105077/current/replicas doesn't exist 
2020-12-03 07:21:52,075 [Thread-224] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/current/BP-1681972789-172.17.0.7-1606980105077/current/replicas doesn't exist 
2020-12-03 07:21:52,075 [Thread-223] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11/current/BP-1681972789-172.17.0.7-1606980105077/current/replicas doesn't exist 
2020-12-03 07:21:52,075 [Thread-217] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1681972789-172.17.0.7-1606980105077/current/replicas doesn't exist 
2020-12-03 07:21:52,077 [Thread-220] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7: 3ms
2020-12-03 07:21:52,077 [Thread-222] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14: 1ms
2020-12-03 07:21:52,077 [Thread-217] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4: 2ms
2020-12-03 07:21:52,077 [Thread-219] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10: 3ms
2020-12-03 07:21:52,077 [Thread-223] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11: 2ms
2020-12-03 07:21:52,080 [Thread-224] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8: 4ms
2020-12-03 07:21:52,080 [Thread-221] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 4ms
2020-12-03 07:21:52,080 [Thread-150] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077: 7ms
2020-12-03 07:21:52,080 [Thread-225] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5: 5ms
2020-12-03 07:21:52,079 [Thread-216] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 5ms
2020-12-03 07:21:52,080 [Thread-106] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077: 8ms
2020-12-03 07:21:52,080 [Thread-128] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077: 7ms
2020-12-03 07:21:52,082 [Thread-60] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077: 9ms
2020-12-03 07:21:52,084 [Thread-84] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077: 11ms
2020-12-03 07:21:52,084 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10
2020-12-03 07:21:52,084 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5
2020-12-03 07:21:52,084 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:21:52,086 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14
2020-12-03 07:21:52,086 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:21:52,084 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:21:52,086 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7
2020-12-03 07:21:52,086 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8
2020-12-03 07:21:52,087 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, DS-03c4e4df-d4c4-413d-a247-bf506f40e039): finished scanning block pool BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:52,086 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11
2020-12-03 07:21:52,087 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8, DS-e617a7b2-64f7-42ba-a4d8-2991b5de0697): finished scanning block pool BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:52,087 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7, DS-65c144b2-b711-4cc2-94b7-e2af5974caa9): finished scanning block pool BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:52,087 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14, DS-d2b02d71-811b-4a1b-beec-e7557cf65bae): finished scanning block pool BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:52,087 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-16ad47b7-6740-4c6f-9eb7-62aad3546c7d): finished scanning block pool BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:52,087 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-a70eae85-caf8-4810-9307-7fdcff1de75a): finished scanning block pool BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:52,087 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10, DS-a4774b34-f21c-4814-aefc-8f3ebb297608): finished scanning block pool BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:52,086 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13
2020-12-03 07:21:52,094 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13, DS-fb09e1ea-5c92-4d14-a724-2616a3983122): finished scanning block pool BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:52,087 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5, DS-167fc370-7419-4f5b-a77d-5974db53e53b): finished scanning block pool BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:52,087 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11, DS-bd99d0d3-dfed-4a67-b8cf-af2132dc7382): finished scanning block pool BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:52,118 [Thread-84] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 12:17 PM with interval of 21600000ms
2020-12-03 07:21:52,119 [Thread-128] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 10:39 AM with interval of 21600000ms
2020-12-03 07:21:52,119 [Thread-150] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 11:15 AM with interval of 21600000ms
2020-12-03 07:21:52,118 [Thread-106] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 12:03 PM with interval of 21600000ms
2020-12-03 07:21:52,119 [Thread-60] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 11:41 AM with interval of 21600000ms
2020-12-03 07:21:52,123 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14, DS-d2b02d71-811b-4a1b-beec-e7557cf65bae): no suitable block pools found to scan.  Waiting 1814399963 ms.
2020-12-03 07:21:52,123 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7, DS-65c144b2-b711-4cc2-94b7-e2af5974caa9): no suitable block pools found to scan.  Waiting 1814399961 ms.
2020-12-03 07:21:52,123 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10, DS-a4774b34-f21c-4814-aefc-8f3ebb297608): no suitable block pools found to scan.  Waiting 1814399961 ms.
2020-12-03 07:21:52,123 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5, DS-167fc370-7419-4f5b-a77d-5974db53e53b): no suitable block pools found to scan.  Waiting 1814399961 ms.
2020-12-03 07:21:52,123 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-16ad47b7-6740-4c6f-9eb7-62aad3546c7d): no suitable block pools found to scan.  Waiting 1814399961 ms.
2020-12-03 07:21:52,123 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13, DS-fb09e1ea-5c92-4d14-a724-2616a3983122): no suitable block pools found to scan.  Waiting 1814399963 ms.
2020-12-03 07:21:52,123 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-a70eae85-caf8-4810-9307-7fdcff1de75a): no suitable block pools found to scan.  Waiting 1814399961 ms.
2020-12-03 07:21:52,123 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11, DS-bd99d0d3-dfed-4a67-b8cf-af2132dc7382): no suitable block pools found to scan.  Waiting 1814399961 ms.
2020-12-03 07:21:52,123 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, DS-03c4e4df-d4c4-413d-a247-bf506f40e039): no suitable block pools found to scan.  Waiting 1814399961 ms.
2020-12-03 07:21:52,123 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8, DS-e617a7b2-64f7-42ba-a4d8-2991b5de0697): no suitable block pools found to scan.  Waiting 1814399963 ms.
2020-12-03 07:21:52,136 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1681972789-172.17.0.7-1606980105077 (Datanode Uuid 6af00937-a528-462e-97d0-7a889bd94f87) service to localhost/127.0.0.1:40508 beginning handshake with NN
2020-12-03 07:21:52,136 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1681972789-172.17.0.7-1606980105077 (Datanode Uuid f693c68b-acb5-4ad0-8980-23d838f7fba7) service to localhost/127.0.0.1:40508 beginning handshake with NN
2020-12-03 07:21:52,136 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1681972789-172.17.0.7-1606980105077 (Datanode Uuid 67671ade-3465-42ec-9f5f-9ccb0a3921b0) service to localhost/127.0.0.1:40508 beginning handshake with NN
2020-12-03 07:21:52,136 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1681972789-172.17.0.7-1606980105077 (Datanode Uuid ac27962c-fa6a-4f23-86c8-18a7f92f9f56) service to localhost/127.0.0.1:40508 beginning handshake with NN
2020-12-03 07:21:52,136 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1681972789-172.17.0.7-1606980105077 (Datanode Uuid a36cabce-9acb-4413-8be0-398b0630ef41) service to localhost/127.0.0.1:40508 beginning handshake with NN
2020-12-03 07:21:52,142 [IPC Server handler 4 on default port 40508] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:52,143 [Listener at localhost/41296] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:21:52,143 [Listener at localhost/41296] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:21:52,153 [IPC Server handler 8 on default port 40508] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:36081, datanodeUuid=a36cabce-9acb-4413-8be0-398b0630ef41, infoPort=45122, infoSecurePort=0, ipcPort=33017, storageInfo=lv=-57;cid=testClusterID;nsid=629858452;c=1606980105077) storage a36cabce-9acb-4413-8be0-398b0630ef41
2020-12-03 07:21:52,155 [IPC Server handler 8 on default port 40508] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:36081
2020-12-03 07:21:52,155 [IPC Server handler 8 on default port 40508] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN a36cabce-9acb-4413-8be0-398b0630ef41 (127.0.0.1:36081).
2020-12-03 07:21:52,158 [IPC Server handler 7 on default port 40508] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:45911, datanodeUuid=6af00937-a528-462e-97d0-7a889bd94f87, infoPort=42011, infoSecurePort=0, ipcPort=40280, storageInfo=lv=-57;cid=testClusterID;nsid=629858452;c=1606980105077) storage 6af00937-a528-462e-97d0-7a889bd94f87
2020-12-03 07:21:52,158 [IPC Server handler 7 on default port 40508] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:45911
2020-12-03 07:21:52,159 [IPC Server handler 7 on default port 40508] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 6af00937-a528-462e-97d0-7a889bd94f87 (127.0.0.1:45911).
2020-12-03 07:21:52,160 [IPC Server handler 5 on default port 40508] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:44964, datanodeUuid=ac27962c-fa6a-4f23-86c8-18a7f92f9f56, infoPort=40204, infoSecurePort=0, ipcPort=43008, storageInfo=lv=-57;cid=testClusterID;nsid=629858452;c=1606980105077) storage ac27962c-fa6a-4f23-86c8-18a7f92f9f56
2020-12-03 07:21:52,160 [IPC Server handler 5 on default port 40508] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:44964
2020-12-03 07:21:52,160 [IPC Server handler 5 on default port 40508] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN ac27962c-fa6a-4f23-86c8-18a7f92f9f56 (127.0.0.1:44964).
2020-12-03 07:21:52,160 [IPC Server handler 3 on default port 40508] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:46335, datanodeUuid=67671ade-3465-42ec-9f5f-9ccb0a3921b0, infoPort=34112, infoSecurePort=0, ipcPort=35345, storageInfo=lv=-57;cid=testClusterID;nsid=629858452;c=1606980105077) storage 67671ade-3465-42ec-9f5f-9ccb0a3921b0
2020-12-03 07:21:52,161 [IPC Server handler 3 on default port 40508] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:46335
2020-12-03 07:21:52,161 [IPC Server handler 3 on default port 40508] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 67671ade-3465-42ec-9f5f-9ccb0a3921b0 (127.0.0.1:46335).
2020-12-03 07:21:52,161 [IPC Server handler 9 on default port 40508] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:43386, datanodeUuid=f693c68b-acb5-4ad0-8980-23d838f7fba7, infoPort=36937, infoSecurePort=0, ipcPort=41623, storageInfo=lv=-57;cid=testClusterID;nsid=629858452;c=1606980105077) storage f693c68b-acb5-4ad0-8980-23d838f7fba7
2020-12-03 07:21:52,162 [IPC Server handler 9 on default port 40508] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:43386
2020-12-03 07:21:52,162 [IPC Server handler 9 on default port 40508] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN f693c68b-acb5-4ad0-8980-23d838f7fba7 (127.0.0.1:43386).
2020-12-03 07:21:52,164 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1681972789-172.17.0.7-1606980105077 (Datanode Uuid 6af00937-a528-462e-97d0-7a889bd94f87) service to localhost/127.0.0.1:40508 successfully registered with NN
2020-12-03 07:21:52,164 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1681972789-172.17.0.7-1606980105077 (Datanode Uuid a36cabce-9acb-4413-8be0-398b0630ef41) service to localhost/127.0.0.1:40508 successfully registered with NN
2020-12-03 07:21:52,164 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1681972789-172.17.0.7-1606980105077 (Datanode Uuid 67671ade-3465-42ec-9f5f-9ccb0a3921b0) service to localhost/127.0.0.1:40508 successfully registered with NN
2020-12-03 07:21:52,164 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1681972789-172.17.0.7-1606980105077 (Datanode Uuid f693c68b-acb5-4ad0-8980-23d838f7fba7) service to localhost/127.0.0.1:40508 successfully registered with NN
2020-12-03 07:21:52,164 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1681972789-172.17.0.7-1606980105077 (Datanode Uuid ac27962c-fa6a-4f23-86c8-18a7f92f9f56) service to localhost/127.0.0.1:40508 successfully registered with NN
2020-12-03 07:21:52,165 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:40508 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:21:52,164 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:40508 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:21:52,164 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:40508 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:21:52,164 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:40508 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:21:52,165 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:40508 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:21:52,186 [IPC Server handler 2 on default port 40508] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-16ad47b7-6740-4c6f-9eb7-62aad3546c7d for DN 127.0.0.1:43386
2020-12-03 07:21:52,188 [IPC Server handler 2 on default port 40508] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-a70eae85-caf8-4810-9307-7fdcff1de75a for DN 127.0.0.1:43386
2020-12-03 07:21:52,190 [IPC Server handler 4 on default port 40508] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-65c144b2-b711-4cc2-94b7-e2af5974caa9 for DN 127.0.0.1:36081
2020-12-03 07:21:52,195 [IPC Server handler 4 on default port 40508] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-e617a7b2-64f7-42ba-a4d8-2991b5de0697 for DN 127.0.0.1:36081
2020-12-03 07:21:52,197 [IPC Server handler 6 on default port 40508] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-fb09e1ea-5c92-4d14-a724-2616a3983122 for DN 127.0.0.1:46335
2020-12-03 07:21:52,197 [IPC Server handler 6 on default port 40508] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-d2b02d71-811b-4a1b-beec-e7557cf65bae for DN 127.0.0.1:46335
2020-12-03 07:21:52,198 [IPC Server handler 0 on default port 40508] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-03c4e4df-d4c4-413d-a247-bf506f40e039 for DN 127.0.0.1:45911
2020-12-03 07:21:52,198 [IPC Server handler 0 on default port 40508] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-167fc370-7419-4f5b-a77d-5974db53e53b for DN 127.0.0.1:45911
2020-12-03 07:21:52,199 [IPC Server handler 1 on default port 40508] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-a4774b34-f21c-4814-aefc-8f3ebb297608 for DN 127.0.0.1:44964
2020-12-03 07:21:52,199 [IPC Server handler 1 on default port 40508] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-bd99d0d3-dfed-4a67-b8cf-af2132dc7382 for DN 127.0.0.1:44964
2020-12-03 07:21:52,234 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x2e25ffc47cccbda: Processing first storage report for DS-a4774b34-f21c-4814-aefc-8f3ebb297608 from datanode ac27962c-fa6a-4f23-86c8-18a7f92f9f56
2020-12-03 07:21:52,237 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x2e25ffc47cccbda: from storage DS-a4774b34-f21c-4814-aefc-8f3ebb297608 node DatanodeRegistration(127.0.0.1:44964, datanodeUuid=ac27962c-fa6a-4f23-86c8-18a7f92f9f56, infoPort=40204, infoSecurePort=0, ipcPort=43008, storageInfo=lv=-57;cid=testClusterID;nsid=629858452;c=1606980105077), blocks: 0, hasStaleStorage: true, processing time: 3 msecs, invalidatedBlocks: 0
2020-12-03 07:21:52,237 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x462ac6b81563ec15: Processing first storage report for DS-03c4e4df-d4c4-413d-a247-bf506f40e039 from datanode 6af00937-a528-462e-97d0-7a889bd94f87
2020-12-03 07:21:52,237 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x462ac6b81563ec15: from storage DS-03c4e4df-d4c4-413d-a247-bf506f40e039 node DatanodeRegistration(127.0.0.1:45911, datanodeUuid=6af00937-a528-462e-97d0-7a889bd94f87, infoPort=42011, infoSecurePort=0, ipcPort=40280, storageInfo=lv=-57;cid=testClusterID;nsid=629858452;c=1606980105077), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:21:52,237 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x1ee46e262a947b8d: Processing first storage report for DS-d2b02d71-811b-4a1b-beec-e7557cf65bae from datanode 67671ade-3465-42ec-9f5f-9ccb0a3921b0
2020-12-03 07:21:52,237 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x1ee46e262a947b8d: from storage DS-d2b02d71-811b-4a1b-beec-e7557cf65bae node DatanodeRegistration(127.0.0.1:46335, datanodeUuid=67671ade-3465-42ec-9f5f-9ccb0a3921b0, infoPort=34112, infoSecurePort=0, ipcPort=35345, storageInfo=lv=-57;cid=testClusterID;nsid=629858452;c=1606980105077), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:21:52,238 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x3d29ce156ebd4ea2: Processing first storage report for DS-65c144b2-b711-4cc2-94b7-e2af5974caa9 from datanode a36cabce-9acb-4413-8be0-398b0630ef41
2020-12-03 07:21:52,238 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x3d29ce156ebd4ea2: from storage DS-65c144b2-b711-4cc2-94b7-e2af5974caa9 node DatanodeRegistration(127.0.0.1:36081, datanodeUuid=a36cabce-9acb-4413-8be0-398b0630ef41, infoPort=45122, infoSecurePort=0, ipcPort=33017, storageInfo=lv=-57;cid=testClusterID;nsid=629858452;c=1606980105077), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:21:52,238 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x5eb8e6c82d52ec53: Processing first storage report for DS-16ad47b7-6740-4c6f-9eb7-62aad3546c7d from datanode f693c68b-acb5-4ad0-8980-23d838f7fba7
2020-12-03 07:21:52,238 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x5eb8e6c82d52ec53: from storage DS-16ad47b7-6740-4c6f-9eb7-62aad3546c7d node DatanodeRegistration(127.0.0.1:43386, datanodeUuid=f693c68b-acb5-4ad0-8980-23d838f7fba7, infoPort=36937, infoSecurePort=0, ipcPort=41623, storageInfo=lv=-57;cid=testClusterID;nsid=629858452;c=1606980105077), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:21:52,238 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x2e25ffc47cccbda: Processing first storage report for DS-bd99d0d3-dfed-4a67-b8cf-af2132dc7382 from datanode ac27962c-fa6a-4f23-86c8-18a7f92f9f56
2020-12-03 07:21:52,238 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x2e25ffc47cccbda: from storage DS-bd99d0d3-dfed-4a67-b8cf-af2132dc7382 node DatanodeRegistration(127.0.0.1:44964, datanodeUuid=ac27962c-fa6a-4f23-86c8-18a7f92f9f56, infoPort=40204, infoSecurePort=0, ipcPort=43008, storageInfo=lv=-57;cid=testClusterID;nsid=629858452;c=1606980105077), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:21:52,239 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x462ac6b81563ec15: Processing first storage report for DS-167fc370-7419-4f5b-a77d-5974db53e53b from datanode 6af00937-a528-462e-97d0-7a889bd94f87
2020-12-03 07:21:52,239 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x462ac6b81563ec15: from storage DS-167fc370-7419-4f5b-a77d-5974db53e53b node DatanodeRegistration(127.0.0.1:45911, datanodeUuid=6af00937-a528-462e-97d0-7a889bd94f87, infoPort=42011, infoSecurePort=0, ipcPort=40280, storageInfo=lv=-57;cid=testClusterID;nsid=629858452;c=1606980105077), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:21:52,239 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x1ee46e262a947b8d: Processing first storage report for DS-fb09e1ea-5c92-4d14-a724-2616a3983122 from datanode 67671ade-3465-42ec-9f5f-9ccb0a3921b0
2020-12-03 07:21:52,239 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x1ee46e262a947b8d: from storage DS-fb09e1ea-5c92-4d14-a724-2616a3983122 node DatanodeRegistration(127.0.0.1:46335, datanodeUuid=67671ade-3465-42ec-9f5f-9ccb0a3921b0, infoPort=34112, infoSecurePort=0, ipcPort=35345, storageInfo=lv=-57;cid=testClusterID;nsid=629858452;c=1606980105077), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:21:52,239 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x3d29ce156ebd4ea2: Processing first storage report for DS-e617a7b2-64f7-42ba-a4d8-2991b5de0697 from datanode a36cabce-9acb-4413-8be0-398b0630ef41
2020-12-03 07:21:52,240 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x3d29ce156ebd4ea2: from storage DS-e617a7b2-64f7-42ba-a4d8-2991b5de0697 node DatanodeRegistration(127.0.0.1:36081, datanodeUuid=a36cabce-9acb-4413-8be0-398b0630ef41, infoPort=45122, infoSecurePort=0, ipcPort=33017, storageInfo=lv=-57;cid=testClusterID;nsid=629858452;c=1606980105077), blocks: 0, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
2020-12-03 07:21:52,240 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x5eb8e6c82d52ec53: Processing first storage report for DS-a70eae85-caf8-4810-9307-7fdcff1de75a from datanode f693c68b-acb5-4ad0-8980-23d838f7fba7
2020-12-03 07:21:52,240 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x5eb8e6c82d52ec53: from storage DS-a70eae85-caf8-4810-9307-7fdcff1de75a node DatanodeRegistration(127.0.0.1:43386, datanodeUuid=f693c68b-acb5-4ad0-8980-23d838f7fba7, infoPort=36937, infoSecurePort=0, ipcPort=41623, storageInfo=lv=-57;cid=testClusterID;nsid=629858452;c=1606980105077), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:21:52,247 [IPC Server handler 2 on default port 40508] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:52,254 [Listener at localhost/41296] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:21:52,254 [Listener at localhost/41296] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:21:52,258 [Thread-172] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:52,258 [Thread-172] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18/current/BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:52,258 [Thread-172] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18 and block pool id BP-1681972789-172.17.0.7-1606980105077 is not formatted. Formatting ...
2020-12-03 07:21:52,259 [Thread-172] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1681972789-172.17.0.7-1606980105077 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18/current/BP-1681972789-172.17.0.7-1606980105077/current
2020-12-03 07:21:52,271 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x2e25ffc47cccbda,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 52 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:21:52,271 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x5eb8e6c82d52ec53,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 5 msec to generate and 54 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:21:52,272 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:52,271 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x462ac6b81563ec15,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 5 msec to generate and 52 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:21:52,272 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:52,271 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x3d29ce156ebd4ea2,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 53 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:21:52,271 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x1ee46e262a947b8d,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 52 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:21:52,273 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:52,272 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:52,273 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:52,357 [IPC Server handler 1 on default port 40508] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:52,359 [Listener at localhost/41296] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:21:52,359 [Listener at localhost/41296] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:21:52,462 [IPC Server handler 4 on default port 40508] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:52,463 [Listener at localhost/41296] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:21:52,463 [Listener at localhost/41296] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:21:52,482 [Thread-172] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=629858452;bpid=BP-1681972789-172.17.0.7-1606980105077;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=629858452;c=1606980105077;bpid=BP-1681972789-172.17.0.7-1606980105077;dnuuid=null
2020-12-03 07:21:52,565 [IPC Server handler 0 on default port 40508] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:52,567 [Listener at localhost/41296] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:21:52,567 [Listener at localhost/41296] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:21:52,669 [IPC Server handler 6 on default port 40508] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:52,671 [Listener at localhost/41296] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:21:52,671 [Listener at localhost/41296] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:21:52,774 [IPC Server handler 2 on default port 40508] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:52,775 [Listener at localhost/41296] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:21:52,775 [Listener at localhost/41296] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:21:52,844 [Thread-172] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID 6568366e-2097-441e-bd45-29423857a859
2020-12-03 07:21:52,848 [Thread-172] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-650414f7-31fe-484d-8973-85ea58ac630c
2020-12-03 07:21:52,848 [Thread-172] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [RAM_DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16, StorageType: RAM_DISK
2020-12-03 07:21:52,850 [Thread-172] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-fcefd356-7d44-40b3-85cf-64c3f285c82c
2020-12-03 07:21:52,851 [Thread-172] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17, StorageType: ARCHIVE
2020-12-03 07:21:52,853 [Thread-172] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-ffc14c33-e3bb-433d-a880-69e3de925b44
2020-12-03 07:21:52,853 [Thread-172] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18, StorageType: ARCHIVE
2020-12-03 07:21:52,854 [Thread-172] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:21:52,856 [Thread-172] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16
2020-12-03 07:21:52,857 [Thread-172] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16
2020-12-03 07:21:52,857 [Thread-172] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17
2020-12-03 07:21:52,857 [Thread-172] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17
2020-12-03 07:21:52,858 [Thread-172] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18
2020-12-03 07:21:52,858 [Thread-172] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18
2020-12-03 07:21:52,858 [Thread-172] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:52,859 [Thread-244] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16...
2020-12-03 07:21:52,859 [Thread-245] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17...
2020-12-03 07:21:52,859 [Thread-246] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18...
2020-12-03 07:21:52,881 [IPC Server handler 8 on default port 40508] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:52,883 [Listener at localhost/41296] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:21:52,883 [Listener at localhost/41296] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:21:52,888 [Thread-245] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1681972789-172.17.0.7-1606980105077 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17: 29ms
2020-12-03 07:21:52,888 [Thread-244] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1681972789-172.17.0.7-1606980105077 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16: 29ms
2020-12-03 07:21:52,891 [Thread-246] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1681972789-172.17.0.7-1606980105077 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18: 32ms
2020-12-03 07:21:52,891 [Thread-172] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1681972789-172.17.0.7-1606980105077: 33ms
2020-12-03 07:21:52,891 [Thread-250] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16...
2020-12-03 07:21:52,892 [Thread-251] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17...
2020-12-03 07:21:52,892 [Thread-252] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18...
2020-12-03 07:21:52,892 [Thread-250] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16/current/BP-1681972789-172.17.0.7-1606980105077/current/replicas doesn't exist 
2020-12-03 07:21:52,892 [Thread-252] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18/current/BP-1681972789-172.17.0.7-1606980105077/current/replicas doesn't exist 
2020-12-03 07:21:52,892 [Thread-251] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17/current/BP-1681972789-172.17.0.7-1606980105077/current/replicas doesn't exist 
2020-12-03 07:21:52,892 [Thread-250] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16: 0ms
2020-12-03 07:21:52,892 [Thread-252] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18: 0ms
2020-12-03 07:21:52,892 [Thread-251] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17: 0ms
2020-12-03 07:21:52,893 [Thread-172] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077: 2ms
2020-12-03 07:21:52,893 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16
2020-12-03 07:21:52,894 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17
2020-12-03 07:21:52,894 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18
2020-12-03 07:21:52,894 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16, DS-650414f7-31fe-484d-8973-85ea58ac630c): finished scanning block pool BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:52,894 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17, DS-fcefd356-7d44-40b3-85cf-64c3f285c82c): finished scanning block pool BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:52,894 [Thread-172] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 8:25 AM with interval of 21600000ms
2020-12-03 07:21:52,894 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18, DS-ffc14c33-e3bb-433d-a880-69e3de925b44): finished scanning block pool BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:52,895 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1681972789-172.17.0.7-1606980105077 (Datanode Uuid 6568366e-2097-441e-bd45-29423857a859) service to localhost/127.0.0.1:40508 beginning handshake with NN
2020-12-03 07:21:52,896 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16, DS-650414f7-31fe-484d-8973-85ea58ac630c): no suitable block pools found to scan.  Waiting 1814399998 ms.
2020-12-03 07:21:52,896 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17, DS-fcefd356-7d44-40b3-85cf-64c3f285c82c): no suitable block pools found to scan.  Waiting 1814399997 ms.
2020-12-03 07:21:52,896 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18, DS-ffc14c33-e3bb-433d-a880-69e3de925b44): no suitable block pools found to scan.  Waiting 1814399998 ms.
2020-12-03 07:21:52,898 [IPC Server handler 3 on default port 40508] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:32804, datanodeUuid=6568366e-2097-441e-bd45-29423857a859, infoPort=42175, infoSecurePort=0, ipcPort=41296, storageInfo=lv=-57;cid=testClusterID;nsid=629858452;c=1606980105077) storage 6568366e-2097-441e-bd45-29423857a859
2020-12-03 07:21:52,898 [IPC Server handler 3 on default port 40508] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:32804
2020-12-03 07:21:52,898 [IPC Server handler 3 on default port 40508] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 6568366e-2097-441e-bd45-29423857a859 (127.0.0.1:32804).
2020-12-03 07:21:52,899 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1681972789-172.17.0.7-1606980105077 (Datanode Uuid 6568366e-2097-441e-bd45-29423857a859) service to localhost/127.0.0.1:40508 successfully registered with NN
2020-12-03 07:21:52,899 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:40508 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:21:52,902 [IPC Server handler 5 on default port 40508] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-650414f7-31fe-484d-8973-85ea58ac630c for DN 127.0.0.1:32804
2020-12-03 07:21:52,902 [IPC Server handler 5 on default port 40508] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-fcefd356-7d44-40b3-85cf-64c3f285c82c for DN 127.0.0.1:32804
2020-12-03 07:21:52,903 [IPC Server handler 5 on default port 40508] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-ffc14c33-e3bb-433d-a880-69e3de925b44 for DN 127.0.0.1:32804
2020-12-03 07:21:52,911 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0xc24cbd5dfed8bd5c: Processing first storage report for DS-650414f7-31fe-484d-8973-85ea58ac630c from datanode 6568366e-2097-441e-bd45-29423857a859
2020-12-03 07:21:52,911 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0xc24cbd5dfed8bd5c: from storage DS-650414f7-31fe-484d-8973-85ea58ac630c node DatanodeRegistration(127.0.0.1:32804, datanodeUuid=6568366e-2097-441e-bd45-29423857a859, infoPort=42175, infoSecurePort=0, ipcPort=41296, storageInfo=lv=-57;cid=testClusterID;nsid=629858452;c=1606980105077), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:21:52,911 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0xc24cbd5dfed8bd5c: Processing first storage report for DS-ffc14c33-e3bb-433d-a880-69e3de925b44 from datanode 6568366e-2097-441e-bd45-29423857a859
2020-12-03 07:21:52,911 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0xc24cbd5dfed8bd5c: from storage DS-ffc14c33-e3bb-433d-a880-69e3de925b44 node DatanodeRegistration(127.0.0.1:32804, datanodeUuid=6568366e-2097-441e-bd45-29423857a859, infoPort=42175, infoSecurePort=0, ipcPort=41296, storageInfo=lv=-57;cid=testClusterID;nsid=629858452;c=1606980105077), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:21:52,912 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0xc24cbd5dfed8bd5c: Processing first storage report for DS-fcefd356-7d44-40b3-85cf-64c3f285c82c from datanode 6568366e-2097-441e-bd45-29423857a859
2020-12-03 07:21:52,912 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0xc24cbd5dfed8bd5c: from storage DS-fcefd356-7d44-40b3-85cf-64c3f285c82c node DatanodeRegistration(127.0.0.1:32804, datanodeUuid=6568366e-2097-441e-bd45-29423857a859, infoPort=42175, infoSecurePort=0, ipcPort=41296, storageInfo=lv=-57;cid=testClusterID;nsid=629858452;c=1606980105077), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:21:52,915 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0xc24cbd5dfed8bd5c,  containing 3 storage report(s), of which we sent 3. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 9 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:21:52,915 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:52,985 [IPC Server handler 9 on default port 40508] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:52,988 [Listener at localhost/41296] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2758)) - Cluster is active
2020-12-03 07:21:52,996 [IPC Server handler 1 on default port 40508] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:52,997 [Listener at localhost/41296] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2758)) - Cluster is active
2020-12-03 07:21:53,016 [IPC Server handler 4 on default port 40508] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-12-03 07:21:53,058 [IPC Server handler 0 on default port 40508] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/blockStatsFile1	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2020-12-03 07:21:53,109 [IPC Server handler 6 on default port 40508] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_1073741825_1001, replicas=127.0.0.1:43386 for /blockStatsFile1
2020-12-03 07:21:53,138 [Thread-256] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:21:53,226 [DataXceiver for client DFSClient_NONMAPREDUCE_-1378863765_24 at /127.0.0.1:49742 [Receiving block BP-1681972789-172.17.0.7-1606980105077:blk_1073741825_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1681972789-172.17.0.7-1606980105077:blk_1073741825_1001 src: /127.0.0.1:49742 dest: /127.0.0.1:43386
2020-12-03 07:21:53,274 [PacketResponder: BP-1681972789-172.17.0.7-1606980105077:blk_1073741825_1001, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:49742, dest: /127.0.0.1:43386, bytes: 1024, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1378863765_24, offset: 0, srvID: f693c68b-acb5-4ad0-8980-23d838f7fba7, blockid: BP-1681972789-172.17.0.7-1606980105077:blk_1073741825_1001, duration(ns): 11540451
2020-12-03 07:21:53,274 [PacketResponder: BP-1681972789-172.17.0.7-1606980105077:blk_1073741825_1001, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1681972789-172.17.0.7-1606980105077:blk_1073741825_1001, type=LAST_IN_PIPELINE terminating
2020-12-03 07:21:53,281 [IPC Server handler 8 on default port 40508] INFO  namenode.FSNamesystem (FSNamesystem.java:checkBlocksComplete(2995)) - BLOCK* blk_1073741825_1001 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /blockStatsFile1
2020-12-03 07:21:53,687 [IPC Server handler 5 on default port 40508] INFO  hdfs.StateChange (FSNamesystem.java:completeFile(2948)) - DIR* completeFile: /blockStatsFile1 is closed by DFSClient_NONMAPREDUCE_-1378863765_24
2020-12-03 07:21:53,690 [IPC Server handler 7 on default port 40508] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-12-03 07:21:53,693 [IPC Server handler 9 on default port 40508] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/blockStatsFile2	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2020-12-03 07:21:53,698 [IPC Server handler 1 on default port 40508] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_1073741826_1002, replicas=127.0.0.1:43386 for /blockStatsFile2
2020-12-03 07:21:53,700 [Thread-261] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:21:53,702 [DataXceiver for client DFSClient_NONMAPREDUCE_-1378863765_24 at /127.0.0.1:49748 [Receiving block BP-1681972789-172.17.0.7-1606980105077:blk_1073741826_1002]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1681972789-172.17.0.7-1606980105077:blk_1073741826_1002 src: /127.0.0.1:49748 dest: /127.0.0.1:43386
2020-12-03 07:21:53,703 [DataXceiver for client DFSClient_NONMAPREDUCE_-1378863765_24 at /127.0.0.1:49748 [Receiving block BP-1681972789-172.17.0.7-1606980105077:blk_1073741826_1002]] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:21:53,708 [DataXceiver for client DFSClient_NONMAPREDUCE_-1378863765_24 at /127.0.0.1:49748 [Receiving block BP-1681972789-172.17.0.7-1606980105077:blk_1073741826_1002]] WARN  datanode.DataNode (BlockReceiver.java:<init>(289)) - IOException in BlockReceiver constructor :Possible disk error: Failed to create /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1681972789-172.17.0.7-1606980105077/current/rbw/blk_1073741826. Cause is 
java.io.IOException: Not a directory
	at java.io.UnixFileSystem.createFileExclusively(Native Method)
	at java.io.File.createNewFile(File.java:1012)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.createFile(FileIoProvider.java:302)
	at org.apache.hadoop.hdfs.server.datanode.DatanodeUtil.createFileWithExistsCheck(DatanodeUtil.java:69)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.createRbwFile(BlockPoolSlice.java:350)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.createRbwFile(FsVolumeImpl.java:945)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.createRbw(FsVolumeImpl.java:1225)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.createRbw(FsDatasetImpl.java:1423)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.<init>(BlockReceiver.java:216)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.getBlockReceiver(DataXceiver.java:1312)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:763)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:21:53,708 [VolumeCheck ResultHandler thread 0] WARN  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:onFailure(380)) - Exception running disk checks against volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
org.apache.hadoop.util.DiskChecker$DiskErrorException: Cannot create directory: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1681972789-172.17.0.7-1606980105077/current/finalized
	at org.apache.hadoop.util.DiskChecker.checkDirInternal(DiskChecker.java:98)
	at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:77)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.checkDirs(BlockPoolSlice.java:399)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.check(FsVolumeImpl.java:992)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.check(FsVolumeImpl.java:100)
	at org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker$1.call(ThrottledAsyncChecker.java:142)
	at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:125)
	at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:57)
	at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:78)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:21:53,710 [DataXceiver for client DFSClient_NONMAPREDUCE_-1378863765_24 at /127.0.0.1:49748 [Receiving block BP-1681972789-172.17.0.7-1606980105077:blk_1073741826_1002]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(939)) - opWriteBlock BP-1681972789-172.17.0.7-1606980105077:blk_1073741826_1002 received exception java.io.IOException: Not a directory
2020-12-03 07:21:53,711 [VolumeCheck ResultHandler thread 0] WARN  datanode.DataNode (DataNode.java:lambda$checkDiskErrorAsync$0(2182)) - checkDiskErrorAsync callback got 1 failed volumes: [/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2]
2020-12-03 07:21:53,713 [VolumeCheck ResultHandler thread 0] INFO  datanode.BlockScanner (BlockScanner.java:removeVolumeScanner(255)) - Removing scanner for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2 (StorageID DS-a70eae85-caf8-4810-9307-7fdcff1de75a)
2020-12-03 07:21:53,713 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-a70eae85-caf8-4810-9307-7fdcff1de75a) exiting.
2020-12-03 07:21:53,716 [DataXceiver for client DFSClient_NONMAPREDUCE_-1378863765_24 at /127.0.0.1:49748 [Receiving block BP-1681972789-172.17.0.7-1606980105077:blk_1073741826_1002]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:43386:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:49748 dst: /127.0.0.1:43386
java.io.IOException: Not a directory
	at java.io.UnixFileSystem.createFileExclusively(Native Method)
	at java.io.File.createNewFile(File.java:1012)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.createFile(FileIoProvider.java:302)
	at org.apache.hadoop.hdfs.server.datanode.DatanodeUtil.createFileWithExistsCheck(DatanodeUtil.java:69)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.createRbwFile(BlockPoolSlice.java:350)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.createRbwFile(FsVolumeImpl.java:945)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.createRbw(FsVolumeImpl.java:1225)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.createRbw(FsDatasetImpl.java:1423)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.<init>(BlockReceiver.java:216)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.getBlockReceiver(DataXceiver.java:1312)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:763)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:21:53,716 [Thread-261] INFO  hdfs.DataStreamer (DataStreamer.java:createBlockOutputStream(1790)) - Exception in createBlockOutputStream blk_1073741826_1002
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:550)
	at org.apache.hadoop.hdfs.DataStreamer.createBlockOutputStream(DataStreamer.java:1762)
	at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1679)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
2020-12-03 07:21:53,717 [Thread-261] WARN  hdfs.DataStreamer (DataStreamer.java:nextBlockOutputStream(1683)) - Abandoning BP-1681972789-172.17.0.7-1606980105077:blk_1073741826_1002
2020-12-03 07:21:53,718 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:21:53,718 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] WARN  datanode.VolumeScanner (VolumeScanner.java:saveBlockIterator(319)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-a70eae85-caf8-4810-9307-7fdcff1de75a): error saving org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$BlockIteratorImpl@4fa4cc0a.
java.io.FileNotFoundException: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1681972789-172.17.0.7-1606980105077/scanner.cursor.tmp (Not a directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider$WrappedFileOutputStream.<init>(FileIoProvider.java:899)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider$WrappedFileOutputStream.<init>(FileIoProvider.java:890)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.getFileOutputStream(FileIoProvider.java:360)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.getFileOutputStream(FileIoProvider.java:384)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$BlockIteratorImpl.save(FsVolumeImpl.java:859)
	at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.saveBlockIterator(VolumeScanner.java:317)
	at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:640)
2020-12-03 07:21:53,719 [VolumeCheck ResultHandler thread 0] WARN  impl.FsDatasetImpl (BlockPoolSlice.java:saveDfsUsed(326)) - Failed to write dfsUsed to /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1681972789-172.17.0.7-1606980105077/current/dfsUsed
java.io.FileNotFoundException: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1681972789-172.17.0.7-1606980105077/current/dfsUsed (Not a directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:162)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.saveDfsUsed(BlockPoolSlice.java:314)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.shutdown(BlockPoolSlice.java:857)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.shutdown(FsVolumeImpl.java:1030)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.removeVolume(FsVolumeList.java:323)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.handleVolumeFailures(FsVolumeList.java:246)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.handleVolumeFailures(FsDatasetImpl.java:2255)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.handleVolumeFailures(DataNode.java:3401)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.lambda$checkDiskErrorAsync$0(DataNode.java:2188)
	at org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$ResultHandler.invokeCallback(DatasetVolumeChecker.java:407)
	at org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$ResultHandler.cleanup(DatasetVolumeChecker.java:400)
	at org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$ResultHandler.onFailure(DatasetVolumeChecker.java:383)
	at com.google.common.util.concurrent.Futures$CallbackListener.run(Futures.java:1056)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:21:53,720 [VolumeCheck ResultHandler thread 1] WARN  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:onFailure(380)) - Exception running disk checks against volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
org.apache.hadoop.util.DiskChecker$DiskErrorException: Cannot create directory: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1681972789-172.17.0.7-1606980105077/current/finalized
	at org.apache.hadoop.util.DiskChecker.checkDirInternal(DiskChecker.java:98)
	at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:77)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.checkDirs(BlockPoolSlice.java:399)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.check(FsVolumeImpl.java:992)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.check(FsVolumeImpl.java:100)
	at org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker$1.call(ThrottledAsyncChecker.java:142)
	at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:125)
	at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:57)
	at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:78)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:21:53,721 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1681972789-172.17.0.7-1606980105077] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:21:53,721 [VolumeCheck ResultHandler thread 1] WARN  datanode.DataNode (DataNode.java:lambda$checkDiskErrorAsync$0(2182)) - checkDiskErrorAsync callback got 1 failed volumes: [/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2]
2020-12-03 07:21:53,721 [VolumeCheck ResultHandler thread 0] INFO  impl.FsDatasetImpl (FsVolumeList.java:removeVolume(325)) - Removed volume: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:21:53,722 [VolumeCheck ResultHandler thread 0] INFO  impl.FsDatasetImpl (FsVolumeList.java:waitVolumeRemoved(278)) - Volume reference is released.
2020-12-03 07:21:53,722 [VolumeCheck ResultHandler thread 1] INFO  impl.FsDatasetImpl (FsVolumeList.java:waitVolumeRemoved(278)) - Volume reference is released.
2020-12-03 07:21:53,722 [VolumeCheck ResultHandler thread 0] INFO  datanode.DataNode (DataNode.java:removeVolumes(872)) - Deactivating volumes (clear failure=false): [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:21:53,722 [VolumeCheck ResultHandler thread 0] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:removeVolumes(534)) - Checking removing StorageLocation [RAM_DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1 with id DS-16ad47b7-6740-4c6f-9eb7-62aad3546c7d
2020-12-03 07:21:53,722 [VolumeCheck ResultHandler thread 0] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:removeVolumes(534)) - Checking removing StorageLocation [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2 with id DS-a70eae85-caf8-4810-9307-7fdcff1de75a
2020-12-03 07:21:53,723 [VolumeCheck ResultHandler thread 0] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:removeVolumes(537)) - Removing StorageLocation [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2 with id DS-a70eae85-caf8-4810-9307-7fdcff1de75a from FsDataset.
2020-12-03 07:21:53,723 [VolumeCheck ResultHandler thread 0] INFO  impl.FsDatasetImpl (FsVolumeList.java:waitVolumeRemoved(278)) - Volume reference is released.
2020-12-03 07:21:53,724 [Thread-261] WARN  hdfs.DataStreamer (DataStreamer.java:nextBlockOutputStream(1688)) - Excluding datanode DatanodeInfoWithStorage[127.0.0.1:43386,DS-a70eae85-caf8-4810-9307-7fdcff1de75a,DISK]
2020-12-03 07:21:53,726 [VolumeCheck ResultHandler thread 0] INFO  common.Storage (BlockPoolSliceStorage.java:remove(297)) - Removing block level storage: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:53,727 [VolumeCheck ResultHandler thread 0] WARN  datanode.DataNode (DataNode.java:handleDiskError(2194)) - DataNode.handleDiskError on: [DataNode failed volumes:[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2;] Keep Running: false
2020-12-03 07:21:53,727 [VolumeCheck ResultHandler thread 1] INFO  datanode.DataNode (DataNode.java:removeVolumes(872)) - Deactivating volumes (clear failure=false): [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:21:53,728 [VolumeCheck ResultHandler thread 1] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:removeVolumes(534)) - Checking removing StorageLocation [RAM_DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1 with id DS-16ad47b7-6740-4c6f-9eb7-62aad3546c7d
2020-12-03 07:21:53,728 [VolumeCheck ResultHandler thread 1] WARN  datanode.DataNode (DataNode.java:handleDiskError(2194)) - DataNode.handleDiskError on: [DataNode failed volumes:[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2;] Keep Running: false
2020-12-03 07:21:53,728 [VolumeCheck ResultHandler thread 0] WARN  datanode.DataNode (DataNode.java:handleDiskError(2213)) - DataNode is shutting down due to failed volumes: [DataNode failed volumes:[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2;]
2020-12-03 07:21:53,728 [VolumeCheck ResultHandler thread 1] WARN  datanode.DataNode (DataNode.java:handleDiskError(2213)) - DataNode is shutting down due to failed volumes: [DataNode failed volumes:[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2;]
2020-12-03 07:21:53,730 [IPC Server handler 0 on default port 40508] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_1073741827_1003, replicas=127.0.0.1:36081 for /blockStatsFile2
2020-12-03 07:21:53,732 [Thread-261] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:21:53,733 [DataXceiver for client DFSClient_NONMAPREDUCE_-1378863765_24 at /127.0.0.1:56628 [Receiving block BP-1681972789-172.17.0.7-1606980105077:blk_1073741827_1003]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1681972789-172.17.0.7-1606980105077:blk_1073741827_1003 src: /127.0.0.1:56628 dest: /127.0.0.1:36081
2020-12-03 07:21:53,733 [DataXceiver for client DFSClient_NONMAPREDUCE_-1378863765_24 at /127.0.0.1:56628 [Receiving block BP-1681972789-172.17.0.7-1606980105077:blk_1073741827_1003]] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8
2020-12-03 07:21:53,733 [DataXceiver for client DFSClient_NONMAPREDUCE_-1378863765_24 at /127.0.0.1:56628 [Receiving block BP-1681972789-172.17.0.7-1606980105077:blk_1073741827_1003]] WARN  datanode.DataNode (BlockReceiver.java:<init>(289)) - IOException in BlockReceiver constructor :Possible disk error: Failed to create /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/current/BP-1681972789-172.17.0.7-1606980105077/current/rbw/blk_1073741827. Cause is 
java.io.IOException: Not a directory
	at java.io.UnixFileSystem.createFileExclusively(Native Method)
	at java.io.File.createNewFile(File.java:1012)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.createFile(FileIoProvider.java:302)
	at org.apache.hadoop.hdfs.server.datanode.DatanodeUtil.createFileWithExistsCheck(DatanodeUtil.java:69)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.createRbwFile(BlockPoolSlice.java:350)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.createRbwFile(FsVolumeImpl.java:945)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.createRbw(FsVolumeImpl.java:1225)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.createRbw(FsDatasetImpl.java:1423)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.<init>(BlockReceiver.java:216)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.getBlockReceiver(DataXceiver.java:1312)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:763)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:21:53,734 [DataXceiver for client DFSClient_NONMAPREDUCE_-1378863765_24 at /127.0.0.1:56628 [Receiving block BP-1681972789-172.17.0.7-1606980105077:blk_1073741827_1003]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(939)) - opWriteBlock BP-1681972789-172.17.0.7-1606980105077:blk_1073741827_1003 received exception java.io.IOException: Not a directory
2020-12-03 07:21:53,734 [DataXceiver for client DFSClient_NONMAPREDUCE_-1378863765_24 at /127.0.0.1:56628 [Receiving block BP-1681972789-172.17.0.7-1606980105077:blk_1073741827_1003]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:36081:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:56628 dst: /127.0.0.1:36081
java.io.IOException: Not a directory
	at java.io.UnixFileSystem.createFileExclusively(Native Method)
	at java.io.File.createNewFile(File.java:1012)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.createFile(FileIoProvider.java:302)
	at org.apache.hadoop.hdfs.server.datanode.DatanodeUtil.createFileWithExistsCheck(DatanodeUtil.java:69)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.createRbwFile(BlockPoolSlice.java:350)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.createRbwFile(FsVolumeImpl.java:945)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.createRbw(FsVolumeImpl.java:1225)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.createRbw(FsDatasetImpl.java:1423)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.<init>(BlockReceiver.java:216)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.getBlockReceiver(DataXceiver.java:1312)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:763)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:21:53,734 [Thread-261] INFO  hdfs.DataStreamer (DataStreamer.java:createBlockOutputStream(1790)) - Exception in createBlockOutputStream blk_1073741827_1003
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:550)
	at org.apache.hadoop.hdfs.DataStreamer.createBlockOutputStream(DataStreamer.java:1762)
	at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1679)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
2020-12-03 07:21:53,735 [VolumeCheck ResultHandler thread 0] WARN  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:onFailure(380)) - Exception running disk checks against volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8
org.apache.hadoop.util.DiskChecker$DiskErrorException: Cannot create directory: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/current/BP-1681972789-172.17.0.7-1606980105077/current/finalized
	at org.apache.hadoop.util.DiskChecker.checkDirInternal(DiskChecker.java:98)
	at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:77)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.checkDirs(BlockPoolSlice.java:399)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.check(FsVolumeImpl.java:992)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.check(FsVolumeImpl.java:100)
	at org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker$1.call(ThrottledAsyncChecker.java:142)
	at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:125)
	at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:57)
	at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:78)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:21:53,735 [Thread-261] WARN  hdfs.DataStreamer (DataStreamer.java:nextBlockOutputStream(1683)) - Abandoning BP-1681972789-172.17.0.7-1606980105077:blk_1073741827_1003
2020-12-03 07:21:53,736 [VolumeCheck ResultHandler thread 0] WARN  datanode.DataNode (DataNode.java:lambda$checkDiskErrorAsync$0(2182)) - checkDiskErrorAsync callback got 1 failed volumes: [/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8]
2020-12-03 07:21:53,737 [VolumeCheck ResultHandler thread 0] INFO  datanode.BlockScanner (BlockScanner.java:removeVolumeScanner(255)) - Removing scanner for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8 (StorageID DS-e617a7b2-64f7-42ba-a4d8-2991b5de0697)
2020-12-03 07:21:53,737 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8, DS-e617a7b2-64f7-42ba-a4d8-2991b5de0697) exiting.
2020-12-03 07:21:53,737 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8)] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8
2020-12-03 07:21:53,737 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8)] WARN  datanode.VolumeScanner (VolumeScanner.java:saveBlockIterator(319)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8, DS-e617a7b2-64f7-42ba-a4d8-2991b5de0697): error saving org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$BlockIteratorImpl@11efcd8e.
java.io.FileNotFoundException: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/current/BP-1681972789-172.17.0.7-1606980105077/scanner.cursor.tmp (Not a directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider$WrappedFileOutputStream.<init>(FileIoProvider.java:899)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider$WrappedFileOutputStream.<init>(FileIoProvider.java:890)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.getFileOutputStream(FileIoProvider.java:360)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.getFileOutputStream(FileIoProvider.java:384)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$BlockIteratorImpl.save(FsVolumeImpl.java:859)
	at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.saveBlockIterator(VolumeScanner.java:317)
	at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:640)
2020-12-03 07:21:53,738 [VolumeCheck ResultHandler thread 0] WARN  impl.FsDatasetImpl (BlockPoolSlice.java:saveDfsUsed(326)) - Failed to write dfsUsed to /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/current/BP-1681972789-172.17.0.7-1606980105077/current/dfsUsed
java.io.FileNotFoundException: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/current/BP-1681972789-172.17.0.7-1606980105077/current/dfsUsed (Not a directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:162)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.saveDfsUsed(BlockPoolSlice.java:314)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.shutdown(BlockPoolSlice.java:857)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.shutdown(FsVolumeImpl.java:1030)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.removeVolume(FsVolumeList.java:323)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.handleVolumeFailures(FsVolumeList.java:246)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.handleVolumeFailures(FsDatasetImpl.java:2255)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.handleVolumeFailures(DataNode.java:3401)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.lambda$checkDiskErrorAsync$0(DataNode.java:2188)
	at org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$ResultHandler.invokeCallback(DatasetVolumeChecker.java:407)
	at org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$ResultHandler.cleanup(DatasetVolumeChecker.java:400)
	at org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$ResultHandler.onFailure(DatasetVolumeChecker.java:383)
	at com.google.common.util.concurrent.Futures$CallbackListener.run(Futures.java:1056)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:21:53,739 [Thread-261] WARN  hdfs.DataStreamer (DataStreamer.java:nextBlockOutputStream(1688)) - Excluding datanode DatanodeInfoWithStorage[127.0.0.1:36081,DS-e617a7b2-64f7-42ba-a4d8-2991b5de0697,DISK]
2020-12-03 07:21:53,738 [VolumeCheck ResultHandler thread 1] WARN  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:onFailure(380)) - Exception running disk checks against volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8
org.apache.hadoop.util.DiskChecker$DiskErrorException: Cannot create directory: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/current/BP-1681972789-172.17.0.7-1606980105077/current/finalized
	at org.apache.hadoop.util.DiskChecker.checkDirInternal(DiskChecker.java:98)
	at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:77)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.checkDirs(BlockPoolSlice.java:399)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.check(FsVolumeImpl.java:992)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.check(FsVolumeImpl.java:100)
	at org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker$1.call(ThrottledAsyncChecker.java:142)
	at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:125)
	at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:57)
	at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:78)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:21:53,739 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/current/BP-1681972789-172.17.0.7-1606980105077] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:21:53,739 [VolumeCheck ResultHandler thread 0] INFO  impl.FsDatasetImpl (FsVolumeList.java:removeVolume(325)) - Removed volume: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8
2020-12-03 07:21:53,740 [VolumeCheck ResultHandler thread 1] WARN  datanode.DataNode (DataNode.java:lambda$checkDiskErrorAsync$0(2182)) - checkDiskErrorAsync callback got 1 failed volumes: [/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8]
2020-12-03 07:21:53,742 [VolumeCheck ResultHandler thread 0] INFO  impl.FsDatasetImpl (FsVolumeList.java:waitVolumeRemoved(278)) - Volume reference is released.
2020-12-03 07:21:53,742 [VolumeCheck ResultHandler thread 1] INFO  impl.FsDatasetImpl (FsVolumeList.java:waitVolumeRemoved(278)) - Volume reference is released.
2020-12-03 07:21:53,743 [VolumeCheck ResultHandler thread 0] INFO  datanode.DataNode (DataNode.java:removeVolumes(872)) - Deactivating volumes (clear failure=false): [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8
2020-12-03 07:21:53,743 [VolumeCheck ResultHandler thread 0] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:removeVolumes(534)) - Checking removing StorageLocation [RAM_DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7 with id DS-65c144b2-b711-4cc2-94b7-e2af5974caa9
2020-12-03 07:21:53,743 [VolumeCheck ResultHandler thread 0] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:removeVolumes(534)) - Checking removing StorageLocation [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8 with id DS-e617a7b2-64f7-42ba-a4d8-2991b5de0697
2020-12-03 07:21:53,743 [VolumeCheck ResultHandler thread 0] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:removeVolumes(537)) - Removing StorageLocation [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8 with id DS-e617a7b2-64f7-42ba-a4d8-2991b5de0697 from FsDataset.
2020-12-03 07:21:53,743 [VolumeCheck ResultHandler thread 0] INFO  impl.FsDatasetImpl (FsVolumeList.java:waitVolumeRemoved(278)) - Volume reference is released.
2020-12-03 07:21:53,744 [VolumeCheck ResultHandler thread 0] INFO  common.Storage (BlockPoolSliceStorage.java:remove(297)) - Removing block level storage: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/current/BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:53,744 [IPC Server handler 2 on default port 40508] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_1073741828_1004, replicas=127.0.0.1:45911 for /blockStatsFile2
2020-12-03 07:21:53,746 [VolumeCheck ResultHandler thread 0] WARN  datanode.DataNode (DataNode.java:handleDiskError(2194)) - DataNode.handleDiskError on: [DataNode failed volumes:[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8;] Keep Running: false
2020-12-03 07:21:53,746 [VolumeCheck ResultHandler thread 0] WARN  datanode.DataNode (DataNode.java:handleDiskError(2213)) - DataNode is shutting down due to failed volumes: [DataNode failed volumes:[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8;]
2020-12-03 07:21:53,747 [Thread-261] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:21:53,748 [DataXceiver for client DFSClient_NONMAPREDUCE_-1378863765_24 at /127.0.0.1:50152 [Receiving block BP-1681972789-172.17.0.7-1606980105077:blk_1073741828_1004]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1681972789-172.17.0.7-1606980105077:blk_1073741828_1004 src: /127.0.0.1:50152 dest: /127.0.0.1:45911
2020-12-03 07:21:53,748 [VolumeCheck ResultHandler thread 1] INFO  datanode.DataNode (DataNode.java:removeVolumes(872)) - Deactivating volumes (clear failure=false): [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8
2020-12-03 07:21:53,749 [VolumeCheck ResultHandler thread 1] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:removeVolumes(534)) - Checking removing StorageLocation [RAM_DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7 with id DS-65c144b2-b711-4cc2-94b7-e2af5974caa9
2020-12-03 07:21:53,749 [DataXceiver for client DFSClient_NONMAPREDUCE_-1378863765_24 at /127.0.0.1:50152 [Receiving block BP-1681972789-172.17.0.7-1606980105077:blk_1073741828_1004]] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5
2020-12-03 07:21:53,749 [VolumeCheck ResultHandler thread 1] WARN  datanode.DataNode (DataNode.java:handleDiskError(2194)) - DataNode.handleDiskError on: [DataNode failed volumes:[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8;] Keep Running: false
2020-12-03 07:21:53,750 [VolumeCheck ResultHandler thread 1] WARN  datanode.DataNode (DataNode.java:handleDiskError(2213)) - DataNode is shutting down due to failed volumes: [DataNode failed volumes:[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8;]
2020-12-03 07:21:53,750 [DataXceiver for client DFSClient_NONMAPREDUCE_-1378863765_24 at /127.0.0.1:50152 [Receiving block BP-1681972789-172.17.0.7-1606980105077:blk_1073741828_1004]] WARN  datanode.DataNode (BlockReceiver.java:<init>(289)) - IOException in BlockReceiver constructor :Possible disk error: Failed to create /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-1681972789-172.17.0.7-1606980105077/current/rbw/blk_1073741828. Cause is 
java.io.IOException: Not a directory
	at java.io.UnixFileSystem.createFileExclusively(Native Method)
	at java.io.File.createNewFile(File.java:1012)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.createFile(FileIoProvider.java:302)
	at org.apache.hadoop.hdfs.server.datanode.DatanodeUtil.createFileWithExistsCheck(DatanodeUtil.java:69)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.createRbwFile(BlockPoolSlice.java:350)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.createRbwFile(FsVolumeImpl.java:945)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.createRbw(FsVolumeImpl.java:1225)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.createRbw(FsDatasetImpl.java:1423)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.<init>(BlockReceiver.java:216)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.getBlockReceiver(DataXceiver.java:1312)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:763)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:21:53,750 [DataXceiver for client DFSClient_NONMAPREDUCE_-1378863765_24 at /127.0.0.1:50152 [Receiving block BP-1681972789-172.17.0.7-1606980105077:blk_1073741828_1004]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(939)) - opWriteBlock BP-1681972789-172.17.0.7-1606980105077:blk_1073741828_1004 received exception java.io.IOException: Not a directory
2020-12-03 07:21:53,751 [DataXceiver for client DFSClient_NONMAPREDUCE_-1378863765_24 at /127.0.0.1:50152 [Receiving block BP-1681972789-172.17.0.7-1606980105077:blk_1073741828_1004]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:45911:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:50152 dst: /127.0.0.1:45911
java.io.IOException: Not a directory
	at java.io.UnixFileSystem.createFileExclusively(Native Method)
	at java.io.File.createNewFile(File.java:1012)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.createFile(FileIoProvider.java:302)
	at org.apache.hadoop.hdfs.server.datanode.DatanodeUtil.createFileWithExistsCheck(DatanodeUtil.java:69)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.createRbwFile(BlockPoolSlice.java:350)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.createRbwFile(FsVolumeImpl.java:945)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.createRbw(FsVolumeImpl.java:1225)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.createRbw(FsDatasetImpl.java:1423)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.<init>(BlockReceiver.java:216)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.getBlockReceiver(DataXceiver.java:1312)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:763)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:21:53,751 [VolumeCheck ResultHandler thread 0] WARN  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:onFailure(380)) - Exception running disk checks against volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5
org.apache.hadoop.util.DiskChecker$DiskErrorException: Cannot create directory: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-1681972789-172.17.0.7-1606980105077/current/finalized
	at org.apache.hadoop.util.DiskChecker.checkDirInternal(DiskChecker.java:98)
	at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:77)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.checkDirs(BlockPoolSlice.java:399)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.check(FsVolumeImpl.java:992)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.check(FsVolumeImpl.java:100)
	at org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker$1.call(ThrottledAsyncChecker.java:142)
	at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:125)
	at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:57)
	at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:78)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:21:53,751 [Thread-261] INFO  hdfs.DataStreamer (DataStreamer.java:createBlockOutputStream(1790)) - Exception in createBlockOutputStream blk_1073741828_1004
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:550)
	at org.apache.hadoop.hdfs.DataStreamer.createBlockOutputStream(DataStreamer.java:1762)
	at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1679)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
2020-12-03 07:21:53,753 [VolumeCheck ResultHandler thread 0] WARN  datanode.DataNode (DataNode.java:lambda$checkDiskErrorAsync$0(2182)) - checkDiskErrorAsync callback got 1 failed volumes: [/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5]
2020-12-03 07:21:53,753 [Thread-261] WARN  hdfs.DataStreamer (DataStreamer.java:nextBlockOutputStream(1683)) - Abandoning BP-1681972789-172.17.0.7-1606980105077:blk_1073741828_1004
2020-12-03 07:21:53,754 [VolumeCheck ResultHandler thread 0] INFO  datanode.BlockScanner (BlockScanner.java:removeVolumeScanner(255)) - Removing scanner for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5 (StorageID DS-167fc370-7419-4f5b-a77d-5974db53e53b)
2020-12-03 07:21:53,754 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5, DS-167fc370-7419-4f5b-a77d-5974db53e53b) exiting.
2020-12-03 07:21:53,754 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5)] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5
2020-12-03 07:21:53,755 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5)] WARN  datanode.VolumeScanner (VolumeScanner.java:saveBlockIterator(319)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5, DS-167fc370-7419-4f5b-a77d-5974db53e53b): error saving org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$BlockIteratorImpl@326b1cbf.
java.io.FileNotFoundException: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-1681972789-172.17.0.7-1606980105077/scanner.cursor.tmp (Not a directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider$WrappedFileOutputStream.<init>(FileIoProvider.java:899)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider$WrappedFileOutputStream.<init>(FileIoProvider.java:890)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.getFileOutputStream(FileIoProvider.java:360)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.getFileOutputStream(FileIoProvider.java:384)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$BlockIteratorImpl.save(FsVolumeImpl.java:859)
	at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.saveBlockIterator(VolumeScanner.java:317)
	at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:640)
2020-12-03 07:21:53,755 [VolumeCheck ResultHandler thread 0] WARN  impl.FsDatasetImpl (BlockPoolSlice.java:saveDfsUsed(326)) - Failed to write dfsUsed to /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-1681972789-172.17.0.7-1606980105077/current/dfsUsed
java.io.FileNotFoundException: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-1681972789-172.17.0.7-1606980105077/current/dfsUsed (Not a directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:162)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.saveDfsUsed(BlockPoolSlice.java:314)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.shutdown(BlockPoolSlice.java:857)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.shutdown(FsVolumeImpl.java:1030)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.removeVolume(FsVolumeList.java:323)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.handleVolumeFailures(FsVolumeList.java:246)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.handleVolumeFailures(FsDatasetImpl.java:2255)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.handleVolumeFailures(DataNode.java:3401)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.lambda$checkDiskErrorAsync$0(DataNode.java:2188)
	at org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$ResultHandler.invokeCallback(DatasetVolumeChecker.java:407)
	at org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$ResultHandler.cleanup(DatasetVolumeChecker.java:400)
	at org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$ResultHandler.onFailure(DatasetVolumeChecker.java:383)
	at com.google.common.util.concurrent.Futures$CallbackListener.run(Futures.java:1056)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:21:53,756 [Thread-261] WARN  hdfs.DataStreamer (DataStreamer.java:nextBlockOutputStream(1688)) - Excluding datanode DatanodeInfoWithStorage[127.0.0.1:45911,DS-167fc370-7419-4f5b-a77d-5974db53e53b,DISK]
2020-12-03 07:21:53,756 [VolumeCheck ResultHandler thread 1] WARN  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:onFailure(380)) - Exception running disk checks against volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5
org.apache.hadoop.util.DiskChecker$DiskErrorException: Cannot create directory: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-1681972789-172.17.0.7-1606980105077/current/finalized
	at org.apache.hadoop.util.DiskChecker.checkDirInternal(DiskChecker.java:98)
	at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:77)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.checkDirs(BlockPoolSlice.java:399)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.check(FsVolumeImpl.java:992)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.check(FsVolumeImpl.java:100)
	at org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker$1.call(ThrottledAsyncChecker.java:142)
	at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:125)
	at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:57)
	at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:78)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:21:53,757 [VolumeCheck ResultHandler thread 0] INFO  impl.FsDatasetImpl (FsVolumeList.java:removeVolume(325)) - Removed volume: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5
2020-12-03 07:21:53,760 [VolumeCheck ResultHandler thread 0] INFO  impl.FsDatasetImpl (FsVolumeList.java:waitVolumeRemoved(278)) - Volume reference is released.
2020-12-03 07:21:53,757 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-1681972789-172.17.0.7-1606980105077] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:21:53,760 [VolumeCheck ResultHandler thread 0] INFO  datanode.DataNode (DataNode.java:removeVolumes(872)) - Deactivating volumes (clear failure=false): [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5
2020-12-03 07:21:53,760 [VolumeCheck ResultHandler thread 1] WARN  datanode.DataNode (DataNode.java:lambda$checkDiskErrorAsync$0(2182)) - checkDiskErrorAsync callback got 1 failed volumes: [/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5]
2020-12-03 07:21:53,761 [VolumeCheck ResultHandler thread 0] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:removeVolumes(534)) - Checking removing StorageLocation [RAM_DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4 with id DS-03c4e4df-d4c4-413d-a247-bf506f40e039
2020-12-03 07:21:53,761 [VolumeCheck ResultHandler thread 1] INFO  impl.FsDatasetImpl (FsVolumeList.java:waitVolumeRemoved(278)) - Volume reference is released.
2020-12-03 07:21:53,761 [VolumeCheck ResultHandler thread 0] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:removeVolumes(534)) - Checking removing StorageLocation [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5 with id DS-167fc370-7419-4f5b-a77d-5974db53e53b
2020-12-03 07:21:53,761 [VolumeCheck ResultHandler thread 0] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:removeVolumes(537)) - Removing StorageLocation [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5 with id DS-167fc370-7419-4f5b-a77d-5974db53e53b from FsDataset.
2020-12-03 07:21:53,762 [VolumeCheck ResultHandler thread 0] INFO  impl.FsDatasetImpl (FsVolumeList.java:waitVolumeRemoved(278)) - Volume reference is released.
2020-12-03 07:21:53,763 [VolumeCheck ResultHandler thread 0] INFO  common.Storage (BlockPoolSliceStorage.java:remove(297)) - Removing block level storage: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:53,764 [VolumeCheck ResultHandler thread 0] WARN  datanode.DataNode (DataNode.java:handleDiskError(2194)) - DataNode.handleDiskError on: [DataNode failed volumes:[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5;] Keep Running: false
2020-12-03 07:21:53,764 [VolumeCheck ResultHandler thread 1] INFO  datanode.DataNode (DataNode.java:removeVolumes(872)) - Deactivating volumes (clear failure=false): [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5
2020-12-03 07:21:53,764 [VolumeCheck ResultHandler thread 0] WARN  datanode.DataNode (DataNode.java:handleDiskError(2213)) - DataNode is shutting down due to failed volumes: [DataNode failed volumes:[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5;]
2020-12-03 07:21:53,765 [VolumeCheck ResultHandler thread 1] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:removeVolumes(534)) - Checking removing StorageLocation [RAM_DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4 with id DS-03c4e4df-d4c4-413d-a247-bf506f40e039
2020-12-03 07:21:53,765 [VolumeCheck ResultHandler thread 1] WARN  datanode.DataNode (DataNode.java:handleDiskError(2194)) - DataNode.handleDiskError on: [DataNode failed volumes:[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5;] Keep Running: false
2020-12-03 07:21:53,766 [VolumeCheck ResultHandler thread 1] WARN  datanode.DataNode (DataNode.java:handleDiskError(2213)) - DataNode is shutting down due to failed volumes: [DataNode failed volumes:[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5;]
2020-12-03 07:21:53,769 [IPC Server handler 3 on default port 40508] WARN  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(450)) - Failed to place enough replicas, still in need of 1 to reach 1 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2020-12-03 07:21:53,770 [IPC Server handler 3 on default port 40508] WARN  protocol.BlockStoragePolicy (BlockStoragePolicy.java:chooseStorageTypes(161)) - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=1, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2020-12-03 07:21:53,771 [IPC Server handler 3 on default port 40508] WARN  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(450)) - Failed to place enough replicas, still in need of 1 to reach 1 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2020-12-03 07:21:53,772 [IPC Server handler 3 on default port 40508] INFO  ipc.Server (Server.java:logException(2976)) - IPC Server handler 3 on default port 40508, call Call#57 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:50690
java.io.IOException: File /blockStatsFile2 could only be written to 0 of the 1 minReplication nodes. There are 6 datanode(s) running and 4 node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2219)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2789)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:892)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:574)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)
2020-12-03 07:21:53,784 [Thread-261] WARN  hdfs.DataStreamer (DataStreamer.java:run(826)) - DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /blockStatsFile2 could only be written to 0 of the 1 minReplication nodes. There are 6 datanode(s) running and 4 node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2219)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2789)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:892)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:574)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1545)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy26.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy27.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
	at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1866)
	at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1668)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
2020-12-03 07:21:55,173 [IPC Server handler 9 on default port 40508] WARN  namenode.NameNode (NameNodeRpcServer.java:errorReport(1666)) - Fatal disk error on DatanodeRegistration(127.0.0.1:43386, datanodeUuid=f693c68b-acb5-4ad0-8980-23d838f7fba7, infoPort=36937, infoSecurePort=0, ipcPort=41623, storageInfo=lv=-57;cid=testClusterID;nsid=629858452;c=1606980105077): DataNode failed volumes:[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2;
2020-12-03 07:21:55,173 [IPC Server handler 1 on default port 40508] WARN  namenode.NameNode (NameNodeRpcServer.java:errorReport(1666)) - Fatal disk error on DatanodeRegistration(127.0.0.1:36081, datanodeUuid=a36cabce-9acb-4413-8be0-398b0630ef41, infoPort=45122, infoSecurePort=0, ipcPort=33017, storageInfo=lv=-57;cid=testClusterID;nsid=629858452;c=1606980105077): DataNode failed volumes:[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8;
2020-12-03 07:21:55,173 [IPC Server handler 4 on default port 40508] WARN  namenode.NameNode (NameNodeRpcServer.java:errorReport(1666)) - Fatal disk error on DatanodeRegistration(127.0.0.1:45911, datanodeUuid=6af00937-a528-462e-97d0-7a889bd94f87, infoPort=42011, infoSecurePort=0, ipcPort=40280, storageInfo=lv=-57;cid=testClusterID;nsid=629858452;c=1606980105077): DataNode failed volumes:[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5;
2020-12-03 07:21:55,176 [IPC Server handler 9 on default port 40508] INFO  net.NetworkTopology (NetworkTopology.java:remove(219)) - Removing a node: /default-rack/127.0.0.1:43386
2020-12-03 07:21:55,176 [IPC Server handler 1 on default port 40508] INFO  net.NetworkTopology (NetworkTopology.java:remove(219)) - Removing a node: /default-rack/127.0.0.1:36081
2020-12-03 07:21:55,177 [IPC Server handler 4 on default port 40508] INFO  net.NetworkTopology (NetworkTopology.java:remove(219)) - Removing a node: /default-rack/127.0.0.1:45911
2020-12-03 07:21:55,177 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1681972789-172.17.0.7-1606980105077 (Datanode Uuid a36cabce-9acb-4413-8be0-398b0630ef41) service to localhost/127.0.0.1:40508
2020-12-03 07:21:55,177 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1681972789-172.17.0.7-1606980105077 (Datanode Uuid f693c68b-acb5-4ad0-8980-23d838f7fba7) service to localhost/127.0.0.1:40508
2020-12-03 07:21:55,177 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1681972789-172.17.0.7-1606980105077 (Datanode Uuid a36cabce-9acb-4413-8be0-398b0630ef41)
2020-12-03 07:21:55,177 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1681972789-172.17.0.7-1606980105077 (Datanode Uuid f693c68b-acb5-4ad0-8980-23d838f7fba7)
2020-12-03 07:21:55,177 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:55,177 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:55,177 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1681972789-172.17.0.7-1606980105077 (Datanode Uuid 6af00937-a528-462e-97d0-7a889bd94f87) service to localhost/127.0.0.1:40508
2020-12-03 07:21:55,178 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1681972789-172.17.0.7-1606980105077 (Datanode Uuid 6af00937-a528-462e-97d0-7a889bd94f87)
2020-12-03 07:21:55,178 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:21:55,178 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1681972789-172.17.0.7-1606980105077] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:21:55,178 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7/current/BP-1681972789-172.17.0.7-1606980105077] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:21:55,178 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1681972789-172.17.0.7-1606980105077] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:21:59,787 [Listener at localhost/41296] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:stopDataNode(2331)) - MiniDFSCluster Stopping DataNode 127.0.0.1:43386 from a total of 6 datanodes.
2020-12-03 07:21:59,787 [Listener at localhost/41296] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:21:59,788 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@22e3e430] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:21:59,789 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-16ad47b7-6740-4c6f-9eb7-62aad3546c7d) exiting.
2020-12-03 07:21:59,875 [Listener at localhost/41296] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@70b01ef0{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:21:59,881 [Listener at localhost/41296] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@2c5ac465{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:21:59,882 [Listener at localhost/41296] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@2d970ddc{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:21:59,882 [Listener at localhost/41296] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@8c58693{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:21:59,886 [Listener at localhost/41296] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 41623
2020-12-03 07:21:59,887 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:21:59,888 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:21:59,916 [Listener at localhost/41296] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:21:59,916 [Listener at localhost/41296] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:21:59,917 [Listener at localhost/41296] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:21:59,917 [Listener at localhost/41296] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:21:59,919 [Listener at localhost/41296] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:21:59,921 [Listener at localhost/41296] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [RAM_DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:21:59,921 [Listener at localhost/41296] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:21:59,922 [Listener at localhost/41296] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:21:59,923 [Listener at localhost/41296] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:21:59,923 [Listener at localhost/41296] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:21:59,924 [Listener at localhost/41296] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:21:59,924 [Listener at localhost/41296] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:21:59,924 [Listener at localhost/41296] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:21:59,925 [Listener at localhost/41296] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:43386
2020-12-03 07:21:59,925 [Listener at localhost/41296] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:21:59,925 [Listener at localhost/41296] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:21:59,926 [Listener at localhost/41296] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:21:59,928 [Listener at localhost/41296] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:21:59,928 [Listener at localhost/41296] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:21:59,928 [Listener at localhost/41296] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:21:59,930 [Listener at localhost/41296] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:21:59,931 [Listener at localhost/41296] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:21:59,931 [Listener at localhost/41296] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:21:59,931 [Listener at localhost/41296] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:21:59,932 [Listener at localhost/41296] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 40954
2020-12-03 07:21:59,932 [Listener at localhost/41296] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:21:59,933 [Listener at localhost/41296] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@68f7a047{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:21:59,934 [Listener at localhost/41296] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@1540b2c9{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:21:59,940 [Listener at localhost/41296] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@7c965c4f{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:21:59,942 [Listener at localhost/41296] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@1d3b96e5{HTTP/1.1,[http/1.1]}{localhost:40954}
2020-12-03 07:21:59,942 [Listener at localhost/41296] INFO  server.Server (Server.java:doStart(419)) - Started @17473ms
2020-12-03 07:21:59,959 [Listener at localhost/41296] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:45167
2020-12-03 07:21:59,960 [Listener at localhost/41296] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:21:59,960 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@6a63d1e3] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:21:59,960 [Listener at localhost/41296] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:21:59,960 [Listener at localhost/41296] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:21:59,961 [Socket Reader #1 for port 41623] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 41623
2020-12-03 07:21:59,965 [Listener at localhost/41623] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:41623
2020-12-03 07:21:59,973 [Listener at localhost/41623] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:21:59,973 [Listener at localhost/41623] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:21:59,974 [Thread-276] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:40508 starting to offer service
2020-12-03 07:21:59,978 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:21:59,978 [IPC Server listener on 41623] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 41623: starting
2020-12-03 07:21:59,982 [Listener at localhost/41623] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:stopDataNode(2331)) - MiniDFSCluster Stopping DataNode 127.0.0.1:45911 from a total of 6 datanodes.
2020-12-03 07:21:59,982 [Listener at localhost/41623] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:21:59,982 [Thread-276] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:40508
2020-12-03 07:21:59,982 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@2e5fc186] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:21:59,983 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, DS-03c4e4df-d4c4-413d-a247-bf506f40e039) exiting.
2020-12-03 07:21:59,983 [Thread-276] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:22:00,005 [Listener at localhost/41623] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@f20ae76{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:22:00,006 [Listener at localhost/41623] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@29a3c24c{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:22:00,006 [Listener at localhost/41623] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@340663de{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:22:00,006 [Listener at localhost/41623] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@4daa429a{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:22:00,007 [Listener at localhost/41623] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 40280
2020-12-03 07:22:00,015 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:22:00,015 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:22:00,025 [Listener at localhost/41623] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:22:00,025 [Listener at localhost/41623] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:22:00,026 [Listener at localhost/41623] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:22:00,026 [Listener at localhost/41623] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:22:00,027 [Listener at localhost/41623] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:22:00,029 [Listener at localhost/41623] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [RAM_DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:22:00,029 [Listener at localhost/41623] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5
2020-12-03 07:22:00,031 [Listener at localhost/41623] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:22:00,031 [Listener at localhost/41623] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:22:00,031 [Listener at localhost/41623] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:22:00,032 [Listener at localhost/41623] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:22:00,032 [Listener at localhost/41623] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:22:00,032 [Listener at localhost/41623] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:22:00,033 [Listener at localhost/41623] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:45911
2020-12-03 07:22:00,033 [Listener at localhost/41623] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:22:00,033 [Thread-276] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/in_use.lock acquired by nodename 2751@a3be22c8fbc1
2020-12-03 07:22:00,033 [Listener at localhost/41623] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:22:00,034 [Listener at localhost/41623] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:22:00,036 [Listener at localhost/41623] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:22:00,036 [Listener at localhost/41623] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:22:00,036 [Listener at localhost/41623] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:22:00,038 [Listener at localhost/41623] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:22:00,040 [Listener at localhost/41623] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:22:00,040 [Listener at localhost/41623] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:22:00,040 [Listener at localhost/41623] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:22:00,041 [Listener at localhost/41623] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 40477
2020-12-03 07:22:00,041 [Listener at localhost/41623] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:22:00,043 [Listener at localhost/41623] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@4f06bb9b{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:22:00,044 [Listener at localhost/41623] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@5bce3f3f{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:22:00,050 [Listener at localhost/41623] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@75e1782{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:22:00,052 [Listener at localhost/41623] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@4f76f3ed{HTTP/1.1,[http/1.1]}{localhost:40477}
2020-12-03 07:22:00,052 [Listener at localhost/41623] INFO  server.Server (Server.java:doStart(419)) - Started @17583ms
2020-12-03 07:22:00,129 [Listener at localhost/41623] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:34912
2020-12-03 07:22:00,129 [Listener at localhost/41623] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:22:00,129 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@1c5c13bb] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:22:00,129 [Listener at localhost/41623] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:22:00,130 [Listener at localhost/41623] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:22:00,130 [Socket Reader #1 for port 40280] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 40280
2020-12-03 07:22:00,135 [Listener at localhost/40280] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:40280
2020-12-03 07:22:00,151 [Listener at localhost/40280] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:22:00,152 [Listener at localhost/40280] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:22:00,155 [Thread-298] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:40508 starting to offer service
2020-12-03 07:22:00,164 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:22:00,164 [IPC Server listener on 40280] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 40280: starting
2020-12-03 07:22:00,167 [Thread-276] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/in_use.lock acquired by nodename 2751@a3be22c8fbc1
2020-12-03 07:22:00,169 [Listener at localhost/40280] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:stopDataNode(2331)) - MiniDFSCluster Stopping DataNode 127.0.0.1:36081 from a total of 6 datanodes.
2020-12-03 07:22:00,170 [Listener at localhost/40280] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:22:00,170 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@4fc70b1b] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:22:00,171 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7, DS-65c144b2-b711-4cc2-94b7-e2af5974caa9) exiting.
2020-12-03 07:22:00,173 [Thread-298] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:40508
2020-12-03 07:22:00,179 [Thread-298] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:22:00,205 [Listener at localhost/40280] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@6868caaa{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:22:00,206 [Listener at localhost/40280] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@13ec55c4{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:22:00,207 [Listener at localhost/40280] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@e027b0a{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:22:00,207 [Listener at localhost/40280] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@2597c40{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:22:00,208 [Listener at localhost/40280] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 33017
2020-12-03 07:22:00,211 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:22:00,211 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:22:00,214 [Listener at localhost/40280] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:22:00,214 [Listener at localhost/40280] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:22:00,216 [Listener at localhost/40280] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:22:00,216 [Listener at localhost/40280] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:22:00,217 [Listener at localhost/40280] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:22:00,219 [Listener at localhost/40280] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [RAM_DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7
2020-12-03 07:22:00,221 [Listener at localhost/40280] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8
2020-12-03 07:22:00,222 [Listener at localhost/40280] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:22:00,223 [Listener at localhost/40280] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:22:00,223 [Listener at localhost/40280] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:22:00,226 [Listener at localhost/40280] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:22:00,226 [Listener at localhost/40280] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:22:00,226 [Listener at localhost/40280] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:22:00,227 [Listener at localhost/40280] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:36081
2020-12-03 07:22:00,227 [Listener at localhost/40280] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:22:00,228 [Listener at localhost/40280] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:22:00,229 [Listener at localhost/40280] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:22:00,230 [Thread-276] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:22:00,231 [Listener at localhost/40280] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:22:00,231 [Thread-276] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:22:00,231 [Listener at localhost/40280] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:22:00,232 [Listener at localhost/40280] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:22:00,234 [Listener at localhost/40280] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:22:00,234 [Listener at localhost/40280] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:22:00,235 [Listener at localhost/40280] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:22:00,235 [Listener at localhost/40280] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:22:00,236 [Listener at localhost/40280] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 37460
2020-12-03 07:22:00,236 [Listener at localhost/40280] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:22:00,238 [Listener at localhost/40280] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@6639127b{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:22:00,239 [Listener at localhost/40280] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@1df68869{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:22:00,239 [Thread-298] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/in_use.lock acquired by nodename 2751@a3be22c8fbc1
2020-12-03 07:22:00,247 [Listener at localhost/40280] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@70106f89{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:22:00,249 [Listener at localhost/40280] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@474c4c83{HTTP/1.1,[http/1.1]}{localhost:37460}
2020-12-03 07:22:00,250 [Listener at localhost/40280] INFO  server.Server (Server.java:doStart(419)) - Started @17781ms
2020-12-03 07:22:00,266 [Listener at localhost/40280] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:41442
2020-12-03 07:22:00,267 [Listener at localhost/40280] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:22:00,267 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@7b1b1240] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:22:00,267 [Listener at localhost/40280] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:22:00,267 [Listener at localhost/40280] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:22:00,268 [Socket Reader #1 for port 33017] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 33017
2020-12-03 07:22:00,272 [Listener at localhost/33017] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:33017
2020-12-03 07:22:00,289 [Listener at localhost/33017] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:22:00,290 [Listener at localhost/33017] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:22:00,290 [Thread-320] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:40508 starting to offer service
2020-12-03 07:22:00,296 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:22:00,298 [IPC Server listener on 33017] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 33017: starting
2020-12-03 07:22:00,302 [Thread-320] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:40508
2020-12-03 07:22:00,304 [Thread-320] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:22:00,307 [Thread-276] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:22:00,308 [Thread-276] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:22:00,371 [Thread-298] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/in_use.lock acquired by nodename 2751@a3be22c8fbc1
2020-12-03 07:22:00,371 [Thread-320] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7/in_use.lock acquired by nodename 2751@a3be22c8fbc1
2020-12-03 07:22:00,417 [Thread-276] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=629858452;bpid=BP-1681972789-172.17.0.7-1606980105077;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=629858452;c=1606980105077;bpid=BP-1681972789-172.17.0.7-1606980105077;dnuuid=f693c68b-acb5-4ad0-8980-23d838f7fba7
2020-12-03 07:22:00,420 [Thread-276] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-16ad47b7-6740-4c6f-9eb7-62aad3546c7d
2020-12-03 07:22:00,422 [Thread-276] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [RAM_DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, StorageType: RAM_DISK
2020-12-03 07:22:00,424 [Thread-276] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-a70eae85-caf8-4810-9307-7fdcff1de75a
2020-12-03 07:22:00,424 [Thread-276] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, StorageType: DISK
2020-12-03 07:22:00,426 [Thread-276] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:22:00,427 [Thread-276] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:22:00,428 [Thread-276] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:22:00,428 [Thread-276] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:22:00,428 [Thread-276] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:22:00,430 [Thread-276] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:22:00,431 [Thread-333] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2020-12-03 07:22:00,431 [Thread-334] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2020-12-03 07:22:00,434 [Thread-333] INFO  impl.FsDatasetImpl (BlockPoolSlice.java:loadDfsUsed(292)) - Cached dfsUsed found for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1681972789-172.17.0.7-1606980105077/current: 24576
2020-12-03 07:22:00,442 [Thread-333] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1681972789-172.17.0.7-1606980105077 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 11ms
2020-12-03 07:22:00,455 [Thread-298] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:22:00,455 [Thread-298] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:22:00,460 [Thread-334] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1681972789-172.17.0.7-1606980105077 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 29ms
2020-12-03 07:22:00,460 [Thread-276] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1681972789-172.17.0.7-1606980105077: 30ms
2020-12-03 07:22:00,460 [Thread-336] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2020-12-03 07:22:00,461 [Thread-336] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1681972789-172.17.0.7-1606980105077/current/replicas doesn't exist 
2020-12-03 07:22:00,461 [Thread-337] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2020-12-03 07:22:00,461 [Thread-337] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1681972789-172.17.0.7-1606980105077/current/replicas doesn't exist 
2020-12-03 07:22:00,461 [Thread-336] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 0ms
2020-12-03 07:22:00,463 [Thread-337] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 3ms
2020-12-03 07:22:00,464 [Thread-276] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077: 4ms
2020-12-03 07:22:00,476 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-16ad47b7-6740-4c6f-9eb7-62aad3546c7d): no suitable block pools found to scan.  Waiting 1814391608 ms.
2020-12-03 07:22:00,477 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-a70eae85-caf8-4810-9307-7fdcff1de75a): no suitable block pools found to scan.  Waiting 1814391607 ms.
2020-12-03 07:22:00,477 [Thread-276] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 12:17 PM with interval of 21600000ms
2020-12-03 07:22:00,479 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1681972789-172.17.0.7-1606980105077 (Datanode Uuid f693c68b-acb5-4ad0-8980-23d838f7fba7) service to localhost/127.0.0.1:40508 beginning handshake with NN
2020-12-03 07:22:00,480 [IPC Server handler 6 on default port 40508] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:43386, datanodeUuid=f693c68b-acb5-4ad0-8980-23d838f7fba7, infoPort=45167, infoSecurePort=0, ipcPort=41623, storageInfo=lv=-57;cid=testClusterID;nsid=629858452;c=1606980105077) storage f693c68b-acb5-4ad0-8980-23d838f7fba7
2020-12-03 07:22:00,480 [IPC Server handler 6 on default port 40508] INFO  net.NetworkTopology (NetworkTopology.java:remove(219)) - Removing a node: /default-rack/127.0.0.1:43386
2020-12-03 07:22:00,481 [IPC Server handler 6 on default port 40508] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:43386
2020-12-03 07:22:00,481 [IPC Server handler 6 on default port 40508] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateFailedStorage(562)) - [RAM_DISK]DS-16ad47b7-6740-4c6f-9eb7-62aad3546c7d:NORMAL:127.0.0.1:43386 failed.
2020-12-03 07:22:00,481 [IPC Server handler 6 on default port 40508] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateFailedStorage(562)) - [DISK]DS-a70eae85-caf8-4810-9307-7fdcff1de75a:NORMAL:127.0.0.1:43386 failed.
2020-12-03 07:22:00,481 [IPC Server handler 6 on default port 40508] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:pruneStorageMap(548)) - Removed storage [RAM_DISK]DS-16ad47b7-6740-4c6f-9eb7-62aad3546c7d:FAILED:127.0.0.1:43386 from DataNode 127.0.0.1:43386
2020-12-03 07:22:00,481 [IPC Server handler 6 on default port 40508] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:pruneStorageMap(548)) - Removed storage [DISK]DS-a70eae85-caf8-4810-9307-7fdcff1de75a:FAILED:127.0.0.1:43386 from DataNode 127.0.0.1:43386
2020-12-03 07:22:00,482 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1681972789-172.17.0.7-1606980105077 (Datanode Uuid f693c68b-acb5-4ad0-8980-23d838f7fba7) service to localhost/127.0.0.1:40508 successfully registered with NN
2020-12-03 07:22:00,482 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:40508 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:22:00,485 [IPC Server handler 2 on default port 40508] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-16ad47b7-6740-4c6f-9eb7-62aad3546c7d for DN 127.0.0.1:43386
2020-12-03 07:22:00,485 [IPC Server handler 2 on default port 40508] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-a70eae85-caf8-4810-9307-7fdcff1de75a for DN 127.0.0.1:43386
2020-12-03 07:22:00,485 [IPC Server handler 2 on default port 40508] WARN  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:requestLease(230)) - DN f693c68b-acb5-4ad0-8980-23d838f7fba7 (127.0.0.1:43386) requested a lease even though it wasn't yet registered.  Registering now.
2020-12-03 07:22:00,485 [IPC Server handler 2 on default port 40508] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN f693c68b-acb5-4ad0-8980-23d838f7fba7 (127.0.0.1:43386).
2020-12-03 07:22:00,489 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x22539d5090fab883: Processing first storage report for DS-16ad47b7-6740-4c6f-9eb7-62aad3546c7d from datanode f693c68b-acb5-4ad0-8980-23d838f7fba7
2020-12-03 07:22:00,489 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x22539d5090fab883: from storage DS-16ad47b7-6740-4c6f-9eb7-62aad3546c7d node DatanodeRegistration(127.0.0.1:43386, datanodeUuid=f693c68b-acb5-4ad0-8980-23d838f7fba7, infoPort=45167, infoSecurePort=0, ipcPort=41623, storageInfo=lv=-57;cid=testClusterID;nsid=629858452;c=1606980105077), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:22:00,489 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x22539d5090fab883: Processing first storage report for DS-a70eae85-caf8-4810-9307-7fdcff1de75a from datanode f693c68b-acb5-4ad0-8980-23d838f7fba7
2020-12-03 07:22:00,489 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x22539d5090fab883: from storage DS-a70eae85-caf8-4810-9307-7fdcff1de75a node DatanodeRegistration(127.0.0.1:43386, datanodeUuid=f693c68b-acb5-4ad0-8980-23d838f7fba7, infoPort=45167, infoSecurePort=0, ipcPort=41623, storageInfo=lv=-57;cid=testClusterID;nsid=629858452;c=1606980105077), blocks: 1, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
2020-12-03 07:22:00,491 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x22539d5090fab883,  containing 2 storage report(s), of which we sent 2. The reports had 1 total blocks and used 1 RPC(s). This took 1 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:22:00,491 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:22:00,524 [Thread-320] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/in_use.lock acquired by nodename 2751@a3be22c8fbc1
2020-12-03 07:22:00,533 [Thread-298] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:22:00,534 [Thread-298] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:22:00,622 [Thread-298] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=629858452;bpid=BP-1681972789-172.17.0.7-1606980105077;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=629858452;c=1606980105077;bpid=BP-1681972789-172.17.0.7-1606980105077;dnuuid=6af00937-a528-462e-97d0-7a889bd94f87
2020-12-03 07:22:00,625 [Thread-298] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-03c4e4df-d4c4-413d-a247-bf506f40e039
2020-12-03 07:22:00,625 [Thread-298] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [RAM_DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, StorageType: RAM_DISK
2020-12-03 07:22:00,628 [Thread-298] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-167fc370-7419-4f5b-a77d-5974db53e53b
2020-12-03 07:22:00,628 [Thread-298] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5, StorageType: DISK
2020-12-03 07:22:00,630 [Thread-298] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:22:00,631 [Thread-298] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:22:00,632 [Thread-298] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:22:00,632 [Thread-298] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5
2020-12-03 07:22:00,632 [Thread-298] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5
2020-12-03 07:22:00,633 [Thread-298] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:22:00,634 [Thread-343] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4...
2020-12-03 07:22:00,634 [Thread-344] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5...
2020-12-03 07:22:00,635 [Thread-320] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:22:00,635 [Thread-320] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7/current/BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:22:00,636 [Thread-343] INFO  impl.FsDatasetImpl (BlockPoolSlice.java:loadDfsUsed(292)) - Cached dfsUsed found for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1681972789-172.17.0.7-1606980105077/current: 24576
2020-12-03 07:22:00,643 [Thread-343] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1681972789-172.17.0.7-1606980105077 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4: 9ms
2020-12-03 07:22:00,660 [Thread-344] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1681972789-172.17.0.7-1606980105077 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5: 26ms
2020-12-03 07:22:00,660 [Thread-298] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1681972789-172.17.0.7-1606980105077: 27ms
2020-12-03 07:22:00,660 [Thread-346] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4...
2020-12-03 07:22:00,660 [Thread-347] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5...
2020-12-03 07:22:00,660 [Thread-346] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1681972789-172.17.0.7-1606980105077/current/replicas doesn't exist 
2020-12-03 07:22:00,660 [Thread-347] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-1681972789-172.17.0.7-1606980105077/current/replicas doesn't exist 
2020-12-03 07:22:00,661 [Thread-346] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4: 1ms
2020-12-03 07:22:00,661 [Thread-347] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5: 0ms
2020-12-03 07:22:00,661 [Thread-298] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077: 1ms
2020-12-03 07:22:00,662 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, DS-03c4e4df-d4c4-413d-a247-bf506f40e039): no suitable block pools found to scan.  Waiting 1814391422 ms.
2020-12-03 07:22:00,662 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5, DS-167fc370-7419-4f5b-a77d-5974db53e53b): no suitable block pools found to scan.  Waiting 1814391422 ms.
2020-12-03 07:22:00,662 [Thread-298] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 9:07 AM with interval of 21600000ms
2020-12-03 07:22:00,664 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1681972789-172.17.0.7-1606980105077 (Datanode Uuid 6af00937-a528-462e-97d0-7a889bd94f87) service to localhost/127.0.0.1:40508 beginning handshake with NN
2020-12-03 07:22:00,665 [IPC Server handler 3 on default port 40508] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:45911, datanodeUuid=6af00937-a528-462e-97d0-7a889bd94f87, infoPort=34912, infoSecurePort=0, ipcPort=40280, storageInfo=lv=-57;cid=testClusterID;nsid=629858452;c=1606980105077) storage 6af00937-a528-462e-97d0-7a889bd94f87
2020-12-03 07:22:00,666 [IPC Server handler 3 on default port 40508] INFO  net.NetworkTopology (NetworkTopology.java:remove(219)) - Removing a node: /default-rack/127.0.0.1:45911
2020-12-03 07:22:00,666 [IPC Server handler 3 on default port 40508] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:45911
2020-12-03 07:22:00,666 [IPC Server handler 3 on default port 40508] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateFailedStorage(562)) - [RAM_DISK]DS-03c4e4df-d4c4-413d-a247-bf506f40e039:NORMAL:127.0.0.1:45911 failed.
2020-12-03 07:22:00,666 [IPC Server handler 3 on default port 40508] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateFailedStorage(562)) - [DISK]DS-167fc370-7419-4f5b-a77d-5974db53e53b:NORMAL:127.0.0.1:45911 failed.
2020-12-03 07:22:00,666 [IPC Server handler 3 on default port 40508] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:pruneStorageMap(548)) - Removed storage [RAM_DISK]DS-03c4e4df-d4c4-413d-a247-bf506f40e039:FAILED:127.0.0.1:45911 from DataNode 127.0.0.1:45911
2020-12-03 07:22:00,666 [IPC Server handler 3 on default port 40508] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:pruneStorageMap(548)) - Removed storage [DISK]DS-167fc370-7419-4f5b-a77d-5974db53e53b:FAILED:127.0.0.1:45911 from DataNode 127.0.0.1:45911
2020-12-03 07:22:00,667 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1681972789-172.17.0.7-1606980105077 (Datanode Uuid 6af00937-a528-462e-97d0-7a889bd94f87) service to localhost/127.0.0.1:40508 successfully registered with NN
2020-12-03 07:22:00,667 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:40508 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:22:00,670 [IPC Server handler 5 on default port 40508] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-03c4e4df-d4c4-413d-a247-bf506f40e039 for DN 127.0.0.1:45911
2020-12-03 07:22:00,670 [IPC Server handler 5 on default port 40508] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-167fc370-7419-4f5b-a77d-5974db53e53b for DN 127.0.0.1:45911
2020-12-03 07:22:00,677 [IPC Server handler 5 on default port 40508] WARN  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:requestLease(230)) - DN 6af00937-a528-462e-97d0-7a889bd94f87 (127.0.0.1:45911) requested a lease even though it wasn't yet registered.  Registering now.
2020-12-03 07:22:00,677 [IPC Server handler 5 on default port 40508] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 6af00937-a528-462e-97d0-7a889bd94f87 (127.0.0.1:45911).
2020-12-03 07:22:00,680 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x3a2bca4d8dbed23: Processing first storage report for DS-03c4e4df-d4c4-413d-a247-bf506f40e039 from datanode 6af00937-a528-462e-97d0-7a889bd94f87
2020-12-03 07:22:00,680 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x3a2bca4d8dbed23: from storage DS-03c4e4df-d4c4-413d-a247-bf506f40e039 node DatanodeRegistration(127.0.0.1:45911, datanodeUuid=6af00937-a528-462e-97d0-7a889bd94f87, infoPort=34912, infoSecurePort=0, ipcPort=40280, storageInfo=lv=-57;cid=testClusterID;nsid=629858452;c=1606980105077), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:22:00,680 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x3a2bca4d8dbed23: Processing first storage report for DS-167fc370-7419-4f5b-a77d-5974db53e53b from datanode 6af00937-a528-462e-97d0-7a889bd94f87
2020-12-03 07:22:00,680 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x3a2bca4d8dbed23: from storage DS-167fc370-7419-4f5b-a77d-5974db53e53b node DatanodeRegistration(127.0.0.1:45911, datanodeUuid=6af00937-a528-462e-97d0-7a889bd94f87, infoPort=34912, infoSecurePort=0, ipcPort=40280, storageInfo=lv=-57;cid=testClusterID;nsid=629858452;c=1606980105077), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:22:00,682 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x3a2bca4d8dbed23,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:22:00,682 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:22:00,716 [Thread-320] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:22:00,717 [Thread-320] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/current/BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:22:00,802 [Thread-320] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=629858452;bpid=BP-1681972789-172.17.0.7-1606980105077;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=629858452;c=1606980105077;bpid=BP-1681972789-172.17.0.7-1606980105077;dnuuid=a36cabce-9acb-4413-8be0-398b0630ef41
2020-12-03 07:22:00,804 [Thread-320] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-65c144b2-b711-4cc2-94b7-e2af5974caa9
2020-12-03 07:22:00,805 [Thread-320] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [RAM_DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7, StorageType: RAM_DISK
2020-12-03 07:22:00,810 [Thread-320] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-e617a7b2-64f7-42ba-a4d8-2991b5de0697
2020-12-03 07:22:00,810 [Thread-320] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8, StorageType: DISK
2020-12-03 07:22:00,813 [Thread-320] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:22:00,814 [Thread-320] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7
2020-12-03 07:22:00,815 [Thread-320] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7
2020-12-03 07:22:00,815 [Thread-320] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8
2020-12-03 07:22:00,815 [Thread-320] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8
2020-12-03 07:22:00,818 [Thread-320] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:22:00,819 [Thread-353] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7...
2020-12-03 07:22:00,819 [Thread-354] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8...
2020-12-03 07:22:00,820 [Thread-353] INFO  impl.FsDatasetImpl (BlockPoolSlice.java:loadDfsUsed(292)) - Cached dfsUsed found for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7/current/BP-1681972789-172.17.0.7-1606980105077/current: 24576
2020-12-03 07:22:00,828 [Thread-353] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1681972789-172.17.0.7-1606980105077 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7: 9ms
2020-12-03 07:22:00,862 [Thread-354] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1681972789-172.17.0.7-1606980105077 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8: 44ms
2020-12-03 07:22:00,863 [Thread-320] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1681972789-172.17.0.7-1606980105077: 45ms
2020-12-03 07:22:00,863 [Thread-356] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7...
2020-12-03 07:22:00,863 [Thread-356] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7/current/BP-1681972789-172.17.0.7-1606980105077/current/replicas doesn't exist 
2020-12-03 07:22:00,863 [Thread-357] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8...
2020-12-03 07:22:00,864 [Thread-357] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/current/BP-1681972789-172.17.0.7-1606980105077/current/replicas doesn't exist 
2020-12-03 07:22:00,864 [Thread-356] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7: 1ms
2020-12-03 07:22:00,864 [Thread-357] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8: 0ms
2020-12-03 07:22:00,864 [Thread-320] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1681972789-172.17.0.7-1606980105077: 1ms
2020-12-03 07:22:00,865 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7, DS-65c144b2-b711-4cc2-94b7-e2af5974caa9): no suitable block pools found to scan.  Waiting 1814391219 ms.
2020-12-03 07:22:00,865 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8, DS-e617a7b2-64f7-42ba-a4d8-2991b5de0697): no suitable block pools found to scan.  Waiting 1814391221 ms.
2020-12-03 07:22:00,865 [Thread-320] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 10:24 AM with interval of 21600000ms
2020-12-03 07:22:00,869 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1681972789-172.17.0.7-1606980105077 (Datanode Uuid a36cabce-9acb-4413-8be0-398b0630ef41) service to localhost/127.0.0.1:40508 beginning handshake with NN
2020-12-03 07:22:00,871 [IPC Server handler 9 on default port 40508] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:36081, datanodeUuid=a36cabce-9acb-4413-8be0-398b0630ef41, infoPort=41442, infoSecurePort=0, ipcPort=33017, storageInfo=lv=-57;cid=testClusterID;nsid=629858452;c=1606980105077) storage a36cabce-9acb-4413-8be0-398b0630ef41
2020-12-03 07:22:00,871 [IPC Server handler 9 on default port 40508] INFO  net.NetworkTopology (NetworkTopology.java:remove(219)) - Removing a node: /default-rack/127.0.0.1:36081
2020-12-03 07:22:00,871 [IPC Server handler 9 on default port 40508] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:36081
2020-12-03 07:22:00,871 [IPC Server handler 9 on default port 40508] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateFailedStorage(562)) - [RAM_DISK]DS-65c144b2-b711-4cc2-94b7-e2af5974caa9:NORMAL:127.0.0.1:36081 failed.
2020-12-03 07:22:00,871 [IPC Server handler 9 on default port 40508] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateFailedStorage(562)) - [DISK]DS-e617a7b2-64f7-42ba-a4d8-2991b5de0697:NORMAL:127.0.0.1:36081 failed.
2020-12-03 07:22:00,872 [IPC Server handler 9 on default port 40508] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:pruneStorageMap(548)) - Removed storage [DISK]DS-e617a7b2-64f7-42ba-a4d8-2991b5de0697:FAILED:127.0.0.1:36081 from DataNode 127.0.0.1:36081
2020-12-03 07:22:00,872 [IPC Server handler 9 on default port 40508] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:pruneStorageMap(548)) - Removed storage [RAM_DISK]DS-65c144b2-b711-4cc2-94b7-e2af5974caa9:FAILED:127.0.0.1:36081 from DataNode 127.0.0.1:36081
2020-12-03 07:22:00,873 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1681972789-172.17.0.7-1606980105077 (Datanode Uuid a36cabce-9acb-4413-8be0-398b0630ef41) service to localhost/127.0.0.1:40508 successfully registered with NN
2020-12-03 07:22:00,873 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:40508 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:22:00,877 [IPC Server handler 4 on default port 40508] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-65c144b2-b711-4cc2-94b7-e2af5974caa9 for DN 127.0.0.1:36081
2020-12-03 07:22:00,878 [IPC Server handler 4 on default port 40508] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-e617a7b2-64f7-42ba-a4d8-2991b5de0697 for DN 127.0.0.1:36081
2020-12-03 07:22:00,878 [IPC Server handler 4 on default port 40508] WARN  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:requestLease(230)) - DN a36cabce-9acb-4413-8be0-398b0630ef41 (127.0.0.1:36081) requested a lease even though it wasn't yet registered.  Registering now.
2020-12-03 07:22:00,878 [IPC Server handler 4 on default port 40508] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN a36cabce-9acb-4413-8be0-398b0630ef41 (127.0.0.1:36081).
2020-12-03 07:22:00,882 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x2344948615e20134: Processing first storage report for DS-65c144b2-b711-4cc2-94b7-e2af5974caa9 from datanode a36cabce-9acb-4413-8be0-398b0630ef41
2020-12-03 07:22:00,882 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x2344948615e20134: from storage DS-65c144b2-b711-4cc2-94b7-e2af5974caa9 node DatanodeRegistration(127.0.0.1:36081, datanodeUuid=a36cabce-9acb-4413-8be0-398b0630ef41, infoPort=41442, infoSecurePort=0, ipcPort=33017, storageInfo=lv=-57;cid=testClusterID;nsid=629858452;c=1606980105077), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:22:00,883 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x2344948615e20134: Processing first storage report for DS-e617a7b2-64f7-42ba-a4d8-2991b5de0697 from datanode a36cabce-9acb-4413-8be0-398b0630ef41
2020-12-03 07:22:00,883 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x2344948615e20134: from storage DS-e617a7b2-64f7-42ba-a4d8-2991b5de0697 node DatanodeRegistration(127.0.0.1:36081, datanodeUuid=a36cabce-9acb-4413-8be0-398b0630ef41, infoPort=41442, infoSecurePort=0, ipcPort=33017, storageInfo=lv=-57;cid=testClusterID;nsid=629858452;c=1606980105077), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:22:00,885 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x2344948615e20134,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:22:00,885 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:22:06,302 [Listener at localhost/33017] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(2049)) - Shutting down the Mini HDFS Cluster
2020-12-03 07:22:06,303 [Listener at localhost/33017] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 5
2020-12-03 07:22:06,303 [Listener at localhost/33017] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:22:06,303 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@1a9b42c] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:22:06,304 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7, DS-65c144b2-b711-4cc2-94b7-e2af5974caa9) exiting.
2020-12-03 07:22:06,304 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8, DS-e617a7b2-64f7-42ba-a4d8-2991b5de0697) exiting.
2020-12-03 07:22:06,329 [Listener at localhost/33017] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@70106f89{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:22:06,336 [Listener at localhost/33017] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@474c4c83{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:22:06,336 [Listener at localhost/33017] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@1df68869{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:22:06,337 [Listener at localhost/33017] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@6639127b{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:22:06,338 [Listener at localhost/33017] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 33017
2020-12-03 07:22:06,339 [IPC Server listener on 33017] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 33017
2020-12-03 07:22:06,341 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:22:06,344 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:22:06,344 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1681972789-172.17.0.7-1606980105077 (Datanode Uuid a36cabce-9acb-4413-8be0-398b0630ef41) service to localhost/127.0.0.1:40508
2020-12-03 07:22:06,344 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1681972789-172.17.0.7-1606980105077 (Datanode Uuid a36cabce-9acb-4413-8be0-398b0630ef41)
2020-12-03 07:22:06,344 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:22:06,345 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7/current/BP-1681972789-172.17.0.7-1606980105077] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:06,347 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/current/BP-1681972789-172.17.0.7-1606980105077] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:06,353 [Listener at localhost/33017] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:22:06,353 [Listener at localhost/33017] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:22:06,354 [Listener at localhost/33017] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:22:06,354 [Listener at localhost/33017] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:22:06,355 [Listener at localhost/33017] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:22:06,356 [Listener at localhost/33017] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 4
2020-12-03 07:22:06,356 [Listener at localhost/33017] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:22:06,356 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@53932917] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:22:06,357 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, DS-03c4e4df-d4c4-413d-a247-bf506f40e039) exiting.
2020-12-03 07:22:06,357 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5, DS-167fc370-7419-4f5b-a77d-5974db53e53b) exiting.
2020-12-03 07:22:06,372 [Listener at localhost/33017] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@75e1782{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:22:06,373 [Listener at localhost/33017] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@4f76f3ed{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:22:06,373 [Listener at localhost/33017] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@5bce3f3f{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:22:06,373 [Listener at localhost/33017] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@4f06bb9b{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:22:06,379 [Listener at localhost/33017] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 40280
2020-12-03 07:22:06,384 [IPC Server listener on 40280] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 40280
2020-12-03 07:22:06,386 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:22:06,390 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:22:06,391 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1681972789-172.17.0.7-1606980105077 (Datanode Uuid 6af00937-a528-462e-97d0-7a889bd94f87) service to localhost/127.0.0.1:40508
2020-12-03 07:22:06,392 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1681972789-172.17.0.7-1606980105077 (Datanode Uuid 6af00937-a528-462e-97d0-7a889bd94f87)
2020-12-03 07:22:06,392 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:22:06,393 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1681972789-172.17.0.7-1606980105077] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:06,393 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-1681972789-172.17.0.7-1606980105077] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:06,396 [Listener at localhost/33017] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:22:06,397 [Listener at localhost/33017] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:22:06,397 [Listener at localhost/33017] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:22:06,398 [Listener at localhost/33017] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:22:06,399 [Listener at localhost/33017] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:22:06,400 [Listener at localhost/33017] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 3
2020-12-03 07:22:06,400 [Listener at localhost/33017] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:22:06,400 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@12773a9c] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:22:06,401 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-16ad47b7-6740-4c6f-9eb7-62aad3546c7d) exiting.
2020-12-03 07:22:06,401 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-a70eae85-caf8-4810-9307-7fdcff1de75a) exiting.
2020-12-03 07:22:06,423 [Listener at localhost/33017] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@7c965c4f{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:22:06,424 [Listener at localhost/33017] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@1d3b96e5{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:22:06,424 [Listener at localhost/33017] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@1540b2c9{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:22:06,425 [Listener at localhost/33017] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@68f7a047{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:22:06,426 [Listener at localhost/33017] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 41623
2020-12-03 07:22:06,428 [IPC Server listener on 41623] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 41623
2020-12-03 07:22:06,431 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:22:06,431 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:22:06,431 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1681972789-172.17.0.7-1606980105077 (Datanode Uuid f693c68b-acb5-4ad0-8980-23d838f7fba7) service to localhost/127.0.0.1:40508
2020-12-03 07:22:06,431 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1681972789-172.17.0.7-1606980105077 (Datanode Uuid f693c68b-acb5-4ad0-8980-23d838f7fba7)
2020-12-03 07:22:06,431 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:22:06,432 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1681972789-172.17.0.7-1606980105077] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:06,432 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1681972789-172.17.0.7-1606980105077] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:06,441 [Listener at localhost/33017] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:22:06,445 [Listener at localhost/33017] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:22:06,448 [Listener at localhost/33017] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:22:06,448 [Listener at localhost/33017] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:22:06,453 [Listener at localhost/33017] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:22:06,453 [Listener at localhost/33017] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 2
2020-12-03 07:22:06,454 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@717c8568] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:22:06,454 [Listener at localhost/33017] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:22:06,457 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16, DS-650414f7-31fe-484d-8973-85ea58ac630c) exiting.
2020-12-03 07:22:06,457 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17, DS-fcefd356-7d44-40b3-85cf-64c3f285c82c) exiting.
2020-12-03 07:22:06,458 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18, DS-ffc14c33-e3bb-433d-a880-69e3de925b44) exiting.
2020-12-03 07:22:06,511 [Listener at localhost/33017] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@354a33d{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:22:06,515 [Listener at localhost/33017] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@7633055f{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:22:06,516 [Listener at localhost/33017] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@58323171{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:22:06,517 [Listener at localhost/33017] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@333a174{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:22:06,518 [Listener at localhost/33017] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 41296
2020-12-03 07:22:06,520 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:22:06,528 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:22:06,529 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:22:06,529 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1681972789-172.17.0.7-1606980105077 (Datanode Uuid 6568366e-2097-441e-bd45-29423857a859) service to localhost/127.0.0.1:40508
2020-12-03 07:22:06,529 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1681972789-172.17.0.7-1606980105077 (Datanode Uuid 6568366e-2097-441e-bd45-29423857a859)
2020-12-03 07:22:06,529 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:22:06,530 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16/current/BP-1681972789-172.17.0.7-1606980105077] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:06,530 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17/current/BP-1681972789-172.17.0.7-1606980105077] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:06,539 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18/current/BP-1681972789-172.17.0.7-1606980105077] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:06,545 [Listener at localhost/33017] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:22:06,545 [Listener at localhost/33017] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:22:06,547 [Listener at localhost/33017] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:22:06,547 [Listener at localhost/33017] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:22:06,556 [Listener at localhost/33017] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:22:06,556 [Listener at localhost/33017] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 1
2020-12-03 07:22:06,557 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@2b68b3af] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:22:06,557 [Listener at localhost/33017] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:22:06,560 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14, DS-d2b02d71-811b-4a1b-beec-e7557cf65bae) exiting.
2020-12-03 07:22:06,560 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13, DS-fb09e1ea-5c92-4d14-a724-2616a3983122) exiting.
2020-12-03 07:22:06,585 [Listener at localhost/33017] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@4f9283ac{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:22:06,586 [Listener at localhost/33017] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@245ed753{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:22:06,586 [Listener at localhost/33017] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@7749d006{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:22:06,587 [Listener at localhost/33017] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@40570e0a{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:22:06,589 [Listener at localhost/33017] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 35345
2020-12-03 07:22:06,636 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:22:06,636 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:22:06,640 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:22:06,640 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1681972789-172.17.0.7-1606980105077 (Datanode Uuid 67671ade-3465-42ec-9f5f-9ccb0a3921b0) service to localhost/127.0.0.1:40508
2020-12-03 07:22:06,640 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1681972789-172.17.0.7-1606980105077 (Datanode Uuid 67671ade-3465-42ec-9f5f-9ccb0a3921b0)
2020-12-03 07:22:06,640 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:22:06,641 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13/current/BP-1681972789-172.17.0.7-1606980105077] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:06,641 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14/current/BP-1681972789-172.17.0.7-1606980105077] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:06,680 [Listener at localhost/33017] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:22:06,680 [Listener at localhost/33017] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:22:06,682 [Listener at localhost/33017] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:22:06,682 [Listener at localhost/33017] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:22:06,685 [Listener at localhost/33017] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:22:06,685 [Listener at localhost/33017] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 0
2020-12-03 07:22:06,686 [Listener at localhost/33017] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:22:06,686 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@6698a68a] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:22:06,689 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10, DS-a4774b34-f21c-4814-aefc-8f3ebb297608) exiting.
2020-12-03 07:22:06,690 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11, DS-bd99d0d3-dfed-4a67-b8cf-af2132dc7382) exiting.
2020-12-03 07:22:06,730 [Listener at localhost/33017] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@70b5d021{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:22:06,731 [Listener at localhost/33017] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@239db63d{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:22:06,731 [Listener at localhost/33017] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@644d2cae{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:22:06,732 [Listener at localhost/33017] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@1ca63fcf{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:22:06,734 [Listener at localhost/33017] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 43008
2020-12-03 07:22:06,745 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:22:06,747 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:22:06,747 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:22:06,758 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1681972789-172.17.0.7-1606980105077 (Datanode Uuid ac27962c-fa6a-4f23-86c8-18a7f92f9f56) service to localhost/127.0.0.1:40508
2020-12-03 07:22:06,863 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1681972789-172.17.0.7-1606980105077 (Datanode Uuid ac27962c-fa6a-4f23-86c8-18a7f92f9f56)
2020-12-03 07:22:06,863 [BP-1681972789-172.17.0.7-1606980105077 heartbeating to localhost/127.0.0.1:40508] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1681972789-172.17.0.7-1606980105077
2020-12-03 07:22:06,863 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10/current/BP-1681972789-172.17.0.7-1606980105077] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:06,864 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11/current/BP-1681972789-172.17.0.7-1606980105077] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:06,900 [Listener at localhost/33017] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:22:06,901 [Listener at localhost/33017] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:22:06,905 [Listener at localhost/33017] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:22:06,905 [Listener at localhost/33017] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:22:06,914 [Listener at localhost/33017] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:22:06,914 [Listener at localhost/33017] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:stopAndJoinNameNode(2130)) - Shutting down the namenode
2020-12-03 07:22:06,915 [Listener at localhost/33017] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1334)) - Stopping services started for active state
2020-12-03 07:22:06,915 [Listener at localhost/33017] INFO  namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1410)) - Ending log segment 1, 19
2020-12-03 07:22:06,916 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@c4648c5] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4198)) - LazyPersistFileScrubber was interrupted, exiting
2020-12-03 07:22:06,917 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@2d3821f2] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4107)) - NameNodeEditLogRoller was interrupted, exiting
2020-12-03 07:22:06,924 [Listener at localhost/33017] INFO  namenode.FSEditLog (FSEditLog.java:printStatistics(778)) - Number of transactions: 20 Total time for transactions(ms): 21 Number of transactions batched in Syncs: 3 Number of syncs: 18 SyncTimes(ms): 4 1 
2020-12-03 07:22:06,926 [Listener at localhost/33017] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(145)) - Finalizing edits file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_inprogress_0000000000000000001 -> /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000001-0000000000000000020
2020-12-03 07:22:06,927 [Listener at localhost/33017] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(145)) - Finalizing edits file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_inprogress_0000000000000000001 -> /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000001-0000000000000000020
2020-12-03 07:22:06,928 [FSEditLogAsync] INFO  namenode.FSEditLog (FSEditLogAsync.java:run(253)) - FSEditLogAsync was interrupted, exiting
2020-12-03 07:22:06,928 [CacheReplicationMonitor(1884595011)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2020-12-03 07:22:06,968 [Listener at localhost/33017] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 40508
2020-12-03 07:22:06,980 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:22:06,980 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:22:07,005 [StorageInfoMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4722)) - Stopping thread.
2020-12-03 07:22:07,005 [RedundancyMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4687)) - Stopping RedundancyMonitor.
2020-12-03 07:22:07,076 [Listener at localhost/33017] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1334)) - Stopping services started for active state
2020-12-03 07:22:07,076 [Listener at localhost/33017] INFO  namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1434)) - Stopping services started for standby state
2020-12-03 07:22:07,078 [Listener at localhost/33017] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@40e12390{/,null,UNAVAILABLE}{/hdfs}
2020-12-03 07:22:07,080 [Listener at localhost/33017] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@23a07e07{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:22:07,080 [Listener at localhost/33017] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@53aea7af{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:22:07,080 [Listener at localhost/33017] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@6ec984c3{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:22:07,082 [Listener at localhost/33017] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2020-12-03 07:22:07,104 [Listener at localhost/33017] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2020-12-03 07:22:07,105 [Listener at localhost/33017] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - DataNode metrics system shutdown complete.
msx-rc 0
