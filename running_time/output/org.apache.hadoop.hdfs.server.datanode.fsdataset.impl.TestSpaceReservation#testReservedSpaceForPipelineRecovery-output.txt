2020-12-03 07:22:04,356 [Thread-1] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(493)) - starting cluster: numNameNodes=1, numDataNodes=3
Formatting using clusterid: testClusterID
2020-12-03 07:22:05,295 [Thread-1] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(229)) - Edit logging is async:true
2020-12-03 07:22:05,317 [Thread-1] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(755)) - KeyProvider: null
2020-12-03 07:22:05,318 [Thread-1] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(123)) - fsLock is fair: true
2020-12-03 07:22:05,319 [Thread-1] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(141)) - Detailed lock hold time metrics enabled: false
2020-12-03 07:22:05,332 [Thread-1] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(780)) - fsOwner             = root (auth:SIMPLE)
2020-12-03 07:22:05,332 [Thread-1] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(781)) - supergroup          = supergroup
2020-12-03 07:22:05,333 [Thread-1] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(782)) - isPermissionEnabled = true
2020-12-03 07:22:05,334 [Thread-1] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(793)) - HA Enabled: false
2020-12-03 07:22:05,406 [Thread-1] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:22:05,412 [Thread-1] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - hadoop.configured.node.mapping is deprecated. Instead, use net.topology.configured.node.mapping
2020-12-03 07:22:05,413 [Thread-1] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(303)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-12-03 07:22:05,413 [Thread-1] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(311)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-12-03 07:22:05,420 [Thread-1] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-12-03 07:22:05,421 [Thread-1] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Dec 03 07:22:05
2020-12-03 07:22:05,424 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-12-03 07:22:05,426 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:22:05,429 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.9 GB = 39.3 MB
2020-12-03 07:22:05,429 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-12-03 07:22:05,455 [Thread-1] INFO  blockmanagement.BlockManager (BlockManager.java:createSPSManager(5183)) - Storage policy satisfier is disabled
2020-12-03 07:22:05,456 [Thread-1] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(595)) - dfs.block.access.token.enable = false
2020-12-03 07:22:05,467 [Thread-1] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-12-03 07:22:05,468 [Thread-1] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-12-03 07:22:05,468 [Thread-1] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-12-03 07:22:05,469 [Thread-1] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-12-03 07:22:05,470 [Thread-1] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(581)) - defaultReplication         = 3
2020-12-03 07:22:05,470 [Thread-1] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(582)) - maxReplication             = 512
2020-12-03 07:22:05,470 [Thread-1] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(583)) - minReplication             = 1
2020-12-03 07:22:05,471 [Thread-1] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(584)) - maxReplicationStreams      = 2
2020-12-03 07:22:05,471 [Thread-1] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(585)) - redundancyRecheckInterval  = 3000ms
2020-12-03 07:22:05,471 [Thread-1] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(586)) - encryptDataTransfer        = false
2020-12-03 07:22:05,472 [Thread-1] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(587)) - maxNumBlocksToLog          = 1000
2020-12-03 07:22:05,529 [Thread-1] INFO  namenode.FSDirectory (SerialNumberManager.java:<clinit>(51)) - GLOBAL serial map: bits=29 maxEntries=536870911
2020-12-03 07:22:05,530 [Thread-1] INFO  namenode.FSDirectory (SerialNumberManager.java:<clinit>(51)) - USER serial map: bits=24 maxEntries=16777215
2020-12-03 07:22:05,530 [Thread-1] INFO  namenode.FSDirectory (SerialNumberManager.java:<clinit>(51)) - GROUP serial map: bits=24 maxEntries=16777215
2020-12-03 07:22:05,530 [Thread-1] INFO  namenode.FSDirectory (SerialNumberManager.java:<clinit>(51)) - XATTR serial map: bits=24 maxEntries=16777215
2020-12-03 07:22:05,557 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-12-03 07:22:05,558 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:22:05,558 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.9 GB = 19.6 MB
2020-12-03 07:22:05,558 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-12-03 07:22:05,566 [Thread-1] INFO  namenode.FSDirectory (FSDirectory.java:<init>(290)) - ACLs enabled? false
2020-12-03 07:22:05,567 [Thread-1] INFO  namenode.FSDirectory (FSDirectory.java:<init>(294)) - POSIX ACL inheritance enabled? true
2020-12-03 07:22:05,567 [Thread-1] INFO  namenode.FSDirectory (FSDirectory.java:<init>(298)) - XAttrs enabled? true
2020-12-03 07:22:05,568 [Thread-1] INFO  namenode.NameNode (FSDirectory.java:<init>(362)) - Caching file names occurring more than 10 times
2020-12-03 07:22:05,577 [Thread-1] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-12-03 07:22:05,580 [Thread-1] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-12-03 07:22:05,587 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-12-03 07:22:05,587 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:22:05,588 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.9 GB = 4.9 MB
2020-12-03 07:22:05,588 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-12-03 07:22:05,604 [Thread-1] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-12-03 07:22:05,605 [Thread-1] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-12-03 07:22:05,605 [Thread-1] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-12-03 07:22:05,614 [Thread-1] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(996)) - Retry cache on namenode is enabled
2020-12-03 07:22:05,614 [Thread-1] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(1004)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-12-03 07:22:05,618 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-12-03 07:22:05,618 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:22:05,619 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.9 GB = 603.0 KB
2020-12-03 07:22:05,619 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-12-03 07:22:05,665 [Thread-1] INFO  namenode.FSImage (FSImage.java:format(185)) - Allocated new BlockPoolId: BP-1723813095-172.17.0.10-1606980125651
2020-12-03 07:22:05,794 [Thread-1] INFO  common.Storage (NNStorage.java:format(595)) - Storage directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1 has been successfully formatted.
2020-12-03 07:22:05,862 [Thread-1] INFO  common.Storage (NNStorage.java:format(595)) - Storage directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2 has been successfully formatted.
2020-12-03 07:22:05,921 [FSImageSaver for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(512)) - Saving image file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/fsimage.ckpt_0000000000000000000 using no compression
2020-12-03 07:22:05,921 [FSImageSaver for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(512)) - Saving image file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage.ckpt_0000000000000000000 using no compression
2020-12-03 07:22:06,069 [FSImageSaver for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(516)) - Image file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage.ckpt_0000000000000000000 of size 396 bytes saved in 0 seconds .
2020-12-03 07:22:06,069 [FSImageSaver for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(516)) - Image file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/fsimage.ckpt_0000000000000000000 of size 396 bytes saved in 0 seconds .
2020-12-03 07:22:06,172 [Thread-1] INFO  namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:getImageTxIdToRetain(203)) - Going to retain 1 images with txid >= 0
2020-12-03 07:22:06,177 [Thread-1] INFO  namenode.NameNode (NameNode.java:createNameNode(1632)) - createNameNode []
2020-12-03 07:22:06,640 [Thread-1] INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(118)) - Loaded properties from hadoop-metrics2.properties
2020-12-03 07:22:06,791 [Thread-1] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 0 second(s).
2020-12-03 07:22:06,792 [Thread-1] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2020-12-03 07:22:06,836 [Thread-1] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://127.0.0.1:0
2020-12-03 07:22:06,898 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@2badf17] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:22:06,924 [Thread-1] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1641)) - Starting Web-server for hdfs at: http://localhost:0
2020-12-03 07:22:06,932 [Thread-1] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:22:06,965 [Thread-1] INFO  util.log (Log.java:initialized(192)) - Logging initialized @4339ms
2020-12-03 07:22:07,128 [Thread-1] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:22:07,132 [Thread-1] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-12-03 07:22:07,132 [Thread-1] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:22:07,144 [Thread-1] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:22:07,148 [Thread-1] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-12-03 07:22:07,148 [Thread-1] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:22:07,148 [Thread-1] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:22:07,184 [Thread-1] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(102)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-12-03 07:22:07,185 [Thread-1] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(806)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-12-03 07:22:07,199 [Thread-1] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 37993
2020-12-03 07:22:07,202 [Thread-1] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:22:07,271 [Thread-1] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@480e98{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:22:07,273 [Thread-1] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@431c1549{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:22:07,339 [Thread-1] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@157444bc{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-12-03 07:22:07,352 [Thread-1] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@7689437{HTTP/1.1,[http/1.1]}{localhost:37993}
2020-12-03 07:22:07,353 [Thread-1] INFO  server.Server (Server.java:doStart(419)) - Started @4727ms
2020-12-03 07:22:07,375 [Thread-1] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(229)) - Edit logging is async:true
2020-12-03 07:22:07,376 [Thread-1] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(755)) - KeyProvider: null
2020-12-03 07:22:07,376 [Thread-1] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(123)) - fsLock is fair: true
2020-12-03 07:22:07,377 [Thread-1] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(141)) - Detailed lock hold time metrics enabled: false
2020-12-03 07:22:07,377 [Thread-1] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(780)) - fsOwner             = root (auth:SIMPLE)
2020-12-03 07:22:07,377 [Thread-1] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(781)) - supergroup          = supergroup
2020-12-03 07:22:07,377 [Thread-1] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(782)) - isPermissionEnabled = true
2020-12-03 07:22:07,378 [Thread-1] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(793)) - HA Enabled: false
2020-12-03 07:22:07,379 [Thread-1] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:22:07,380 [Thread-1] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(303)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-12-03 07:22:07,380 [Thread-1] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(311)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-12-03 07:22:07,380 [Thread-1] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-12-03 07:22:07,381 [Thread-1] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Dec 03 07:22:07
2020-12-03 07:22:07,381 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-12-03 07:22:07,382 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:22:07,382 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.8 GB = 36.4 MB
2020-12-03 07:22:07,382 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-12-03 07:22:07,401 [Thread-1] INFO  blockmanagement.BlockManager (BlockManager.java:createSPSManager(5183)) - Storage policy satisfier is disabled
2020-12-03 07:22:07,402 [Thread-1] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(595)) - dfs.block.access.token.enable = false
2020-12-03 07:22:07,402 [Thread-1] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-12-03 07:22:07,402 [Thread-1] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-12-03 07:22:07,403 [Thread-1] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-12-03 07:22:07,403 [Thread-1] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-12-03 07:22:07,404 [Thread-1] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(581)) - defaultReplication         = 3
2020-12-03 07:22:07,404 [Thread-1] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(582)) - maxReplication             = 512
2020-12-03 07:22:07,404 [Thread-1] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(583)) - minReplication             = 1
2020-12-03 07:22:07,404 [Thread-1] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(584)) - maxReplicationStreams      = 2
2020-12-03 07:22:07,405 [Thread-1] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(585)) - redundancyRecheckInterval  = 3000ms
2020-12-03 07:22:07,405 [Thread-1] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(586)) - encryptDataTransfer        = false
2020-12-03 07:22:07,405 [Thread-1] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(587)) - maxNumBlocksToLog          = 1000
2020-12-03 07:22:07,406 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-12-03 07:22:07,406 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:22:07,411 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.8 GB = 18.2 MB
2020-12-03 07:22:07,411 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-12-03 07:22:07,416 [Thread-1] INFO  namenode.FSDirectory (FSDirectory.java:<init>(290)) - ACLs enabled? false
2020-12-03 07:22:07,416 [Thread-1] INFO  namenode.FSDirectory (FSDirectory.java:<init>(294)) - POSIX ACL inheritance enabled? true
2020-12-03 07:22:07,421 [Thread-1] INFO  namenode.FSDirectory (FSDirectory.java:<init>(298)) - XAttrs enabled? true
2020-12-03 07:22:07,422 [Thread-1] INFO  namenode.NameNode (FSDirectory.java:<init>(362)) - Caching file names occurring more than 10 times
2020-12-03 07:22:07,422 [Thread-1] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-12-03 07:22:07,422 [Thread-1] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-12-03 07:22:07,423 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-12-03 07:22:07,423 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:22:07,423 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.8 GB = 4.6 MB
2020-12-03 07:22:07,424 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-12-03 07:22:07,428 [Thread-1] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-12-03 07:22:07,428 [Thread-1] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-12-03 07:22:07,428 [Thread-1] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-12-03 07:22:07,429 [Thread-1] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(996)) - Retry cache on namenode is enabled
2020-12-03 07:22:07,429 [Thread-1] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(1004)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-12-03 07:22:07,429 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-12-03 07:22:07,429 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:22:07,430 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.8 GB = 559.3 KB
2020-12-03 07:22:07,430 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-12-03 07:22:07,502 [Thread-1] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/in_use.lock acquired by nodename 2804@19b93cb8e93f
2020-12-03 07:22:07,602 [Thread-1] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/in_use.lock acquired by nodename 2804@19b93cb8e93f
2020-12-03 07:22:07,607 [Thread-1] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(428)) - Recovering unfinalized segments in /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current
2020-12-03 07:22:07,607 [Thread-1] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(428)) - Recovering unfinalized segments in /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current
2020-12-03 07:22:07,608 [Thread-1] INFO  namenode.FSImage (FSImage.java:loadFSImage(733)) - No edit log streams selected.
2020-12-03 07:22:07,609 [Thread-1] INFO  namenode.FSImage (FSImage.java:loadFSImageFile(797)) - Planning to load image: FSImageFile(file=/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2020-12-03 07:22:07,651 [Thread-1] INFO  namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(234)) - Loading 1 INodes.
2020-12-03 07:22:07,657 [Thread-1] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(246)) - Loaded FSImage in 0 seconds.
2020-12-03 07:22:07,658 [Thread-1] INFO  namenode.FSImage (FSImage.java:loadFSImage(978)) - Loaded image for txid 0 from /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000000
2020-12-03 07:22:07,663 [Thread-1] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1110)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-12-03 07:22:07,663 [Thread-1] INFO  namenode.FSEditLog (FSEditLog.java:startLogSegment(1365)) - Starting log segment at 1
2020-12-03 07:22:07,782 [Thread-1] INFO  namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2020-12-03 07:22:07,782 [Thread-1] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(727)) - Finished loading FSImage in 349 msecs
2020-12-03 07:22:07,993 [Thread-1] INFO  namenode.NameNode (NameNodeRpcServer.java:<init>(448)) - RPC server is binding to localhost:0
2020-12-03 07:22:08,037 [Thread-1] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:22:08,051 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:22:08,422 [Listener at localhost/36555] INFO  namenode.NameNode (NameNode.java:initialize(722)) - Clients are to use localhost:36555 to access this namenode/service.
2020-12-03 07:22:08,427 [Listener at localhost/36555] INFO  namenode.FSNamesystem (FSNamesystem.java:registerMBean(5090)) - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2020-12-03 07:22:08,457 [Listener at localhost/36555] INFO  namenode.LeaseManager (LeaseManager.java:getNumUnderConstructionBlocks(171)) - Number of blocks under construction: 0
2020-12-03 07:22:08,473 [Listener at localhost/36555] INFO  blockmanagement.BlockManager (BlockManager.java:initializeReplQueues(4922)) - initializing replication queues
2020-12-03 07:22:08,474 [Listener at localhost/36555] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(400)) - STATE* Leaving safe mode after 0 secs
2020-12-03 07:22:08,474 [Listener at localhost/36555] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(406)) - STATE* Network topology has 0 racks and 0 datanodes
2020-12-03 07:22:08,475 [Listener at localhost/36555] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(408)) - STATE* UnderReplicatedBlocks has 0 blocks
2020-12-03 07:22:08,478 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3585)) - Total number of blocks            = 0
2020-12-03 07:22:08,478 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3586)) - Number of invalid blocks          = 0
2020-12-03 07:22:08,478 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3587)) - Number of under-replicated blocks = 0
2020-12-03 07:22:08,478 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3588)) - Number of  over-replicated blocks = 0
2020-12-03 07:22:08,479 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3590)) - Number of blocks being written    = 0
2020-12-03 07:22:08,479 [Reconstruction Queue Initializer] INFO  hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(3593)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 5 msec
2020-12-03 07:22:08,519 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:22:08,527 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:22:08,536 [Listener at localhost/36555] INFO  namenode.NameNode (NameNode.java:startCommonServices(828)) - NameNode RPC up at: localhost/127.0.0.1:36555
2020-12-03 07:22:08,541 [Listener at localhost/36555] INFO  namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1222)) - Starting services required for active state
2020-12-03 07:22:08,541 [Listener at localhost/36555] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(777)) - Initializing quota with 4 thread(s)
2020-12-03 07:22:08,551 [Listener at localhost/36555] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(786)) - Quota initialization completed in 9 milliseconds
name space=1
storage space=0
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2020-12-03 07:22:08,565 [Listener at localhost/36555] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:22:08,571 [CacheReplicationMonitor(1675367542)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-12-03 07:22:08,679 [Listener at localhost/36555] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:22:08,724 [Listener at localhost/36555] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:22:08,733 [Listener at localhost/36555] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:22:08,738 [Listener at localhost/36555] INFO  datanode.BlockScanner (BlockScanner.java:<init>(187)) - Disabled block scanner.
2020-12-03 07:22:08,743 [Listener at localhost/36555] DEBUG datanode.DataNode (DataNode.java:<init>(493)) - File descriptor passing was not configured.
2020-12-03 07:22:08,744 [Listener at localhost/36555] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:22:08,746 [Listener at localhost/36555] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:22:08,753 [Listener at localhost/36555] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:22:08,764 [Listener at localhost/36555] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:36884
2020-12-03 07:22:08,767 [Listener at localhost/36555] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:22:08,768 [Listener at localhost/36555] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:22:08,796 [Listener at localhost/36555] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:22:08,799 [Listener at localhost/36555] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:22:08,800 [Listener at localhost/36555] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:22:08,801 [Listener at localhost/36555] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:22:08,807 [Listener at localhost/36555] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:22:08,809 [Listener at localhost/36555] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:22:08,809 [Listener at localhost/36555] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:22:08,809 [Listener at localhost/36555] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:22:08,814 [Listener at localhost/36555] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 38715
2020-12-03 07:22:08,815 [Listener at localhost/36555] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:22:08,817 [Listener at localhost/36555] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@5137d29e{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:22:08,818 [Listener at localhost/36555] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@a220320{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:22:08,828 [Listener at localhost/36555] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@7d63cb72{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:22:08,829 [Listener at localhost/36555] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@31a4bd1d{HTTP/1.1,[http/1.1]}{localhost:38715}
2020-12-03 07:22:08,830 [Listener at localhost/36555] INFO  server.Server (Server.java:doStart(419)) - Started @6204ms
2020-12-03 07:22:09,833 [Listener at localhost/36555] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:43176
2020-12-03 07:22:09,839 [Listener at localhost/36555] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:22:09,839 [Listener at localhost/36555] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:22:09,842 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@32a4f1d5] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:22:09,863 [Listener at localhost/36555] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:22:09,869 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:22:09,889 [Listener at localhost/35379] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:35379
2020-12-03 07:22:09,908 [Listener at localhost/35379] DEBUG datanode.DataNode (ErasureCodingWorker.java:initializeStripedReadThreadPool(76)) - Using striped reads
2020-12-03 07:22:09,909 [Listener at localhost/35379] DEBUG datanode.DataNode (ErasureCodingWorker.java:initializeStripedBlkReconstructionThreadPool(107)) - Using striped block reconstruction; pool threads=8
2020-12-03 07:22:09,912 [Listener at localhost/35379] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:22:09,914 [Listener at localhost/35379] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:22:09,928 [Thread-60] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:36555 starting to offer service
2020-12-03 07:22:09,936 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:22:09,936 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:22:09,961 [Listener at localhost/35379] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 1 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:22:09,969 [Listener at localhost/35379] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:22:09,990 [Listener at localhost/35379] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:22:09,991 [Listener at localhost/35379] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:22:09,992 [Listener at localhost/35379] INFO  datanode.BlockScanner (BlockScanner.java:<init>(187)) - Disabled block scanner.
2020-12-03 07:22:09,993 [Listener at localhost/35379] DEBUG datanode.DataNode (DataNode.java:<init>(493)) - File descriptor passing was not configured.
2020-12-03 07:22:09,993 [Listener at localhost/35379] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:22:09,997 [Listener at localhost/35379] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:22:09,998 [Listener at localhost/35379] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:22:09,999 [Listener at localhost/35379] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:34135
2020-12-03 07:22:09,999 [Listener at localhost/35379] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:22:10,000 [Listener at localhost/35379] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:22:10,002 [Listener at localhost/35379] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:22:10,006 [Listener at localhost/35379] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:22:10,007 [Listener at localhost/35379] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:22:10,007 [Listener at localhost/35379] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:22:10,011 [Listener at localhost/35379] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:22:10,013 [Listener at localhost/35379] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:22:10,013 [Listener at localhost/35379] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:22:10,014 [Listener at localhost/35379] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:22:10,015 [Listener at localhost/35379] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 42863
2020-12-03 07:22:10,016 [Listener at localhost/35379] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:22:10,019 [Listener at localhost/35379] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@b149f15{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:22:10,020 [Listener at localhost/35379] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@793ca543{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:22:10,047 [Listener at localhost/35379] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@4f239add{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:22:10,048 [Listener at localhost/35379] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@7b54a74{HTTP/1.1,[http/1.1]}{localhost:42863}
2020-12-03 07:22:10,049 [Listener at localhost/35379] INFO  server.Server (Server.java:doStart(419)) - Started @7423ms
2020-12-03 07:22:10,201 [Listener at localhost/35379] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:41079
2020-12-03 07:22:10,203 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@5eb1dc6d] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:22:10,203 [Listener at localhost/35379] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:22:10,204 [Listener at localhost/35379] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:22:10,205 [Listener at localhost/35379] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:22:10,208 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:22:10,244 [Listener at localhost/45866] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:45866
2020-12-03 07:22:10,264 [Listener at localhost/45866] DEBUG datanode.DataNode (ErasureCodingWorker.java:initializeStripedReadThreadPool(76)) - Using striped reads
2020-12-03 07:22:10,265 [Listener at localhost/45866] DEBUG datanode.DataNode (ErasureCodingWorker.java:initializeStripedBlkReconstructionThreadPool(107)) - Using striped block reconstruction; pool threads=8
2020-12-03 07:22:10,265 [Listener at localhost/45866] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:22:10,266 [Listener at localhost/45866] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:22:10,267 [Thread-84] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:36555 starting to offer service
2020-12-03 07:22:10,275 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:22:10,275 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:22:10,278 [Listener at localhost/45866] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 2 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
2020-12-03 07:22:10,319 [Listener at localhost/45866] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
2020-12-03 07:22:10,321 [Listener at localhost/45866] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:22:10,321 [Listener at localhost/45866] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:22:10,322 [Listener at localhost/45866] INFO  datanode.BlockScanner (BlockScanner.java:<init>(187)) - Disabled block scanner.
2020-12-03 07:22:10,322 [Listener at localhost/45866] DEBUG datanode.DataNode (DataNode.java:<init>(493)) - File descriptor passing was not configured.
2020-12-03 07:22:10,322 [Listener at localhost/45866] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:22:10,323 [Listener at localhost/45866] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:22:10,323 [Listener at localhost/45866] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:22:10,324 [Listener at localhost/45866] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:40715
2020-12-03 07:22:10,324 [Listener at localhost/45866] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:22:10,324 [Listener at localhost/45866] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:22:10,326 [Listener at localhost/45866] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:22:10,328 [Listener at localhost/45866] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:22:10,330 [Listener at localhost/45866] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:22:10,331 [Listener at localhost/45866] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:22:10,343 [Listener at localhost/45866] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:22:10,345 [Listener at localhost/45866] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:22:10,345 [Listener at localhost/45866] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:22:10,345 [Listener at localhost/45866] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:22:10,347 [Listener at localhost/45866] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 39746
2020-12-03 07:22:10,347 [Listener at localhost/45866] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:22:10,354 [Listener at localhost/45866] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@34177a0c{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:22:10,356 [Listener at localhost/45866] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@cb2313a{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:22:10,366 [Listener at localhost/45866] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@27e71c23{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:22:10,367 [Listener at localhost/45866] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@499c8e70{HTTP/1.1,[http/1.1]}{localhost:39746}
2020-12-03 07:22:10,368 [Listener at localhost/45866] INFO  server.Server (Server.java:doStart(419)) - Started @7742ms
2020-12-03 07:22:10,410 [Listener at localhost/45866] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:38815
2020-12-03 07:22:10,411 [Listener at localhost/45866] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:22:10,411 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@6d3cb237] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:22:10,411 [Listener at localhost/45866] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:22:10,412 [Listener at localhost/45866] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:22:10,413 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:22:10,421 [Listener at localhost/34293] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:34293
2020-12-03 07:22:10,430 [Listener at localhost/34293] DEBUG datanode.DataNode (ErasureCodingWorker.java:initializeStripedReadThreadPool(76)) - Using striped reads
2020-12-03 07:22:10,430 [Listener at localhost/34293] DEBUG datanode.DataNode (ErasureCodingWorker.java:initializeStripedBlkReconstructionThreadPool(107)) - Using striped block reconstruction; pool threads=8
2020-12-03 07:22:10,431 [Listener at localhost/34293] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:22:10,431 [Listener at localhost/34293] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:22:10,432 [Thread-106] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:36555 starting to offer service
2020-12-03 07:22:10,451 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:22:10,453 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:22:10,567 [Thread-84] DEBUG datanode.DataNode (BPServiceActor.java:retrieveNamespaceInfo(232)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:36555 received versionRequest response: lv=-65;cid=testClusterID;nsid=156303761;c=1606980125651;bpid=BP-1723813095-172.17.0.10-1606980125651
2020-12-03 07:22:10,567 [Thread-106] DEBUG datanode.DataNode (BPServiceActor.java:retrieveNamespaceInfo(232)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:36555 received versionRequest response: lv=-65;cid=testClusterID;nsid=156303761;c=1606980125651;bpid=BP-1723813095-172.17.0.10-1606980125651
2020-12-03 07:22:10,568 [Thread-60] DEBUG datanode.DataNode (BPServiceActor.java:retrieveNamespaceInfo(232)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:36555 received versionRequest response: lv=-65;cid=testClusterID;nsid=156303761;c=1606980125651;bpid=BP-1723813095-172.17.0.10-1606980125651
2020-12-03 07:22:10,576 [Thread-106] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:36555
2020-12-03 07:22:10,578 [Thread-60] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:36555
2020-12-03 07:22:10,580 [Thread-106] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2020-12-03 07:22:10,575 [Thread-84] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:36555
2020-12-03 07:22:10,582 [Thread-60] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2020-12-03 07:22:10,584 [Thread-84] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2020-12-03 07:22:10,641 [Thread-60] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/in_use.lock acquired by nodename 2804@19b93cb8e93f
2020-12-03 07:22:10,642 [Thread-60] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1 is not formatted for namespace 156303761. Formatting...
2020-12-03 07:22:10,644 [Thread-60] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-f9cddab7-0df0-41ff-8e2b-7e03980d036c for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1 
2020-12-03 07:22:10,668 [Thread-106] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/in_use.lock acquired by nodename 2804@19b93cb8e93f
2020-12-03 07:22:10,668 [Thread-106] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3 is not formatted for namespace 156303761. Formatting...
2020-12-03 07:22:10,669 [Thread-106] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-c1dccd31-33e7-4e8c-aaa0-047e42da1cc4 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3 
2020-12-03 07:22:10,669 [Thread-84] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/in_use.lock acquired by nodename 2804@19b93cb8e93f
2020-12-03 07:22:10,670 [Thread-84] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2 is not formatted for namespace 156303761. Formatting...
2020-12-03 07:22:10,670 [Thread-84] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-c8853e3c-4a70-4eb7-ab66-5a6255afa86b for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2 
2020-12-03 07:22:10,794 [Thread-60] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1723813095-172.17.0.10-1606980125651
2020-12-03 07:22:10,795 [Thread-60] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1723813095-172.17.0.10-1606980125651
2020-12-03 07:22:10,796 [Thread-60] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1 and block pool id BP-1723813095-172.17.0.10-1606980125651 is not formatted. Formatting ...
2020-12-03 07:22:10,796 [Thread-60] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1723813095-172.17.0.10-1606980125651 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1723813095-172.17.0.10-1606980125651/current
2020-12-03 07:22:10,806 [Thread-106] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1723813095-172.17.0.10-1606980125651
2020-12-03 07:22:10,806 [Thread-106] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1723813095-172.17.0.10-1606980125651
2020-12-03 07:22:10,807 [Thread-106] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3 and block pool id BP-1723813095-172.17.0.10-1606980125651 is not formatted. Formatting ...
2020-12-03 07:22:10,807 [Thread-106] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1723813095-172.17.0.10-1606980125651 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1723813095-172.17.0.10-1606980125651/current
2020-12-03 07:22:10,822 [Thread-84] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1723813095-172.17.0.10-1606980125651
2020-12-03 07:22:10,823 [Thread-84] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1723813095-172.17.0.10-1606980125651
2020-12-03 07:22:10,823 [Thread-84] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2 and block pool id BP-1723813095-172.17.0.10-1606980125651 is not formatted. Formatting ...
2020-12-03 07:22:10,823 [Thread-84] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1723813095-172.17.0.10-1606980125651 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1723813095-172.17.0.10-1606980125651/current
2020-12-03 07:22:10,960 [Thread-60] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=156303761;bpid=BP-1723813095-172.17.0.10-1606980125651;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=156303761;c=1606980125651;bpid=BP-1723813095-172.17.0.10-1606980125651;dnuuid=null
2020-12-03 07:22:11,011 [Thread-106] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=156303761;bpid=BP-1723813095-172.17.0.10-1606980125651;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=156303761;c=1606980125651;bpid=BP-1723813095-172.17.0.10-1606980125651;dnuuid=null
2020-12-03 07:22:11,011 [Thread-84] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=156303761;bpid=BP-1723813095-172.17.0.10-1606980125651;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=156303761;c=1606980125651;bpid=BP-1723813095-172.17.0.10-1606980125651;dnuuid=null
2020-12-03 07:22:11,053 [IPC Server handler 6 on default port 36555] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:22:11,063 [Listener at localhost/34293] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:22:11,064 [Listener at localhost/34293] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:22:11,072 [Thread-60] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID 04aa09e8-2060-44fc-bf98-148164c6bc7c
2020-12-03 07:22:11,116 [Thread-84] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID df5771e4-65f1-4ee5-86ea-ae3a8b274cb3
2020-12-03 07:22:11,117 [Thread-106] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID b518f08d-a98e-47d6-9057-a679367cc8bc
2020-12-03 07:22:11,170 [IPC Server handler 8 on default port 36555] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:22:11,174 [Listener at localhost/34293] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:22:11,175 [Listener at localhost/34293] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:22:11,212 [Thread-84] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: incr /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 1
2020-12-03 07:22:11,213 [Thread-84] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.reference(FsVolumeImpl.java:240)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$000(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.<init>(FsVolumeImpl.java:266)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.obtainReference(FsVolumeImpl.java:289)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.addVolume(FsDatasetImpl.java:445)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.<init>(FsDatasetImpl.java:334)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory.newInstance(FsDatasetFactory.java:34)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory.newInstance(FsDatasetFactory.java:30)
org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1757)
org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1679)
org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:390)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:282)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:824)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:11,217 [Thread-106] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: incr /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3: 1
2020-12-03 07:22:11,217 [Thread-106] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.reference(FsVolumeImpl.java:240)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$000(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.<init>(FsVolumeImpl.java:266)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.obtainReference(FsVolumeImpl.java:289)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.addVolume(FsDatasetImpl.java:445)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.<init>(FsDatasetImpl.java:334)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory.newInstance(FsDatasetFactory.java:34)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory.newInstance(FsDatasetFactory.java:30)
org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1757)
org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1679)
org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:390)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:282)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:824)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:11,218 [Thread-106] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: desc /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3: 1
2020-12-03 07:22:11,217 [Thread-84] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: desc /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 1
2020-12-03 07:22:11,220 [Thread-60] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: incr /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 1
2020-12-03 07:22:11,228 [Thread-60] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.reference(FsVolumeImpl.java:240)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$000(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.<init>(FsVolumeImpl.java:266)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.obtainReference(FsVolumeImpl.java:289)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.addVolume(FsDatasetImpl.java:445)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.<init>(FsDatasetImpl.java:334)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory.newInstance(FsDatasetFactory.java:34)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory.newInstance(FsDatasetFactory.java:30)
org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1757)
org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1679)
org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:390)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:282)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:824)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:11,229 [Thread-60] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: desc /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 1
2020-12-03 07:22:11,229 [Thread-60] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.unreference(FsVolumeImpl.java:249)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$100(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.close(FsVolumeImpl.java:276)
org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:258)
org.apache.hadoop.hdfs.server.datanode.BlockScanner.addVolumeScanner(BlockScanner.java:231)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.addVolume(FsVolumeList.java:295)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.activateVolume(FsDatasetImpl.java:428)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.addVolume(FsDatasetImpl.java:449)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.<init>(FsDatasetImpl.java:334)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory.newInstance(FsDatasetFactory.java:34)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory.newInstance(FsDatasetFactory.java:30)
org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1757)
org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1679)
org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:390)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:282)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:824)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:11,229 [Thread-60] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-f9cddab7-0df0-41ff-8e2b-7e03980d036c
2020-12-03 07:22:11,229 [Thread-60] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, StorageType: DISK
2020-12-03 07:22:11,218 [Thread-106] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.unreference(FsVolumeImpl.java:249)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$100(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.close(FsVolumeImpl.java:276)
org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:258)
org.apache.hadoop.hdfs.server.datanode.BlockScanner.addVolumeScanner(BlockScanner.java:231)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.addVolume(FsVolumeList.java:295)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.activateVolume(FsDatasetImpl.java:428)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.addVolume(FsDatasetImpl.java:449)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.<init>(FsDatasetImpl.java:334)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory.newInstance(FsDatasetFactory.java:34)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory.newInstance(FsDatasetFactory.java:30)
org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1757)
org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1679)
org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:390)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:282)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:824)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:11,234 [Thread-106] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-c1dccd31-33e7-4e8c-aaa0-047e42da1cc4
2020-12-03 07:22:11,228 [Thread-84] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.unreference(FsVolumeImpl.java:249)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$100(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.close(FsVolumeImpl.java:276)
org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:258)
org.apache.hadoop.hdfs.server.datanode.BlockScanner.addVolumeScanner(BlockScanner.java:231)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.addVolume(FsVolumeList.java:295)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.activateVolume(FsDatasetImpl.java:428)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.addVolume(FsDatasetImpl.java:449)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.<init>(FsDatasetImpl.java:334)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory.newInstance(FsDatasetFactory.java:34)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory.newInstance(FsDatasetFactory.java:30)
org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1757)
org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1679)
org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:390)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:282)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:824)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:11,235 [Thread-106] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3, StorageType: DISK
2020-12-03 07:22:11,235 [Thread-84] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-c8853e3c-4a70-4eb7-ab66-5a6255afa86b
2020-12-03 07:22:11,236 [Thread-84] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, StorageType: DISK
2020-12-03 07:22:11,237 [Thread-84] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:22:11,237 [Thread-60] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:22:11,237 [Thread-106] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:22:11,242 [Thread-84] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: incr /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 1
2020-12-03 07:22:11,251 [Thread-84] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.reference(FsVolumeImpl.java:240)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$000(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.<init>(FsVolumeImpl.java:266)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.obtainReference(FsVolumeImpl.java:289)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.getCapacity(FsVolumeList.java:166)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getCapacity(FsDatasetImpl.java:632)
org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeMetricHelper.getMetrics(DataNodeMetricHelper.java:52)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getMetrics(FsDatasetImpl.java:738)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMetrics(MetricsSourceAdapter.java:200)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.updateJmxCache(MetricsSourceAdapter.java:183)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMBeanInfo(MetricsSourceAdapter.java:156)
com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getNewMBeanClassName(DefaultMBeanServerInterceptor.java:333)
com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:319)
com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:98)
org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:72)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:222)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.start(MetricsSourceAdapter.java:101)
org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSource(MetricsSystemImpl.java:268)
org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:233)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.<init>(FsDatasetImpl.java:359)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory.newInstance(FsDatasetFactory.java:34)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory.newInstance(FsDatasetFactory.java:30)
org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1757)
org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1679)
org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:390)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:282)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:824)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:11,252 [Thread-84] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: desc /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 1
2020-12-03 07:22:11,252 [Thread-84] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.unreference(FsVolumeImpl.java:249)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$100(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.close(FsVolumeImpl.java:276)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.getCapacity(FsVolumeList.java:168)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getCapacity(FsDatasetImpl.java:632)
org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeMetricHelper.getMetrics(DataNodeMetricHelper.java:52)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getMetrics(FsDatasetImpl.java:738)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMetrics(MetricsSourceAdapter.java:200)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.updateJmxCache(MetricsSourceAdapter.java:183)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMBeanInfo(MetricsSourceAdapter.java:156)
com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getNewMBeanClassName(DefaultMBeanServerInterceptor.java:333)
com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:319)
com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:98)
org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:72)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:222)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.start(MetricsSourceAdapter.java:101)
org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSource(MetricsSystemImpl.java:268)
org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:233)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.<init>(FsDatasetImpl.java:359)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory.newInstance(FsDatasetFactory.java:34)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory.newInstance(FsDatasetFactory.java:30)
org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1757)
org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1679)
org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:390)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:282)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:824)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:11,253 [Thread-84] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: incr /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 1
2020-12-03 07:22:11,253 [Thread-84] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.reference(FsVolumeImpl.java:240)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$000(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.<init>(FsVolumeImpl.java:266)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.obtainReference(FsVolumeImpl.java:289)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.getRemaining(FsVolumeList.java:178)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getRemaining(FsDatasetImpl.java:640)
org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeMetricHelper.getMetrics(DataNodeMetricHelper.java:56)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getMetrics(FsDatasetImpl.java:738)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMetrics(MetricsSourceAdapter.java:200)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.updateJmxCache(MetricsSourceAdapter.java:183)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMBeanInfo(MetricsSourceAdapter.java:156)
com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getNewMBeanClassName(DefaultMBeanServerInterceptor.java:333)
com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:319)
com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:98)
org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:72)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:222)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.start(MetricsSourceAdapter.java:101)
org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSource(MetricsSystemImpl.java:268)
org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:233)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.<init>(FsDatasetImpl.java:359)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory.newInstance(FsDatasetFactory.java:34)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory.newInstance(FsDatasetFactory.java:30)
org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1757)
org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1679)
org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:390)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:282)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:824)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:11,254 [Thread-84] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: desc /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 1
2020-12-03 07:22:11,258 [Thread-84] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.unreference(FsVolumeImpl.java:249)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$100(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.close(FsVolumeImpl.java:276)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.getRemaining(FsVolumeList.java:180)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getRemaining(FsDatasetImpl.java:640)
org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeMetricHelper.getMetrics(DataNodeMetricHelper.java:56)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getMetrics(FsDatasetImpl.java:738)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMetrics(MetricsSourceAdapter.java:200)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.updateJmxCache(MetricsSourceAdapter.java:183)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMBeanInfo(MetricsSourceAdapter.java:156)
com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getNewMBeanClassName(DefaultMBeanServerInterceptor.java:333)
com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:319)
com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:98)
org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:72)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:222)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.start(MetricsSourceAdapter.java:101)
org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSource(MetricsSystemImpl.java:268)
org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:233)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.<init>(FsDatasetImpl.java:359)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory.newInstance(FsDatasetFactory.java:34)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory.newInstance(FsDatasetFactory.java:30)
org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1757)
org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1679)
org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:390)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:282)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:824)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:11,261 [Thread-84] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: incr /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 1
2020-12-03 07:22:11,261 [Thread-84] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.reference(FsVolumeImpl.java:240)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$000(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.<init>(FsVolumeImpl.java:266)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.obtainReference(FsVolumeImpl.java:289)
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi$FsVolumeReferences.<init>(FsDatasetSpi.java:110)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getFsVolumeReferences(FsDatasetImpl.java:145)
org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker.checkAllVolumes(DatasetVolumeChecker.java:203)
org.apache.hadoop.hdfs.server.datanode.DataNode.checkDiskError(DataNode.java:3378)
org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1683)
org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:390)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:282)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:824)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:11,262 [Thread-106] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: incr /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3: 1
2020-12-03 07:22:11,262 [Thread-84] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:22:11,277 [Thread-106] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.reference(FsVolumeImpl.java:240)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$000(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.<init>(FsVolumeImpl.java:266)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.obtainReference(FsVolumeImpl.java:289)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.getCapacity(FsVolumeList.java:166)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getCapacity(FsDatasetImpl.java:632)
org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeMetricHelper.getMetrics(DataNodeMetricHelper.java:52)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getMetrics(FsDatasetImpl.java:738)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMetrics(MetricsSourceAdapter.java:200)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.updateJmxCache(MetricsSourceAdapter.java:183)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMBeanInfo(MetricsSourceAdapter.java:156)
com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getNewMBeanClassName(DefaultMBeanServerInterceptor.java:333)
com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:319)
com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:98)
org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:72)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:222)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.start(MetricsSourceAdapter.java:101)
org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSource(MetricsSystemImpl.java:268)
org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:233)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.<init>(FsDatasetImpl.java:359)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory.newInstance(FsDatasetFactory.java:34)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory.newInstance(FsDatasetFactory.java:30)
org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1757)
org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1679)
org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:390)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:282)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:824)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:11,277 [IPC Server handler 9 on default port 36555] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:22:11,279 [Listener at localhost/34293] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:22:11,286 [Listener at localhost/34293] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:22:11,286 [Thread-106] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: desc /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3: 1
2020-12-03 07:22:11,287 [Thread-84] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:22:11,289 [Thread-84] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: desc /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 1
2020-12-03 07:22:11,290 [Thread-84] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.unreference(FsVolumeImpl.java:249)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$100(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.close(FsVolumeImpl.java:276)
org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:258)
org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$ResultHandler.cleanup(DatasetVolumeChecker.java:399)
org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$ResultHandler.onSuccess(DatasetVolumeChecker.java:373)
org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$ResultHandler.onSuccess(DatasetVolumeChecker.java:320)
com.google.common.util.concurrent.Futures$CallbackListener.run(Futures.java:1062)
com.google.common.util.concurrent.DirectExecutor.execute(DirectExecutor.java:30)
org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture.executeListener(AbstractFuture.java:992)
org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture.addListener(AbstractFuture.java:695)
org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture$TrustedFuture.addListener(AbstractFuture.java:108)
com.google.common.util.concurrent.Futures.addCallback(Futures.java:1037)
org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker.checkAllVolumes(DatasetVolumeChecker.java:225)
org.apache.hadoop.hdfs.server.datanode.DataNode.checkDiskError(DataNode.java:3378)
org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1683)
org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:390)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:282)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:824)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:11,290 [Thread-84] DEBUG datanode.DataNode (DataNode.java:checkDiskError(3390)) - checkDiskError encountered no failures
2020-12-03 07:22:11,290 [Thread-84] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1723813095-172.17.0.10-1606980125651
2020-12-03 07:22:11,287 [Thread-106] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.unreference(FsVolumeImpl.java:249)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$100(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.close(FsVolumeImpl.java:276)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.getCapacity(FsVolumeList.java:168)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getCapacity(FsDatasetImpl.java:632)
org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeMetricHelper.getMetrics(DataNodeMetricHelper.java:52)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getMetrics(FsDatasetImpl.java:738)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMetrics(MetricsSourceAdapter.java:200)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.updateJmxCache(MetricsSourceAdapter.java:183)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMBeanInfo(MetricsSourceAdapter.java:156)
com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getNewMBeanClassName(DefaultMBeanServerInterceptor.java:333)
com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:319)
com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:98)
org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:72)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:222)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.start(MetricsSourceAdapter.java:101)
org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSource(MetricsSystemImpl.java:268)
org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:233)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.<init>(FsDatasetImpl.java:359)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory.newInstance(FsDatasetFactory.java:34)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory.newInstance(FsDatasetFactory.java:30)
org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1757)
org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1679)
org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:390)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:282)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:824)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:11,291 [Thread-120] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: incr /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 1
2020-12-03 07:22:11,292 [Thread-120] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.reference(FsVolumeImpl.java:240)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$000(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.<init>(FsVolumeImpl.java:266)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.obtainReference(FsVolumeImpl.java:289)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$2.run(FsVolumeList.java:405)
2020-12-03 07:22:11,292 [Thread-106] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: incr /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3: 1
2020-12-03 07:22:11,293 [Thread-120] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1723813095-172.17.0.10-1606980125651 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2020-12-03 07:22:11,293 [Thread-106] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.reference(FsVolumeImpl.java:240)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$000(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.<init>(FsVolumeImpl.java:266)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.obtainReference(FsVolumeImpl.java:289)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.getRemaining(FsVolumeList.java:178)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getRemaining(FsDatasetImpl.java:640)
org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeMetricHelper.getMetrics(DataNodeMetricHelper.java:56)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getMetrics(FsDatasetImpl.java:738)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMetrics(MetricsSourceAdapter.java:200)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.updateJmxCache(MetricsSourceAdapter.java:183)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMBeanInfo(MetricsSourceAdapter.java:156)
com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getNewMBeanClassName(DefaultMBeanServerInterceptor.java:333)
com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:319)
com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:98)
org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:72)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:222)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.start(MetricsSourceAdapter.java:101)
org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSource(MetricsSystemImpl.java:268)
org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:233)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.<init>(FsDatasetImpl.java:359)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory.newInstance(FsDatasetFactory.java:34)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory.newInstance(FsDatasetFactory.java:30)
org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1757)
org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1679)
org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:390)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:282)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:824)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:11,295 [Thread-106] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: desc /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3: 1
2020-12-03 07:22:11,295 [Thread-106] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.unreference(FsVolumeImpl.java:249)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$100(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.close(FsVolumeImpl.java:276)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.getRemaining(FsVolumeList.java:180)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getRemaining(FsDatasetImpl.java:640)
org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeMetricHelper.getMetrics(DataNodeMetricHelper.java:56)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getMetrics(FsDatasetImpl.java:738)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMetrics(MetricsSourceAdapter.java:200)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.updateJmxCache(MetricsSourceAdapter.java:183)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMBeanInfo(MetricsSourceAdapter.java:156)
com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getNewMBeanClassName(DefaultMBeanServerInterceptor.java:333)
com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:319)
com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:98)
org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:72)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:222)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.start(MetricsSourceAdapter.java:101)
org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSource(MetricsSystemImpl.java:268)
org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:233)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.<init>(FsDatasetImpl.java:359)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory.newInstance(FsDatasetFactory.java:34)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory.newInstance(FsDatasetFactory.java:30)
org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1757)
org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1679)
org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:390)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:282)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:824)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:11,296 [Thread-106] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: incr /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3: 1
2020-12-03 07:22:11,296 [Thread-106] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.reference(FsVolumeImpl.java:240)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$000(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.<init>(FsVolumeImpl.java:266)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.obtainReference(FsVolumeImpl.java:289)
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi$FsVolumeReferences.<init>(FsDatasetSpi.java:110)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getFsVolumeReferences(FsDatasetImpl.java:145)
org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker.checkAllVolumes(DatasetVolumeChecker.java:203)
org.apache.hadoop.hdfs.server.datanode.DataNode.checkDiskError(DataNode.java:3378)
org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1683)
org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:390)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:282)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:824)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:11,297 [Thread-106] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
2020-12-03 07:22:11,297 [Thread-60] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: incr /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 1
2020-12-03 07:22:11,299 [Thread-60] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.reference(FsVolumeImpl.java:240)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$000(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.<init>(FsVolumeImpl.java:266)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.obtainReference(FsVolumeImpl.java:289)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.getCapacity(FsVolumeList.java:166)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getCapacity(FsDatasetImpl.java:632)
org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeMetricHelper.getMetrics(DataNodeMetricHelper.java:52)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getMetrics(FsDatasetImpl.java:738)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMetrics(MetricsSourceAdapter.java:200)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.updateJmxCache(MetricsSourceAdapter.java:183)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMBeanInfo(MetricsSourceAdapter.java:156)
com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getNewMBeanClassName(DefaultMBeanServerInterceptor.java:333)
com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:319)
com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:98)
org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:72)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:222)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.start(MetricsSourceAdapter.java:101)
org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSource(MetricsSystemImpl.java:268)
org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:233)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.<init>(FsDatasetImpl.java:359)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory.newInstance(FsDatasetFactory.java:34)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory.newInstance(FsDatasetFactory.java:30)
org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1757)
org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1679)
org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:390)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:282)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:824)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:11,299 [Thread-106] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
2020-12-03 07:22:11,300 [Thread-60] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: desc /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 1
2020-12-03 07:22:11,302 [Thread-60] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.unreference(FsVolumeImpl.java:249)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$100(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.close(FsVolumeImpl.java:276)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.getCapacity(FsVolumeList.java:168)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getCapacity(FsDatasetImpl.java:632)
org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeMetricHelper.getMetrics(DataNodeMetricHelper.java:52)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getMetrics(FsDatasetImpl.java:738)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMetrics(MetricsSourceAdapter.java:200)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.updateJmxCache(MetricsSourceAdapter.java:183)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMBeanInfo(MetricsSourceAdapter.java:156)
com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getNewMBeanClassName(DefaultMBeanServerInterceptor.java:333)
com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:319)
com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:98)
org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:72)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:222)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.start(MetricsSourceAdapter.java:101)
org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSource(MetricsSystemImpl.java:268)
org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:233)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.<init>(FsDatasetImpl.java:359)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory.newInstance(FsDatasetFactory.java:34)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory.newInstance(FsDatasetFactory.java:30)
org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1757)
org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1679)
org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:390)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:282)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:824)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:11,303 [Thread-60] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: incr /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 1
2020-12-03 07:22:11,303 [Thread-60] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.reference(FsVolumeImpl.java:240)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$000(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.<init>(FsVolumeImpl.java:266)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.obtainReference(FsVolumeImpl.java:289)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.getRemaining(FsVolumeList.java:178)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getRemaining(FsDatasetImpl.java:640)
org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeMetricHelper.getMetrics(DataNodeMetricHelper.java:56)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getMetrics(FsDatasetImpl.java:738)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMetrics(MetricsSourceAdapter.java:200)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.updateJmxCache(MetricsSourceAdapter.java:183)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMBeanInfo(MetricsSourceAdapter.java:156)
com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getNewMBeanClassName(DefaultMBeanServerInterceptor.java:333)
com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:319)
com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:98)
org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:72)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:222)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.start(MetricsSourceAdapter.java:101)
org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSource(MetricsSystemImpl.java:268)
org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:233)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.<init>(FsDatasetImpl.java:359)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory.newInstance(FsDatasetFactory.java:34)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory.newInstance(FsDatasetFactory.java:30)
org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1757)
org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1679)
org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:390)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:282)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:824)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:11,303 [Thread-106] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: desc /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3: 1
2020-12-03 07:22:11,304 [Thread-60] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: desc /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 1
2020-12-03 07:22:11,306 [Thread-106] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.unreference(FsVolumeImpl.java:249)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$100(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.close(FsVolumeImpl.java:276)
org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:258)
org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$ResultHandler.cleanup(DatasetVolumeChecker.java:399)
org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$ResultHandler.onSuccess(DatasetVolumeChecker.java:373)
org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$ResultHandler.onSuccess(DatasetVolumeChecker.java:320)
com.google.common.util.concurrent.Futures$CallbackListener.run(Futures.java:1062)
com.google.common.util.concurrent.DirectExecutor.execute(DirectExecutor.java:30)
org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture.executeListener(AbstractFuture.java:992)
org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture.addListener(AbstractFuture.java:695)
org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture$TrustedFuture.addListener(AbstractFuture.java:108)
com.google.common.util.concurrent.Futures.addCallback(Futures.java:1037)
org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker.checkAllVolumes(DatasetVolumeChecker.java:225)
org.apache.hadoop.hdfs.server.datanode.DataNode.checkDiskError(DataNode.java:3378)
org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1683)
org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:390)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:282)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:824)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:11,306 [Thread-60] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.unreference(FsVolumeImpl.java:249)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$100(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.close(FsVolumeImpl.java:276)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.getRemaining(FsVolumeList.java:180)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getRemaining(FsDatasetImpl.java:640)
org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeMetricHelper.getMetrics(DataNodeMetricHelper.java:56)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getMetrics(FsDatasetImpl.java:738)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMetrics(MetricsSourceAdapter.java:200)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.updateJmxCache(MetricsSourceAdapter.java:183)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMBeanInfo(MetricsSourceAdapter.java:156)
com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getNewMBeanClassName(DefaultMBeanServerInterceptor.java:333)
com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:319)
com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:98)
org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:72)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:222)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.start(MetricsSourceAdapter.java:101)
org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSource(MetricsSystemImpl.java:268)
org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:233)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.<init>(FsDatasetImpl.java:359)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory.newInstance(FsDatasetFactory.java:34)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory.newInstance(FsDatasetFactory.java:30)
org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1757)
org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1679)
org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:390)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:282)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:824)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:11,306 [Thread-106] DEBUG datanode.DataNode (DataNode.java:checkDiskError(3390)) - checkDiskError encountered no failures
2020-12-03 07:22:11,307 [Thread-106] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1723813095-172.17.0.10-1606980125651
2020-12-03 07:22:11,307 [Thread-60] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: incr /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 1
2020-12-03 07:22:11,307 [Thread-121] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: incr /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3: 1
2020-12-03 07:22:11,312 [Thread-60] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.reference(FsVolumeImpl.java:240)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$000(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.<init>(FsVolumeImpl.java:266)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.obtainReference(FsVolumeImpl.java:289)
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi$FsVolumeReferences.<init>(FsDatasetSpi.java:110)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getFsVolumeReferences(FsDatasetImpl.java:145)
org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker.checkAllVolumes(DatasetVolumeChecker.java:203)
org.apache.hadoop.hdfs.server.datanode.DataNode.checkDiskError(DataNode.java:3378)
org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1683)
org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:390)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:282)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:824)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:11,312 [Thread-121] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.reference(FsVolumeImpl.java:240)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$000(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.<init>(FsVolumeImpl.java:266)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.obtainReference(FsVolumeImpl.java:289)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$2.run(FsVolumeList.java:405)
2020-12-03 07:22:11,312 [Thread-60] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:22:11,312 [Thread-121] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1723813095-172.17.0.10-1606980125651 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3...
2020-12-03 07:22:11,314 [Thread-60] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:22:11,314 [Thread-60] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: desc /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 1
2020-12-03 07:22:11,315 [Thread-60] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.unreference(FsVolumeImpl.java:249)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$100(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.close(FsVolumeImpl.java:276)
org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:258)
org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$ResultHandler.cleanup(DatasetVolumeChecker.java:399)
org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$ResultHandler.onSuccess(DatasetVolumeChecker.java:373)
org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$ResultHandler.onSuccess(DatasetVolumeChecker.java:320)
com.google.common.util.concurrent.Futures$CallbackListener.run(Futures.java:1062)
com.google.common.util.concurrent.DirectExecutor.execute(DirectExecutor.java:30)
org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture.executeListener(AbstractFuture.java:992)
org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture.addListener(AbstractFuture.java:695)
org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture$TrustedFuture.addListener(AbstractFuture.java:108)
com.google.common.util.concurrent.Futures.addCallback(Futures.java:1037)
org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker.checkAllVolumes(DatasetVolumeChecker.java:225)
org.apache.hadoop.hdfs.server.datanode.DataNode.checkDiskError(DataNode.java:3378)
org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1683)
org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:390)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:282)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:824)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:11,315 [Thread-60] DEBUG datanode.DataNode (DataNode.java:checkDiskError(3390)) - checkDiskError encountered no failures
2020-12-03 07:22:11,315 [Thread-60] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1723813095-172.17.0.10-1606980125651
2020-12-03 07:22:11,315 [Thread-122] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: incr /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 1
2020-12-03 07:22:11,316 [Thread-122] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.reference(FsVolumeImpl.java:240)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$000(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.<init>(FsVolumeImpl.java:266)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.obtainReference(FsVolumeImpl.java:289)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$2.run(FsVolumeList.java:405)
2020-12-03 07:22:11,316 [Thread-122] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1723813095-172.17.0.10-1606980125651 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2020-12-03 07:22:11,341 [Thread-122] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1723813095-172.17.0.10-1606980125651 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 26ms
2020-12-03 07:22:11,342 [Thread-122] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: desc /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 1
2020-12-03 07:22:11,342 [Thread-122] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.unreference(FsVolumeImpl.java:249)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$100(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.close(FsVolumeImpl.java:276)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$2.run(FsVolumeList.java:413)
2020-12-03 07:22:11,366 [Thread-60] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1723813095-172.17.0.10-1606980125651: 47ms
2020-12-03 07:22:11,389 [Thread-121] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1723813095-172.17.0.10-1606980125651 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3: 76ms
2020-12-03 07:22:11,391 [Thread-129] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: incr /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 1
2020-12-03 07:22:11,392 [Thread-121] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: desc /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3: 1
2020-12-03 07:22:11,392 [Thread-129] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.reference(FsVolumeImpl.java:240)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$000(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.<init>(FsVolumeImpl.java:266)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.obtainReference(FsVolumeImpl.java:289)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$1.run(FsVolumeList.java:198)
2020-12-03 07:22:11,392 [Thread-121] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.unreference(FsVolumeImpl.java:249)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$100(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.close(FsVolumeImpl.java:276)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$2.run(FsVolumeList.java:413)
2020-12-03 07:22:11,392 [Thread-129] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1723813095-172.17.0.10-1606980125651 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2020-12-03 07:22:11,393 [Thread-129] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1723813095-172.17.0.10-1606980125651/current/replicas doesn't exist 
2020-12-03 07:22:11,393 [IPC Server handler 7 on default port 36555] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:22:11,393 [Thread-120] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1723813095-172.17.0.10-1606980125651 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 99ms
2020-12-03 07:22:11,394 [Thread-120] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: desc /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 1
2020-12-03 07:22:11,394 [Thread-120] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.unreference(FsVolumeImpl.java:249)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$100(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.close(FsVolumeImpl.java:276)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$2.run(FsVolumeList.java:413)
2020-12-03 07:22:11,394 [Thread-84] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1723813095-172.17.0.10-1606980125651: 103ms
2020-12-03 07:22:11,394 [Thread-106] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1723813095-172.17.0.10-1606980125651: 87ms
2020-12-03 07:22:11,395 [Thread-129] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1723813095-172.17.0.10-1606980125651 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 3ms
2020-12-03 07:22:11,396 [Thread-129] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: desc /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 1
2020-12-03 07:22:11,396 [Thread-129] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.unreference(FsVolumeImpl.java:249)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$100(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.close(FsVolumeImpl.java:276)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$1.run(FsVolumeList.java:206)
2020-12-03 07:22:11,396 [Thread-60] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1723813095-172.17.0.10-1606980125651: 9ms
2020-12-03 07:22:11,395 [Thread-130] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: incr /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 1
2020-12-03 07:22:11,397 [Thread-131] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: incr /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3: 1
2020-12-03 07:22:11,397 [Thread-130] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.reference(FsVolumeImpl.java:240)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$000(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.<init>(FsVolumeImpl.java:266)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.obtainReference(FsVolumeImpl.java:289)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$1.run(FsVolumeList.java:198)
2020-12-03 07:22:11,397 [Thread-131] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.reference(FsVolumeImpl.java:240)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$000(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.<init>(FsVolumeImpl.java:266)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.obtainReference(FsVolumeImpl.java:289)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$1.run(FsVolumeList.java:198)
2020-12-03 07:22:11,397 [Thread-131] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1723813095-172.17.0.10-1606980125651 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3...
2020-12-03 07:22:11,397 [Thread-130] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1723813095-172.17.0.10-1606980125651 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2020-12-03 07:22:11,398 [Thread-131] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1723813095-172.17.0.10-1606980125651/current/replicas doesn't exist 
2020-12-03 07:22:11,398 [Thread-130] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1723813095-172.17.0.10-1606980125651/current/replicas doesn't exist 
2020-12-03 07:22:11,398 [Listener at localhost/34293] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:22:11,399 [Listener at localhost/34293] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:22:11,399 [Thread-131] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1723813095-172.17.0.10-1606980125651 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3: 2ms
2020-12-03 07:22:11,399 [Thread-131] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: desc /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3: 1
2020-12-03 07:22:11,399 [Thread-131] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.unreference(FsVolumeImpl.java:249)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$100(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.close(FsVolumeImpl.java:276)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$1.run(FsVolumeList.java:206)
2020-12-03 07:22:11,399 [Thread-106] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1723813095-172.17.0.10-1606980125651: 5ms
2020-12-03 07:22:11,400 [Thread-130] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1723813095-172.17.0.10-1606980125651 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 2ms
2020-12-03 07:22:11,400 [Thread-130] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: desc /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 1
2020-12-03 07:22:11,400 [Thread-130] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.unreference(FsVolumeImpl.java:249)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$100(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.close(FsVolumeImpl.java:276)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$1.run(FsVolumeList.java:206)
2020-12-03 07:22:11,401 [Thread-84] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1723813095-172.17.0.10-1606980125651: 7ms
2020-12-03 07:22:11,423 [Thread-60] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 11:54 AM with interval of 21600000ms
2020-12-03 07:22:11,423 [Thread-84] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 11:13 AM with interval of 21600000ms
2020-12-03 07:22:11,423 [Thread-106] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 11:18 AM with interval of 21600000ms
2020-12-03 07:22:11,432 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1723813095-172.17.0.10-1606980125651 (Datanode Uuid 04aa09e8-2060-44fc-bf98-148164c6bc7c) service to localhost/127.0.0.1:36555 beginning handshake with NN
2020-12-03 07:22:11,432 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1723813095-172.17.0.10-1606980125651 (Datanode Uuid df5771e4-65f1-4ee5-86ea-ae3a8b274cb3) service to localhost/127.0.0.1:36555 beginning handshake with NN
2020-12-03 07:22:11,433 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1723813095-172.17.0.10-1606980125651 (Datanode Uuid b518f08d-a98e-47d6-9057-a679367cc8bc) service to localhost/127.0.0.1:36555 beginning handshake with NN
2020-12-03 07:22:11,447 [IPC Server handler 4 on default port 36555] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:34135, datanodeUuid=df5771e4-65f1-4ee5-86ea-ae3a8b274cb3, infoPort=41079, infoSecurePort=0, ipcPort=45866, storageInfo=lv=-57;cid=testClusterID;nsid=156303761;c=1606980125651) storage df5771e4-65f1-4ee5-86ea-ae3a8b274cb3
2020-12-03 07:22:11,450 [IPC Server handler 4 on default port 36555] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:34135
2020-12-03 07:22:11,450 [IPC Server handler 4 on default port 36555] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN df5771e4-65f1-4ee5-86ea-ae3a8b274cb3 (127.0.0.1:34135).
2020-12-03 07:22:11,457 [IPC Server handler 3 on default port 36555] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:40715, datanodeUuid=b518f08d-a98e-47d6-9057-a679367cc8bc, infoPort=38815, infoSecurePort=0, ipcPort=34293, storageInfo=lv=-57;cid=testClusterID;nsid=156303761;c=1606980125651) storage b518f08d-a98e-47d6-9057-a679367cc8bc
2020-12-03 07:22:11,457 [IPC Server handler 3 on default port 36555] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:40715
2020-12-03 07:22:11,458 [IPC Server handler 3 on default port 36555] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN b518f08d-a98e-47d6-9057-a679367cc8bc (127.0.0.1:40715).
2020-12-03 07:22:11,458 [IPC Server handler 5 on default port 36555] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:36884, datanodeUuid=04aa09e8-2060-44fc-bf98-148164c6bc7c, infoPort=43176, infoSecurePort=0, ipcPort=35379, storageInfo=lv=-57;cid=testClusterID;nsid=156303761;c=1606980125651) storage 04aa09e8-2060-44fc-bf98-148164c6bc7c
2020-12-03 07:22:11,459 [IPC Server handler 5 on default port 36555] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:36884
2020-12-03 07:22:11,459 [IPC Server handler 5 on default port 36555] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 04aa09e8-2060-44fc-bf98-148164c6bc7c (127.0.0.1:36884).
2020-12-03 07:22:11,459 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1723813095-172.17.0.10-1606980125651 (Datanode Uuid df5771e4-65f1-4ee5-86ea-ae3a8b274cb3) service to localhost/127.0.0.1:36555 successfully registered with NN
2020-12-03 07:22:11,459 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1723813095-172.17.0.10-1606980125651 (Datanode Uuid b518f08d-a98e-47d6-9057-a679367cc8bc) service to localhost/127.0.0.1:36555 successfully registered with NN
2020-12-03 07:22:11,459 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:36555 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:22:11,460 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1723813095-172.17.0.10-1606980125651 (Datanode Uuid 04aa09e8-2060-44fc-bf98-148164c6bc7c) service to localhost/127.0.0.1:36555 successfully registered with NN
2020-12-03 07:22:11,460 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:36555 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:22:11,460 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: incr /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 1
2020-12-03 07:22:11,460 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:36555 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:22:11,460 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.reference(FsVolumeImpl.java:240)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$000(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.<init>(FsVolumeImpl.java:266)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.obtainReference(FsVolumeImpl.java:289)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getStorageReports(FsDatasetImpl.java:162)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:495)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:648)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:849)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:11,460 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: incr /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3: 1
2020-12-03 07:22:11,461 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.reference(FsVolumeImpl.java:240)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$000(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.<init>(FsVolumeImpl.java:266)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.obtainReference(FsVolumeImpl.java:289)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getStorageReports(FsDatasetImpl.java:162)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:495)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:648)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:849)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:11,461 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: desc /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 1
2020-12-03 07:22:11,460 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: incr /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 1
2020-12-03 07:22:11,462 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.unreference(FsVolumeImpl.java:249)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$100(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.close(FsVolumeImpl.java:276)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getStorageReports(FsDatasetImpl.java:171)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:495)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:648)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:849)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:11,461 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: desc /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3: 1
2020-12-03 07:22:11,462 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] DEBUG datanode.DataNode (BPServiceActor.java:sendHeartBeat(497)) - Sending heartbeat with 1 storage reports from service actor: Block pool BP-1723813095-172.17.0.10-1606980125651 (Datanode Uuid df5771e4-65f1-4ee5-86ea-ae3a8b274cb3) service to localhost/127.0.0.1:36555
2020-12-03 07:22:11,462 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.reference(FsVolumeImpl.java:240)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$000(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.<init>(FsVolumeImpl.java:266)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.obtainReference(FsVolumeImpl.java:289)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getStorageReports(FsDatasetImpl.java:162)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:495)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:648)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:849)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:11,462 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.unreference(FsVolumeImpl.java:249)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$100(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.close(FsVolumeImpl.java:276)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getStorageReports(FsDatasetImpl.java:171)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:495)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:648)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:849)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:11,462 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] DEBUG datanode.DataNode (BPServiceActor.java:sendHeartBeat(497)) - Sending heartbeat with 1 storage reports from service actor: Block pool BP-1723813095-172.17.0.10-1606980125651 (Datanode Uuid b518f08d-a98e-47d6-9057-a679367cc8bc) service to localhost/127.0.0.1:36555
2020-12-03 07:22:11,463 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: desc /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 1
2020-12-03 07:22:11,464 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.unreference(FsVolumeImpl.java:249)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$100(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.close(FsVolumeImpl.java:276)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getStorageReports(FsDatasetImpl.java:171)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:495)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:648)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:849)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:11,464 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] DEBUG datanode.DataNode (BPServiceActor.java:sendHeartBeat(497)) - Sending heartbeat with 1 storage reports from service actor: Block pool BP-1723813095-172.17.0.10-1606980125651 (Datanode Uuid 04aa09e8-2060-44fc-bf98-148164c6bc7c) service to localhost/127.0.0.1:36555
2020-12-03 07:22:11,481 [IPC Server handler 1 on default port 36555] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-c8853e3c-4a70-4eb7-ab66-5a6255afa86b for DN 127.0.0.1:34135
2020-12-03 07:22:11,483 [IPC Server handler 0 on default port 36555] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-c1dccd31-33e7-4e8c-aaa0-047e42da1cc4 for DN 127.0.0.1:40715
2020-12-03 07:22:11,485 [IPC Server handler 2 on default port 36555] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-f9cddab7-0df0-41ff-8e2b-7e03980d036c for DN 127.0.0.1:36884
2020-12-03 07:22:11,510 [IPC Server handler 6 on default port 36555] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:22:11,522 [Listener at localhost/34293] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2758)) - Cluster is active
2020-12-03 07:22:11,522 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0xb483d3284db523b4: Processing first storage report for DS-c8853e3c-4a70-4eb7-ab66-5a6255afa86b from datanode df5771e4-65f1-4ee5-86ea-ae3a8b274cb3
2020-12-03 07:22:11,530 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0xb483d3284db523b4: from storage DS-c8853e3c-4a70-4eb7-ab66-5a6255afa86b node DatanodeRegistration(127.0.0.1:34135, datanodeUuid=df5771e4-65f1-4ee5-86ea-ae3a8b274cb3, infoPort=41079, infoSecurePort=0, ipcPort=45866, storageInfo=lv=-57;cid=testClusterID;nsid=156303761;c=1606980125651), blocks: 0, hasStaleStorage: false, processing time: 7 msecs, invalidatedBlocks: 0
2020-12-03 07:22:11,531 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x21b5d55bd9fcf48: Processing first storage report for DS-f9cddab7-0df0-41ff-8e2b-7e03980d036c from datanode 04aa09e8-2060-44fc-bf98-148164c6bc7c
2020-12-03 07:22:11,531 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x21b5d55bd9fcf48: from storage DS-f9cddab7-0df0-41ff-8e2b-7e03980d036c node DatanodeRegistration(127.0.0.1:36884, datanodeUuid=04aa09e8-2060-44fc-bf98-148164c6bc7c, infoPort=43176, infoSecurePort=0, ipcPort=35379, storageInfo=lv=-57;cid=testClusterID;nsid=156303761;c=1606980125651), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:22:11,531 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x5f460ee949b6f7a5: Processing first storage report for DS-c1dccd31-33e7-4e8c-aaa0-047e42da1cc4 from datanode b518f08d-a98e-47d6-9057-a679367cc8bc
2020-12-03 07:22:11,531 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x5f460ee949b6f7a5: from storage DS-c1dccd31-33e7-4e8c-aaa0-047e42da1cc4 node DatanodeRegistration(127.0.0.1:40715, datanodeUuid=b518f08d-a98e-47d6-9057-a679367cc8bc, infoPort=38815, infoSecurePort=0, ipcPort=34293, storageInfo=lv=-57;cid=testClusterID;nsid=156303761;c=1606980125651), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:22:11,539 [IPC Server handler 4 on default port 36555] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:22:11,541 [Listener at localhost/34293] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2758)) - Cluster is active
2020-12-03 07:22:11,553 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0xb483d3284db523b4,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 5 msec to generate and 50 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:22:11,553 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x21b5d55bd9fcf48,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 5 msec to generate and 51 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:22:11,553 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x5f460ee949b6f7a5,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 5 msec to generate and 50 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:22:11,554 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1723813095-172.17.0.10-1606980125651
2020-12-03 07:22:11,554 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1723813095-172.17.0.10-1606980125651
2020-12-03 07:22:11,554 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1723813095-172.17.0.10-1606980125651
2020-12-03 07:22:11,608 [IPC Server handler 3 on default port 36555] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/testReservedSpaceForPipelineRecovery.01.dat	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2020-12-03 07:22:11,675 [IPC Server handler 5 on default port 36555] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_1073741825_1001, replicas=127.0.0.1:36884, 127.0.0.1:40715, 127.0.0.1:34135 for /testReservedSpaceForPipelineRecovery.01.dat
2020-12-03 07:22:11,705 [Thread-141] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:11,726 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@65b1d5cf] DEBUG datanode.DataNode (DataXceiver.java:<init>(154)) - Number of active connections is: 1
2020-12-03 07:22:11,780 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:59442 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741825_1001]] DEBUG datanode.DataNode (DataXceiver.java:writeBlock(728)) - opWriteBlock: stage=PIPELINE_SETUP_CREATE, clientname=DFSClient_NONMAPREDUCE_-2011940974_25
  block  =BP-1723813095-172.17.0.10-1606980125651:blk_1073741825_1001, newGs=0, bytesRcvd=[0, 0]
  targets=[127.0.0.1:40715, 127.0.0.1:34135]; pipelineSize=3, srcDataNode=:0, pinning=false
2020-12-03 07:22:11,780 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:59442 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741825_1001]] DEBUG datanode.DataNode (DataXceiver.java:writeBlock(734)) - isDatanode=false, isClient=true, isTransfer=false
2020-12-03 07:22:11,780 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:59442 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741825_1001]] DEBUG datanode.DataNode (DataXceiver.java:writeBlock(736)) - writeBlock receive buf size 531128 tcp no delay true
2020-12-03 07:22:11,781 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:59442 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741825_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1723813095-172.17.0.10-1606980125651:blk_1073741825_1001 src: /127.0.0.1:59442 dest: /127.0.0.1:36884
2020-12-03 07:22:11,793 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:59442 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741825_1001]] DEBUG datanode.DataNode (BlockReceiver.java:<init>(191)) - BlockReceiver: BP-1723813095-172.17.0.10-1606980125651:blk_1073741825_1001
 storageType=DISK, inAddr=/127.0.0.1:59442, myAddr=/127.0.0.1:36884
 stage=PIPELINE_SETUP_CREATE, newGs=0, minBytesRcvd=0, maxBytesRcvd=0
 clientname=DFSClient_NONMAPREDUCE_-2011940974_25, srcDataNode=:0, datanode=127.0.0.1:36884
 requestedChecksum=DataChecksum(type=CRC32C, chunkSize=512)
 cachingStrategy=CachingStrategy(dropBehind=null, readahead=null)
 allowLazyPersist=false, pinning=false, isClient=true, isDatanode=false, responseInterval=30000, storageID=DS-f9cddab7-0df0-41ff-8e2b-7e03980d036c
2020-12-03 07:22:11,794 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:59442 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741825_1001]] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: incr /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 1
2020-12-03 07:22:11,794 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:59442 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741825_1001]] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.reference(FsVolumeImpl.java:240)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$000(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.<init>(FsVolumeImpl.java:266)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.obtainReference(FsVolumeImpl.java:289)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.chooseVolume(FsVolumeList.java:91)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.getNextVolume(FsVolumeList.java:118)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.createRbw(FsDatasetImpl.java:1411)
org.apache.hadoop.hdfs.server.datanode.BlockReceiver.<init>(BlockReceiver.java:216)
org.apache.hadoop.hdfs.server.datanode.DataXceiver.getBlockReceiver(DataXceiver.java:1312)
org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:763)
org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:11,806 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:59442 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741825_1001]] DEBUG datanode.DataNode (LocalReplicaInPipeline.java:createStreams(252)) - writeTo blockfile is /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1723813095-172.17.0.10-1606980125651/current/rbw/blk_1073741825 of size 0
2020-12-03 07:22:11,806 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:59442 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741825_1001]] DEBUG datanode.DataNode (LocalReplicaInPipeline.java:createStreams(254)) - writeTo metafile is /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1723813095-172.17.0.10-1606980125651/current/rbw/blk_1073741825_1001.meta of size 0
2020-12-03 07:22:11,809 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:59442 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741825_1001]] DEBUG datanode.DataNode (DataXceiver.java:writeBlock(784)) - Connecting to datanode 127.0.0.1:40715
2020-12-03 07:22:11,819 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:59442 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741825_1001]] ERROR datanode.DataNode (DataXceiver.java:writeBlock(880)) - DataNode{data=FSDataset{dirpath='[/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1]'}, localName='127.0.0.1:36884', datanodeUuid='04aa09e8-2060-44fc-bf98-148164c6bc7c', xmitsInProgress=0}:Exception transfering block BP-1723813095-172.17.0.10-1606980125651:blk_1073741825_1001 to mirror 127.0.0.1:40715
java.io.IOException: Failing Mirror for space reservation
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestSpaceReservation$2.failMirrorConnection(TestSpaceReservation.java:644)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:789)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:11,819 [Thread-141] INFO  hdfs.DataStreamer (DataStreamer.java:createBlockOutputStream(1790)) - Exception in createBlockOutputStream blk_1073741825_1001
java.io.IOException: Got error, status=ERROR, status message , ack with firstBadLink as 127.0.0.1:40715
	at org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:110)
	at org.apache.hadoop.hdfs.DataStreamer.createBlockOutputStream(DataStreamer.java:1778)
	at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1679)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
2020-12-03 07:22:11,821 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:59442 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741825_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(939)) - opWriteBlock BP-1723813095-172.17.0.10-1606980125651:blk_1073741825_1001 received exception java.io.IOException: Failing Mirror for space reservation
2020-12-03 07:22:11,822 [Thread-141] WARN  hdfs.DataStreamer (DataStreamer.java:nextBlockOutputStream(1683)) - Abandoning BP-1723813095-172.17.0.10-1606980125651:blk_1073741825_1001
2020-12-03 07:22:11,825 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:59442 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741825_1001]] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: desc /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 1
2020-12-03 07:22:11,825 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:59442 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741825_1001]] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.unreference(FsVolumeImpl.java:249)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$100(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.close(FsVolumeImpl.java:276)
org.apache.hadoop.hdfs.server.datanode.ReplicaHandler.close(ReplicaHandler.java:42)
org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:258)
org.apache.hadoop.hdfs.server.datanode.BlockReceiver.close(BlockReceiver.java:369)
org.apache.hadoop.io.IOUtils.cleanupWithLogger(IOUtils.java:280)
org.apache.hadoop.io.IOUtils.closeStream(IOUtils.java:298)
org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:949)
org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:11,826 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:59442 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741825_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:36884:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:59442 dst: /127.0.0.1:36884
java.io.IOException: Failing Mirror for space reservation
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestSpaceReservation$2.failMirrorConnection(TestSpaceReservation.java:644)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:789)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:11,826 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:59442 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741825_1001]] DEBUG datanode.DataNode (DataXceiver.java:run(328)) - 127.0.0.1:36884:Number of active connections is: 2
2020-12-03 07:22:11,832 [Thread-141] WARN  hdfs.DataStreamer (DataStreamer.java:nextBlockOutputStream(1688)) - Excluding datanode DatanodeInfoWithStorage[127.0.0.1:40715,DS-c1dccd31-33e7-4e8c-aaa0-047e42da1cc4,DISK]
2020-12-03 07:22:11,840 [IPC Server handler 1 on default port 36555] WARN  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(450)) - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2020-12-03 07:22:11,841 [IPC Server handler 1 on default port 36555] WARN  protocol.BlockStoragePolicy (BlockStoragePolicy.java:chooseStorageTypes(161)) - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2020-12-03 07:22:11,841 [IPC Server handler 1 on default port 36555] WARN  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(450)) - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2020-12-03 07:22:11,842 [IPC Server handler 1 on default port 36555] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_1073741826_1002, replicas=127.0.0.1:34135, 127.0.0.1:36884 for /testReservedSpaceForPipelineRecovery.01.dat
2020-12-03 07:22:11,845 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@5fe78640] DEBUG datanode.DataNode (DataXceiver.java:<init>(154)) - Number of active connections is: 1
2020-12-03 07:22:11,846 [Thread-141] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:11,847 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:46094 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002]] DEBUG datanode.DataNode (DataXceiver.java:writeBlock(728)) - opWriteBlock: stage=PIPELINE_SETUP_CREATE, clientname=DFSClient_NONMAPREDUCE_-2011940974_25
  block  =BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002, newGs=0, bytesRcvd=[0, 0]
  targets=[127.0.0.1:36884]; pipelineSize=2, srcDataNode=:0, pinning=false
2020-12-03 07:22:11,848 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:46094 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002]] DEBUG datanode.DataNode (DataXceiver.java:writeBlock(734)) - isDatanode=false, isClient=true, isTransfer=false
2020-12-03 07:22:11,848 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:46094 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002]] DEBUG datanode.DataNode (DataXceiver.java:writeBlock(736)) - writeBlock receive buf size 531128 tcp no delay true
2020-12-03 07:22:11,848 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:46094 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002 src: /127.0.0.1:46094 dest: /127.0.0.1:34135
2020-12-03 07:22:11,848 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:46094 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002]] DEBUG datanode.DataNode (BlockReceiver.java:<init>(191)) - BlockReceiver: BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002
 storageType=DISK, inAddr=/127.0.0.1:46094, myAddr=/127.0.0.1:34135
 stage=PIPELINE_SETUP_CREATE, newGs=0, minBytesRcvd=0, maxBytesRcvd=0
 clientname=DFSClient_NONMAPREDUCE_-2011940974_25, srcDataNode=:0, datanode=127.0.0.1:34135
 requestedChecksum=DataChecksum(type=CRC32C, chunkSize=512)
 cachingStrategy=CachingStrategy(dropBehind=null, readahead=null)
 allowLazyPersist=false, pinning=false, isClient=true, isDatanode=false, responseInterval=30000, storageID=DS-c8853e3c-4a70-4eb7-ab66-5a6255afa86b
2020-12-03 07:22:11,848 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:46094 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002]] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: incr /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 1
2020-12-03 07:22:11,849 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:46094 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002]] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.reference(FsVolumeImpl.java:240)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$000(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.<init>(FsVolumeImpl.java:266)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.obtainReference(FsVolumeImpl.java:289)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.chooseVolume(FsVolumeList.java:91)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.getNextVolume(FsVolumeList.java:118)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.createRbw(FsDatasetImpl.java:1411)
org.apache.hadoop.hdfs.server.datanode.BlockReceiver.<init>(BlockReceiver.java:216)
org.apache.hadoop.hdfs.server.datanode.DataXceiver.getBlockReceiver(DataXceiver.java:1312)
org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:763)
org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:11,854 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:46094 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002]] DEBUG datanode.DataNode (LocalReplicaInPipeline.java:createStreams(252)) - writeTo blockfile is /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1723813095-172.17.0.10-1606980125651/current/rbw/blk_1073741826 of size 0
2020-12-03 07:22:11,854 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:46094 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002]] DEBUG datanode.DataNode (LocalReplicaInPipeline.java:createStreams(254)) - writeTo metafile is /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1723813095-172.17.0.10-1606980125651/current/rbw/blk_1073741826_1002.meta of size 0
2020-12-03 07:22:11,855 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:46094 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002]] DEBUG datanode.DataNode (DataXceiver.java:writeBlock(784)) - Connecting to datanode 127.0.0.1:36884
2020-12-03 07:22:11,856 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@65b1d5cf] DEBUG datanode.DataNode (DataXceiver.java:<init>(154)) - Number of active connections is: 1
2020-12-03 07:22:11,861 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:46094 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:11,875 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:59446 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002]] DEBUG datanode.DataNode (DataXceiver.java:writeBlock(728)) - opWriteBlock: stage=PIPELINE_SETUP_CREATE, clientname=DFSClient_NONMAPREDUCE_-2011940974_25
  block  =BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002, newGs=0, bytesRcvd=[0, 0]
  targets=[]; pipelineSize=2, srcDataNode=:0, pinning=false
2020-12-03 07:22:11,875 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:59446 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002]] DEBUG datanode.DataNode (DataXceiver.java:writeBlock(734)) - isDatanode=false, isClient=true, isTransfer=false
2020-12-03 07:22:11,875 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:59446 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002]] DEBUG datanode.DataNode (DataXceiver.java:writeBlock(736)) - writeBlock receive buf size 531128 tcp no delay true
2020-12-03 07:22:11,875 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:59446 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002 src: /127.0.0.1:59446 dest: /127.0.0.1:36884
2020-12-03 07:22:11,876 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:59446 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002]] DEBUG datanode.DataNode (BlockReceiver.java:<init>(191)) - BlockReceiver: BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002
 storageType=DISK, inAddr=/127.0.0.1:59446, myAddr=/127.0.0.1:36884
 stage=PIPELINE_SETUP_CREATE, newGs=0, minBytesRcvd=0, maxBytesRcvd=0
 clientname=DFSClient_NONMAPREDUCE_-2011940974_25, srcDataNode=:0, datanode=127.0.0.1:36884
 requestedChecksum=DataChecksum(type=CRC32C, chunkSize=512)
 cachingStrategy=CachingStrategy(dropBehind=null, readahead=null)
 allowLazyPersist=false, pinning=false, isClient=true, isDatanode=false, responseInterval=30000, storageID=DS-f9cddab7-0df0-41ff-8e2b-7e03980d036c
2020-12-03 07:22:11,876 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:59446 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002]] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: incr /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 1
2020-12-03 07:22:11,876 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:59446 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002]] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.reference(FsVolumeImpl.java:240)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$000(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.<init>(FsVolumeImpl.java:266)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.obtainReference(FsVolumeImpl.java:289)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.chooseVolume(FsVolumeList.java:91)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.getNextVolume(FsVolumeList.java:118)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.createRbw(FsDatasetImpl.java:1411)
org.apache.hadoop.hdfs.server.datanode.BlockReceiver.<init>(BlockReceiver.java:216)
org.apache.hadoop.hdfs.server.datanode.DataXceiver.getBlockReceiver(DataXceiver.java:1312)
org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:763)
org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:11,877 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:59446 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002]] DEBUG datanode.DataNode (LocalReplicaInPipeline.java:createStreams(252)) - writeTo blockfile is /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1723813095-172.17.0.10-1606980125651/current/rbw/blk_1073741826 of size 0
2020-12-03 07:22:11,877 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:59446 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002]] DEBUG datanode.DataNode (LocalReplicaInPipeline.java:createStreams(254)) - writeTo metafile is /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1723813095-172.17.0.10-1606980125651/current/rbw/blk_1073741826_1002.meta of size 0
2020-12-03 07:22:11,890 [PacketResponder: BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002, type=LAST_IN_PIPELINE] DEBUG datanode.DataNode (BlockReceiver.java:waitForAckHead(1327)) - PacketResponder: BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002, type=LAST_IN_PIPELINE: seqno=-2 waiting for local datanode to finish write.
2020-12-03 07:22:11,906 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:46094 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002]] DEBUG datanode.DataNode (BlockReceiver.java:receivePacket(532)) - Receiving one packet for block BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002: PacketHeader with packetLen=9 header data: offsetInBlock: 0
seqno: 0
lastPacketInBlock: false
dataLen: 1

2020-12-03 07:22:11,906 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:46094 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002]] DEBUG datanode.DataNode (BlockReceiver.java:enqueue(1268)) - PacketResponder: BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:36884]: enqueue Packet(seqno=0, lastPacketInBlock=false, offsetInBlock=1, ackEnqueueNanoTime=141123395967331, ackStatus=SUCCESS)
2020-12-03 07:22:11,908 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:59446 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002]] DEBUG datanode.DataNode (BlockReceiver.java:receivePacket(532)) - Receiving one packet for block BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002: PacketHeader with packetLen=9 header data: offsetInBlock: 0
seqno: 0
lastPacketInBlock: false
dataLen: 1

2020-12-03 07:22:11,908 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:59446 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002]] DEBUG datanode.DataNode (BlockReceiver.java:enqueue(1268)) - PacketResponder: BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002, type=LAST_IN_PIPELINE: enqueue Packet(seqno=0, lastPacketInBlock=false, offsetInBlock=1, ackEnqueueNanoTime=141123397570956, ackStatus=SUCCESS)
2020-12-03 07:22:11,921 [PacketResponder: BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:36884]] DEBUG datanode.DataNode (BlockReceiver.java:run(1387)) - PacketResponder: BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:36884] got seqno: 0 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
2020-12-03 07:22:11,921 [PacketResponder: BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002, type=LAST_IN_PIPELINE] DEBUG datanode.DataNode (BlockReceiver.java:sendAckUpstreamUnprotected(1645)) - PacketResponder: BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002, type=LAST_IN_PIPELINE, replyAck=seqno: 0 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
2020-12-03 07:22:11,922 [PacketResponder: BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002, type=LAST_IN_PIPELINE] DEBUG datanode.DataNode (BlockReceiver.java:waitForAckHead(1327)) - PacketResponder: BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002, type=LAST_IN_PIPELINE: seqno=-2 waiting for local datanode to finish write.
2020-12-03 07:22:11,923 [PacketResponder: BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:36884]] DEBUG datanode.DataNode (BlockReceiver.java:sendAckUpstreamUnprotected(1645)) - PacketResponder: BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:36884], replyAck=seqno: 0 reply: SUCCESS reply: SUCCESS downstreamAckTimeNanos: 11262682 flag: 0 flag: 0
2020-12-03 07:22:11,932 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:46094 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002]] DEBUG datanode.DataNode (BlockReceiver.java:receivePacket(532)) - Receiving one packet for block BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002: PacketHeader with packetLen=4 header data: offsetInBlock: 1
seqno: 1
lastPacketInBlock: true
dataLen: 0

2020-12-03 07:22:11,932 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:46094 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002]] DEBUG datanode.DataNode (BlockReceiver.java:enqueue(1268)) - PacketResponder: BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:36884]: enqueue Packet(seqno=1, lastPacketInBlock=true, offsetInBlock=1, ackEnqueueNanoTime=141123421418724, ackStatus=SUCCESS)
2020-12-03 07:22:11,932 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:46094 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002]] DEBUG datanode.DataNode (BlockReceiver.java:receivePacket(613)) - Receiving an empty packet or the end of the block BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002
2020-12-03 07:22:11,932 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:59446 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002]] DEBUG datanode.DataNode (BlockReceiver.java:receivePacket(532)) - Receiving one packet for block BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002: PacketHeader with packetLen=4 header data: offsetInBlock: 1
seqno: 1
lastPacketInBlock: true
dataLen: 0

2020-12-03 07:22:11,932 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:59446 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002]] DEBUG datanode.DataNode (BlockReceiver.java:receivePacket(613)) - Receiving an empty packet or the end of the block BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002
2020-12-03 07:22:11,933 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:59446 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002]] DEBUG datanode.DataNode (BlockReceiver.java:enqueue(1268)) - PacketResponder: BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002, type=LAST_IN_PIPELINE: enqueue Packet(seqno=1, lastPacketInBlock=true, offsetInBlock=1, ackEnqueueNanoTime=141123422193061, ackStatus=SUCCESS)
2020-12-03 07:22:11,940 [PacketResponder: BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002, type=LAST_IN_PIPELINE] DEBUG impl.FsDatasetImpl (FsDatasetImpl.java:moveBlockFiles(885)) - addFinalizedBlock: Moved file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1723813095-172.17.0.10-1606980125651/current/rbw/blk_1073741826_1002.meta to /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1723813095-172.17.0.10-1606980125651/current/finalized/subdir0/subdir0/blk_1073741826_1002.meta and file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1723813095-172.17.0.10-1606980125651/current/rbw/blk_1073741826 to /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1723813095-172.17.0.10-1606980125651/current/finalized/subdir0/subdir0/blk_1073741826
2020-12-03 07:22:11,943 [PacketResponder: BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002, type=LAST_IN_PIPELINE] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: desc /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 1
2020-12-03 07:22:11,943 [PacketResponder: BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002, type=LAST_IN_PIPELINE] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.unreference(FsVolumeImpl.java:249)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$100(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.close(FsVolumeImpl.java:276)
org.apache.hadoop.hdfs.server.datanode.ReplicaHandler.close(ReplicaHandler.java:42)
org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.finalizeBlock(BlockReceiver.java:1521)
org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.run(BlockReceiver.java:1477)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:11,944 [PacketResponder: BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:59446, dest: /127.0.0.1:36884, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2011940974_25, offset: 0, srvID: 04aa09e8-2060-44fc-bf98-148164c6bc7c, blockid: BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002, duration(ns): 44165664
2020-12-03 07:22:11,944 [PacketResponder: BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002, type=LAST_IN_PIPELINE] DEBUG datanode.DataNode (BlockReceiver.java:sendAckUpstreamUnprotected(1645)) - PacketResponder: BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002, type=LAST_IN_PIPELINE, replyAck=seqno: 1 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
2020-12-03 07:22:11,945 [PacketResponder: BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002, type=LAST_IN_PIPELINE terminating
2020-12-03 07:22:11,945 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:59446 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002]] DEBUG datanode.DataNode (BlockReceiver.java:close(1351)) - PacketResponder: BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002, type=LAST_IN_PIPELINE: closing
2020-12-03 07:22:11,948 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:59446 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002]] DEBUG datanode.DataNode (DataXceiver.java:run(328)) - 127.0.0.1:36884:Number of active connections is: 2
2020-12-03 07:22:11,948 [PacketResponder: BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:36884]] DEBUG datanode.DataNode (BlockReceiver.java:run(1387)) - PacketResponder: BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:36884] got seqno: 1 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
2020-12-03 07:22:11,950 [PacketResponder: BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:36884]] DEBUG impl.FsDatasetImpl (FsDatasetImpl.java:moveBlockFiles(885)) - addFinalizedBlock: Moved file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1723813095-172.17.0.10-1606980125651/current/rbw/blk_1073741826_1002.meta to /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1723813095-172.17.0.10-1606980125651/current/finalized/subdir0/subdir0/blk_1073741826_1002.meta and file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1723813095-172.17.0.10-1606980125651/current/rbw/blk_1073741826 to /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1723813095-172.17.0.10-1606980125651/current/finalized/subdir0/subdir0/blk_1073741826
2020-12-03 07:22:11,951 [PacketResponder: BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:36884]] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: desc /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 1
2020-12-03 07:22:11,951 [PacketResponder: BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:36884]] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.unreference(FsVolumeImpl.java:249)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$100(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.close(FsVolumeImpl.java:276)
org.apache.hadoop.hdfs.server.datanode.ReplicaHandler.close(ReplicaHandler.java:42)
org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.finalizeBlock(BlockReceiver.java:1521)
org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.run(BlockReceiver.java:1477)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:11,952 [PacketResponder: BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:36884]] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:46094, dest: /127.0.0.1:34135, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2011940974_25, offset: 0, srvID: df5771e4-65f1-4ee5-86ea-ae3a8b274cb3, blockid: BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002, duration(ns): 49531558
2020-12-03 07:22:11,952 [PacketResponder: BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:36884]] DEBUG datanode.DataNode (BlockReceiver.java:sendAckUpstreamUnprotected(1645)) - PacketResponder: BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:36884], replyAck=seqno: 1 reply: SUCCESS reply: SUCCESS downstreamAckTimeNanos: 16112376 flag: 0 flag: 0
2020-12-03 07:22:11,952 [PacketResponder: BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:36884]] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:36884] terminating
2020-12-03 07:22:11,952 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:46094 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002]] DEBUG datanode.DataNode (BlockReceiver.java:close(1351)) - PacketResponder: BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:36884]: closing
2020-12-03 07:22:11,954 [DataXceiver for client DFSClient_NONMAPREDUCE_-2011940974_25 at /127.0.0.1:46094 [Receiving block BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002]] DEBUG datanode.DataNode (DataXceiver.java:run(328)) - 127.0.0.1:34135:Number of active connections is: 2
2020-12-03 07:22:11,975 [IPC Server handler 6 on default port 36555] INFO  namenode.FSNamesystem (FSNamesystem.java:checkBlocksComplete(2995)) - BLOCK* blk_1073741826_1002 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /testReservedSpaceForPipelineRecovery.01.dat
2020-12-03 07:22:12,394 [IPC Server handler 8 on default port 36555] INFO  hdfs.StateChange (FSNamesystem.java:completeFile(2948)) - DIR* completeFile: /testReservedSpaceForPipelineRecovery.01.dat is closed by DFSClient_NONMAPREDUCE_-2011940974_25
2020-12-03 07:22:12,397 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: incr /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 1
2020-12-03 07:22:12,397 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.reference(FsVolumeImpl.java:240)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$000(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.<init>(FsVolumeImpl.java:266)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.obtainReference(FsVolumeImpl.java:289)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getStorageReports(FsDatasetImpl.java:162)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:495)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:648)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:849)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:12,397 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: desc /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 1
2020-12-03 07:22:12,398 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.unreference(FsVolumeImpl.java:249)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$100(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.close(FsVolumeImpl.java:276)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getStorageReports(FsDatasetImpl.java:171)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:495)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:648)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:849)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:12,398 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] DEBUG datanode.DataNode (BPServiceActor.java:sendHeartBeat(497)) - Sending heartbeat with 1 storage reports from service actor: Block pool BP-1723813095-172.17.0.10-1606980125651 (Datanode Uuid 04aa09e8-2060-44fc-bf98-148164c6bc7c) service to localhost/127.0.0.1:36555
2020-12-03 07:22:12,403 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] INFO  datanode.DataNode (BPServiceActor.java:offerService(698)) - Forcing a full block report to localhost/127.0.0.1:36555
2020-12-03 07:22:12,407 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x21b5d55bd9fcf49: from storage DS-f9cddab7-0df0-41ff-8e2b-7e03980d036c node DatanodeRegistration(127.0.0.1:36884, datanodeUuid=04aa09e8-2060-44fc-bf98-148164c6bc7c, infoPort=43176, infoSecurePort=0, ipcPort=35379, storageInfo=lv=-57;cid=testClusterID;nsid=156303761;c=1606980125651), blocks: 2, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 1
2020-12-03 07:22:12,408 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x21b5d55bd9fcf49,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 1 msec to generate and 4 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:22:12,409 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1723813095-172.17.0.10-1606980125651
2020-12-03 07:22:12,497 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: incr /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 1
2020-12-03 07:22:12,497 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.reference(FsVolumeImpl.java:240)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$000(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.<init>(FsVolumeImpl.java:266)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.obtainReference(FsVolumeImpl.java:289)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getStorageReports(FsDatasetImpl.java:162)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:495)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:648)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:849)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:12,498 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: desc /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 1
2020-12-03 07:22:12,498 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.unreference(FsVolumeImpl.java:249)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$100(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.close(FsVolumeImpl.java:276)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getStorageReports(FsDatasetImpl.java:171)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:495)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:648)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:849)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:12,498 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] DEBUG datanode.DataNode (BPServiceActor.java:sendHeartBeat(497)) - Sending heartbeat with 1 storage reports from service actor: Block pool BP-1723813095-172.17.0.10-1606980125651 (Datanode Uuid df5771e4-65f1-4ee5-86ea-ae3a8b274cb3) service to localhost/127.0.0.1:36555
2020-12-03 07:22:12,503 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] INFO  datanode.DataNode (BPServiceActor.java:offerService(698)) - Forcing a full block report to localhost/127.0.0.1:36555
2020-12-03 07:22:12,505 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0xb483d3284db523b5: from storage DS-c8853e3c-4a70-4eb7-ab66-5a6255afa86b node DatanodeRegistration(127.0.0.1:34135, datanodeUuid=df5771e4-65f1-4ee5-86ea-ae3a8b274cb3, infoPort=41079, infoSecurePort=0, ipcPort=45866, storageInfo=lv=-57;cid=testClusterID;nsid=156303761;c=1606980125651), blocks: 1, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:22:12,506 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0xb483d3284db523b5,  containing 1 storage report(s), of which we sent 1. The reports had 1 total blocks and used 1 RPC(s). This took 1 msec to generate and 3 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:22:12,507 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1723813095-172.17.0.10-1606980125651
2020-12-03 07:22:12,597 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: incr /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3: 1
2020-12-03 07:22:12,597 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.reference(FsVolumeImpl.java:240)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$000(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.<init>(FsVolumeImpl.java:266)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.obtainReference(FsVolumeImpl.java:289)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getStorageReports(FsDatasetImpl.java:162)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:495)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:648)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:849)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:12,598 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: desc /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3: 1
2020-12-03 07:22:12,598 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.unreference(FsVolumeImpl.java:249)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$100(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.close(FsVolumeImpl.java:276)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getStorageReports(FsDatasetImpl.java:171)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:495)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:648)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:849)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:12,598 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] DEBUG datanode.DataNode (BPServiceActor.java:sendHeartBeat(497)) - Sending heartbeat with 1 storage reports from service actor: Block pool BP-1723813095-172.17.0.10-1606980125651 (Datanode Uuid b518f08d-a98e-47d6-9057-a679367cc8bc) service to localhost/127.0.0.1:36555
2020-12-03 07:22:12,601 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] INFO  datanode.DataNode (BPServiceActor.java:offerService(698)) - Forcing a full block report to localhost/127.0.0.1:36555
2020-12-03 07:22:12,604 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x5f460ee949b6f7a6: from storage DS-c1dccd31-33e7-4e8c-aaa0-047e42da1cc4 node DatanodeRegistration(127.0.0.1:40715, datanodeUuid=b518f08d-a98e-47d6-9057-a679367cc8bc, infoPort=38815, infoSecurePort=0, ipcPort=34293, storageInfo=lv=-57;cid=testClusterID;nsid=156303761;c=1606980125651), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:22:12,606 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x5f460ee949b6f7a6,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 4 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:22:12,606 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1723813095-172.17.0.10-1606980125651
2020-12-03 07:22:12,697 [Listener at localhost/34293] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: incr /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 1
2020-12-03 07:22:12,698 [Listener at localhost/34293] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.reference(FsVolumeImpl.java:240)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$000(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.<init>(FsVolumeImpl.java:266)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.obtainReference(FsVolumeImpl.java:289)
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi$FsVolumeReferences.<init>(FsDatasetSpi.java:110)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getFsVolumeReferences(FsDatasetImpl.java:145)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestSpaceReservation.testReservedSpaceForPipelineRecovery(TestSpaceReservation.java:657)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-12-03 07:22:12,699 [Listener at localhost/34293] INFO  impl.TestSpaceReservation (TestSpaceReservation.java:get(662)) - dn 127.0.0.1:36884 space : 1048576
2020-12-03 07:22:12,802 [Listener at localhost/34293] INFO  impl.TestSpaceReservation (TestSpaceReservation.java:get(662)) - dn 127.0.0.1:36884 space : 1048576
2020-12-03 07:22:12,903 [Listener at localhost/34293] INFO  impl.TestSpaceReservation (TestSpaceReservation.java:get(662)) - dn 127.0.0.1:36884 space : 1048576
2020-12-03 07:22:13,003 [Listener at localhost/34293] INFO  impl.TestSpaceReservation (TestSpaceReservation.java:get(662)) - dn 127.0.0.1:36884 space : 1048576
2020-12-03 07:22:13,104 [Listener at localhost/34293] INFO  impl.TestSpaceReservation (TestSpaceReservation.java:get(662)) - dn 127.0.0.1:36884 space : 1048576
2020-12-03 07:22:13,204 [Listener at localhost/34293] INFO  impl.TestSpaceReservation (TestSpaceReservation.java:get(662)) - dn 127.0.0.1:36884 space : 1048576
2020-12-03 07:22:13,305 [Listener at localhost/34293] INFO  impl.TestSpaceReservation (TestSpaceReservation.java:get(662)) - dn 127.0.0.1:36884 space : 1048576
2020-12-03 07:22:13,405 [Listener at localhost/34293] INFO  impl.TestSpaceReservation (TestSpaceReservation.java:get(662)) - dn 127.0.0.1:36884 space : 1048576
2020-12-03 07:22:13,506 [Listener at localhost/34293] INFO  impl.TestSpaceReservation (TestSpaceReservation.java:get(662)) - dn 127.0.0.1:36884 space : 1048576
2020-12-03 07:22:13,610 [Listener at localhost/34293] INFO  impl.TestSpaceReservation (TestSpaceReservation.java:get(662)) - dn 127.0.0.1:36884 space : 1048576
2020-12-03 07:22:13,711 [Listener at localhost/34293] INFO  impl.TestSpaceReservation (TestSpaceReservation.java:get(662)) - dn 127.0.0.1:36884 space : 1048576
2020-12-03 07:22:13,811 [Listener at localhost/34293] INFO  impl.TestSpaceReservation (TestSpaceReservation.java:get(662)) - dn 127.0.0.1:36884 space : 1048576
2020-12-03 07:22:13,912 [Listener at localhost/34293] INFO  impl.TestSpaceReservation (TestSpaceReservation.java:get(662)) - dn 127.0.0.1:36884 space : 1048576
2020-12-03 07:22:14,012 [Listener at localhost/34293] INFO  impl.TestSpaceReservation (TestSpaceReservation.java:get(662)) - dn 127.0.0.1:36884 space : 1048576
2020-12-03 07:22:14,113 [Listener at localhost/34293] INFO  impl.TestSpaceReservation (TestSpaceReservation.java:get(662)) - dn 127.0.0.1:36884 space : 1048576
2020-12-03 07:22:14,213 [Listener at localhost/34293] INFO  impl.TestSpaceReservation (TestSpaceReservation.java:get(662)) - dn 127.0.0.1:36884 space : 1048576
2020-12-03 07:22:14,314 [Listener at localhost/34293] INFO  impl.TestSpaceReservation (TestSpaceReservation.java:get(662)) - dn 127.0.0.1:36884 space : 1048576
2020-12-03 07:22:14,414 [Listener at localhost/34293] INFO  impl.TestSpaceReservation (TestSpaceReservation.java:get(662)) - dn 127.0.0.1:36884 space : 1048576
2020-12-03 07:22:14,518 [Listener at localhost/34293] INFO  impl.TestSpaceReservation (TestSpaceReservation.java:get(662)) - dn 127.0.0.1:36884 space : 1048576
2020-12-03 07:22:14,622 [Listener at localhost/34293] INFO  impl.TestSpaceReservation (TestSpaceReservation.java:get(662)) - dn 127.0.0.1:36884 space : 1048576
2020-12-03 07:22:14,723 [Listener at localhost/34293] INFO  impl.TestSpaceReservation (TestSpaceReservation.java:get(662)) - dn 127.0.0.1:36884 space : 1048576
2020-12-03 07:22:14,823 [Listener at localhost/34293] INFO  impl.TestSpaceReservation (TestSpaceReservation.java:get(662)) - dn 127.0.0.1:36884 space : 1048576
2020-12-03 07:22:14,924 [Listener at localhost/34293] INFO  impl.TestSpaceReservation (TestSpaceReservation.java:get(662)) - dn 127.0.0.1:36884 space : 1048576
2020-12-03 07:22:15,024 [Listener at localhost/34293] INFO  impl.TestSpaceReservation (TestSpaceReservation.java:get(662)) - dn 127.0.0.1:36884 space : 1048576
2020-12-03 07:22:15,125 [Listener at localhost/34293] INFO  impl.TestSpaceReservation (TestSpaceReservation.java:get(662)) - dn 127.0.0.1:36884 space : 1048576
2020-12-03 07:22:15,225 [Listener at localhost/34293] INFO  impl.TestSpaceReservation (TestSpaceReservation.java:get(662)) - dn 127.0.0.1:36884 space : 1048576
2020-12-03 07:22:15,326 [Listener at localhost/34293] INFO  impl.TestSpaceReservation (TestSpaceReservation.java:get(662)) - dn 127.0.0.1:36884 space : 1048576
2020-12-03 07:22:15,397 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: incr /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 2
2020-12-03 07:22:15,398 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.reference(FsVolumeImpl.java:240)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$000(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.<init>(FsVolumeImpl.java:266)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.obtainReference(FsVolumeImpl.java:289)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getStorageReports(FsDatasetImpl.java:162)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:495)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:648)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:849)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:15,398 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: desc /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 2
2020-12-03 07:22:15,398 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.unreference(FsVolumeImpl.java:249)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$100(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.close(FsVolumeImpl.java:276)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getStorageReports(FsDatasetImpl.java:171)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:495)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:648)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:849)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:15,399 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] DEBUG datanode.DataNode (BPServiceActor.java:sendHeartBeat(497)) - Sending heartbeat with 1 storage reports from service actor: Block pool BP-1723813095-172.17.0.10-1606980125651 (Datanode Uuid 04aa09e8-2060-44fc-bf98-148164c6bc7c) service to localhost/127.0.0.1:36555
2020-12-03 07:22:15,413 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] DEBUG impl.FsDatasetImpl (FsDatasetImpl.java:invalidate(2081)) - Block file file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1723813095-172.17.0.10-1606980125651/current/rbw/blk_1073741825 is to be deleted
2020-12-03 07:22:15,416 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: incr /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 2
2020-12-03 07:22:15,416 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.reference(FsVolumeImpl.java:240)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$000(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.<init>(FsVolumeImpl.java:266)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.obtainReference(FsVolumeImpl.java:289)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.invalidate(FsDatasetImpl.java:2115)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.invalidate(FsDatasetImpl.java:2034)
org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromActive(BPOfferService.java:734)
org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromActor(BPOfferService.java:680)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.processCommand(BPServiceActor.java:883)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:678)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:849)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:15,416 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:deleteAsync(226)) - Scheduling blk_1073741825_1001 replica ReplicaBeingWritten, blk_1073741825_1001, RBW
  getNumBytes()     = 0
  getBytesOnDisk()  = 0
  getVisibleLength()= 0
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1723813095-172.17.0.10-1606980125651/current/rbw/blk_1073741825
  bytesAcked=0
  bytesOnDisk=0 for deletion
2020-12-03 07:22:15,419 [Async disk worker #0 for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:run(334)) - Deleted BP-1723813095-172.17.0.10-1606980125651 blk_1073741825_1001 URI file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1723813095-172.17.0.10-1606980125651/current/rbw/blk_1073741825
2020-12-03 07:22:15,419 [Async disk worker #0 for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: desc /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 2
2020-12-03 07:22:15,420 [Async disk worker #0 for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.unreference(FsVolumeImpl.java:249)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$100(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.close(FsVolumeImpl.java:276)
org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:258)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService$ReplicaFileDeleteTask.run(FsDatasetAsyncDiskService.java:338)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:15,426 [Listener at localhost/34293] INFO  impl.TestSpaceReservation (TestSpaceReservation.java:get(662)) - dn 127.0.0.1:36884 space : 0
2020-12-03 07:22:15,426 [Listener at localhost/34293] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: desc /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 1
2020-12-03 07:22:15,427 [Listener at localhost/34293] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.unreference(FsVolumeImpl.java:249)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$100(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.close(FsVolumeImpl.java:276)
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi$FsVolumeReferences.close(FsDatasetSpi.java:174)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestSpaceReservation.testReservedSpaceForPipelineRecovery(TestSpaceReservation.java:667)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-12-03 07:22:15,427 [Listener at localhost/34293] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: incr /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 1
2020-12-03 07:22:15,428 [Listener at localhost/34293] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.reference(FsVolumeImpl.java:240)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$000(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.<init>(FsVolumeImpl.java:266)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.obtainReference(FsVolumeImpl.java:289)
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi$FsVolumeReferences.<init>(FsDatasetSpi.java:110)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getFsVolumeReferences(FsDatasetImpl.java:145)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestSpaceReservation.testReservedSpaceForPipelineRecovery(TestSpaceReservation.java:657)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-12-03 07:22:15,428 [Listener at localhost/34293] INFO  impl.TestSpaceReservation (TestSpaceReservation.java:get(662)) - dn 127.0.0.1:34135 space : 0
2020-12-03 07:22:15,428 [Listener at localhost/34293] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: desc /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 1
2020-12-03 07:22:15,429 [Listener at localhost/34293] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.unreference(FsVolumeImpl.java:249)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$100(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.close(FsVolumeImpl.java:276)
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi$FsVolumeReferences.close(FsDatasetSpi.java:174)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestSpaceReservation.testReservedSpaceForPipelineRecovery(TestSpaceReservation.java:667)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-12-03 07:22:15,429 [Listener at localhost/34293] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: incr /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3: 1
2020-12-03 07:22:15,429 [Listener at localhost/34293] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.reference(FsVolumeImpl.java:240)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$000(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.<init>(FsVolumeImpl.java:266)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.obtainReference(FsVolumeImpl.java:289)
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi$FsVolumeReferences.<init>(FsDatasetSpi.java:110)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getFsVolumeReferences(FsDatasetImpl.java:145)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestSpaceReservation.testReservedSpaceForPipelineRecovery(TestSpaceReservation.java:657)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-12-03 07:22:15,430 [Listener at localhost/34293] INFO  impl.TestSpaceReservation (TestSpaceReservation.java:get(662)) - dn 127.0.0.1:40715 space : 0
2020-12-03 07:22:15,430 [Listener at localhost/34293] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: desc /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3: 1
2020-12-03 07:22:15,430 [Listener at localhost/34293] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.unreference(FsVolumeImpl.java:249)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$100(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.close(FsVolumeImpl.java:276)
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi$FsVolumeReferences.close(FsDatasetSpi.java:174)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestSpaceReservation.testReservedSpaceForPipelineRecovery(TestSpaceReservation.java:667)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-12-03 07:22:15,431 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(2049)) - Shutting down the Mini HDFS Cluster
2020-12-03 07:22:15,432 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 2
2020-12-03 07:22:15,433 [main] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:22:15,439 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@9431d06] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:22:15,497 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: incr /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 1
2020-12-03 07:22:15,498 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.reference(FsVolumeImpl.java:240)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$000(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.<init>(FsVolumeImpl.java:266)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.obtainReference(FsVolumeImpl.java:289)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getStorageReports(FsDatasetImpl.java:162)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:495)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:648)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:849)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:15,498 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(225)) - Reference count: desc /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 1
2020-12-03 07:22:15,498 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] TRACE impl.FsDatasetImpl (FsVolumeImpl.java:printReferenceTraceInfo(227)) - java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.printReferenceTraceInfo(FsVolumeImpl.java:228)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.unreference(FsVolumeImpl.java:249)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.access$100(FsVolumeImpl.java:102)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl.close(FsVolumeImpl.java:276)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getStorageReports(FsDatasetImpl.java:171)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:495)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:648)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:849)
java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:15,498 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] DEBUG datanode.DataNode (BPServiceActor.java:sendHeartBeat(497)) - Sending heartbeat with 1 storage reports from service actor: Block pool BP-1723813095-172.17.0.10-1606980125651 (Datanode Uuid df5771e4-65f1-4ee5-86ea-ae3a8b274cb3) service to localhost/127.0.0.1:36555
2020-12-03 07:22:15,512 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@27e71c23{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:22:15,522 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@499c8e70{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:22:15,525 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@cb2313a{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:22:15,528 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@34177a0c{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:22:15,532 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] INFO  datanode.DataNode (DataNode.java:transferBlock(2358)) - DatanodeRegistration(127.0.0.1:34135, datanodeUuid=df5771e4-65f1-4ee5-86ea-ae3a8b274cb3, infoPort=41079, infoSecurePort=0, ipcPort=45866, storageInfo=lv=-57;cid=testClusterID;nsid=156303761;c=1606980125651) Starting thread to transfer BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002 to 127.0.0.1:40715 
2020-12-03 07:22:15,538 [org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer@24ca9a69] DEBUG datanode.DataNode (DataNode.java:run(2527)) - Connecting to datanode 127.0.0.1:40715
2020-12-03 07:22:15,540 [main] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 34293
2020-12-03 07:22:15,544 [org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer@24ca9a69] WARN  datanode.DataNode (DataNode.java:run(2595)) - DatanodeRegistration(127.0.0.1:34135, datanodeUuid=df5771e4-65f1-4ee5-86ea-ae3a8b274cb3, infoPort=41079, infoSecurePort=0, ipcPort=45866, storageInfo=lv=-57;cid=testClusterID;nsid=156303761;c=1606980125651):Failed to transfer BP-1723813095-172.17.0.10-1606980125651:blk_1073741826_1002 to 127.0.0.1:40715 got
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:533)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:497)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2529)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:15,545 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:22:15,545 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:22:15,548 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:22:15,548 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1723813095-172.17.0.10-1606980125651 (Datanode Uuid b518f08d-a98e-47d6-9057-a679367cc8bc) service to localhost/127.0.0.1:36555
2020-12-03 07:22:15,548 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1723813095-172.17.0.10-1606980125651 (Datanode Uuid b518f08d-a98e-47d6-9057-a679367cc8bc)
2020-12-03 07:22:15,549 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1723813095-172.17.0.10-1606980125651
2020-12-03 07:22:15,550 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1723813095-172.17.0.10-1606980125651] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:15,552 [main] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:22:15,552 [main] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:22:15,553 [main] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:22:15,553 [main] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:22:15,558 [main] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:22:15,559 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 1
2020-12-03 07:22:15,560 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@5fe78640] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:22:15,560 [main] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:22:15,604 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@4f239add{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:22:15,605 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@7b54a74{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:22:15,606 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@793ca543{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:22:15,608 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@b149f15{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:22:15,613 [main] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 45866
2020-12-03 07:22:15,620 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:22:15,620 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:22:15,623 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:22:15,624 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1723813095-172.17.0.10-1606980125651 (Datanode Uuid df5771e4-65f1-4ee5-86ea-ae3a8b274cb3) service to localhost/127.0.0.1:36555
2020-12-03 07:22:15,624 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1723813095-172.17.0.10-1606980125651 (Datanode Uuid df5771e4-65f1-4ee5-86ea-ae3a8b274cb3)
2020-12-03 07:22:15,624 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1723813095-172.17.0.10-1606980125651
2020-12-03 07:22:15,626 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1723813095-172.17.0.10-1606980125651] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:15,628 [main] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:22:15,629 [main] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:22:15,629 [main] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:22:15,630 [main] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:22:15,631 [main] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:22:15,631 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 0
2020-12-03 07:22:15,632 [main] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:22:15,632 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@65b1d5cf] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:22:15,830 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@7d63cb72{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:22:15,831 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@31a4bd1d{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:22:15,833 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@a220320{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:22:15,835 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@5137d29e{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:22:15,845 [main] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 35379
2020-12-03 07:22:15,855 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:22:15,856 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:22:15,861 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:22:15,861 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1723813095-172.17.0.10-1606980125651 (Datanode Uuid 04aa09e8-2060-44fc-bf98-148164c6bc7c) service to localhost/127.0.0.1:36555
2020-12-03 07:22:15,962 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1723813095-172.17.0.10-1606980125651 (Datanode Uuid 04aa09e8-2060-44fc-bf98-148164c6bc7c)
2020-12-03 07:22:15,962 [BP-1723813095-172.17.0.10-1606980125651 heartbeating to localhost/127.0.0.1:36555] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1723813095-172.17.0.10-1606980125651
2020-12-03 07:22:15,967 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1723813095-172.17.0.10-1606980125651] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:15,967 [main] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:22:15,977 [main] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:22:15,979 [main] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:22:15,981 [main] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:22:15,984 [main] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:22:15,984 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:stopAndJoinNameNode(2130)) - Shutting down the namenode
2020-12-03 07:22:15,984 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1334)) - Stopping services started for active state
2020-12-03 07:22:15,986 [main] INFO  namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1410)) - Ending log segment 1, 10
2020-12-03 07:22:15,986 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@1f546957] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4198)) - LazyPersistFileScrubber was interrupted, exiting
2020-12-03 07:22:15,992 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@2e438ce8] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4107)) - NameNodeEditLogRoller was interrupted, exiting
2020-12-03 07:22:16,005 [main] INFO  namenode.FSEditLog (FSEditLog.java:printStatistics(778)) - Number of transactions: 11 Total time for transactions(ms): 28 Number of transactions batched in Syncs: 1 Number of syncs: 11 SyncTimes(ms): 4 1 
2020-12-03 07:22:16,010 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(145)) - Finalizing edits file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_inprogress_0000000000000000001 -> /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000001-0000000000000000011
2020-12-03 07:22:16,013 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(145)) - Finalizing edits file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_inprogress_0000000000000000001 -> /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000001-0000000000000000011
2020-12-03 07:22:16,014 [FSEditLogAsync] INFO  namenode.FSEditLog (FSEditLogAsync.java:run(253)) - FSEditLogAsync was interrupted, exiting
2020-12-03 07:22:16,020 [CacheReplicationMonitor(1675367542)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2020-12-03 07:22:16,025 [main] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 36555
2020-12-03 07:22:16,028 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:22:16,031 [RedundancyMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4687)) - Stopping RedundancyMonitor.
2020-12-03 07:22:16,031 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:22:16,033 [StorageInfoMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4722)) - Stopping thread.
2020-12-03 07:22:16,077 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1334)) - Stopping services started for active state
2020-12-03 07:22:16,077 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1434)) - Stopping services started for standby state
2020-12-03 07:22:16,095 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@157444bc{/,null,UNAVAILABLE}{/hdfs}
2020-12-03 07:22:16,115 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@7689437{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:22:16,120 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@431c1549{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:22:16,122 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@480e98{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:22:16,131 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2020-12-03 07:22:16,186 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2020-12-03 07:22:16,187 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - DataNode metrics system shutdown complete.
msx-rc 0
