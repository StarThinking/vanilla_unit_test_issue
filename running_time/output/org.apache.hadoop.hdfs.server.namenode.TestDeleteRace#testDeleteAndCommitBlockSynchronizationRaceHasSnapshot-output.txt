2020-12-03 07:22:46,228 [Thread-1] INFO  namenode.TestDeleteRace (TestDeleteRace.java:testDeleteAndCommitBlockSynchronizationRace(243)) - Start testing, hasSnapshot: true
2020-12-03 07:22:46,499 [Thread-1] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(493)) - starting cluster: numNameNodes=1, numDataNodes=3
Formatting using clusterid: testClusterID
2020-12-03 07:22:47,292 [Thread-1] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(229)) - Edit logging is async:true
2020-12-03 07:22:47,311 [Thread-1] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(755)) - KeyProvider: null
2020-12-03 07:22:47,312 [Thread-1] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(123)) - fsLock is fair: true
2020-12-03 07:22:47,313 [Thread-1] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(141)) - Detailed lock hold time metrics enabled: false
2020-12-03 07:22:47,321 [Thread-1] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(780)) - fsOwner             = root (auth:SIMPLE)
2020-12-03 07:22:47,321 [Thread-1] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(781)) - supergroup          = supergroup
2020-12-03 07:22:47,321 [Thread-1] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(782)) - isPermissionEnabled = false
2020-12-03 07:22:47,322 [Thread-1] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(793)) - HA Enabled: false
2020-12-03 07:22:47,379 [Thread-1] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:22:47,385 [Thread-1] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - hadoop.configured.node.mapping is deprecated. Instead, use net.topology.configured.node.mapping
2020-12-03 07:22:47,385 [Thread-1] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(303)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-12-03 07:22:47,385 [Thread-1] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(311)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-12-03 07:22:47,391 [Thread-1] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-12-03 07:22:47,392 [Thread-1] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Dec 03 07:22:47
2020-12-03 07:22:47,396 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-12-03 07:22:47,396 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:22:47,398 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.9 GB = 39.3 MB
2020-12-03 07:22:47,398 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-12-03 07:22:47,419 [Thread-1] INFO  blockmanagement.BlockManager (BlockManager.java:createSPSManager(5183)) - Storage policy satisfier is disabled
2020-12-03 07:22:47,419 [Thread-1] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(595)) - dfs.block.access.token.enable = false
2020-12-03 07:22:47,426 [Thread-1] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-12-03 07:22:47,427 [Thread-1] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-12-03 07:22:47,427 [Thread-1] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-12-03 07:22:47,427 [Thread-1] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-12-03 07:22:47,428 [Thread-1] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(581)) - defaultReplication         = 3
2020-12-03 07:22:47,428 [Thread-1] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(582)) - maxReplication             = 512
2020-12-03 07:22:47,428 [Thread-1] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(583)) - minReplication             = 1
2020-12-03 07:22:47,428 [Thread-1] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(584)) - maxReplicationStreams      = 2
2020-12-03 07:22:47,429 [Thread-1] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(585)) - redundancyRecheckInterval  = 3000ms
2020-12-03 07:22:47,429 [Thread-1] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(586)) - encryptDataTransfer        = false
2020-12-03 07:22:47,429 [Thread-1] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(587)) - maxNumBlocksToLog          = 1000
2020-12-03 07:22:47,468 [Thread-1] INFO  namenode.FSDirectory (SerialNumberManager.java:<clinit>(51)) - GLOBAL serial map: bits=29 maxEntries=536870911
2020-12-03 07:22:47,468 [Thread-1] INFO  namenode.FSDirectory (SerialNumberManager.java:<clinit>(51)) - USER serial map: bits=24 maxEntries=16777215
2020-12-03 07:22:47,468 [Thread-1] INFO  namenode.FSDirectory (SerialNumberManager.java:<clinit>(51)) - GROUP serial map: bits=24 maxEntries=16777215
2020-12-03 07:22:47,468 [Thread-1] INFO  namenode.FSDirectory (SerialNumberManager.java:<clinit>(51)) - XATTR serial map: bits=24 maxEntries=16777215
2020-12-03 07:22:47,485 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-12-03 07:22:47,485 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:22:47,486 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.9 GB = 19.6 MB
2020-12-03 07:22:47,486 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-12-03 07:22:47,492 [Thread-1] INFO  namenode.FSDirectory (FSDirectory.java:<init>(290)) - ACLs enabled? false
2020-12-03 07:22:47,493 [Thread-1] INFO  namenode.FSDirectory (FSDirectory.java:<init>(294)) - POSIX ACL inheritance enabled? true
2020-12-03 07:22:47,493 [Thread-1] INFO  namenode.FSDirectory (FSDirectory.java:<init>(298)) - XAttrs enabled? true
2020-12-03 07:22:47,493 [Thread-1] INFO  namenode.NameNode (FSDirectory.java:<init>(362)) - Caching file names occurring more than 10 times
2020-12-03 07:22:47,499 [Thread-1] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-12-03 07:22:47,502 [Thread-1] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-12-03 07:22:47,506 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-12-03 07:22:47,507 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:22:47,507 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.9 GB = 4.9 MB
2020-12-03 07:22:47,507 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-12-03 07:22:47,518 [Thread-1] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-12-03 07:22:47,519 [Thread-1] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-12-03 07:22:47,519 [Thread-1] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-12-03 07:22:47,523 [Thread-1] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(996)) - Retry cache on namenode is enabled
2020-12-03 07:22:47,524 [Thread-1] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(1004)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-12-03 07:22:47,527 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-12-03 07:22:47,527 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:22:47,527 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.9 GB = 603.0 KB
2020-12-03 07:22:47,528 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-12-03 07:22:47,569 [Thread-1] INFO  namenode.FSImage (FSImage.java:format(185)) - Allocated new BlockPoolId: BP-279225940-172.17.0.11-1606980167551
2020-12-03 07:22:47,822 [Thread-1] INFO  common.Storage (NNStorage.java:format(595)) - Storage directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1 has been successfully formatted.
2020-12-03 07:22:48,039 [Thread-1] INFO  common.Storage (NNStorage.java:format(595)) - Storage directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2 has been successfully formatted.
2020-12-03 07:22:48,076 [FSImageSaver for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(512)) - Saving image file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage.ckpt_0000000000000000000 using no compression
2020-12-03 07:22:48,076 [FSImageSaver for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(512)) - Saving image file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/fsimage.ckpt_0000000000000000000 using no compression
2020-12-03 07:22:48,221 [FSImageSaver for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(516)) - Image file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .
2020-12-03 07:22:48,221 [FSImageSaver for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(516)) - Image file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .
2020-12-03 07:22:48,338 [Thread-1] INFO  namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:getImageTxIdToRetain(203)) - Going to retain 1 images with txid >= 0
2020-12-03 07:22:48,343 [Thread-1] INFO  namenode.NameNode (NameNode.java:createNameNode(1632)) - createNameNode []
2020-12-03 07:22:48,468 [Thread-1] INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(118)) - Loaded properties from hadoop-metrics2.properties
2020-12-03 07:22:48,784 [Thread-1] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 0 second(s).
2020-12-03 07:22:48,784 [Thread-1] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2020-12-03 07:22:48,818 [Thread-1] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://127.0.0.1:0
2020-12-03 07:22:48,876 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@37b453e5] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:22:48,894 [Thread-1] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1641)) - Starting Web-server for hdfs at: http://localhost:0
2020-12-03 07:22:48,900 [Thread-1] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:22:48,922 [Thread-1] INFO  util.log (Log.java:initialized(192)) - Logging initialized @3482ms
2020-12-03 07:22:49,100 [Thread-1] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:22:49,104 [Thread-1] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-12-03 07:22:49,105 [Thread-1] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:22:49,116 [Thread-1] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:22:49,119 [Thread-1] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-12-03 07:22:49,120 [Thread-1] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:22:49,120 [Thread-1] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:22:49,159 [Thread-1] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(102)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-12-03 07:22:49,160 [Thread-1] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(806)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-12-03 07:22:49,172 [Thread-1] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 41335
2020-12-03 07:22:49,176 [Thread-1] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:22:49,233 [Thread-1] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@10fc84d4{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:22:49,235 [Thread-1] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@5f7ad052{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:22:49,290 [Thread-1] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@689a2f9{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-12-03 07:22:49,300 [Thread-1] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@7f867186{HTTP/1.1,[http/1.1]}{localhost:41335}
2020-12-03 07:22:49,301 [Thread-1] INFO  server.Server (Server.java:doStart(419)) - Started @3861ms
2020-12-03 07:22:49,315 [Thread-1] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(229)) - Edit logging is async:true
2020-12-03 07:22:49,316 [Thread-1] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(755)) - KeyProvider: null
2020-12-03 07:22:49,316 [Thread-1] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(123)) - fsLock is fair: true
2020-12-03 07:22:49,316 [Thread-1] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(141)) - Detailed lock hold time metrics enabled: false
2020-12-03 07:22:49,317 [Thread-1] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(780)) - fsOwner             = root (auth:SIMPLE)
2020-12-03 07:22:49,317 [Thread-1] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(781)) - supergroup          = supergroup
2020-12-03 07:22:49,317 [Thread-1] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(782)) - isPermissionEnabled = false
2020-12-03 07:22:49,318 [Thread-1] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(793)) - HA Enabled: false
2020-12-03 07:22:49,319 [Thread-1] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:22:49,320 [Thread-1] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(303)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-12-03 07:22:49,320 [Thread-1] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(311)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-12-03 07:22:49,321 [Thread-1] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-12-03 07:22:49,321 [Thread-1] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Dec 03 07:22:49
2020-12-03 07:22:49,322 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-12-03 07:22:49,322 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:22:49,323 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.8 GB = 36.4 MB
2020-12-03 07:22:49,323 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-12-03 07:22:49,333 [Thread-1] INFO  blockmanagement.BlockManager (BlockManager.java:createSPSManager(5183)) - Storage policy satisfier is disabled
2020-12-03 07:22:49,334 [Thread-1] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(595)) - dfs.block.access.token.enable = false
2020-12-03 07:22:49,334 [Thread-1] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-12-03 07:22:49,335 [Thread-1] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-12-03 07:22:49,335 [Thread-1] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-12-03 07:22:49,335 [Thread-1] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-12-03 07:22:49,336 [Thread-1] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(581)) - defaultReplication         = 3
2020-12-03 07:22:49,336 [Thread-1] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(582)) - maxReplication             = 512
2020-12-03 07:22:49,336 [Thread-1] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(583)) - minReplication             = 1
2020-12-03 07:22:49,337 [Thread-1] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(584)) - maxReplicationStreams      = 2
2020-12-03 07:22:49,337 [Thread-1] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(585)) - redundancyRecheckInterval  = 3000ms
2020-12-03 07:22:49,337 [Thread-1] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(586)) - encryptDataTransfer        = false
2020-12-03 07:22:49,337 [Thread-1] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(587)) - maxNumBlocksToLog          = 1000
2020-12-03 07:22:49,338 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-12-03 07:22:49,338 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:22:49,339 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.8 GB = 18.2 MB
2020-12-03 07:22:49,339 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-12-03 07:22:49,343 [Thread-1] INFO  namenode.FSDirectory (FSDirectory.java:<init>(290)) - ACLs enabled? false
2020-12-03 07:22:49,344 [Thread-1] INFO  namenode.FSDirectory (FSDirectory.java:<init>(294)) - POSIX ACL inheritance enabled? true
2020-12-03 07:22:49,344 [Thread-1] INFO  namenode.FSDirectory (FSDirectory.java:<init>(298)) - XAttrs enabled? true
2020-12-03 07:22:49,344 [Thread-1] INFO  namenode.NameNode (FSDirectory.java:<init>(362)) - Caching file names occurring more than 10 times
2020-12-03 07:22:49,345 [Thread-1] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-12-03 07:22:49,345 [Thread-1] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-12-03 07:22:49,345 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-12-03 07:22:49,345 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:22:49,346 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.8 GB = 4.6 MB
2020-12-03 07:22:49,346 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-12-03 07:22:49,347 [Thread-1] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-12-03 07:22:49,348 [Thread-1] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-12-03 07:22:49,348 [Thread-1] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-12-03 07:22:49,348 [Thread-1] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(996)) - Retry cache on namenode is enabled
2020-12-03 07:22:49,348 [Thread-1] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(1004)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-12-03 07:22:49,348 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-12-03 07:22:49,348 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:22:49,349 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.8 GB = 559.3 KB
2020-12-03 07:22:49,349 [Thread-1] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-12-03 07:22:49,495 [Thread-1] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/in_use.lock acquired by nodename 4867@1b16378f84d5
2020-12-03 07:22:49,605 [Thread-1] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/in_use.lock acquired by nodename 4867@1b16378f84d5
2020-12-03 07:22:49,610 [Thread-1] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(428)) - Recovering unfinalized segments in /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current
2020-12-03 07:22:49,611 [Thread-1] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(428)) - Recovering unfinalized segments in /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current
2020-12-03 07:22:49,612 [Thread-1] INFO  namenode.FSImage (FSImage.java:loadFSImage(733)) - No edit log streams selected.
2020-12-03 07:22:49,612 [Thread-1] INFO  namenode.FSImage (FSImage.java:loadFSImageFile(797)) - Planning to load image: FSImageFile(file=/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2020-12-03 07:22:49,647 [Thread-1] INFO  namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(234)) - Loading 1 INodes.
2020-12-03 07:22:49,654 [Thread-1] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(246)) - Loaded FSImage in 0 seconds.
2020-12-03 07:22:49,655 [Thread-1] INFO  namenode.FSImage (FSImage.java:loadFSImage(978)) - Loaded image for txid 0 from /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000000
2020-12-03 07:22:49,660 [Thread-1] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1110)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-12-03 07:22:49,662 [Thread-1] INFO  namenode.FSEditLog (FSEditLog.java:startLogSegment(1365)) - Starting log segment at 1
2020-12-03 07:22:49,920 [Thread-1] INFO  namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2020-12-03 07:22:49,920 [Thread-1] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(727)) - Finished loading FSImage in 568 msecs
2020-12-03 07:22:50,121 [Thread-1] INFO  namenode.NameNode (NameNodeRpcServer.java:<init>(448)) - RPC server is binding to localhost:0
2020-12-03 07:22:50,169 [Thread-1] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:22:50,185 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:22:50,514 [Listener at localhost/37371] INFO  namenode.NameNode (NameNode.java:initialize(722)) - Clients are to use localhost:37371 to access this namenode/service.
2020-12-03 07:22:50,517 [Listener at localhost/37371] INFO  namenode.FSNamesystem (FSNamesystem.java:registerMBean(5090)) - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2020-12-03 07:22:50,541 [Listener at localhost/37371] INFO  namenode.LeaseManager (LeaseManager.java:getNumUnderConstructionBlocks(171)) - Number of blocks under construction: 0
2020-12-03 07:22:50,559 [Listener at localhost/37371] INFO  blockmanagement.BlockManager (BlockManager.java:initializeReplQueues(4922)) - initializing replication queues
2020-12-03 07:22:50,563 [Listener at localhost/37371] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(400)) - STATE* Leaving safe mode after 0 secs
2020-12-03 07:22:50,564 [Listener at localhost/37371] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(406)) - STATE* Network topology has 0 racks and 0 datanodes
2020-12-03 07:22:50,564 [Listener at localhost/37371] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(408)) - STATE* UnderReplicatedBlocks has 0 blocks
2020-12-03 07:22:50,569 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3585)) - Total number of blocks            = 0
2020-12-03 07:22:50,570 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3586)) - Number of invalid blocks          = 0
2020-12-03 07:22:50,570 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3587)) - Number of under-replicated blocks = 0
2020-12-03 07:22:50,570 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3588)) - Number of  over-replicated blocks = 0
2020-12-03 07:22:50,570 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3590)) - Number of blocks being written    = 0
2020-12-03 07:22:50,571 [Reconstruction Queue Initializer] INFO  hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(3593)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 8 msec
2020-12-03 07:22:50,611 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:22:50,611 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:22:50,629 [Listener at localhost/37371] INFO  namenode.NameNode (NameNode.java:startCommonServices(828)) - NameNode RPC up at: localhost/127.0.0.1:37371
2020-12-03 07:22:50,651 [Listener at localhost/37371] INFO  namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1222)) - Starting services required for active state
2020-12-03 07:22:50,652 [Listener at localhost/37371] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(777)) - Initializing quota with 4 thread(s)
2020-12-03 07:22:50,689 [Listener at localhost/37371] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(786)) - Quota initialization completed in 37 milliseconds
name space=1
storage space=0
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2020-12-03 07:22:50,701 [CacheReplicationMonitor(79752299)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-12-03 07:22:50,712 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1,[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:22:50,806 [Listener at localhost/37371] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:22:50,825 [Listener at localhost/37371] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:22:50,846 [Listener at localhost/37371] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:22:50,852 [Listener at localhost/37371] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:22:50,855 [Listener at localhost/37371] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:22:50,860 [Listener at localhost/37371] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:22:50,861 [Listener at localhost/37371] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:22:50,867 [Listener at localhost/37371] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:22:50,875 [Listener at localhost/37371] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:42073
2020-12-03 07:22:50,878 [Listener at localhost/37371] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:22:50,879 [Listener at localhost/37371] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:22:50,908 [Listener at localhost/37371] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:22:50,911 [Listener at localhost/37371] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:22:50,912 [Listener at localhost/37371] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:22:50,913 [Listener at localhost/37371] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:22:50,917 [Listener at localhost/37371] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:22:50,918 [Listener at localhost/37371] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:22:50,918 [Listener at localhost/37371] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:22:50,918 [Listener at localhost/37371] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:22:50,922 [Listener at localhost/37371] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 38397
2020-12-03 07:22:50,922 [Listener at localhost/37371] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:22:50,925 [Listener at localhost/37371] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@2ea7b934{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:22:50,926 [Listener at localhost/37371] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@233ca3ea{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:22:50,940 [Listener at localhost/37371] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@5a1ec39c{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:22:50,942 [Listener at localhost/37371] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@116522b2{HTTP/1.1,[http/1.1]}{localhost:38397}
2020-12-03 07:22:50,943 [Listener at localhost/37371] INFO  server.Server (Server.java:doStart(419)) - Started @5503ms
2020-12-03 07:22:51,355 [Listener at localhost/37371] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:45021
2020-12-03 07:22:51,357 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@37d0c922] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:22:51,358 [Listener at localhost/37371] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:22:51,359 [Listener at localhost/37371] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:22:51,379 [Listener at localhost/37371] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:22:51,380 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:22:51,393 [Listener at localhost/46834] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:46834
2020-12-03 07:22:51,690 [Listener at localhost/46834] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:22:51,692 [Listener at localhost/46834] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:22:51,704 [Thread-61] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:37371 starting to offer service
2020-12-03 07:22:51,712 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:22:51,712 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:22:51,725 [Listener at localhost/46834] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 1 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3,[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:22:51,729 [Listener at localhost/46834] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
2020-12-03 07:22:51,730 [Listener at localhost/46834] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:22:51,735 [Listener at localhost/46834] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:22:51,735 [Listener at localhost/46834] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:22:51,736 [Listener at localhost/46834] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:22:51,737 [Listener at localhost/46834] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:22:51,737 [Listener at localhost/46834] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:22:51,737 [Listener at localhost/46834] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:22:51,738 [Listener at localhost/46834] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:45120
2020-12-03 07:22:51,739 [Listener at localhost/46834] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:22:51,739 [Listener at localhost/46834] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:22:51,741 [Listener at localhost/46834] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:22:51,744 [Listener at localhost/46834] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:22:51,745 [Listener at localhost/46834] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:22:51,745 [Listener at localhost/46834] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:22:51,749 [Listener at localhost/46834] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:22:51,750 [Listener at localhost/46834] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:22:51,750 [Listener at localhost/46834] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:22:51,750 [Listener at localhost/46834] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:22:51,752 [Listener at localhost/46834] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 34109
2020-12-03 07:22:51,752 [Listener at localhost/46834] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:22:51,756 [Listener at localhost/46834] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@38bac53e{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:22:51,759 [Listener at localhost/46834] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@e6a4f6e{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:22:51,773 [Listener at localhost/46834] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@175319e7{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:22:51,775 [Listener at localhost/46834] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@4549f1d3{HTTP/1.1,[http/1.1]}{localhost:34109}
2020-12-03 07:22:51,776 [Listener at localhost/46834] INFO  server.Server (Server.java:doStart(419)) - Started @6336ms
2020-12-03 07:22:51,869 [Listener at localhost/46834] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:41729
2020-12-03 07:22:51,870 [Listener at localhost/46834] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:22:51,870 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@2d4be38d] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:22:51,870 [Listener at localhost/46834] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:22:51,871 [Listener at localhost/46834] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:22:51,872 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:22:51,876 [Listener at localhost/44735] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:44735
2020-12-03 07:22:51,887 [Listener at localhost/44735] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:22:51,888 [Listener at localhost/44735] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:22:51,889 [Thread-85] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:37371 starting to offer service
2020-12-03 07:22:51,892 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:22:51,892 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:22:51,895 [Listener at localhost/44735] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 2 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5,[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6
2020-12-03 07:22:51,903 [Listener at localhost/44735] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5
2020-12-03 07:22:51,906 [Listener at localhost/44735] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6
2020-12-03 07:22:51,912 [Listener at localhost/44735] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:22:51,913 [Listener at localhost/44735] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:22:51,913 [Listener at localhost/44735] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:22:51,914 [Listener at localhost/44735] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:22:51,915 [Listener at localhost/44735] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:22:51,915 [Listener at localhost/44735] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:22:51,917 [Listener at localhost/44735] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:37424
2020-12-03 07:22:51,917 [Listener at localhost/44735] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:22:51,917 [Listener at localhost/44735] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:22:51,919 [Listener at localhost/44735] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:22:51,922 [Listener at localhost/44735] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:22:51,923 [Listener at localhost/44735] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:22:51,924 [Listener at localhost/44735] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:22:51,927 [Listener at localhost/44735] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:22:51,929 [Listener at localhost/44735] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:22:51,929 [Listener at localhost/44735] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:22:51,930 [Listener at localhost/44735] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:22:51,931 [Listener at localhost/44735] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 35060
2020-12-03 07:22:51,931 [Listener at localhost/44735] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:22:51,934 [Listener at localhost/44735] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@145d1600{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:22:51,935 [Listener at localhost/44735] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@79ea3dc6{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:22:51,945 [Listener at localhost/44735] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@4b6eb0c5{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:22:51,946 [Listener at localhost/44735] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@2fbc5263{HTTP/1.1,[http/1.1]}{localhost:35060}
2020-12-03 07:22:51,947 [Listener at localhost/44735] INFO  server.Server (Server.java:doStart(419)) - Started @6507ms
2020-12-03 07:22:51,992 [Listener at localhost/44735] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:42090
2020-12-03 07:22:51,993 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@f5dda4f] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:22:51,993 [Listener at localhost/44735] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:22:51,994 [Listener at localhost/44735] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:22:51,995 [Listener at localhost/44735] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:22:51,996 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:22:52,003 [Listener at localhost/42873] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:42873
2020-12-03 07:22:52,024 [Listener at localhost/42873] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:22:52,024 [Listener at localhost/42873] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:22:52,025 [Thread-107] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:37371 starting to offer service
2020-12-03 07:22:52,028 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:22:52,029 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:22:52,149 [Thread-61] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:37371
2020-12-03 07:22:52,149 [Thread-85] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:37371
2020-12-03 07:22:52,149 [Thread-107] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:37371
2020-12-03 07:22:52,151 [Thread-61] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:22:52,151 [Thread-107] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:22:52,151 [Thread-85] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:22:52,235 [Thread-85] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/in_use.lock acquired by nodename 4867@1b16378f84d5
2020-12-03 07:22:52,235 [Thread-107] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/in_use.lock acquired by nodename 4867@1b16378f84d5
2020-12-03 07:22:52,235 [Thread-61] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/in_use.lock acquired by nodename 4867@1b16378f84d5
2020-12-03 07:22:52,236 [Thread-107] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5 is not formatted for namespace 1405290069. Formatting...
2020-12-03 07:22:52,236 [Thread-61] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1 is not formatted for namespace 1405290069. Formatting...
2020-12-03 07:22:52,238 [Thread-85] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3 is not formatted for namespace 1405290069. Formatting...
2020-12-03 07:22:52,238 [Thread-61] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-2ce793de-7ca9-4eff-a497-239e428e643d for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1 
2020-12-03 07:22:52,238 [Thread-85] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-5d75b3b6-c3c3-415c-9506-a037c96d984c for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3 
2020-12-03 07:22:52,239 [Thread-107] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-0c4ace5c-737b-46b0-9a95-af6b01d0e34c for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5 
2020-12-03 07:22:52,574 [Thread-85] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/in_use.lock acquired by nodename 4867@1b16378f84d5
2020-12-03 07:22:52,574 [IPC Server handler 7 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:22:52,575 [Thread-85] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4 is not formatted for namespace 1405290069. Formatting...
2020-12-03 07:22:52,575 [Thread-85] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-3024a9be-1033-48db-8cf1-7792eec124ef for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4 
2020-12-03 07:22:52,577 [Thread-61] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/in_use.lock acquired by nodename 4867@1b16378f84d5
2020-12-03 07:22:52,578 [Thread-107] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/in_use.lock acquired by nodename 4867@1b16378f84d5
2020-12-03 07:22:52,579 [Thread-107] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6 is not formatted for namespace 1405290069. Formatting...
2020-12-03 07:22:52,579 [Thread-107] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-ff826779-4412-445b-8f25-01fd55cacd1d for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6 
2020-12-03 07:22:52,584 [Listener at localhost/42873] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:22:52,592 [Listener at localhost/42873] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:22:52,592 [Thread-61] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2 is not formatted for namespace 1405290069. Formatting...
2020-12-03 07:22:52,593 [Thread-61] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-4d64cd8d-286f-4d38-9cd8-eaa887a5ffa1 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2 
2020-12-03 07:22:52,694 [IPC Server handler 8 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:22:52,696 [Listener at localhost/42873] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:22:52,696 [Listener at localhost/42873] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:22:52,716 [Thread-85] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-279225940-172.17.0.11-1606980167551
2020-12-03 07:22:52,717 [Thread-85] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-279225940-172.17.0.11-1606980167551
2020-12-03 07:22:52,717 [Thread-85] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3 and block pool id BP-279225940-172.17.0.11-1606980167551 is not formatted. Formatting ...
2020-12-03 07:22:52,718 [Thread-85] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-279225940-172.17.0.11-1606980167551 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-279225940-172.17.0.11-1606980167551/current
2020-12-03 07:22:52,765 [Thread-107] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-279225940-172.17.0.11-1606980167551
2020-12-03 07:22:52,765 [Thread-107] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-279225940-172.17.0.11-1606980167551
2020-12-03 07:22:52,765 [Thread-61] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-279225940-172.17.0.11-1606980167551
2020-12-03 07:22:52,765 [Thread-107] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5 and block pool id BP-279225940-172.17.0.11-1606980167551 is not formatted. Formatting ...
2020-12-03 07:22:52,766 [Thread-61] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-279225940-172.17.0.11-1606980167551
2020-12-03 07:22:52,766 [Thread-107] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-279225940-172.17.0.11-1606980167551 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-279225940-172.17.0.11-1606980167551/current
2020-12-03 07:22:52,766 [Thread-61] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1 and block pool id BP-279225940-172.17.0.11-1606980167551 is not formatted. Formatting ...
2020-12-03 07:22:52,766 [Thread-61] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-279225940-172.17.0.11-1606980167551 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-279225940-172.17.0.11-1606980167551/current
2020-12-03 07:22:52,799 [IPC Server handler 1 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:22:52,800 [Listener at localhost/42873] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:22:52,800 [Listener at localhost/42873] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:22:52,910 [IPC Server handler 9 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:22:52,911 [Listener at localhost/42873] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:22:52,911 [Listener at localhost/42873] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:22:52,912 [Thread-85] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-279225940-172.17.0.11-1606980167551
2020-12-03 07:22:52,912 [Thread-85] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-279225940-172.17.0.11-1606980167551
2020-12-03 07:22:52,912 [Thread-85] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4 and block pool id BP-279225940-172.17.0.11-1606980167551 is not formatted. Formatting ...
2020-12-03 07:22:52,913 [Thread-85] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-279225940-172.17.0.11-1606980167551 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-279225940-172.17.0.11-1606980167551/current
2020-12-03 07:22:52,928 [Thread-107] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-279225940-172.17.0.11-1606980167551
2020-12-03 07:22:52,928 [Thread-107] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/current/BP-279225940-172.17.0.11-1606980167551
2020-12-03 07:22:52,929 [Thread-107] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6 and block pool id BP-279225940-172.17.0.11-1606980167551 is not formatted. Formatting ...
2020-12-03 07:22:52,929 [Thread-107] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-279225940-172.17.0.11-1606980167551 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/current/BP-279225940-172.17.0.11-1606980167551/current
2020-12-03 07:22:52,933 [Thread-61] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-279225940-172.17.0.11-1606980167551
2020-12-03 07:22:52,934 [Thread-61] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-279225940-172.17.0.11-1606980167551
2020-12-03 07:22:52,934 [Thread-61] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2 and block pool id BP-279225940-172.17.0.11-1606980167551 is not formatted. Formatting ...
2020-12-03 07:22:52,934 [Thread-61] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-279225940-172.17.0.11-1606980167551 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-279225940-172.17.0.11-1606980167551/current
2020-12-03 07:22:53,014 [IPC Server handler 7 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:22:53,015 [Listener at localhost/42873] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:22:53,015 [Listener at localhost/42873] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:22:53,077 [Thread-85] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=1405290069;bpid=BP-279225940-172.17.0.11-1606980167551;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=1405290069;c=1606980167551;bpid=BP-279225940-172.17.0.11-1606980167551;dnuuid=null
2020-12-03 07:22:53,113 [Thread-61] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=1405290069;bpid=BP-279225940-172.17.0.11-1606980167551;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=1405290069;c=1606980167551;bpid=BP-279225940-172.17.0.11-1606980167551;dnuuid=null
2020-12-03 07:22:53,113 [Thread-107] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=1405290069;bpid=BP-279225940-172.17.0.11-1606980167551;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=1405290069;c=1606980167551;bpid=BP-279225940-172.17.0.11-1606980167551;dnuuid=null
2020-12-03 07:22:53,118 [IPC Server handler 6 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:22:53,119 [Listener at localhost/42873] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:22:53,119 [Listener at localhost/42873] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:22:53,221 [IPC Server handler 5 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:22:53,223 [Listener at localhost/42873] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:22:53,223 [Listener at localhost/42873] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:22:53,267 [Thread-85] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID a9f6fc87-7b25-48f4-8425-d3620813d33e
2020-12-03 07:22:53,303 [Thread-107] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID a4d0b9c3-5325-4729-a835-ddff222c3d3f
2020-12-03 07:22:53,304 [Thread-61] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID 06f17149-3c92-4fc6-af33-7ac1a1270c19
2020-12-03 07:22:53,325 [IPC Server handler 4 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:22:53,326 [Listener at localhost/42873] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:22:53,326 [Listener at localhost/42873] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:22:53,392 [Thread-85] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-5d75b3b6-c3c3-415c-9506-a037c96d984c
2020-12-03 07:22:53,392 [Thread-107] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-0c4ace5c-737b-46b0-9a95-af6b01d0e34c
2020-12-03 07:22:53,392 [Thread-61] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-2ce793de-7ca9-4eff-a497-239e428e643d
2020-12-03 07:22:53,394 [Thread-107] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5, StorageType: DISK
2020-12-03 07:22:53,393 [Thread-85] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3, StorageType: DISK
2020-12-03 07:22:53,394 [Thread-61] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, StorageType: DISK
2020-12-03 07:22:53,396 [Thread-107] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-ff826779-4412-445b-8f25-01fd55cacd1d
2020-12-03 07:22:53,409 [Thread-107] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6, StorageType: DISK
2020-12-03 07:22:53,411 [Thread-85] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-3024a9be-1033-48db-8cf1-7792eec124ef
2020-12-03 07:22:53,411 [Thread-85] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, StorageType: DISK
2020-12-03 07:22:53,409 [Thread-61] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-4d64cd8d-286f-4d38-9cd8-eaa887a5ffa1
2020-12-03 07:22:53,413 [Thread-61] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, StorageType: DISK
2020-12-03 07:22:53,429 [IPC Server handler 3 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:22:53,431 [Listener at localhost/42873] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:22:53,431 [Listener at localhost/42873] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:22:53,432 [Thread-107] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:22:53,432 [Thread-61] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:22:53,432 [Thread-85] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:22:53,441 [Thread-107] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5
2020-12-03 07:22:53,442 [Thread-85] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
2020-12-03 07:22:53,443 [Thread-61] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:22:53,459 [Thread-107] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5
2020-12-03 07:22:53,459 [Thread-61] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:22:53,459 [Thread-85] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
2020-12-03 07:22:53,461 [Thread-61] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:22:53,461 [Thread-107] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6
2020-12-03 07:22:53,461 [Thread-85] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:22:53,462 [Thread-107] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6
2020-12-03 07:22:53,462 [Thread-61] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:22:53,462 [Thread-85] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:22:53,463 [Thread-61] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-279225940-172.17.0.11-1606980167551
2020-12-03 07:22:53,463 [Thread-85] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-279225940-172.17.0.11-1606980167551
2020-12-03 07:22:53,463 [Thread-107] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-279225940-172.17.0.11-1606980167551
2020-12-03 07:22:53,464 [Thread-129] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-279225940-172.17.0.11-1606980167551 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2020-12-03 07:22:53,464 [Thread-128] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-279225940-172.17.0.11-1606980167551 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5...
2020-12-03 07:22:53,464 [Thread-127] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-279225940-172.17.0.11-1606980167551 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3...
2020-12-03 07:22:53,464 [Thread-130] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-279225940-172.17.0.11-1606980167551 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2020-12-03 07:22:53,464 [Thread-131] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-279225940-172.17.0.11-1606980167551 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6...
2020-12-03 07:22:53,464 [Thread-132] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-279225940-172.17.0.11-1606980167551 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4...
2020-12-03 07:22:53,512 [Thread-130] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-279225940-172.17.0.11-1606980167551 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 48ms
2020-12-03 07:22:53,512 [Thread-132] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-279225940-172.17.0.11-1606980167551 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4: 47ms
2020-12-03 07:22:53,519 [Thread-128] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-279225940-172.17.0.11-1606980167551 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5: 55ms
2020-12-03 07:22:53,521 [Thread-129] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-279225940-172.17.0.11-1606980167551 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 57ms
2020-12-03 07:22:53,522 [Thread-61] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-279225940-172.17.0.11-1606980167551: 59ms
2020-12-03 07:22:53,523 [Thread-127] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-279225940-172.17.0.11-1606980167551 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3: 60ms
2020-12-03 07:22:53,524 [Thread-85] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-279225940-172.17.0.11-1606980167551: 61ms
2020-12-03 07:22:53,524 [Thread-131] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-279225940-172.17.0.11-1606980167551 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6: 59ms
2020-12-03 07:22:53,524 [Thread-107] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-279225940-172.17.0.11-1606980167551: 62ms
2020-12-03 07:22:53,525 [Thread-142] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-279225940-172.17.0.11-1606980167551 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4...
2020-12-03 07:22:53,525 [Thread-139] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-279225940-172.17.0.11-1606980167551 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2020-12-03 07:22:53,526 [Thread-144] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-279225940-172.17.0.11-1606980167551 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6...
2020-12-03 07:22:53,525 [Thread-141] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-279225940-172.17.0.11-1606980167551 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2020-12-03 07:22:53,525 [Thread-140] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-279225940-172.17.0.11-1606980167551 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3...
2020-12-03 07:22:53,527 [Thread-141] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-279225940-172.17.0.11-1606980167551/current/replicas doesn't exist 
2020-12-03 07:22:53,526 [Thread-139] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-279225940-172.17.0.11-1606980167551/current/replicas doesn't exist 
2020-12-03 07:22:53,527 [Thread-144] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/current/BP-279225940-172.17.0.11-1606980167551/current/replicas doesn't exist 
2020-12-03 07:22:53,526 [Thread-143] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-279225940-172.17.0.11-1606980167551 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5...
2020-12-03 07:22:53,525 [Thread-142] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-279225940-172.17.0.11-1606980167551/current/replicas doesn't exist 
2020-12-03 07:22:53,528 [Thread-143] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-279225940-172.17.0.11-1606980167551/current/replicas doesn't exist 
2020-12-03 07:22:53,527 [Thread-140] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-279225940-172.17.0.11-1606980167551/current/replicas doesn't exist 
2020-12-03 07:22:53,530 [Thread-142] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-279225940-172.17.0.11-1606980167551 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4: 5ms
2020-12-03 07:22:53,533 [Thread-143] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-279225940-172.17.0.11-1606980167551 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5: 5ms
2020-12-03 07:22:53,533 [Thread-140] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-279225940-172.17.0.11-1606980167551 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3: 6ms
2020-12-03 07:22:53,533 [Thread-85] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-279225940-172.17.0.11-1606980167551: 9ms
2020-12-03 07:22:53,533 [IPC Server handler 0 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:22:53,534 [Listener at localhost/42873] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:22:53,535 [Thread-139] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-279225940-172.17.0.11-1606980167551 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 9ms
2020-12-03 07:22:53,534 [Thread-144] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-279225940-172.17.0.11-1606980167551 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6: 8ms
2020-12-03 07:22:53,535 [Thread-107] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-279225940-172.17.0.11-1606980167551: 10ms
2020-12-03 07:22:53,535 [Thread-141] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-279225940-172.17.0.11-1606980167551 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 8ms
2020-12-03 07:22:53,535 [Listener at localhost/42873] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:22:53,536 [Thread-61] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-279225940-172.17.0.11-1606980167551: 12ms
2020-12-03 07:22:53,536 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-279225940-172.17.0.11-1606980167551 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:22:53,537 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-279225940-172.17.0.11-1606980167551 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:22:53,537 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-279225940-172.17.0.11-1606980167551 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
2020-12-03 07:22:53,537 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-279225940-172.17.0.11-1606980167551 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:22:53,537 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-279225940-172.17.0.11-1606980167551 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6
2020-12-03 07:22:53,536 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-279225940-172.17.0.11-1606980167551 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5
2020-12-03 07:22:53,539 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6, DS-ff826779-4412-445b-8f25-01fd55cacd1d): finished scanning block pool BP-279225940-172.17.0.11-1606980167551
2020-12-03 07:22:53,539 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, DS-3024a9be-1033-48db-8cf1-7792eec124ef): finished scanning block pool BP-279225940-172.17.0.11-1606980167551
2020-12-03 07:22:53,539 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3, DS-5d75b3b6-c3c3-415c-9506-a037c96d984c): finished scanning block pool BP-279225940-172.17.0.11-1606980167551
2020-12-03 07:22:53,539 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-4d64cd8d-286f-4d38-9cd8-eaa887a5ffa1): finished scanning block pool BP-279225940-172.17.0.11-1606980167551
2020-12-03 07:22:53,539 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-2ce793de-7ca9-4eff-a497-239e428e643d): finished scanning block pool BP-279225940-172.17.0.11-1606980167551
2020-12-03 07:22:53,541 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5, DS-0c4ace5c-737b-46b0-9a95-af6b01d0e34c): finished scanning block pool BP-279225940-172.17.0.11-1606980167551
2020-12-03 07:22:53,564 [Thread-61] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 9:23 AM with interval of 21600000ms
2020-12-03 07:22:53,566 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-2ce793de-7ca9-4eff-a497-239e428e643d): no suitable block pools found to scan.  Waiting 1814399971 ms.
2020-12-03 07:22:53,567 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-4d64cd8d-286f-4d38-9cd8-eaa887a5ffa1): no suitable block pools found to scan.  Waiting 1814399970 ms.
2020-12-03 07:22:53,567 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6, DS-ff826779-4412-445b-8f25-01fd55cacd1d): no suitable block pools found to scan.  Waiting 1814399969 ms.
2020-12-03 07:22:53,567 [Thread-85] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 8:04 AM with interval of 21600000ms
2020-12-03 07:22:53,569 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, DS-3024a9be-1033-48db-8cf1-7792eec124ef): no suitable block pools found to scan.  Waiting 1814399967 ms.
2020-12-03 07:22:53,569 [Thread-107] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 11:16 AM with interval of 21600000ms
2020-12-03 07:22:53,570 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5, DS-0c4ace5c-737b-46b0-9a95-af6b01d0e34c): no suitable block pools found to scan.  Waiting 1814399966 ms.
2020-12-03 07:22:53,570 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3, DS-5d75b3b6-c3c3-415c-9506-a037c96d984c): no suitable block pools found to scan.  Waiting 1814399967 ms.
2020-12-03 07:22:53,574 [BP-279225940-172.17.0.11-1606980167551 heartbeating to localhost/127.0.0.1:37371] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-279225940-172.17.0.11-1606980167551 (Datanode Uuid 06f17149-3c92-4fc6-af33-7ac1a1270c19) service to localhost/127.0.0.1:37371 beginning handshake with NN
2020-12-03 07:22:53,574 [BP-279225940-172.17.0.11-1606980167551 heartbeating to localhost/127.0.0.1:37371] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-279225940-172.17.0.11-1606980167551 (Datanode Uuid a9f6fc87-7b25-48f4-8425-d3620813d33e) service to localhost/127.0.0.1:37371 beginning handshake with NN
2020-12-03 07:22:53,574 [BP-279225940-172.17.0.11-1606980167551 heartbeating to localhost/127.0.0.1:37371] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-279225940-172.17.0.11-1606980167551 (Datanode Uuid a4d0b9c3-5325-4729-a835-ddff222c3d3f) service to localhost/127.0.0.1:37371 beginning handshake with NN
2020-12-03 07:22:53,587 [IPC Server handler 2 on default port 37371] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:45120, datanodeUuid=a9f6fc87-7b25-48f4-8425-d3620813d33e, infoPort=41729, infoSecurePort=0, ipcPort=44735, storageInfo=lv=-57;cid=testClusterID;nsid=1405290069;c=1606980167551) storage a9f6fc87-7b25-48f4-8425-d3620813d33e
2020-12-03 07:22:53,589 [IPC Server handler 2 on default port 37371] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:45120
2020-12-03 07:22:53,589 [IPC Server handler 2 on default port 37371] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN a9f6fc87-7b25-48f4-8425-d3620813d33e (127.0.0.1:45120).
2020-12-03 07:22:53,591 [IPC Server handler 1 on default port 37371] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:42073, datanodeUuid=06f17149-3c92-4fc6-af33-7ac1a1270c19, infoPort=45021, infoSecurePort=0, ipcPort=46834, storageInfo=lv=-57;cid=testClusterID;nsid=1405290069;c=1606980167551) storage 06f17149-3c92-4fc6-af33-7ac1a1270c19
2020-12-03 07:22:53,591 [IPC Server handler 1 on default port 37371] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:42073
2020-12-03 07:22:53,591 [IPC Server handler 1 on default port 37371] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 06f17149-3c92-4fc6-af33-7ac1a1270c19 (127.0.0.1:42073).
2020-12-03 07:22:53,592 [IPC Server handler 8 on default port 37371] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:37424, datanodeUuid=a4d0b9c3-5325-4729-a835-ddff222c3d3f, infoPort=42090, infoSecurePort=0, ipcPort=42873, storageInfo=lv=-57;cid=testClusterID;nsid=1405290069;c=1606980167551) storage a4d0b9c3-5325-4729-a835-ddff222c3d3f
2020-12-03 07:22:53,592 [IPC Server handler 8 on default port 37371] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:37424
2020-12-03 07:22:53,592 [IPC Server handler 8 on default port 37371] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN a4d0b9c3-5325-4729-a835-ddff222c3d3f (127.0.0.1:37424).
2020-12-03 07:22:53,594 [BP-279225940-172.17.0.11-1606980167551 heartbeating to localhost/127.0.0.1:37371] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-279225940-172.17.0.11-1606980167551 (Datanode Uuid a4d0b9c3-5325-4729-a835-ddff222c3d3f) service to localhost/127.0.0.1:37371 successfully registered with NN
2020-12-03 07:22:53,594 [BP-279225940-172.17.0.11-1606980167551 heartbeating to localhost/127.0.0.1:37371] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-279225940-172.17.0.11-1606980167551 (Datanode Uuid a9f6fc87-7b25-48f4-8425-d3620813d33e) service to localhost/127.0.0.1:37371 successfully registered with NN
2020-12-03 07:22:53,594 [BP-279225940-172.17.0.11-1606980167551 heartbeating to localhost/127.0.0.1:37371] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:37371 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:22:53,594 [BP-279225940-172.17.0.11-1606980167551 heartbeating to localhost/127.0.0.1:37371] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:37371 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:22:53,594 [BP-279225940-172.17.0.11-1606980167551 heartbeating to localhost/127.0.0.1:37371] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-279225940-172.17.0.11-1606980167551 (Datanode Uuid 06f17149-3c92-4fc6-af33-7ac1a1270c19) service to localhost/127.0.0.1:37371 successfully registered with NN
2020-12-03 07:22:53,594 [BP-279225940-172.17.0.11-1606980167551 heartbeating to localhost/127.0.0.1:37371] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:37371 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:22:53,614 [IPC Server handler 9 on default port 37371] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-5d75b3b6-c3c3-415c-9506-a037c96d984c for DN 127.0.0.1:45120
2020-12-03 07:22:53,615 [IPC Server handler 9 on default port 37371] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-3024a9be-1033-48db-8cf1-7792eec124ef for DN 127.0.0.1:45120
2020-12-03 07:22:53,616 [IPC Server handler 6 on default port 37371] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-2ce793de-7ca9-4eff-a497-239e428e643d for DN 127.0.0.1:42073
2020-12-03 07:22:53,617 [IPC Server handler 6 on default port 37371] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-4d64cd8d-286f-4d38-9cd8-eaa887a5ffa1 for DN 127.0.0.1:42073
2020-12-03 07:22:53,618 [IPC Server handler 7 on default port 37371] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-0c4ace5c-737b-46b0-9a95-af6b01d0e34c for DN 127.0.0.1:37424
2020-12-03 07:22:53,618 [IPC Server handler 7 on default port 37371] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-ff826779-4412-445b-8f25-01fd55cacd1d for DN 127.0.0.1:37424
2020-12-03 07:22:53,643 [IPC Server handler 5 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:22:53,661 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x6eed81299680a476: Processing first storage report for DS-ff826779-4412-445b-8f25-01fd55cacd1d from datanode a4d0b9c3-5325-4729-a835-ddff222c3d3f
2020-12-03 07:22:53,664 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x6eed81299680a476: from storage DS-ff826779-4412-445b-8f25-01fd55cacd1d node DatanodeRegistration(127.0.0.1:37424, datanodeUuid=a4d0b9c3-5325-4729-a835-ddff222c3d3f, infoPort=42090, infoSecurePort=0, ipcPort=42873, storageInfo=lv=-57;cid=testClusterID;nsid=1405290069;c=1606980167551), blocks: 0, hasStaleStorage: true, processing time: 3 msecs, invalidatedBlocks: 0
2020-12-03 07:22:53,664 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x9c417a18999f855d: Processing first storage report for DS-3024a9be-1033-48db-8cf1-7792eec124ef from datanode a9f6fc87-7b25-48f4-8425-d3620813d33e
2020-12-03 07:22:53,664 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x9c417a18999f855d: from storage DS-3024a9be-1033-48db-8cf1-7792eec124ef node DatanodeRegistration(127.0.0.1:45120, datanodeUuid=a9f6fc87-7b25-48f4-8425-d3620813d33e, infoPort=41729, infoSecurePort=0, ipcPort=44735, storageInfo=lv=-57;cid=testClusterID;nsid=1405290069;c=1606980167551), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:22:53,664 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x66ddcd24d6f38c62: Processing first storage report for DS-2ce793de-7ca9-4eff-a497-239e428e643d from datanode 06f17149-3c92-4fc6-af33-7ac1a1270c19
2020-12-03 07:22:53,665 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x66ddcd24d6f38c62: from storage DS-2ce793de-7ca9-4eff-a497-239e428e643d node DatanodeRegistration(127.0.0.1:42073, datanodeUuid=06f17149-3c92-4fc6-af33-7ac1a1270c19, infoPort=45021, infoSecurePort=0, ipcPort=46834, storageInfo=lv=-57;cid=testClusterID;nsid=1405290069;c=1606980167551), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:22:53,665 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x6eed81299680a476: Processing first storage report for DS-0c4ace5c-737b-46b0-9a95-af6b01d0e34c from datanode a4d0b9c3-5325-4729-a835-ddff222c3d3f
2020-12-03 07:22:53,665 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x6eed81299680a476: from storage DS-0c4ace5c-737b-46b0-9a95-af6b01d0e34c node DatanodeRegistration(127.0.0.1:37424, datanodeUuid=a4d0b9c3-5325-4729-a835-ddff222c3d3f, infoPort=42090, infoSecurePort=0, ipcPort=42873, storageInfo=lv=-57;cid=testClusterID;nsid=1405290069;c=1606980167551), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:22:53,665 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x9c417a18999f855d: Processing first storage report for DS-5d75b3b6-c3c3-415c-9506-a037c96d984c from datanode a9f6fc87-7b25-48f4-8425-d3620813d33e
2020-12-03 07:22:53,665 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x9c417a18999f855d: from storage DS-5d75b3b6-c3c3-415c-9506-a037c96d984c node DatanodeRegistration(127.0.0.1:45120, datanodeUuid=a9f6fc87-7b25-48f4-8425-d3620813d33e, infoPort=41729, infoSecurePort=0, ipcPort=44735, storageInfo=lv=-57;cid=testClusterID;nsid=1405290069;c=1606980167551), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:22:53,666 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x66ddcd24d6f38c62: Processing first storage report for DS-4d64cd8d-286f-4d38-9cd8-eaa887a5ffa1 from datanode 06f17149-3c92-4fc6-af33-7ac1a1270c19
2020-12-03 07:22:53,666 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x66ddcd24d6f38c62: from storage DS-4d64cd8d-286f-4d38-9cd8-eaa887a5ffa1 node DatanodeRegistration(127.0.0.1:42073, datanodeUuid=06f17149-3c92-4fc6-af33-7ac1a1270c19, infoPort=45021, infoSecurePort=0, ipcPort=46834, storageInfo=lv=-57;cid=testClusterID;nsid=1405290069;c=1606980167551), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:22:53,667 [Listener at localhost/42873] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2758)) - Cluster is active
2020-12-03 07:22:53,676 [IPC Server handler 2 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:22:53,679 [Listener at localhost/42873] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2758)) - Cluster is active
2020-12-03 07:22:53,690 [Listener at localhost/42873] INFO  namenode.TestDeleteRace (TestDeleteRace.java:testDeleteAndCommitBlockSynchronizationRace(277)) - test on /test-file mkSameDir: false snapshot: true
2020-12-03 07:22:53,700 [BP-279225940-172.17.0.11-1606980167551 heartbeating to localhost/127.0.0.1:37371] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x6eed81299680a476,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 62 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:22:53,700 [BP-279225940-172.17.0.11-1606980167551 heartbeating to localhost/127.0.0.1:37371] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-279225940-172.17.0.11-1606980167551
2020-12-03 07:22:53,702 [BP-279225940-172.17.0.11-1606980167551 heartbeating to localhost/127.0.0.1:37371] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x66ddcd24d6f38c62,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 62 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:22:53,702 [BP-279225940-172.17.0.11-1606980167551 heartbeating to localhost/127.0.0.1:37371] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x9c417a18999f855d,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 62 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:22:53,702 [BP-279225940-172.17.0.11-1606980167551 heartbeating to localhost/127.0.0.1:37371] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-279225940-172.17.0.11-1606980167551
2020-12-03 07:22:53,703 [BP-279225940-172.17.0.11-1606980167551 heartbeating to localhost/127.0.0.1:37371] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-279225940-172.17.0.11-1606980167551
2020-12-03 07:22:53,762 [IPC Server handler 8 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/test-file	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2020-12-03 07:22:53,802 [Listener at localhost/42873] INFO  namenode.TestDeleteRace (TestDeleteRace.java:testDeleteAndCommitBlockSynchronizationRace(286)) - test on /test-file created /test-file
2020-12-03 07:22:53,804 [Listener at localhost/42873] INFO  hdfs.AppendTestUtil (AppendTestUtil.java:<clinit>(53)) - seed=-1755000232696165261
2020-12-03 07:22:53,831 [IPC Server handler 1 on default port 37371] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_1073741825_1001, replicas=127.0.0.1:37424, 127.0.0.1:45120, 127.0.0.1:42073 for /test-file
2020-12-03 07:22:53,850 [Thread-154] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:53,925 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:36902 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001 src: /127.0.0.1:36902 dest: /127.0.0.1:37424
2020-12-03 07:22:53,951 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:36902 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:53,953 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:50738 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001 src: /127.0.0.1:50738 dest: /127.0.0.1:45120
2020-12-03 07:22:53,954 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:50738 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:53,955 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:49232 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001 src: /127.0.0.1:49232 dest: /127.0.0.1:42073
2020-12-03 07:22:53,992 [IPC Server handler 7 on default port 37371] INFO  hdfs.StateChange (FSNamesystem.java:fsync(3361)) - BLOCK* fsync: /test-file for DFSClient_NONMAPREDUCE_1516886611_25
2020-12-03 07:22:53,998 [Listener at localhost/42873] INFO  snapshot.SnapshotTestHelper (SnapshotTestHelper.java:createSnapshot(133)) - createSnapshot st0 for /
2020-12-03 07:22:54,003 [IPC Server handler 6 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/	dst=null	perm=null	proto=rpc
2020-12-03 07:22:54,015 [IPC Server handler 5 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=allowSnapshot	src=/	dst=null	perm=null	proto=rpc
2020-12-03 07:22:54,030 [IPC Server handler 2 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=createSnapshot	src=/	dst=/.snapshot/st0	perm=null	proto=rpc
2020-12-03 07:22:54,039 [IPC Server handler 0 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setSpaceQuota	src=/	dst=null	perm=null	proto=rpc
2020-12-03 07:22:54,047 [IPC Server handler 4 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/test-file	dst=null	perm=null	proto=rpc
2020-12-03 07:22:54,172 [Listener at localhost/42873] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:54,206 [Listener at localhost/42873] INFO  namenode.TestDeleteRace (TestDeleteRace.java:testDeleteAndCommitBlockSynchronizationRace(304)) - Expecting block recovery to be triggered on DN 127.0.0.1:37424
2020-12-03 07:22:54,378 [IPC Server handler 3 on default port 37371] INFO  namenode.FSNamesystem (FSNamesystem.java:recoverLeaseInternal(2656)) - recoverLease: [Lease.  Holder: DFSClient_NONMAPREDUCE_1516886611_25, pending creates: 1], src=/test-file from client DFSClient_NONMAPREDUCE_1516886611_25
2020-12-03 07:22:54,378 [IPC Server handler 3 on default port 37371] INFO  namenode.FSNamesystem (FSNamesystem.java:internalReleaseLease(3398)) - Recovering [Lease.  Holder: DFSClient_NONMAPREDUCE_1516886611_25, pending creates: 1], src=/test-file
2020-12-03 07:22:54,381 [IPC Server handler 3 on default port 37371] WARN  hdfs.StateChange (FSNamesystem.java:internalReleaseLease(3524)) - DIR* NameSystem.internalReleaseLease: File /test-file has not been closed. Lease recovery is in progress. RecoveryId = 1002 for block blk_1073741825_1001
2020-12-03 07:22:54,386 [Listener at localhost/42873] INFO  namenode.TestDeleteRace (TestDeleteRace.java:testDeleteAndCommitBlockSynchronizationRace(329)) - Waiting for commitBlockSynchronization call from primary
2020-12-03 07:22:56,665 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@203bd739] INFO  datanode.DataNode (BlockRecoveryWorker.java:logRecoverBlock(549)) - BlockRecoveryWorker: NameNode at localhost/127.0.0.1:37371 calls recoverBlock(BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001, targets=[DatanodeInfoWithStorage[127.0.0.1:37424,null,null], DatanodeInfoWithStorage[127.0.0.1:45120,null,null], DatanodeInfoWithStorage[127.0.0.1:42073,null,null]], newGenerationStamp=1002, newBlock=null, isStriped=false)
2020-12-03 07:22:56,668 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@203bd739] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:initReplicaRecoveryImpl(2588)) - initReplicaRecovery: blk_1073741825_1001, recoveryId=1002, replica=ReplicaBeingWritten, blk_1073741825_1001, RBW
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741825
  bytesAcked=2048
  bytesOnDisk=2048
2020-12-03 07:22:56,670 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:36902 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001]] INFO  datanode.DataNode (BlockReceiver.java:receiveBlock(1010)) - Exception for BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/127.0.0.1:37424 remote=/127.0.0.1:36902]. 60000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:210)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:528)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:971)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:908)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:56,673 [PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[127.0.0.1:45120, 127.0.0.1:42073]] INFO  datanode.DataNode (BlockReceiver.java:run(1470)) - PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[127.0.0.1:45120, 127.0.0.1:42073]: Thread is interrupted.
2020-12-03 07:22:56,673 [PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[127.0.0.1:45120, 127.0.0.1:42073]] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[127.0.0.1:45120, 127.0.0.1:42073] terminating
2020-12-03 07:22:56,674 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:36902 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(939)) - opWriteBlock BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001 received exception java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/127.0.0.1:37424 remote=/127.0.0.1:36902]. 60000 millis timeout left.
2020-12-03 07:22:56,677 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:36902 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:37424:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:36902 dst: /127.0.0.1:37424
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/127.0.0.1:37424 remote=/127.0.0.1:36902]. 60000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:210)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:528)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:971)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:908)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:56,678 [ResponseProcessor for block BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001] WARN  hdfs.DataStreamer (DataStreamer.java:run(1196)) - Exception for BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:550)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:213)
	at org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1086)
2020-12-03 07:22:56,679 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@203bd739] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:initReplicaRecoveryImpl(2588)) - initReplicaRecovery: blk_1073741825_1001, recoveryId=1002, replica=ReplicaBeingWritten, blk_1073741825_1001, RBW
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741825
  bytesAcked=2048
  bytesOnDisk=2048
2020-12-03 07:22:56,677 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:50738 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001]] INFO  datanode.DataNode (BlockReceiver.java:receiveBlock(1010)) - Exception for BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001
java.io.IOException: Premature EOF from inputStream
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:212)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:528)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:971)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:908)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:56,679 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@203bd739] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:initReplicaRecoveryImpl(2646)) - initReplicaRecovery: changing replica state for blk_1073741825_1001 from RBW to RUR
2020-12-03 07:22:56,680 [PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:42073]] INFO  datanode.DataNode (BlockReceiver.java:run(1470)) - PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:42073]: Thread is interrupted.
2020-12-03 07:22:56,680 [PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:42073]] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:42073] terminating
2020-12-03 07:22:56,680 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:50738 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(939)) - opWriteBlock BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001 received exception java.io.IOException: Premature EOF from inputStream
2020-12-03 07:22:56,680 [DataStreamer for file /test-file block BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001] WARN  hdfs.DataStreamer (DataStreamer.java:handleBadDatanode(1571)) - Error Recovery for BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001 in pipeline [DatanodeInfoWithStorage[127.0.0.1:37424,DS-ff826779-4412-445b-8f25-01fd55cacd1d,DISK], DatanodeInfoWithStorage[127.0.0.1:45120,DS-3024a9be-1033-48db-8cf1-7792eec124ef,DISK], DatanodeInfoWithStorage[127.0.0.1:42073,DS-2ce793de-7ca9-4eff-a497-239e428e643d,DISK]]: datanode 0(DatanodeInfoWithStorage[127.0.0.1:37424,DS-ff826779-4412-445b-8f25-01fd55cacd1d,DISK]) is bad.
2020-12-03 07:22:56,682 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:50738 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:45120:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:50738 dst: /127.0.0.1:45120
java.io.IOException: Premature EOF from inputStream
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:212)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:528)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:971)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:908)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:56,682 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:49232 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001]] INFO  datanode.DataNode (BlockReceiver.java:receiveBlock(1010)) - Exception for BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001
java.io.IOException: Premature EOF from inputStream
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:212)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:528)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:971)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:908)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:56,683 [PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1470)) - PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001, type=LAST_IN_PIPELINE: Thread is interrupted.
2020-12-03 07:22:56,683 [PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001, type=LAST_IN_PIPELINE terminating
2020-12-03 07:22:56,685 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:49232 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(939)) - opWriteBlock BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001 received exception java.io.IOException: Premature EOF from inputStream
2020-12-03 07:22:56,686 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:49232 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:42073:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:49232 dst: /127.0.0.1:42073
java.io.IOException: Premature EOF from inputStream
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:212)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:528)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:971)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:908)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:56,693 [IPC Server handler 2 on default port 37371] WARN  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(450)) - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2020-12-03 07:22:56,694 [IPC Server handler 2 on default port 37371] WARN  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(450)) - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2020-12-03 07:22:56,694 [IPC Server handler 2 on default port 37371] WARN  protocol.BlockStoragePolicy (BlockStoragePolicy.java:chooseStorageTypes(161)) - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK, ARCHIVE], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2020-12-03 07:22:56,694 [IPC Server handler 2 on default port 37371] WARN  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(450)) - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK, ARCHIVE], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) All required storage types are unavailable:  unavailableStorages=[DISK, ARCHIVE], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2020-12-03 07:22:56,700 [DataStreamer for file /test-file block BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001] WARN  hdfs.DataStreamer (DataStreamer.java:run(826)) - DataStreamer Exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42073,DS-2ce793de-7ca9-4eff-a497-239e428e643d,DISK], DatanodeInfoWithStorage[127.0.0.1:45120,DS-3024a9be-1033-48db-8cf1-7792eec124ef,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45120,DS-3024a9be-1033-48db-8cf1-7792eec124ef,DISK], DatanodeInfoWithStorage[127.0.0.1:42073,DS-2ce793de-7ca9-4eff-a497-239e428e643d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)
2020-12-03 07:22:56,707 [IPC Server handler 0 on default port 44735] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:initReplicaRecoveryImpl(2588)) - initReplicaRecovery: blk_1073741825_1001, recoveryId=1002, replica=ReplicaBeingWritten, blk_1073741825_1001, RBW
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741825
  bytesAcked=2048
  bytesOnDisk=2048
2020-12-03 07:22:56,708 [IPC Server handler 0 on default port 44735] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:initReplicaRecoveryImpl(2588)) - initReplicaRecovery: blk_1073741825_1001, recoveryId=1002, replica=ReplicaBeingWritten, blk_1073741825_1001, RBW
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741825
  bytesAcked=2048
  bytesOnDisk=2048
2020-12-03 07:22:56,708 [IPC Server handler 0 on default port 44735] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:initReplicaRecoveryImpl(2646)) - initReplicaRecovery: changing replica state for blk_1073741825_1001 from RBW to RUR
2020-12-03 07:22:56,717 [IPC Server handler 2 on default port 46834] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:initReplicaRecoveryImpl(2588)) - initReplicaRecovery: blk_1073741825_1001, recoveryId=1002, replica=ReplicaBeingWritten, blk_1073741825_1001, RBW
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741825
  bytesAcked=2048
  bytesOnDisk=2048
2020-12-03 07:22:56,718 [IPC Server handler 2 on default port 46834] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:initReplicaRecoveryImpl(2588)) - initReplicaRecovery: blk_1073741825_1001, recoveryId=1002, replica=ReplicaBeingWritten, blk_1073741825_1001, RBW
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741825
  bytesAcked=2048
  bytesOnDisk=2048
2020-12-03 07:22:56,718 [IPC Server handler 2 on default port 46834] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:initReplicaRecoveryImpl(2646)) - initReplicaRecovery: changing replica state for blk_1073741825_1001 from RBW to RUR
2020-12-03 07:22:56,719 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@203bd739] INFO  datanode.DataNode (BlockRecoveryWorker.java:syncBlock(200)) - BlockRecoveryWorker: block=BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001 (length=2048), isTruncateRecovery=false, syncList=[block:blk_1073741825_1001[numBytes=2048,originalReplicaState=RBW] node:DatanodeInfoWithStorage[127.0.0.1:37424,null,null], block:blk_1073741825_1001[numBytes=2048,originalReplicaState=RBW] node:DatanodeInfoWithStorage[127.0.0.1:45120,null,null], block:blk_1073741825_1001[numBytes=2048,originalReplicaState=RBW] node:DatanodeInfoWithStorage[127.0.0.1:42073,null,null]]
2020-12-03 07:22:56,721 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@203bd739] INFO  datanode.DataNode (BlockRecoveryWorker.java:syncBlock(293)) - BlockRecoveryWorker: block=BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001 (length=2048), bestState=RBW, newBlock=BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1002 (length=2048), participatingList=[block:blk_1073741825_1001[numBytes=2048,originalReplicaState=RBW] node:DatanodeInfoWithStorage[127.0.0.1:37424,null,null], block:blk_1073741825_1001[numBytes=2048,originalReplicaState=RBW] node:DatanodeInfoWithStorage[127.0.0.1:45120,null,null], block:blk_1073741825_1001[numBytes=2048,originalReplicaState=RBW] node:DatanodeInfoWithStorage[127.0.0.1:42073,null,null]]
2020-12-03 07:22:56,721 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@203bd739] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:updateReplicaUnderRecovery(2667)) - updateReplica: BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001[numBytes=2048,originalReplicaState=RBW], recoveryId=1002, length=2048, replica=ReplicaUnderRecovery, blk_1073741825_1001, RUR
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741825
  recoveryId=1002
  original=ReplicaBeingWritten, blk_1073741825_1001, RBW
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741825
  bytesAcked=2048
  bytesOnDisk=2048
2020-12-03 07:22:56,726 [IPC Server handler 1 on default port 44735] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:updateReplicaUnderRecovery(2667)) - updateReplica: BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001, recoveryId=1002, length=2048, replica=ReplicaUnderRecovery, blk_1073741825_1001, RUR
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741825
  recoveryId=1002
  original=ReplicaBeingWritten, blk_1073741825_1001, RBW
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741825
  bytesAcked=2048
  bytesOnDisk=2048
2020-12-03 07:22:56,733 [IPC Server handler 1 on default port 46834] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:updateReplicaUnderRecovery(2667)) - updateReplica: BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001, recoveryId=1002, length=2048, replica=ReplicaUnderRecovery, blk_1073741825_1001, RUR
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741825
  recoveryId=1002
  original=ReplicaBeingWritten, blk_1073741825_1001, RBW
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741825
  bytesAcked=2048
  bytesOnDisk=2048
2020-12-03 07:22:56,739 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@203bd739] INFO  namenode.TestDeleteRace (GenericTestUtils.java:answer(538)) - DelayAnswer firing fireLatch
2020-12-03 07:22:56,740 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@203bd739] INFO  namenode.TestDeleteRace (GenericTestUtils.java:answer(542)) - DelayAnswer waiting on waitLatch
2020-12-03 07:22:56,740 [Listener at localhost/42873] INFO  namenode.TestDeleteRace (TestDeleteRace.java:testDeleteAndCommitBlockSynchronizationRace(332)) - Deleting recursively /test-file
2020-12-03 07:22:56,765 [IPC Server handler 8 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=delete	src=/test-file	dst=null	perm=null	proto=rpc
2020-12-03 07:22:56,770 [Listener at localhost/42873] INFO  namenode.TestDeleteRace (TestDeleteRace.java:testDeleteAndCommitBlockSynchronizationRace(340)) - Now wait for result
2020-12-03 07:22:56,770 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@203bd739] INFO  namenode.TestDeleteRace (GenericTestUtils.java:answer(544)) - DelayAnswer delay complete
2020-12-03 07:22:56,776 [IPC Server handler 1 on default port 37371] INFO  namenode.FSNamesystem (FSNamesystem.java:commitBlockSynchronization(3655)) - commitBlockSynchronization(oldBlock=BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001, newgenerationstamp=1002, newlength=2048, newtargets=[127.0.0.1:37424, 127.0.0.1:45120, 127.0.0.1:42073], closeFile=true, deleteBlock=false)
2020-12-03 07:22:56,777 [IPC Server handler 1 on default port 37371] INFO  ipc.Server (Server.java:logException(2969)) - IPC Server handler 1 on default port 37371, call Call#50 Retry#0 org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.commitBlockSynchronization from 127.0.0.1:40948: java.io.FileNotFoundException: File not found: /test-file, likely due to delayed block removal
2020-12-03 07:22:56,782 [Listener at localhost/42873] INFO  namenode.TestDeleteRace (TestDeleteRace.java:testDeleteAndCommitBlockSynchronizationRace(344)) - Result exception (snapshot: true): org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File not found: /test-file, likely due to delayed block removal
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.commitBlockSynchronization(FSNamesystem.java:3710)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.commitBlockSynchronization(NameNodeRpcServer.java:998)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.commitBlockSynchronization(DatanodeProtocolServerSideTranslatorPB.java:302)
	at org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(DatanodeProtocolProtos.java:31676)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)

2020-12-03 07:22:56,782 [Listener at localhost/42873] INFO  namenode.TestDeleteRace (TestDeleteRace.java:testDeleteAndCommitBlockSynchronizationRace(277)) - test on /test-file1 mkSameDir: true snapshot: true
2020-12-03 07:22:56,783 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@203bd739] WARN  datanode.DataNode (BlockRecoveryWorker.java:run(607)) - recoverBlocks FAILED: RecoveringBlock{BP-279225940-172.17.0.11-1606980167551:blk_1073741825_1001; getBlockSize()=2048; corrupt=false; offset=-1; locs=[DatanodeInfoWithStorage[127.0.0.1:37424,null,null], DatanodeInfoWithStorage[127.0.0.1:45120,null,null], DatanodeInfoWithStorage[127.0.0.1:42073,null,null]]}
org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File not found: /test-file, likely due to delayed block removal
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.commitBlockSynchronization(FSNamesystem.java:3710)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.commitBlockSynchronization(NameNodeRpcServer.java:998)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.commitBlockSynchronization(DatanodeProtocolServerSideTranslatorPB.java:302)
	at org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(DatanodeProtocolProtos.java:31676)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1545)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy23.commitBlockSynchronization(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.commitBlockSynchronization(DatanodeProtocolClientSideTranslatorPB.java:327)
	at org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$RecoveryTaskContiguous.syncBlock(BlockRecoveryWorker.java:333)
	at org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$RecoveryTaskContiguous.recover(BlockRecoveryWorker.java:188)
	at org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1.run(BlockRecoveryWorker.java:604)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:56,785 [IPC Server handler 9 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/test-file1	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2020-12-03 07:22:56,793 [Listener at localhost/42873] INFO  namenode.TestDeleteRace (TestDeleteRace.java:testDeleteAndCommitBlockSynchronizationRace(286)) - test on /test-file1 created /test-file1
2020-12-03 07:22:56,796 [IPC Server handler 7 on default port 37371] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_1073741826_1003, replicas=127.0.0.1:42073, 127.0.0.1:45120, 127.0.0.1:37424 for /test-file1
2020-12-03 07:22:56,798 [Thread-169] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:56,800 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:49254 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003 src: /127.0.0.1:49254 dest: /127.0.0.1:42073
2020-12-03 07:22:56,801 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:49254 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:56,803 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:50764 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003 src: /127.0.0.1:50764 dest: /127.0.0.1:45120
2020-12-03 07:22:56,804 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:50764 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:56,805 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:36932 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003 src: /127.0.0.1:36932 dest: /127.0.0.1:37424
2020-12-03 07:22:56,813 [IPC Server handler 6 on default port 37371] INFO  hdfs.StateChange (FSNamesystem.java:fsync(3361)) - BLOCK* fsync: /test-file1 for DFSClient_NONMAPREDUCE_1516886611_25
2020-12-03 07:22:56,814 [Listener at localhost/42873] INFO  snapshot.SnapshotTestHelper (SnapshotTestHelper.java:createSnapshot(133)) - createSnapshot st1 for /
2020-12-03 07:22:56,815 [IPC Server handler 5 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/	dst=null	perm=null	proto=rpc
2020-12-03 07:22:56,817 [IPC Server handler 2 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=allowSnapshot	src=/	dst=null	perm=null	proto=rpc
2020-12-03 07:22:56,818 [IPC Server handler 0 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=createSnapshot	src=/	dst=/.snapshot/st1	perm=null	proto=rpc
2020-12-03 07:22:56,820 [IPC Server handler 4 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setSpaceQuota	src=/	dst=null	perm=null	proto=rpc
2020-12-03 07:22:56,822 [IPC Server handler 3 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/test-file1	dst=null	perm=null	proto=rpc
2020-12-03 07:22:56,832 [Listener at localhost/42873] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:56,836 [Listener at localhost/42873] INFO  namenode.TestDeleteRace (TestDeleteRace.java:testDeleteAndCommitBlockSynchronizationRace(304)) - Expecting block recovery to be triggered on DN 127.0.0.1:37424
2020-12-03 07:22:56,839 [IPC Server handler 8 on default port 37371] INFO  namenode.FSNamesystem (FSNamesystem.java:recoverLeaseInternal(2656)) - recoverLease: [Lease.  Holder: DFSClient_NONMAPREDUCE_1516886611_25, pending creates: 1], src=/test-file1 from client DFSClient_NONMAPREDUCE_1516886611_25
2020-12-03 07:22:56,839 [IPC Server handler 8 on default port 37371] INFO  namenode.FSNamesystem (FSNamesystem.java:internalReleaseLease(3398)) - Recovering [Lease.  Holder: DFSClient_NONMAPREDUCE_1516886611_25, pending creates: 1], src=/test-file1
2020-12-03 07:22:56,839 [IPC Server handler 8 on default port 37371] WARN  hdfs.StateChange (FSNamesystem.java:internalReleaseLease(3524)) - DIR* NameSystem.internalReleaseLease: File /test-file1 has not been closed. Lease recovery is in progress. RecoveryId = 1004 for block blk_1073741826_1003
2020-12-03 07:22:56,840 [Listener at localhost/42873] INFO  namenode.TestDeleteRace (TestDeleteRace.java:testDeleteAndCommitBlockSynchronizationRace(329)) - Waiting for commitBlockSynchronization call from primary
2020-12-03 07:22:59,599 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@3ce258aa] INFO  datanode.DataNode (BlockRecoveryWorker.java:logRecoverBlock(549)) - BlockRecoveryWorker: NameNode at localhost/127.0.0.1:37371 calls recoverBlock(BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003, targets=[DatanodeInfoWithStorage[127.0.0.1:42073,null,null], DatanodeInfoWithStorage[127.0.0.1:45120,null,null], DatanodeInfoWithStorage[127.0.0.1:37424,null,null]], newGenerationStamp=1004, newBlock=null, isStriped=false)
2020-12-03 07:22:59,601 [IPC Server handler 0 on default port 46834] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:initReplicaRecoveryImpl(2588)) - initReplicaRecovery: blk_1073741826_1003, recoveryId=1004, replica=ReplicaBeingWritten, blk_1073741826_1003, RBW
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741826
  bytesAcked=2048
  bytesOnDisk=2048
2020-12-03 07:22:59,601 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:49254 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003]] INFO  datanode.DataNode (BlockReceiver.java:receiveBlock(1010)) - Exception for BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/127.0.0.1:42073 remote=/127.0.0.1:49254]. 60000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:210)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:528)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:971)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:908)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:59,602 [PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[127.0.0.1:45120, 127.0.0.1:37424]] INFO  datanode.DataNode (BlockReceiver.java:run(1470)) - PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[127.0.0.1:45120, 127.0.0.1:37424]: Thread is interrupted.
2020-12-03 07:22:59,603 [PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[127.0.0.1:45120, 127.0.0.1:37424]] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[127.0.0.1:45120, 127.0.0.1:37424] terminating
2020-12-03 07:22:59,603 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:49254 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(939)) - opWriteBlock BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003 received exception java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/127.0.0.1:42073 remote=/127.0.0.1:49254]. 60000 millis timeout left.
2020-12-03 07:22:59,604 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:49254 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:42073:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:49254 dst: /127.0.0.1:42073
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/127.0.0.1:42073 remote=/127.0.0.1:49254]. 60000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:210)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:528)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:971)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:908)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:59,604 [ResponseProcessor for block BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003] WARN  hdfs.DataStreamer (DataStreamer.java:run(1196)) - Exception for BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:550)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:213)
	at org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1086)
2020-12-03 07:22:59,604 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:50764 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003]] INFO  datanode.DataNode (BlockReceiver.java:receiveBlock(1010)) - Exception for BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003
java.io.IOException: Premature EOF from inputStream
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:212)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:528)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:971)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:908)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:59,605 [DataStreamer for file /test-file1 block BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003] WARN  hdfs.DataStreamer (DataStreamer.java:handleBadDatanode(1571)) - Error Recovery for BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003 in pipeline [DatanodeInfoWithStorage[127.0.0.1:42073,DS-2ce793de-7ca9-4eff-a497-239e428e643d,DISK], DatanodeInfoWithStorage[127.0.0.1:45120,DS-3024a9be-1033-48db-8cf1-7792eec124ef,DISK], DatanodeInfoWithStorage[127.0.0.1:37424,DS-ff826779-4412-445b-8f25-01fd55cacd1d,DISK]]: datanode 0(DatanodeInfoWithStorage[127.0.0.1:42073,DS-2ce793de-7ca9-4eff-a497-239e428e643d,DISK]) is bad.
2020-12-03 07:22:59,605 [IPC Server handler 0 on default port 46834] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:initReplicaRecoveryImpl(2588)) - initReplicaRecovery: blk_1073741826_1003, recoveryId=1004, replica=ReplicaBeingWritten, blk_1073741826_1003, RBW
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741826
  bytesAcked=2048
  bytesOnDisk=2048
2020-12-03 07:22:59,606 [PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:37424]] INFO  datanode.DataNode (BlockReceiver.java:run(1470)) - PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:37424]: Thread is interrupted.
2020-12-03 07:22:59,606 [PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:37424]] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:37424] terminating
2020-12-03 07:22:59,606 [IPC Server handler 0 on default port 46834] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:initReplicaRecoveryImpl(2646)) - initReplicaRecovery: changing replica state for blk_1073741826_1003 from RBW to RUR
2020-12-03 07:22:59,607 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:50764 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(939)) - opWriteBlock BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003 received exception java.io.IOException: Premature EOF from inputStream
2020-12-03 07:22:59,608 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:50764 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:45120:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:50764 dst: /127.0.0.1:45120
java.io.IOException: Premature EOF from inputStream
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:212)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:528)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:971)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:908)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:59,608 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:36932 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003]] INFO  datanode.DataNode (BlockReceiver.java:receiveBlock(1010)) - Exception for BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003
java.io.IOException: Premature EOF from inputStream
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:212)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:528)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:971)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:908)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:59,609 [IPC Server handler 0 on default port 37371] WARN  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(450)) - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2020-12-03 07:22:59,609 [PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1470)) - PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003, type=LAST_IN_PIPELINE: Thread is interrupted.
2020-12-03 07:22:59,609 [PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003, type=LAST_IN_PIPELINE terminating
2020-12-03 07:22:59,609 [IPC Server handler 0 on default port 37371] WARN  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(450)) - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2020-12-03 07:22:59,609 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:36932 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(939)) - opWriteBlock BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003 received exception java.io.IOException: Premature EOF from inputStream
2020-12-03 07:22:59,609 [IPC Server handler 0 on default port 44735] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:initReplicaRecoveryImpl(2588)) - initReplicaRecovery: blk_1073741826_1003, recoveryId=1004, replica=ReplicaBeingWritten, blk_1073741826_1003, RBW
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741826
  bytesAcked=2048
  bytesOnDisk=2048
2020-12-03 07:22:59,612 [IPC Server handler 0 on default port 37371] WARN  protocol.BlockStoragePolicy (BlockStoragePolicy.java:chooseStorageTypes(161)) - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK, ARCHIVE], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2020-12-03 07:22:59,612 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:36932 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:37424:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:36932 dst: /127.0.0.1:37424
java.io.IOException: Premature EOF from inputStream
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:212)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:528)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:971)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:908)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:59,613 [IPC Server handler 0 on default port 37371] WARN  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(450)) - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK, ARCHIVE], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) All required storage types are unavailable:  unavailableStorages=[DISK, ARCHIVE], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2020-12-03 07:22:59,613 [IPC Server handler 0 on default port 44735] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:initReplicaRecoveryImpl(2588)) - initReplicaRecovery: blk_1073741826_1003, recoveryId=1004, replica=ReplicaBeingWritten, blk_1073741826_1003, RBW
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741826
  bytesAcked=2048
  bytesOnDisk=2048
2020-12-03 07:22:59,614 [IPC Server handler 0 on default port 44735] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:initReplicaRecoveryImpl(2646)) - initReplicaRecovery: changing replica state for blk_1073741826_1003 from RBW to RUR
2020-12-03 07:22:59,614 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@3ce258aa] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:initReplicaRecoveryImpl(2588)) - initReplicaRecovery: blk_1073741826_1003, recoveryId=1004, replica=ReplicaBeingWritten, blk_1073741826_1003, RBW
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741826
  bytesAcked=2048
  bytesOnDisk=2048
2020-12-03 07:22:59,615 [DataStreamer for file /test-file1 block BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003] WARN  hdfs.DataStreamer (DataStreamer.java:run(826)) - DataStreamer Exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37424,DS-ff826779-4412-445b-8f25-01fd55cacd1d,DISK], DatanodeInfoWithStorage[127.0.0.1:45120,DS-3024a9be-1033-48db-8cf1-7792eec124ef,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45120,DS-3024a9be-1033-48db-8cf1-7792eec124ef,DISK], DatanodeInfoWithStorage[127.0.0.1:37424,DS-ff826779-4412-445b-8f25-01fd55cacd1d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)
2020-12-03 07:22:59,615 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@3ce258aa] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:initReplicaRecoveryImpl(2588)) - initReplicaRecovery: blk_1073741826_1003, recoveryId=1004, replica=ReplicaBeingWritten, blk_1073741826_1003, RBW
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741826
  bytesAcked=2048
  bytesOnDisk=2048
2020-12-03 07:22:59,616 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@3ce258aa] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:initReplicaRecoveryImpl(2646)) - initReplicaRecovery: changing replica state for blk_1073741826_1003 from RBW to RUR
2020-12-03 07:22:59,616 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@3ce258aa] INFO  datanode.DataNode (BlockRecoveryWorker.java:syncBlock(200)) - BlockRecoveryWorker: block=BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003 (length=2048), isTruncateRecovery=false, syncList=[block:blk_1073741826_1003[numBytes=2048,originalReplicaState=RBW] node:DatanodeInfoWithStorage[127.0.0.1:42073,null,null], block:blk_1073741826_1003[numBytes=2048,originalReplicaState=RBW] node:DatanodeInfoWithStorage[127.0.0.1:45120,null,null], block:blk_1073741826_1003[numBytes=2048,originalReplicaState=RBW] node:DatanodeInfoWithStorage[127.0.0.1:37424,null,null]]
2020-12-03 07:22:59,616 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@3ce258aa] INFO  datanode.DataNode (BlockRecoveryWorker.java:syncBlock(293)) - BlockRecoveryWorker: block=BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003 (length=2048), bestState=RBW, newBlock=BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1004 (length=2048), participatingList=[block:blk_1073741826_1003[numBytes=2048,originalReplicaState=RBW] node:DatanodeInfoWithStorage[127.0.0.1:42073,null,null], block:blk_1073741826_1003[numBytes=2048,originalReplicaState=RBW] node:DatanodeInfoWithStorage[127.0.0.1:45120,null,null], block:blk_1073741826_1003[numBytes=2048,originalReplicaState=RBW] node:DatanodeInfoWithStorage[127.0.0.1:37424,null,null]]
2020-12-03 07:22:59,618 [IPC Server handler 4 on default port 46834] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:updateReplicaUnderRecovery(2667)) - updateReplica: BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003, recoveryId=1004, length=2048, replica=ReplicaUnderRecovery, blk_1073741826_1003, RUR
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741826
  recoveryId=1004
  original=ReplicaBeingWritten, blk_1073741826_1003, RBW
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741826
  bytesAcked=2048
  bytesOnDisk=2048
2020-12-03 07:22:59,621 [IPC Server handler 1 on default port 44735] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:updateReplicaUnderRecovery(2667)) - updateReplica: BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003, recoveryId=1004, length=2048, replica=ReplicaUnderRecovery, blk_1073741826_1003, RUR
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741826
  recoveryId=1004
  original=ReplicaBeingWritten, blk_1073741826_1003, RBW
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741826
  bytesAcked=2048
  bytesOnDisk=2048
2020-12-03 07:22:59,623 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@3ce258aa] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:updateReplicaUnderRecovery(2667)) - updateReplica: BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003[numBytes=2048,originalReplicaState=RBW], recoveryId=1004, length=2048, replica=ReplicaUnderRecovery, blk_1073741826_1003, RUR
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741826
  recoveryId=1004
  original=ReplicaBeingWritten, blk_1073741826_1003, RBW
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741826
  bytesAcked=2048
  bytesOnDisk=2048
2020-12-03 07:22:59,625 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@3ce258aa] INFO  namenode.TestDeleteRace (GenericTestUtils.java:answer(538)) - DelayAnswer firing fireLatch
2020-12-03 07:22:59,625 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@3ce258aa] INFO  namenode.TestDeleteRace (GenericTestUtils.java:answer(542)) - DelayAnswer waiting on waitLatch
2020-12-03 07:22:59,625 [Listener at localhost/42873] INFO  namenode.TestDeleteRace (TestDeleteRace.java:testDeleteAndCommitBlockSynchronizationRace(332)) - Deleting recursively /test-file1
2020-12-03 07:22:59,627 [IPC Server handler 1 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=delete	src=/test-file1	dst=null	perm=null	proto=rpc
2020-12-03 07:22:59,628 [Listener at localhost/42873] INFO  namenode.TestDeleteRace (TestDeleteRace.java:testDeleteAndCommitBlockSynchronizationRace(340)) - Now wait for result
2020-12-03 07:22:59,628 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@3ce258aa] INFO  namenode.TestDeleteRace (GenericTestUtils.java:answer(544)) - DelayAnswer delay complete
2020-12-03 07:22:59,629 [IPC Server handler 9 on default port 37371] INFO  namenode.FSNamesystem (FSNamesystem.java:commitBlockSynchronization(3655)) - commitBlockSynchronization(oldBlock=BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003, newgenerationstamp=1004, newlength=2048, newtargets=[127.0.0.1:42073, 127.0.0.1:45120, 127.0.0.1:37424], closeFile=true, deleteBlock=false)
2020-12-03 07:22:59,629 [IPC Server handler 9 on default port 37371] INFO  ipc.Server (Server.java:logException(2969)) - IPC Server handler 9 on default port 37371, call Call#76 Retry#0 org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.commitBlockSynchronization from 127.0.0.1:40948: java.io.FileNotFoundException: File not found: /test-file1, likely due to delayed block removal
2020-12-03 07:22:59,631 [Listener at localhost/42873] INFO  namenode.TestDeleteRace (TestDeleteRace.java:testDeleteAndCommitBlockSynchronizationRace(344)) - Result exception (snapshot: true): org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File not found: /test-file1, likely due to delayed block removal
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.commitBlockSynchronization(FSNamesystem.java:3710)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.commitBlockSynchronization(NameNodeRpcServer.java:998)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.commitBlockSynchronization(DatanodeProtocolServerSideTranslatorPB.java:302)
	at org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(DatanodeProtocolProtos.java:31676)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)

2020-12-03 07:22:59,631 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@3ce258aa] WARN  datanode.DataNode (BlockRecoveryWorker.java:run(607)) - recoverBlocks FAILED: RecoveringBlock{BP-279225940-172.17.0.11-1606980167551:blk_1073741826_1003; getBlockSize()=2048; corrupt=false; offset=-1; locs=[DatanodeInfoWithStorage[127.0.0.1:42073,null,null], DatanodeInfoWithStorage[127.0.0.1:45120,null,null], DatanodeInfoWithStorage[127.0.0.1:37424,null,null]]}
org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File not found: /test-file1, likely due to delayed block removal
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.commitBlockSynchronization(FSNamesystem.java:3710)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.commitBlockSynchronization(NameNodeRpcServer.java:998)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.commitBlockSynchronization(DatanodeProtocolServerSideTranslatorPB.java:302)
	at org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(DatanodeProtocolProtos.java:31676)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1545)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy23.commitBlockSynchronization(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.commitBlockSynchronization(DatanodeProtocolClientSideTranslatorPB.java:327)
	at org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$RecoveryTaskContiguous.syncBlock(BlockRecoveryWorker.java:333)
	at org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$RecoveryTaskContiguous.recover(BlockRecoveryWorker.java:188)
	at org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1.run(BlockRecoveryWorker.java:604)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:59,631 [Listener at localhost/42873] INFO  namenode.TestDeleteRace (TestDeleteRace.java:testDeleteAndCommitBlockSynchronizationRace(277)) - test on /testdir/testdir1/test-file mkSameDir: false snapshot: true
2020-12-03 07:22:59,635 [IPC Server handler 7 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/testdir/testdir1/test-file	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2020-12-03 07:22:59,636 [Listener at localhost/42873] INFO  namenode.TestDeleteRace (TestDeleteRace.java:testDeleteAndCommitBlockSynchronizationRace(286)) - test on /testdir/testdir1/test-file created /testdir/testdir1/test-file
2020-12-03 07:22:59,639 [IPC Server handler 6 on default port 37371] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_1073741827_1005, replicas=127.0.0.1:37424, 127.0.0.1:42073, 127.0.0.1:45120 for /testdir/testdir1/test-file
2020-12-03 07:22:59,641 [Thread-180] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:59,642 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:36960 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005 src: /127.0.0.1:36960 dest: /127.0.0.1:37424
2020-12-03 07:22:59,643 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:36960 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:59,644 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:49288 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005 src: /127.0.0.1:49288 dest: /127.0.0.1:42073
2020-12-03 07:22:59,645 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:49288 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:59,646 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:50798 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005 src: /127.0.0.1:50798 dest: /127.0.0.1:45120
2020-12-03 07:22:59,653 [IPC Server handler 5 on default port 37371] INFO  hdfs.StateChange (FSNamesystem.java:fsync(3361)) - BLOCK* fsync: /testdir/testdir1/test-file for DFSClient_NONMAPREDUCE_1516886611_25
2020-12-03 07:22:59,654 [Listener at localhost/42873] INFO  snapshot.SnapshotTestHelper (SnapshotTestHelper.java:createSnapshot(133)) - createSnapshot st2 for /
2020-12-03 07:22:59,656 [IPC Server handler 2 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/	dst=null	perm=null	proto=rpc
2020-12-03 07:22:59,657 [IPC Server handler 0 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=allowSnapshot	src=/	dst=null	perm=null	proto=rpc
2020-12-03 07:22:59,658 [IPC Server handler 4 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=createSnapshot	src=/	dst=/.snapshot/st2	perm=null	proto=rpc
2020-12-03 07:22:59,660 [IPC Server handler 3 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setSpaceQuota	src=/	dst=null	perm=null	proto=rpc
2020-12-03 07:22:59,661 [IPC Server handler 8 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/testdir/testdir1/test-file	dst=null	perm=null	proto=rpc
2020-12-03 07:22:59,673 [Listener at localhost/42873] INFO  namenode.TestDeleteRace (TestDeleteRace.java:testDeleteAndCommitBlockSynchronizationRace(304)) - Expecting block recovery to be triggered on DN 127.0.0.1:45120
2020-12-03 07:22:59,675 [IPC Server handler 1 on default port 37371] INFO  namenode.FSNamesystem (FSNamesystem.java:recoverLeaseInternal(2656)) - recoverLease: [Lease.  Holder: DFSClient_NONMAPREDUCE_1516886611_25, pending creates: 1], src=/testdir/testdir1/test-file from client DFSClient_NONMAPREDUCE_1516886611_25
2020-12-03 07:22:59,675 [IPC Server handler 1 on default port 37371] INFO  namenode.FSNamesystem (FSNamesystem.java:internalReleaseLease(3398)) - Recovering [Lease.  Holder: DFSClient_NONMAPREDUCE_1516886611_25, pending creates: 1], src=/testdir/testdir1/test-file
2020-12-03 07:22:59,676 [IPC Server handler 1 on default port 37371] WARN  hdfs.StateChange (FSNamesystem.java:internalReleaseLease(3524)) - DIR* NameSystem.internalReleaseLease: File /testdir/testdir1/test-file has not been closed. Lease recovery is in progress. RecoveryId = 1006 for block blk_1073741827_1005
2020-12-03 07:22:59,676 [Listener at localhost/42873] INFO  namenode.TestDeleteRace (TestDeleteRace.java:testDeleteAndCommitBlockSynchronizationRace(329)) - Waiting for commitBlockSynchronization call from primary
2020-12-03 07:23:02,601 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@71af1094] INFO  datanode.DataNode (BlockRecoveryWorker.java:logRecoverBlock(549)) - BlockRecoveryWorker: NameNode at localhost/127.0.0.1:37371 calls recoverBlock(BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005, targets=[DatanodeInfoWithStorage[127.0.0.1:37424,null,null], DatanodeInfoWithStorage[127.0.0.1:42073,null,null], DatanodeInfoWithStorage[127.0.0.1:45120,null,null]], newGenerationStamp=1006, newBlock=null, isStriped=false)
2020-12-03 07:23:02,607 [IPC Server handler 0 on default port 42873] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:initReplicaRecoveryImpl(2588)) - initReplicaRecovery: blk_1073741827_1005, recoveryId=1006, replica=ReplicaBeingWritten, blk_1073741827_1005, RBW
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741827
  bytesAcked=2048
  bytesOnDisk=2048
2020-12-03 07:23:02,611 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:36960 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005]] INFO  datanode.DataNode (BlockReceiver.java:receiveBlock(1010)) - Exception for BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/127.0.0.1:37424 remote=/127.0.0.1:36960]. 60000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:210)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:528)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:971)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:908)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:23:02,618 [PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[127.0.0.1:42073, 127.0.0.1:45120]] INFO  datanode.DataNode (BlockReceiver.java:run(1470)) - PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[127.0.0.1:42073, 127.0.0.1:45120]: Thread is interrupted.
2020-12-03 07:23:02,620 [PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[127.0.0.1:42073, 127.0.0.1:45120]] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[127.0.0.1:42073, 127.0.0.1:45120] terminating
2020-12-03 07:23:02,620 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:36960 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(939)) - opWriteBlock BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005 received exception java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/127.0.0.1:37424 remote=/127.0.0.1:36960]. 60000 millis timeout left.
2020-12-03 07:23:02,620 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:36960 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:37424:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:36960 dst: /127.0.0.1:37424
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/127.0.0.1:37424 remote=/127.0.0.1:36960]. 60000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:210)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:528)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:971)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:908)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:23:02,620 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:49288 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005]] INFO  datanode.DataNode (BlockReceiver.java:receiveBlock(1010)) - Exception for BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005
java.io.IOException: Premature EOF from inputStream
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:212)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:528)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:971)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:908)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:23:02,624 [IPC Server handler 0 on default port 42873] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:initReplicaRecoveryImpl(2588)) - initReplicaRecovery: blk_1073741827_1005, recoveryId=1006, replica=ReplicaBeingWritten, blk_1073741827_1005, RBW
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741827
  bytesAcked=2048
  bytesOnDisk=2048
2020-12-03 07:23:02,623 [ResponseProcessor for block BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005] WARN  hdfs.DataStreamer (DataStreamer.java:run(1196)) - Exception for BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:550)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:213)
	at org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1086)
2020-12-03 07:23:02,625 [IPC Server handler 0 on default port 42873] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:initReplicaRecoveryImpl(2646)) - initReplicaRecovery: changing replica state for blk_1073741827_1005 from RBW to RUR
2020-12-03 07:23:02,628 [PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:45120]] INFO  datanode.DataNode (BlockReceiver.java:run(1470)) - PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:45120]: Thread is interrupted.
2020-12-03 07:23:02,634 [PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:45120]] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:45120] terminating
2020-12-03 07:23:02,634 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:49288 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(939)) - opWriteBlock BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005 received exception java.io.IOException: Premature EOF from inputStream
2020-12-03 07:23:02,625 [DataStreamer for file /testdir/testdir1/test-file block BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005] WARN  hdfs.DataStreamer (DataStreamer.java:handleBadDatanode(1571)) - Error Recovery for BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005 in pipeline [DatanodeInfoWithStorage[127.0.0.1:37424,DS-ff826779-4412-445b-8f25-01fd55cacd1d,DISK], DatanodeInfoWithStorage[127.0.0.1:42073,DS-2ce793de-7ca9-4eff-a497-239e428e643d,DISK], DatanodeInfoWithStorage[127.0.0.1:45120,DS-3024a9be-1033-48db-8cf1-7792eec124ef,DISK]]: datanode 0(DatanodeInfoWithStorage[127.0.0.1:37424,DS-ff826779-4412-445b-8f25-01fd55cacd1d,DISK]) is bad.
2020-12-03 07:23:02,634 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:50798 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005]] INFO  datanode.DataNode (BlockReceiver.java:receiveBlock(1010)) - Exception for BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005
java.io.IOException: Premature EOF from inputStream
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:212)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:528)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:971)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:908)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:23:02,635 [PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1470)) - PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005, type=LAST_IN_PIPELINE: Thread is interrupted.
2020-12-03 07:23:02,636 [PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005, type=LAST_IN_PIPELINE terminating
2020-12-03 07:23:02,636 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:49288 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:42073:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:49288 dst: /127.0.0.1:42073
java.io.IOException: Premature EOF from inputStream
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:212)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:528)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:971)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:908)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:23:02,636 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:50798 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(939)) - opWriteBlock BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005 received exception java.io.IOException: Premature EOF from inputStream
2020-12-03 07:23:02,637 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:50798 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:45120:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:50798 dst: /127.0.0.1:45120
java.io.IOException: Premature EOF from inputStream
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:212)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:528)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:971)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:908)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:23:02,637 [IPC Server handler 3 on default port 46834] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:initReplicaRecoveryImpl(2588)) - initReplicaRecovery: blk_1073741827_1005, recoveryId=1006, replica=ReplicaBeingWritten, blk_1073741827_1005, RBW
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741827
  bytesAcked=2048
  bytesOnDisk=2048
2020-12-03 07:23:02,637 [IPC Server handler 4 on default port 37371] WARN  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(450)) - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2020-12-03 07:23:02,638 [IPC Server handler 3 on default port 46834] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:initReplicaRecoveryImpl(2588)) - initReplicaRecovery: blk_1073741827_1005, recoveryId=1006, replica=ReplicaBeingWritten, blk_1073741827_1005, RBW
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741827
  bytesAcked=2048
  bytesOnDisk=2048
2020-12-03 07:23:02,638 [IPC Server handler 4 on default port 37371] WARN  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(450)) - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2020-12-03 07:23:02,639 [IPC Server handler 3 on default port 46834] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:initReplicaRecoveryImpl(2646)) - initReplicaRecovery: changing replica state for blk_1073741827_1005 from RBW to RUR
2020-12-03 07:23:02,639 [IPC Server handler 4 on default port 37371] WARN  protocol.BlockStoragePolicy (BlockStoragePolicy.java:chooseStorageTypes(161)) - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK, ARCHIVE], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2020-12-03 07:23:02,640 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@71af1094] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:initReplicaRecoveryImpl(2588)) - initReplicaRecovery: blk_1073741827_1005, recoveryId=1006, replica=ReplicaBeingWritten, blk_1073741827_1005, RBW
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741827
  bytesAcked=2048
  bytesOnDisk=2048
2020-12-03 07:23:02,641 [IPC Server handler 4 on default port 37371] WARN  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(450)) - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK, ARCHIVE], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) All required storage types are unavailable:  unavailableStorages=[DISK, ARCHIVE], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2020-12-03 07:23:02,641 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@71af1094] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:initReplicaRecoveryImpl(2588)) - initReplicaRecovery: blk_1073741827_1005, recoveryId=1006, replica=ReplicaBeingWritten, blk_1073741827_1005, RBW
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741827
  bytesAcked=2048
  bytesOnDisk=2048
2020-12-03 07:23:02,643 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@71af1094] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:initReplicaRecoveryImpl(2646)) - initReplicaRecovery: changing replica state for blk_1073741827_1005 from RBW to RUR
2020-12-03 07:23:02,643 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@71af1094] INFO  datanode.DataNode (BlockRecoveryWorker.java:syncBlock(200)) - BlockRecoveryWorker: block=BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005 (length=2048), isTruncateRecovery=false, syncList=[block:blk_1073741827_1005[numBytes=2048,originalReplicaState=RBW] node:DatanodeInfoWithStorage[127.0.0.1:37424,null,null], block:blk_1073741827_1005[numBytes=2048,originalReplicaState=RBW] node:DatanodeInfoWithStorage[127.0.0.1:42073,null,null], block:blk_1073741827_1005[numBytes=2048,originalReplicaState=RBW] node:DatanodeInfoWithStorage[127.0.0.1:45120,null,null]]
2020-12-03 07:23:02,644 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@71af1094] INFO  datanode.DataNode (BlockRecoveryWorker.java:syncBlock(293)) - BlockRecoveryWorker: block=BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005 (length=2048), bestState=RBW, newBlock=BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1006 (length=2048), participatingList=[block:blk_1073741827_1005[numBytes=2048,originalReplicaState=RBW] node:DatanodeInfoWithStorage[127.0.0.1:37424,null,null], block:blk_1073741827_1005[numBytes=2048,originalReplicaState=RBW] node:DatanodeInfoWithStorage[127.0.0.1:42073,null,null], block:blk_1073741827_1005[numBytes=2048,originalReplicaState=RBW] node:DatanodeInfoWithStorage[127.0.0.1:45120,null,null]]
2020-12-03 07:23:02,645 [DataStreamer for file /testdir/testdir1/test-file block BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005] WARN  hdfs.DataStreamer (DataStreamer.java:run(826)) - DataStreamer Exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42073,DS-2ce793de-7ca9-4eff-a497-239e428e643d,DISK], DatanodeInfoWithStorage[127.0.0.1:45120,DS-3024a9be-1033-48db-8cf1-7792eec124ef,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42073,DS-2ce793de-7ca9-4eff-a497-239e428e643d,DISK], DatanodeInfoWithStorage[127.0.0.1:45120,DS-3024a9be-1033-48db-8cf1-7792eec124ef,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)
2020-12-03 07:23:02,645 [IPC Server handler 3 on default port 42873] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:updateReplicaUnderRecovery(2667)) - updateReplica: BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005, recoveryId=1006, length=2048, replica=ReplicaUnderRecovery, blk_1073741827_1005, RUR
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741827
  recoveryId=1006
  original=ReplicaBeingWritten, blk_1073741827_1005, RBW
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741827
  bytesAcked=2048
  bytesOnDisk=2048
2020-12-03 07:23:02,652 [IPC Server handler 6 on default port 46834] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:updateReplicaUnderRecovery(2667)) - updateReplica: BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005, recoveryId=1006, length=2048, replica=ReplicaUnderRecovery, blk_1073741827_1005, RUR
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741827
  recoveryId=1006
  original=ReplicaBeingWritten, blk_1073741827_1005, RBW
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741827
  bytesAcked=2048
  bytesOnDisk=2048
2020-12-03 07:23:02,654 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@71af1094] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:updateReplicaUnderRecovery(2667)) - updateReplica: BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005[numBytes=2048,originalReplicaState=RBW], recoveryId=1006, length=2048, replica=ReplicaUnderRecovery, blk_1073741827_1005, RUR
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741827
  recoveryId=1006
  original=ReplicaBeingWritten, blk_1073741827_1005, RBW
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741827
  bytesAcked=2048
  bytesOnDisk=2048
2020-12-03 07:23:02,657 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@71af1094] INFO  namenode.TestDeleteRace (GenericTestUtils.java:answer(538)) - DelayAnswer firing fireLatch
2020-12-03 07:23:02,657 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@71af1094] INFO  namenode.TestDeleteRace (GenericTestUtils.java:answer(542)) - DelayAnswer waiting on waitLatch
2020-12-03 07:23:02,657 [Listener at localhost/42873] INFO  namenode.TestDeleteRace (TestDeleteRace.java:testDeleteAndCommitBlockSynchronizationRace(332)) - Deleting recursively /testdir
2020-12-03 07:23:02,660 [IPC Server handler 7 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=delete	src=/testdir	dst=null	perm=null	proto=rpc
2020-12-03 07:23:02,661 [Listener at localhost/42873] INFO  namenode.TestDeleteRace (TestDeleteRace.java:testDeleteAndCommitBlockSynchronizationRace(340)) - Now wait for result
2020-12-03 07:23:02,661 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@71af1094] INFO  namenode.TestDeleteRace (GenericTestUtils.java:answer(544)) - DelayAnswer delay complete
2020-12-03 07:23:02,664 [IPC Server handler 9 on default port 37371] INFO  namenode.FSNamesystem (FSNamesystem.java:commitBlockSynchronization(3655)) - commitBlockSynchronization(oldBlock=BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005, newgenerationstamp=1006, newlength=2048, newtargets=[127.0.0.1:37424, 127.0.0.1:42073, 127.0.0.1:45120], closeFile=true, deleteBlock=false)
2020-12-03 07:23:02,664 [IPC Server handler 9 on default port 37371] INFO  ipc.Server (Server.java:logException(2969)) - IPC Server handler 9 on default port 37371, call Call#102 Retry#0 org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.commitBlockSynchronization from 127.0.0.1:40948: java.io.FileNotFoundException: File not found: /testdir/testdir1/test-file, likely due to delayed block removal
2020-12-03 07:23:02,666 [Listener at localhost/42873] INFO  namenode.TestDeleteRace (TestDeleteRace.java:testDeleteAndCommitBlockSynchronizationRace(344)) - Result exception (snapshot: true): org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File not found: /testdir/testdir1/test-file, likely due to delayed block removal
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.commitBlockSynchronization(FSNamesystem.java:3710)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.commitBlockSynchronization(NameNodeRpcServer.java:998)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.commitBlockSynchronization(DatanodeProtocolServerSideTranslatorPB.java:302)
	at org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(DatanodeProtocolProtos.java:31676)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)

2020-12-03 07:23:02,666 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@71af1094] WARN  datanode.DataNode (BlockRecoveryWorker.java:run(607)) - recoverBlocks FAILED: RecoveringBlock{BP-279225940-172.17.0.11-1606980167551:blk_1073741827_1005; getBlockSize()=2048; corrupt=false; offset=-1; locs=[DatanodeInfoWithStorage[127.0.0.1:37424,null,null], DatanodeInfoWithStorage[127.0.0.1:42073,null,null], DatanodeInfoWithStorage[127.0.0.1:45120,null,null]]}
org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File not found: /testdir/testdir1/test-file, likely due to delayed block removal
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.commitBlockSynchronization(FSNamesystem.java:3710)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.commitBlockSynchronization(NameNodeRpcServer.java:998)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.commitBlockSynchronization(DatanodeProtocolServerSideTranslatorPB.java:302)
	at org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(DatanodeProtocolProtos.java:31676)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1545)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy23.commitBlockSynchronization(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.commitBlockSynchronization(DatanodeProtocolClientSideTranslatorPB.java:327)
	at org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$RecoveryTaskContiguous.syncBlock(BlockRecoveryWorker.java:333)
	at org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$RecoveryTaskContiguous.recover(BlockRecoveryWorker.java:188)
	at org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1.run(BlockRecoveryWorker.java:604)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:23:02,666 [Listener at localhost/42873] INFO  namenode.TestDeleteRace (TestDeleteRace.java:testDeleteAndCommitBlockSynchronizationRace(277)) - test on /testdir/testdir1/test-file1 mkSameDir: true snapshot: true
2020-12-03 07:23:02,670 [IPC Server handler 5 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/testdir/testdir1/test-file1	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2020-12-03 07:23:02,671 [Listener at localhost/42873] INFO  namenode.TestDeleteRace (TestDeleteRace.java:testDeleteAndCommitBlockSynchronizationRace(286)) - test on /testdir/testdir1/test-file1 created /testdir/testdir1/test-file1
2020-12-03 07:23:02,675 [IPC Server handler 2 on default port 37371] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_1073741828_1007, replicas=127.0.0.1:42073, 127.0.0.1:45120, 127.0.0.1:37424 for /testdir/testdir1/test-file1
2020-12-03 07:23:02,677 [Thread-191] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:23:02,680 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:49318 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007 src: /127.0.0.1:49318 dest: /127.0.0.1:42073
2020-12-03 07:23:02,682 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:49318 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:23:02,685 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:50828 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007 src: /127.0.0.1:50828 dest: /127.0.0.1:45120
2020-12-03 07:23:02,687 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:50828 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:23:02,689 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:36996 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007 src: /127.0.0.1:36996 dest: /127.0.0.1:37424
2020-12-03 07:23:02,709 [IPC Server handler 0 on default port 37371] INFO  hdfs.StateChange (FSNamesystem.java:fsync(3361)) - BLOCK* fsync: /testdir/testdir1/test-file1 for DFSClient_NONMAPREDUCE_1516886611_25
2020-12-03 07:23:02,710 [Listener at localhost/42873] INFO  snapshot.SnapshotTestHelper (SnapshotTestHelper.java:createSnapshot(133)) - createSnapshot st3 for /
2020-12-03 07:23:02,712 [IPC Server handler 6 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/	dst=null	perm=null	proto=rpc
2020-12-03 07:23:02,714 [IPC Server handler 4 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=allowSnapshot	src=/	dst=null	perm=null	proto=rpc
2020-12-03 07:23:02,715 [IPC Server handler 3 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=createSnapshot	src=/	dst=/.snapshot/st3	perm=null	proto=rpc
2020-12-03 07:23:02,717 [IPC Server handler 8 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setSpaceQuota	src=/	dst=null	perm=null	proto=rpc
2020-12-03 07:23:02,719 [IPC Server handler 1 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/testdir/testdir1/test-file1	dst=null	perm=null	proto=rpc
2020-12-03 07:23:02,743 [Listener at localhost/42873] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:23:02,760 [Listener at localhost/42873] INFO  namenode.TestDeleteRace (TestDeleteRace.java:testDeleteAndCommitBlockSynchronizationRace(304)) - Expecting block recovery to be triggered on DN 127.0.0.1:45120
2020-12-03 07:23:02,763 [IPC Server handler 7 on default port 37371] INFO  namenode.FSNamesystem (FSNamesystem.java:recoverLeaseInternal(2656)) - recoverLease: [Lease.  Holder: DFSClient_NONMAPREDUCE_1516886611_25, pending creates: 1], src=/testdir/testdir1/test-file1 from client DFSClient_NONMAPREDUCE_1516886611_25
2020-12-03 07:23:02,764 [IPC Server handler 7 on default port 37371] INFO  namenode.FSNamesystem (FSNamesystem.java:internalReleaseLease(3398)) - Recovering [Lease.  Holder: DFSClient_NONMAPREDUCE_1516886611_25, pending creates: 1], src=/testdir/testdir1/test-file1
2020-12-03 07:23:02,764 [IPC Server handler 7 on default port 37371] WARN  hdfs.StateChange (FSNamesystem.java:internalReleaseLease(3524)) - DIR* NameSystem.internalReleaseLease: File /testdir/testdir1/test-file1 has not been closed. Lease recovery is in progress. RecoveryId = 1008 for block blk_1073741828_1007
2020-12-03 07:23:02,765 [Listener at localhost/42873] INFO  namenode.TestDeleteRace (TestDeleteRace.java:testDeleteAndCommitBlockSynchronizationRace(329)) - Waiting for commitBlockSynchronization call from primary
2020-12-03 07:23:05,608 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@574e94c3] INFO  datanode.DataNode (BlockRecoveryWorker.java:logRecoverBlock(549)) - BlockRecoveryWorker: NameNode at localhost/127.0.0.1:37371 calls recoverBlock(BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007, targets=[DatanodeInfoWithStorage[127.0.0.1:42073,null,null], DatanodeInfoWithStorage[127.0.0.1:45120,null,null], DatanodeInfoWithStorage[127.0.0.1:37424,null,null]], newGenerationStamp=1008, newBlock=null, isStriped=false)
2020-12-03 07:23:05,611 [IPC Server handler 4 on default port 46834] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:initReplicaRecoveryImpl(2588)) - initReplicaRecovery: blk_1073741828_1007, recoveryId=1008, replica=ReplicaBeingWritten, blk_1073741828_1007, RBW
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741828
  bytesAcked=2048
  bytesOnDisk=2048
2020-12-03 07:23:05,611 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:49318 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007]] INFO  datanode.DataNode (BlockReceiver.java:receiveBlock(1010)) - Exception for BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/127.0.0.1:42073 remote=/127.0.0.1:49318]. 60000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:210)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:528)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:971)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:908)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:23:05,612 [PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[127.0.0.1:45120, 127.0.0.1:37424]] INFO  datanode.DataNode (BlockReceiver.java:run(1470)) - PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[127.0.0.1:45120, 127.0.0.1:37424]: Thread is interrupted.
2020-12-03 07:23:05,612 [PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[127.0.0.1:45120, 127.0.0.1:37424]] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[127.0.0.1:45120, 127.0.0.1:37424] terminating
2020-12-03 07:23:05,612 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:49318 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(939)) - opWriteBlock BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007 received exception java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/127.0.0.1:42073 remote=/127.0.0.1:49318]. 60000 millis timeout left.
2020-12-03 07:23:05,613 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:49318 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:42073:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:49318 dst: /127.0.0.1:42073
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/127.0.0.1:42073 remote=/127.0.0.1:49318]. 60000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:210)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:528)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:971)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:908)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:23:05,614 [ResponseProcessor for block BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007] WARN  hdfs.DataStreamer (DataStreamer.java:run(1196)) - Exception for BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:550)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:213)
	at org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1086)
2020-12-03 07:23:05,613 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:50828 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007]] INFO  datanode.DataNode (BlockReceiver.java:receiveBlock(1010)) - Exception for BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007
java.io.IOException: Premature EOF from inputStream
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:212)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:528)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:971)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:908)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:23:05,616 [DataStreamer for file /testdir/testdir1/test-file1 block BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007] WARN  hdfs.DataStreamer (DataStreamer.java:handleBadDatanode(1571)) - Error Recovery for BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007 in pipeline [DatanodeInfoWithStorage[127.0.0.1:42073,DS-2ce793de-7ca9-4eff-a497-239e428e643d,DISK], DatanodeInfoWithStorage[127.0.0.1:45120,DS-3024a9be-1033-48db-8cf1-7792eec124ef,DISK], DatanodeInfoWithStorage[127.0.0.1:37424,DS-ff826779-4412-445b-8f25-01fd55cacd1d,DISK]]: datanode 0(DatanodeInfoWithStorage[127.0.0.1:42073,DS-2ce793de-7ca9-4eff-a497-239e428e643d,DISK]) is bad.
2020-12-03 07:23:05,616 [IPC Server handler 4 on default port 46834] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:initReplicaRecoveryImpl(2588)) - initReplicaRecovery: blk_1073741828_1007, recoveryId=1008, replica=ReplicaBeingWritten, blk_1073741828_1007, RBW
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741828
  bytesAcked=2048
  bytesOnDisk=2048
2020-12-03 07:23:05,617 [PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:37424]] INFO  datanode.DataNode (BlockReceiver.java:run(1470)) - PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:37424]: Thread is interrupted.
2020-12-03 07:23:05,617 [PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:37424]] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:37424] terminating
2020-12-03 07:23:05,617 [IPC Server handler 4 on default port 46834] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:initReplicaRecoveryImpl(2646)) - initReplicaRecovery: changing replica state for blk_1073741828_1007 from RBW to RUR
2020-12-03 07:23:05,617 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:50828 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(939)) - opWriteBlock BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007 received exception java.io.IOException: Premature EOF from inputStream
2020-12-03 07:23:05,617 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:50828 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:45120:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:50828 dst: /127.0.0.1:45120
java.io.IOException: Premature EOF from inputStream
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:212)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:528)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:971)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:908)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:23:05,618 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@574e94c3] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:initReplicaRecoveryImpl(2588)) - initReplicaRecovery: blk_1073741828_1007, recoveryId=1008, replica=ReplicaBeingWritten, blk_1073741828_1007, RBW
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741828
  bytesAcked=2048
  bytesOnDisk=2048
2020-12-03 07:23:05,618 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@574e94c3] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:initReplicaRecoveryImpl(2588)) - initReplicaRecovery: blk_1073741828_1007, recoveryId=1008, replica=ReplicaBeingWritten, blk_1073741828_1007, RBW
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741828
  bytesAcked=2048
  bytesOnDisk=2048
2020-12-03 07:23:05,618 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@574e94c3] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:initReplicaRecoveryImpl(2646)) - initReplicaRecovery: changing replica state for blk_1073741828_1007 from RBW to RUR
2020-12-03 07:23:05,619 [IPC Server handler 3 on default port 37371] WARN  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(450)) - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2020-12-03 07:23:05,618 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:36996 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007]] INFO  datanode.DataNode (BlockReceiver.java:receiveBlock(1010)) - Exception for BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007
java.io.IOException: Premature EOF from inputStream
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:212)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:528)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:971)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:908)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:23:05,619 [IPC Server handler 3 on default port 37371] WARN  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(450)) - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2020-12-03 07:23:05,620 [IPC Server handler 3 on default port 37371] WARN  protocol.BlockStoragePolicy (BlockStoragePolicy.java:chooseStorageTypes(161)) - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK, ARCHIVE], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2020-12-03 07:23:05,620 [IPC Server handler 3 on default port 37371] WARN  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(450)) - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK, ARCHIVE], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) All required storage types are unavailable:  unavailableStorages=[DISK, ARCHIVE], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2020-12-03 07:23:05,621 [IPC Server handler 0 on default port 42873] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:initReplicaRecoveryImpl(2588)) - initReplicaRecovery: blk_1073741828_1007, recoveryId=1008, replica=ReplicaBeingWritten, blk_1073741828_1007, RBW
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741828
  bytesAcked=2048
  bytesOnDisk=2048
2020-12-03 07:23:05,621 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:36996 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(939)) - opWriteBlock BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007 received exception java.io.IOException: Interrupted receiveBlock
2020-12-03 07:23:05,623 [DataXceiver for client DFSClient_NONMAPREDUCE_1516886611_25 at /127.0.0.1:36996 [Receiving block BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:37424:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:36996 dst: /127.0.0.1:37424
java.io.IOException: Interrupted receiveBlock
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:1070)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:908)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:23:05,623 [DataStreamer for file /testdir/testdir1/test-file1 block BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007] WARN  hdfs.DataStreamer (DataStreamer.java:run(826)) - DataStreamer Exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45120,DS-3024a9be-1033-48db-8cf1-7792eec124ef,DISK], DatanodeInfoWithStorage[127.0.0.1:37424,DS-ff826779-4412-445b-8f25-01fd55cacd1d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45120,DS-3024a9be-1033-48db-8cf1-7792eec124ef,DISK], DatanodeInfoWithStorage[127.0.0.1:37424,DS-ff826779-4412-445b-8f25-01fd55cacd1d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)
2020-12-03 07:23:05,624 [IPC Server handler 0 on default port 42873] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:initReplicaRecoveryImpl(2588)) - initReplicaRecovery: blk_1073741828_1007, recoveryId=1008, replica=ReplicaBeingWritten, blk_1073741828_1007, RBW
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741828
  bytesAcked=2048
  bytesOnDisk=2048
2020-12-03 07:23:05,624 [PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1470)) - PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007, type=LAST_IN_PIPELINE: Thread is interrupted.
2020-12-03 07:23:05,628 [PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007, type=LAST_IN_PIPELINE terminating
2020-12-03 07:23:05,628 [IPC Server handler 0 on default port 42873] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:initReplicaRecoveryImpl(2646)) - initReplicaRecovery: changing replica state for blk_1073741828_1007 from RBW to RUR
2020-12-03 07:23:05,629 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@574e94c3] INFO  datanode.DataNode (BlockRecoveryWorker.java:syncBlock(200)) - BlockRecoveryWorker: block=BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007 (length=2048), isTruncateRecovery=false, syncList=[block:blk_1073741828_1007[numBytes=2048,originalReplicaState=RBW] node:DatanodeInfoWithStorage[127.0.0.1:42073,null,null], block:blk_1073741828_1007[numBytes=2048,originalReplicaState=RBW] node:DatanodeInfoWithStorage[127.0.0.1:45120,null,null], block:blk_1073741828_1007[numBytes=2048,originalReplicaState=RBW] node:DatanodeInfoWithStorage[127.0.0.1:37424,null,null]]
2020-12-03 07:23:05,629 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@574e94c3] INFO  datanode.DataNode (BlockRecoveryWorker.java:syncBlock(293)) - BlockRecoveryWorker: block=BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007 (length=2048), bestState=RBW, newBlock=BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1008 (length=2048), participatingList=[block:blk_1073741828_1007[numBytes=2048,originalReplicaState=RBW] node:DatanodeInfoWithStorage[127.0.0.1:42073,null,null], block:blk_1073741828_1007[numBytes=2048,originalReplicaState=RBW] node:DatanodeInfoWithStorage[127.0.0.1:45120,null,null], block:blk_1073741828_1007[numBytes=2048,originalReplicaState=RBW] node:DatanodeInfoWithStorage[127.0.0.1:37424,null,null]]
2020-12-03 07:23:05,631 [IPC Server handler 3 on default port 46834] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:updateReplicaUnderRecovery(2667)) - updateReplica: BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007, recoveryId=1008, length=2048, replica=ReplicaUnderRecovery, blk_1073741828_1007, RUR
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741828
  recoveryId=1008
  original=ReplicaBeingWritten, blk_1073741828_1007, RBW
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741828
  bytesAcked=2048
  bytesOnDisk=2048
2020-12-03 07:23:05,741 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@574e94c3] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:updateReplicaUnderRecovery(2667)) - updateReplica: BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007[numBytes=2048,originalReplicaState=RBW], recoveryId=1008, length=2048, replica=ReplicaUnderRecovery, blk_1073741828_1007, RUR
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741828
  recoveryId=1008
  original=ReplicaBeingWritten, blk_1073741828_1007, RBW
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741828
  bytesAcked=2048
  bytesOnDisk=2048
2020-12-03 07:23:05,745 [IPC Server handler 1 on default port 42873] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:updateReplicaUnderRecovery(2667)) - updateReplica: BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007, recoveryId=1008, length=2048, replica=ReplicaUnderRecovery, blk_1073741828_1007, RUR
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741828
  recoveryId=1008
  original=ReplicaBeingWritten, blk_1073741828_1007, RBW
  getNumBytes()     = 2048
  getBytesOnDisk()  = 2048
  getVisibleLength()= 2048
  getVolume()       = /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6
  getBlockURI()     = file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/current/BP-279225940-172.17.0.11-1606980167551/current/rbw/blk_1073741828
  bytesAcked=2048
  bytesOnDisk=2048
2020-12-03 07:23:05,748 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@574e94c3] INFO  namenode.TestDeleteRace (GenericTestUtils.java:answer(538)) - DelayAnswer firing fireLatch
2020-12-03 07:23:05,748 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@574e94c3] INFO  namenode.TestDeleteRace (GenericTestUtils.java:answer(542)) - DelayAnswer waiting on waitLatch
2020-12-03 07:23:05,748 [Listener at localhost/42873] INFO  namenode.TestDeleteRace (TestDeleteRace.java:testDeleteAndCommitBlockSynchronizationRace(332)) - Deleting recursively /testdir
2020-12-03 07:23:05,751 [IPC Server handler 0 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=delete	src=/testdir	dst=null	perm=null	proto=rpc
2020-12-03 07:23:05,752 [Listener at localhost/42873] INFO  namenode.TestDeleteRace (TestDeleteRace.java:testDeleteAndCommitBlockSynchronizationRace(335)) - Recreate dir /testdir testpath: /testdir/testdir1/test-file1
2020-12-03 07:23:05,761 [IPC Server handler 6 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/testdir	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-12-03 07:23:05,765 [Listener at localhost/42873] INFO  namenode.TestDeleteRace (TestDeleteRace.java:testDeleteAndCommitBlockSynchronizationRace(340)) - Now wait for result
2020-12-03 07:23:05,765 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@574e94c3] INFO  namenode.TestDeleteRace (GenericTestUtils.java:answer(544)) - DelayAnswer delay complete
2020-12-03 07:23:05,769 [IPC Server handler 4 on default port 37371] INFO  namenode.FSNamesystem (FSNamesystem.java:commitBlockSynchronization(3655)) - commitBlockSynchronization(oldBlock=BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007, newgenerationstamp=1008, newlength=2048, newtargets=[127.0.0.1:42073, 127.0.0.1:45120, 127.0.0.1:37424], closeFile=true, deleteBlock=false)
2020-12-03 07:23:05,769 [IPC Server handler 4 on default port 37371] INFO  ipc.Server (Server.java:logException(2969)) - IPC Server handler 4 on default port 37371, call Call#129 Retry#0 org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.commitBlockSynchronization from 127.0.0.1:40948: java.io.FileNotFoundException: File not found: /testdir/testdir1/test-file1, likely due to delayed block removal
2020-12-03 07:23:05,771 [Listener at localhost/42873] INFO  namenode.TestDeleteRace (TestDeleteRace.java:testDeleteAndCommitBlockSynchronizationRace(344)) - Result exception (snapshot: true): org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File not found: /testdir/testdir1/test-file1, likely due to delayed block removal
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.commitBlockSynchronization(FSNamesystem.java:3710)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.commitBlockSynchronization(NameNodeRpcServer.java:998)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.commitBlockSynchronization(DatanodeProtocolServerSideTranslatorPB.java:302)
	at org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(DatanodeProtocolProtos.java:31676)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)

2020-12-03 07:23:05,771 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@574e94c3] WARN  datanode.DataNode (BlockRecoveryWorker.java:run(607)) - recoverBlocks FAILED: RecoveringBlock{BP-279225940-172.17.0.11-1606980167551:blk_1073741828_1007; getBlockSize()=2048; corrupt=false; offset=-1; locs=[DatanodeInfoWithStorage[127.0.0.1:42073,null,null], DatanodeInfoWithStorage[127.0.0.1:45120,null,null], DatanodeInfoWithStorage[127.0.0.1:37424,null,null]]}
org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File not found: /testdir/testdir1/test-file1, likely due to delayed block removal
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.commitBlockSynchronization(FSNamesystem.java:3710)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.commitBlockSynchronization(NameNodeRpcServer.java:998)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.commitBlockSynchronization(DatanodeProtocolServerSideTranslatorPB.java:302)
	at org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(DatanodeProtocolProtos.java:31676)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1545)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy23.commitBlockSynchronization(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.commitBlockSynchronization(DatanodeProtocolClientSideTranslatorPB.java:327)
	at org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$RecoveryTaskContiguous.syncBlock(BlockRecoveryWorker.java:333)
	at org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$RecoveryTaskContiguous.recover(BlockRecoveryWorker.java:188)
	at org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1.run(BlockRecoveryWorker.java:604)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:23:05,771 [Listener at localhost/42873] INFO  namenode.TestDeleteRace (TestDeleteRace.java:testDeleteAndCommitBlockSynchronizationRace(347)) - Now check we can restart
2020-12-03 07:23:05,772 [Listener at localhost/42873] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:stopAndJoinNameNode(2130)) - Shutting down the namenode
2020-12-03 07:23:05,772 [Listener at localhost/42873] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1334)) - Stopping services started for active state
2020-12-03 07:23:05,784 [Listener at localhost/42873] INFO  namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1410)) - Ending log segment 1, 47
2020-12-03 07:23:05,784 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@2b9915e9] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4198)) - LazyPersistFileScrubber was interrupted, exiting
2020-12-03 07:23:05,785 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@9e8b183] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4107)) - NameNodeEditLogRoller was interrupted, exiting
2020-12-03 07:23:05,788 [Listener at localhost/42873] INFO  namenode.FSEditLog (FSEditLog.java:printStatistics(778)) - Number of transactions: 48 Total time for transactions(ms): 38 Number of transactions batched in Syncs: 6 Number of syncs: 43 SyncTimes(ms): 4 4 
2020-12-03 07:23:05,790 [Listener at localhost/42873] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(145)) - Finalizing edits file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_inprogress_0000000000000000001 -> /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000001-0000000000000000048
2020-12-03 07:23:05,791 [Listener at localhost/42873] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(145)) - Finalizing edits file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_inprogress_0000000000000000001 -> /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000001-0000000000000000048
2020-12-03 07:23:05,795 [FSEditLogAsync] INFO  namenode.FSEditLog (FSEditLogAsync.java:run(253)) - FSEditLogAsync was interrupted, exiting
2020-12-03 07:23:05,802 [Listener at localhost/42873] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 37371
2020-12-03 07:23:05,803 [CacheReplicationMonitor(79752299)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2020-12-03 07:23:05,826 [StorageInfoMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4722)) - Stopping thread.
2020-12-03 07:23:05,831 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:23:05,827 [RedundancyMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4687)) - Stopping RedundancyMonitor.
2020-12-03 07:23:05,826 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:23:05,887 [Listener at localhost/42873] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1334)) - Stopping services started for active state
2020-12-03 07:23:05,888 [Listener at localhost/42873] INFO  namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1434)) - Stopping services started for standby state
2020-12-03 07:23:05,894 [Listener at localhost/42873] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@689a2f9{/,null,UNAVAILABLE}{/hdfs}
2020-12-03 07:23:05,900 [Listener at localhost/42873] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@7f867186{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:23:05,901 [Listener at localhost/42873] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@5f7ad052{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:23:05,901 [Listener at localhost/42873] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@10fc84d4{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:23:05,915 [Listener at localhost/42873] INFO  namenode.NameNode (NameNode.java:createNameNode(1632)) - createNameNode []
2020-12-03 07:23:05,917 [Listener at localhost/42873] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - NameNode metrics system started (again)
2020-12-03 07:23:05,918 [Listener at localhost/42873] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://localhost:37371
2020-12-03 07:23:05,919 [Listener at localhost/42873] INFO  namenode.NameNode (NameNode.java:<init>(944)) - Clients should use localhost:37371 to access this namenode/service.
2020-12-03 07:23:05,931 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@17fee192] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:23:05,931 [Listener at localhost/42873] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1641)) - Starting Web-server for hdfs at: http://localhost:41335
2020-12-03 07:23:05,931 [Listener at localhost/42873] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:23:05,935 [Listener at localhost/42873] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:23:05,936 [Listener at localhost/42873] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-12-03 07:23:05,936 [Listener at localhost/42873] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:23:05,940 [Listener at localhost/42873] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:23:05,941 [Listener at localhost/42873] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-12-03 07:23:05,941 [Listener at localhost/42873] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:23:05,941 [Listener at localhost/42873] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:23:05,943 [Listener at localhost/42873] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(102)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-12-03 07:23:05,943 [Listener at localhost/42873] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(806)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-12-03 07:23:05,944 [Listener at localhost/42873] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 41335
2020-12-03 07:23:05,944 [Listener at localhost/42873] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:23:05,957 [Listener at localhost/42873] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@d0e0821{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:23:05,957 [Listener at localhost/42873] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@796de4dd{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:23:05,972 [Listener at localhost/42873] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@617fd96c{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-12-03 07:23:05,973 [Listener at localhost/42873] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@75a30575{HTTP/1.1,[http/1.1]}{localhost:41335}
2020-12-03 07:23:05,974 [Listener at localhost/42873] INFO  server.Server (Server.java:doStart(419)) - Started @20534ms
2020-12-03 07:23:05,979 [Listener at localhost/42873] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(229)) - Edit logging is async:true
2020-12-03 07:23:05,979 [Listener at localhost/42873] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(755)) - KeyProvider: null
2020-12-03 07:23:05,979 [Listener at localhost/42873] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(123)) - fsLock is fair: true
2020-12-03 07:23:05,980 [Listener at localhost/42873] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(141)) - Detailed lock hold time metrics enabled: false
2020-12-03 07:23:05,980 [Listener at localhost/42873] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(780)) - fsOwner             = root (auth:SIMPLE)
2020-12-03 07:23:05,980 [Listener at localhost/42873] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(781)) - supergroup          = supergroup
2020-12-03 07:23:05,980 [Listener at localhost/42873] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(782)) - isPermissionEnabled = false
2020-12-03 07:23:05,981 [Listener at localhost/42873] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(793)) - HA Enabled: false
2020-12-03 07:23:05,981 [Listener at localhost/42873] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:23:05,982 [Listener at localhost/42873] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(303)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-12-03 07:23:05,982 [Listener at localhost/42873] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(311)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-12-03 07:23:05,982 [Listener at localhost/42873] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-12-03 07:23:05,983 [Listener at localhost/42873] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Dec 03 07:23:05
2020-12-03 07:23:05,983 [Listener at localhost/42873] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-12-03 07:23:05,983 [Listener at localhost/42873] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:23:05,984 [Listener at localhost/42873] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.8 GB = 36.4 MB
2020-12-03 07:23:05,984 [Listener at localhost/42873] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-12-03 07:23:05,998 [Listener at localhost/42873] INFO  blockmanagement.BlockManager (BlockManager.java:createSPSManager(5183)) - Storage policy satisfier is disabled
2020-12-03 07:23:05,998 [Listener at localhost/42873] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(595)) - dfs.block.access.token.enable = false
2020-12-03 07:23:05,999 [Listener at localhost/42873] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-12-03 07:23:05,999 [Listener at localhost/42873] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-12-03 07:23:05,999 [Listener at localhost/42873] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-12-03 07:23:05,999 [Listener at localhost/42873] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-12-03 07:23:06,000 [Listener at localhost/42873] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(581)) - defaultReplication         = 3
2020-12-03 07:23:06,000 [Listener at localhost/42873] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(582)) - maxReplication             = 512
2020-12-03 07:23:06,000 [Listener at localhost/42873] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(583)) - minReplication             = 1
2020-12-03 07:23:06,000 [Listener at localhost/42873] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(584)) - maxReplicationStreams      = 2
2020-12-03 07:23:06,000 [Listener at localhost/42873] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(585)) - redundancyRecheckInterval  = 3000ms
2020-12-03 07:23:06,000 [Listener at localhost/42873] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(586)) - encryptDataTransfer        = false
2020-12-03 07:23:06,000 [Listener at localhost/42873] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(587)) - maxNumBlocksToLog          = 1000
2020-12-03 07:23:06,001 [Listener at localhost/42873] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-12-03 07:23:06,001 [Listener at localhost/42873] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:23:06,002 [Listener at localhost/42873] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.8 GB = 18.2 MB
2020-12-03 07:23:06,002 [Listener at localhost/42873] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-12-03 07:23:06,010 [Listener at localhost/42873] INFO  namenode.FSDirectory (FSDirectory.java:<init>(290)) - ACLs enabled? false
2020-12-03 07:23:06,010 [Listener at localhost/42873] INFO  namenode.FSDirectory (FSDirectory.java:<init>(294)) - POSIX ACL inheritance enabled? true
2020-12-03 07:23:06,010 [Listener at localhost/42873] INFO  namenode.FSDirectory (FSDirectory.java:<init>(298)) - XAttrs enabled? true
2020-12-03 07:23:06,010 [Listener at localhost/42873] INFO  namenode.NameNode (FSDirectory.java:<init>(362)) - Caching file names occurring more than 10 times
2020-12-03 07:23:06,011 [Listener at localhost/42873] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-12-03 07:23:06,011 [Listener at localhost/42873] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-12-03 07:23:06,011 [Listener at localhost/42873] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-12-03 07:23:06,011 [Listener at localhost/42873] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:23:06,012 [Listener at localhost/42873] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.8 GB = 4.6 MB
2020-12-03 07:23:06,012 [Listener at localhost/42873] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-12-03 07:23:06,013 [Listener at localhost/42873] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-12-03 07:23:06,013 [Listener at localhost/42873] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-12-03 07:23:06,013 [Listener at localhost/42873] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-12-03 07:23:06,014 [Listener at localhost/42873] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(996)) - Retry cache on namenode is enabled
2020-12-03 07:23:06,014 [Listener at localhost/42873] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(1004)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-12-03 07:23:06,014 [Listener at localhost/42873] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-12-03 07:23:06,014 [Listener at localhost/42873] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:23:06,015 [Listener at localhost/42873] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.8 GB = 559.3 KB
2020-12-03 07:23:06,015 [Listener at localhost/42873] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-12-03 07:23:06,154 [Listener at localhost/42873] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/in_use.lock acquired by nodename 4867@1b16378f84d5
2020-12-03 07:23:06,209 [Listener at localhost/42873] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/in_use.lock acquired by nodename 4867@1b16378f84d5
2020-12-03 07:23:06,213 [Listener at localhost/42873] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(428)) - Recovering unfinalized segments in /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current
2020-12-03 07:23:06,216 [Listener at localhost/42873] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(428)) - Recovering unfinalized segments in /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current
2020-12-03 07:23:06,229 [Listener at localhost/42873] INFO  namenode.FSImage (FSImage.java:loadFSImageFile(797)) - Planning to load image: FSImageFile(file=/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2020-12-03 07:23:06,232 [Listener at localhost/42873] INFO  namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(234)) - Loading 1 INodes.
2020-12-03 07:23:06,234 [Listener at localhost/42873] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(246)) - Loaded FSImage in 0 seconds.
2020-12-03 07:23:06,234 [Listener at localhost/42873] INFO  namenode.FSImage (FSImage.java:loadFSImage(978)) - Loaded image for txid 0 from /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000000
2020-12-03 07:23:06,237 [Listener at localhost/42873] INFO  namenode.FSImage (FSImage.java:loadEdits(910)) - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@5d5a1a42 expecting start txid #1
2020-12-03 07:23:06,238 [Listener at localhost/42873] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(178)) - Start loading edits file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000001-0000000000000000048, /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000001-0000000000000000048 maxTxnsToRead = 9223372036854775807
2020-12-03 07:23:06,239 [Listener at localhost/42873] INFO  namenode.RedundantEditLogInputStream (RedundantEditLogInputStream.java:nextOp(186)) - Fast-forwarding stream '/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000001-0000000000000000048' to transaction ID 1
2020-12-03 07:23:06,310 [Listener at localhost/42873] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(188)) - Loaded 1 edits file(s) (the last named /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000001-0000000000000000048, /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000001-0000000000000000048) of total size 2879.0, total edits 48.0, total load time 24.0 ms
2020-12-03 07:23:06,310 [Listener at localhost/42873] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1110)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-12-03 07:23:06,311 [Listener at localhost/42873] INFO  namenode.FSEditLog (FSEditLog.java:startLogSegment(1365)) - Starting log segment at 49
2020-12-03 07:23:06,404 [Listener at localhost/42873] INFO  namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2020-12-03 07:23:06,405 [Listener at localhost/42873] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(727)) - Finished loading FSImage in 388 msecs
2020-12-03 07:23:06,405 [Listener at localhost/42873] INFO  namenode.NameNode (NameNodeRpcServer.java:<init>(448)) - RPC server is binding to localhost:37371
2020-12-03 07:23:06,407 [Listener at localhost/42873] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:23:06,408 [Socket Reader #1 for port 37371] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 37371
2020-12-03 07:23:06,416 [Listener at localhost/37371] INFO  namenode.FSNamesystem (FSNamesystem.java:registerMBean(5090)) - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2020-12-03 07:23:06,456 [Listener at localhost/37371] INFO  namenode.LeaseManager (LeaseManager.java:getNumUnderConstructionBlocks(171)) - Number of blocks under construction: 0
2020-12-03 07:23:06,458 [Listener at localhost/37371] INFO  hdfs.StateChange (BlockManagerSafeMode.java:reportStatus(617)) - STATE* Safe mode ON. 
The reported blocks 0 needs additional 3 blocks to reach the threshold 0.9990 of total blocks 4.
The minimum number of live datanodes is not required. Safe mode will be turned off automatically once the thresholds have been reached.
2020-12-03 07:23:06,465 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:23:06,465 [IPC Server listener on 37371] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 37371: starting
2020-12-03 07:23:06,471 [Listener at localhost/37371] INFO  namenode.NameNode (NameNode.java:startCommonServices(828)) - NameNode RPC up at: localhost/127.0.0.1:37371
2020-12-03 07:23:06,471 [Listener at localhost/37371] INFO  namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1222)) - Starting services required for active state
2020-12-03 07:23:06,472 [Listener at localhost/37371] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(777)) - Initializing quota with 4 thread(s)
2020-12-03 07:23:06,473 [Listener at localhost/37371] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(786)) - Quota initialization completed in 1 milliseconds
name space=10
storage space=49152
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2020-12-03 07:23:06,481 [CacheReplicationMonitor(2034990691)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-12-03 07:23:06,513 [IPC Server handler 0 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:06,521 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:06,521 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:06,624 [IPC Server handler 1 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:06,625 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:06,625 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:06,727 [IPC Server handler 2 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:06,728 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:06,728 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:06,830 [IPC Server handler 3 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:06,831 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:06,831 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:06,933 [IPC Server handler 4 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:06,934 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:06,934 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:07,035 [IPC Server handler 5 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:07,036 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:07,037 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:07,139 [IPC Server handler 6 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:07,140 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:07,140 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:07,242 [IPC Server handler 7 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:07,242 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:07,243 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:07,347 [IPC Server handler 8 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:07,348 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:07,348 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:07,450 [IPC Server handler 9 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:07,450 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:07,451 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:07,552 [IPC Server handler 1 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:07,554 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:07,554 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:07,655 [IPC Server handler 2 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:07,657 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:07,657 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:07,759 [IPC Server handler 3 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:07,760 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:07,760 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:07,861 [IPC Server handler 4 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:07,862 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:07,862 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:07,963 [IPC Server handler 5 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:07,964 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:07,964 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:08,066 [IPC Server handler 6 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:08,066 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:08,067 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:08,168 [IPC Server handler 7 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:08,169 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:08,169 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:08,270 [IPC Server handler 8 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:08,271 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:08,271 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:08,372 [IPC Server handler 9 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:08,373 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:08,373 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:08,474 [IPC Server handler 0 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:08,475 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:08,475 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:08,577 [IPC Server handler 2 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:08,577 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:08,577 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:08,598 [BP-279225940-172.17.0.11-1606980167551 heartbeating to localhost/127.0.0.1:37371] WARN  datanode.DataNode (BPServiceActor.java:offerService(731)) - IOException in offerService
java.io.EOFException: End of File Exception between local host is: "1b16378f84d5/172.17.0.11"; destination host is: "localhost":37371; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:833)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:791)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy23.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:168)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:517)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:648)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:849)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1850)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1183)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1079)
2020-12-03 07:23:08,606 [BP-279225940-172.17.0.11-1606980167551 heartbeating to localhost/127.0.0.1:37371] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActor(672)) - DatanodeCommand action : DNA_REGISTER from localhost/127.0.0.1:37371 with active state
2020-12-03 07:23:08,606 [BP-279225940-172.17.0.11-1606980167551 heartbeating to localhost/127.0.0.1:37371] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActor(672)) - DatanodeCommand action : DNA_REGISTER from localhost/127.0.0.1:37371 with active state
2020-12-03 07:23:08,608 [BP-279225940-172.17.0.11-1606980167551 heartbeating to localhost/127.0.0.1:37371] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-279225940-172.17.0.11-1606980167551 (Datanode Uuid a9f6fc87-7b25-48f4-8425-d3620813d33e) service to localhost/127.0.0.1:37371 beginning handshake with NN
2020-12-03 07:23:08,608 [BP-279225940-172.17.0.11-1606980167551 heartbeating to localhost/127.0.0.1:37371] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-279225940-172.17.0.11-1606980167551 (Datanode Uuid a4d0b9c3-5325-4729-a835-ddff222c3d3f) service to localhost/127.0.0.1:37371 beginning handshake with NN
2020-12-03 07:23:08,610 [IPC Server handler 7 on default port 37371] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:37424, datanodeUuid=a4d0b9c3-5325-4729-a835-ddff222c3d3f, infoPort=42090, infoSecurePort=0, ipcPort=42873, storageInfo=lv=-57;cid=testClusterID;nsid=1405290069;c=1606980167551) storage a4d0b9c3-5325-4729-a835-ddff222c3d3f
2020-12-03 07:23:08,610 [IPC Server handler 7 on default port 37371] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:37424
2020-12-03 07:23:08,610 [IPC Server handler 7 on default port 37371] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN a4d0b9c3-5325-4729-a835-ddff222c3d3f (127.0.0.1:37424).
2020-12-03 07:23:08,612 [IPC Server handler 8 on default port 37371] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:45120, datanodeUuid=a9f6fc87-7b25-48f4-8425-d3620813d33e, infoPort=41729, infoSecurePort=0, ipcPort=44735, storageInfo=lv=-57;cid=testClusterID;nsid=1405290069;c=1606980167551) storage a9f6fc87-7b25-48f4-8425-d3620813d33e
2020-12-03 07:23:08,612 [IPC Server handler 8 on default port 37371] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:45120
2020-12-03 07:23:08,612 [IPC Server handler 8 on default port 37371] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN a9f6fc87-7b25-48f4-8425-d3620813d33e (127.0.0.1:45120).
2020-12-03 07:23:08,612 [BP-279225940-172.17.0.11-1606980167551 heartbeating to localhost/127.0.0.1:37371] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-279225940-172.17.0.11-1606980167551 (Datanode Uuid a4d0b9c3-5325-4729-a835-ddff222c3d3f) service to localhost/127.0.0.1:37371 successfully registered with NN
2020-12-03 07:23:08,613 [BP-279225940-172.17.0.11-1606980167551 heartbeating to localhost/127.0.0.1:37371] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-279225940-172.17.0.11-1606980167551 (Datanode Uuid a9f6fc87-7b25-48f4-8425-d3620813d33e) service to localhost/127.0.0.1:37371 successfully registered with NN
2020-12-03 07:23:08,614 [IPC Server handler 9 on default port 37371] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-0c4ace5c-737b-46b0-9a95-af6b01d0e34c for DN 127.0.0.1:37424
2020-12-03 07:23:08,615 [IPC Server handler 9 on default port 37371] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-ff826779-4412-445b-8f25-01fd55cacd1d for DN 127.0.0.1:37424
2020-12-03 07:23:08,615 [IPC Server handler 0 on default port 37371] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-5d75b3b6-c3c3-415c-9506-a037c96d984c for DN 127.0.0.1:45120
2020-12-03 07:23:08,616 [IPC Server handler 0 on default port 37371] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-3024a9be-1033-48db-8cf1-7792eec124ef for DN 127.0.0.1:45120
2020-12-03 07:23:08,621 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x6eed81299680a477: Processing first storage report for DS-ff826779-4412-445b-8f25-01fd55cacd1d from datanode a4d0b9c3-5325-4729-a835-ddff222c3d3f
2020-12-03 07:23:08,624 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x6eed81299680a477: from storage DS-ff826779-4412-445b-8f25-01fd55cacd1d node DatanodeRegistration(127.0.0.1:37424, datanodeUuid=a4d0b9c3-5325-4729-a835-ddff222c3d3f, infoPort=42090, infoSecurePort=0, ipcPort=42873, storageInfo=lv=-57;cid=testClusterID;nsid=1405290069;c=1606980167551), blocks: 2, hasStaleStorage: true, processing time: 1 msecs, invalidatedBlocks: 0
2020-12-03 07:23:08,624 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x6eed81299680a477: Processing first storage report for DS-0c4ace5c-737b-46b0-9a95-af6b01d0e34c from datanode a4d0b9c3-5325-4729-a835-ddff222c3d3f
2020-12-03 07:23:08,625 [Block report processor] INFO  blockmanagement.BlockManager (BlockManager.java:initializeReplQueues(4922)) - initializing replication queues
2020-12-03 07:23:08,625 [Block report processor] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(395)) - STATE* Safe mode is OFF
2020-12-03 07:23:08,625 [Block report processor] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(400)) - STATE* Leaving safe mode after 2 secs
2020-12-03 07:23:08,626 [Block report processor] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(406)) - STATE* Network topology has 1 racks and 2 datanodes
2020-12-03 07:23:08,626 [Block report processor] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(408)) - STATE* UnderReplicatedBlocks has 0 blocks
2020-12-03 07:23:08,627 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x6eed81299680a477: from storage DS-0c4ace5c-737b-46b0-9a95-af6b01d0e34c node DatanodeRegistration(127.0.0.1:37424, datanodeUuid=a4d0b9c3-5325-4729-a835-ddff222c3d3f, infoPort=42090, infoSecurePort=0, ipcPort=42873, storageInfo=lv=-57;cid=testClusterID;nsid=1405290069;c=1606980167551), blocks: 2, hasStaleStorage: false, processing time: 3 msecs, invalidatedBlocks: 0
2020-12-03 07:23:08,631 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3585)) - Total number of blocks            = 4
2020-12-03 07:23:08,631 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3586)) - Number of invalid blocks          = 0
2020-12-03 07:23:08,631 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3587)) - Number of under-replicated blocks = 0
2020-12-03 07:23:08,631 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3588)) - Number of  over-replicated blocks = 0
2020-12-03 07:23:08,631 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3590)) - Number of blocks being written    = 4
2020-12-03 07:23:08,631 [Reconstruction Queue Initializer] INFO  hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(3593)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 6 msec
2020-12-03 07:23:08,632 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x9c417a18999f855e: Processing first storage report for DS-3024a9be-1033-48db-8cf1-7792eec124ef from datanode a9f6fc87-7b25-48f4-8425-d3620813d33e
2020-12-03 07:23:08,632 [BP-279225940-172.17.0.11-1606980167551 heartbeating to localhost/127.0.0.1:37371] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x6eed81299680a477,  containing 2 storage report(s), of which we sent 2. The reports had 4 total blocks and used 1 RPC(s). This took 1 msec to generate and 14 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:23:08,632 [BP-279225940-172.17.0.11-1606980167551 heartbeating to localhost/127.0.0.1:37371] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-279225940-172.17.0.11-1606980167551
2020-12-03 07:23:08,632 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x9c417a18999f855e: from storage DS-3024a9be-1033-48db-8cf1-7792eec124ef node DatanodeRegistration(127.0.0.1:45120, datanodeUuid=a9f6fc87-7b25-48f4-8425-d3620813d33e, infoPort=41729, infoSecurePort=0, ipcPort=44735, storageInfo=lv=-57;cid=testClusterID;nsid=1405290069;c=1606980167551), blocks: 2, hasStaleStorage: true, processing time: 1 msecs, invalidatedBlocks: 0
2020-12-03 07:23:08,633 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x9c417a18999f855e: Processing first storage report for DS-5d75b3b6-c3c3-415c-9506-a037c96d984c from datanode a9f6fc87-7b25-48f4-8425-d3620813d33e
2020-12-03 07:23:08,634 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x9c417a18999f855e: from storage DS-5d75b3b6-c3c3-415c-9506-a037c96d984c node DatanodeRegistration(127.0.0.1:45120, datanodeUuid=a9f6fc87-7b25-48f4-8425-d3620813d33e, infoPort=41729, infoSecurePort=0, ipcPort=44735, storageInfo=lv=-57;cid=testClusterID;nsid=1405290069;c=1606980167551), blocks: 2, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
2020-12-03 07:23:08,634 [BP-279225940-172.17.0.11-1606980167551 heartbeating to localhost/127.0.0.1:37371] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x9c417a18999f855e,  containing 2 storage report(s), of which we sent 2. The reports had 4 total blocks and used 1 RPC(s). This took 1 msec to generate and 17 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:23:08,635 [BP-279225940-172.17.0.11-1606980167551 heartbeating to localhost/127.0.0.1:37371] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-279225940-172.17.0.11-1606980167551
2020-12-03 07:23:08,679 [IPC Server handler 3 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:08,681 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:08,681 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:08,782 [IPC Server handler 4 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:08,783 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:08,784 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:08,885 [IPC Server handler 6 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:08,887 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:08,887 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:08,988 [IPC Server handler 5 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:08,990 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:08,990 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:09,092 [IPC Server handler 7 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:09,093 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:09,093 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:09,195 [IPC Server handler 8 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:09,196 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:09,197 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:09,299 [IPC Server handler 9 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:09,300 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:09,300 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:09,402 [IPC Server handler 0 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:09,406 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:09,406 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:09,507 [IPC Server handler 1 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:09,509 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:09,509 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:09,611 [IPC Server handler 2 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:09,612 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:09,613 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:09,714 [IPC Server handler 4 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:09,715 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:09,716 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:09,817 [IPC Server handler 6 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:09,818 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:09,818 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:09,921 [IPC Server handler 5 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:09,922 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:09,922 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:10,024 [IPC Server handler 7 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:10,026 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:10,026 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:10,128 [IPC Server handler 8 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:10,129 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:10,129 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:10,231 [IPC Server handler 9 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:10,233 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:10,233 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:10,335 [IPC Server handler 0 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:10,336 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:10,336 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:10,438 [IPC Server handler 1 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:10,439 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:10,439 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:10,540 [IPC Server handler 2 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:10,542 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:10,542 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:10,644 [IPC Server handler 3 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:10,645 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:10,645 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:10,747 [IPC Server handler 6 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:10,748 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:10,748 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:10,850 [IPC Server handler 5 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:10,852 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:10,852 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:10,954 [IPC Server handler 7 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:10,955 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:10,955 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:11,057 [IPC Server handler 8 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:11,059 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:11,059 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:11,161 [IPC Server handler 9 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:11,162 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:11,162 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:11,264 [IPC Server handler 0 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:11,265 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:11,265 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:11,367 [IPC Server handler 1 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:11,368 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:11,368 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:11,469 [IPC Server handler 2 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:11,471 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:11,471 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:11,577 [IPC Server handler 3 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:11,579 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:23:11,579 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:23:11,600 [BP-279225940-172.17.0.11-1606980167551 heartbeating to localhost/127.0.0.1:37371] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActor(672)) - DatanodeCommand action : DNA_REGISTER from localhost/127.0.0.1:37371 with active state
2020-12-03 07:23:11,602 [BP-279225940-172.17.0.11-1606980167551 heartbeating to localhost/127.0.0.1:37371] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-279225940-172.17.0.11-1606980167551 (Datanode Uuid 06f17149-3c92-4fc6-af33-7ac1a1270c19) service to localhost/127.0.0.1:37371 beginning handshake with NN
2020-12-03 07:23:11,603 [IPC Server handler 5 on default port 37371] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:42073, datanodeUuid=06f17149-3c92-4fc6-af33-7ac1a1270c19, infoPort=45021, infoSecurePort=0, ipcPort=46834, storageInfo=lv=-57;cid=testClusterID;nsid=1405290069;c=1606980167551) storage 06f17149-3c92-4fc6-af33-7ac1a1270c19
2020-12-03 07:23:11,603 [IPC Server handler 5 on default port 37371] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:42073
2020-12-03 07:23:11,604 [IPC Server handler 5 on default port 37371] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 06f17149-3c92-4fc6-af33-7ac1a1270c19 (127.0.0.1:42073).
2020-12-03 07:23:11,604 [BP-279225940-172.17.0.11-1606980167551 heartbeating to localhost/127.0.0.1:37371] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-279225940-172.17.0.11-1606980167551 (Datanode Uuid 06f17149-3c92-4fc6-af33-7ac1a1270c19) service to localhost/127.0.0.1:37371 successfully registered with NN
2020-12-03 07:23:11,606 [IPC Server handler 7 on default port 37371] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-2ce793de-7ca9-4eff-a497-239e428e643d for DN 127.0.0.1:42073
2020-12-03 07:23:11,610 [IPC Server handler 7 on default port 37371] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-4d64cd8d-286f-4d38-9cd8-eaa887a5ffa1 for DN 127.0.0.1:42073
2020-12-03 07:23:11,612 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x66ddcd24d6f38c63: Processing first storage report for DS-2ce793de-7ca9-4eff-a497-239e428e643d from datanode 06f17149-3c92-4fc6-af33-7ac1a1270c19
2020-12-03 07:23:11,613 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x66ddcd24d6f38c63: from storage DS-2ce793de-7ca9-4eff-a497-239e428e643d node DatanodeRegistration(127.0.0.1:42073, datanodeUuid=06f17149-3c92-4fc6-af33-7ac1a1270c19, infoPort=45021, infoSecurePort=0, ipcPort=46834, storageInfo=lv=-57;cid=testClusterID;nsid=1405290069;c=1606980167551), blocks: 2, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:23:11,614 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x66ddcd24d6f38c63: Processing first storage report for DS-4d64cd8d-286f-4d38-9cd8-eaa887a5ffa1 from datanode 06f17149-3c92-4fc6-af33-7ac1a1270c19
2020-12-03 07:23:11,614 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x66ddcd24d6f38c63: from storage DS-4d64cd8d-286f-4d38-9cd8-eaa887a5ffa1 node DatanodeRegistration(127.0.0.1:42073, datanodeUuid=06f17149-3c92-4fc6-af33-7ac1a1270c19, infoPort=45021, infoSecurePort=0, ipcPort=46834, storageInfo=lv=-57;cid=testClusterID;nsid=1405290069;c=1606980167551), blocks: 2, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:23:11,615 [BP-279225940-172.17.0.11-1606980167551 heartbeating to localhost/127.0.0.1:37371] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x66ddcd24d6f38c63,  containing 2 storage report(s), of which we sent 2. The reports had 4 total blocks and used 1 RPC(s). This took 0 msec to generate and 4 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:23:11,615 [BP-279225940-172.17.0.11-1606980167551 heartbeating to localhost/127.0.0.1:37371] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-279225940-172.17.0.11-1606980167551
2020-12-03 07:23:11,681 [IPC Server handler 1 on default port 37371] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:23:11,684 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2758)) - Cluster is active
2020-12-03 07:23:11,685 [Listener at localhost/37371] INFO  namenode.TestDeleteRace (TestDeleteRace.java:testDeleteAndCommitBlockSynchronizationRace(349)) - Restart finished
2020-12-03 07:23:11,685 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(2049)) - Shutting down the Mini HDFS Cluster
2020-12-03 07:23:11,685 [Listener at localhost/37371] ERROR hdfs.DFSClient (DFSClient.java:closeAllFilesBeingWritten(617)) - Failed to close file: /test-file with inode: 16386
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42073,DS-2ce793de-7ca9-4eff-a497-239e428e643d,DISK], DatanodeInfoWithStorage[127.0.0.1:45120,DS-3024a9be-1033-48db-8cf1-7792eec124ef,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45120,DS-3024a9be-1033-48db-8cf1-7792eec124ef,DISK], DatanodeInfoWithStorage[127.0.0.1:42073,DS-2ce793de-7ca9-4eff-a497-239e428e643d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)
2020-12-03 07:23:11,686 [Listener at localhost/37371] ERROR hdfs.DFSClient (DFSClient.java:closeAllFilesBeingWritten(617)) - Failed to close file: /test-file1 with inode: 16387
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37424,DS-ff826779-4412-445b-8f25-01fd55cacd1d,DISK], DatanodeInfoWithStorage[127.0.0.1:45120,DS-3024a9be-1033-48db-8cf1-7792eec124ef,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45120,DS-3024a9be-1033-48db-8cf1-7792eec124ef,DISK], DatanodeInfoWithStorage[127.0.0.1:37424,DS-ff826779-4412-445b-8f25-01fd55cacd1d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)
2020-12-03 07:23:11,686 [Listener at localhost/37371] ERROR hdfs.DFSClient (DFSClient.java:closeAllFilesBeingWritten(617)) - Failed to close file: /testdir/testdir1/test-file with inode: 16390
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42073,DS-2ce793de-7ca9-4eff-a497-239e428e643d,DISK], DatanodeInfoWithStorage[127.0.0.1:45120,DS-3024a9be-1033-48db-8cf1-7792eec124ef,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42073,DS-2ce793de-7ca9-4eff-a497-239e428e643d,DISK], DatanodeInfoWithStorage[127.0.0.1:45120,DS-3024a9be-1033-48db-8cf1-7792eec124ef,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)
2020-12-03 07:23:11,686 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 2
2020-12-03 07:23:11,687 [Listener at localhost/37371] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:23:11,687 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@831e3bf] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:23:11,691 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5, DS-0c4ace5c-737b-46b0-9a95-af6b01d0e34c) exiting.
2020-12-03 07:23:11,693 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6, DS-ff826779-4412-445b-8f25-01fd55cacd1d) exiting.
2020-12-03 07:23:11,857 [Listener at localhost/37371] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@4b6eb0c5{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:23:11,861 [Listener at localhost/37371] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@2fbc5263{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:23:11,862 [Listener at localhost/37371] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@79ea3dc6{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:23:11,863 [Listener at localhost/37371] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@145d1600{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:23:11,879 [Listener at localhost/37371] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 42873
2020-12-03 07:23:11,886 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:23:11,886 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:23:11,889 [BP-279225940-172.17.0.11-1606980167551 heartbeating to localhost/127.0.0.1:37371] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:23:11,893 [BP-279225940-172.17.0.11-1606980167551 heartbeating to localhost/127.0.0.1:37371] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-279225940-172.17.0.11-1606980167551 (Datanode Uuid a4d0b9c3-5325-4729-a835-ddff222c3d3f) service to localhost/127.0.0.1:37371
2020-12-03 07:23:11,894 [BP-279225940-172.17.0.11-1606980167551 heartbeating to localhost/127.0.0.1:37371] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-279225940-172.17.0.11-1606980167551 (Datanode Uuid a4d0b9c3-5325-4729-a835-ddff222c3d3f)
2020-12-03 07:23:11,895 [BP-279225940-172.17.0.11-1606980167551 heartbeating to localhost/127.0.0.1:37371] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-279225940-172.17.0.11-1606980167551
2020-12-03 07:23:11,898 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-279225940-172.17.0.11-1606980167551] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:23:11,899 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/current/BP-279225940-172.17.0.11-1606980167551] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:23:11,906 [Listener at localhost/37371] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:23:11,907 [Listener at localhost/37371] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:23:11,907 [Listener at localhost/37371] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:23:11,908 [Listener at localhost/37371] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:23:11,911 [Listener at localhost/37371] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:23:11,911 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 1
2020-12-03 07:23:11,912 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@f16c15d] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:23:11,912 [Listener at localhost/37371] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:23:11,914 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3, DS-5d75b3b6-c3c3-415c-9506-a037c96d984c) exiting.
2020-12-03 07:23:11,914 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, DS-3024a9be-1033-48db-8cf1-7792eec124ef) exiting.
2020-12-03 07:23:12,242 [Listener at localhost/37371] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@175319e7{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:23:12,250 [Listener at localhost/37371] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@4549f1d3{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:23:12,251 [Listener at localhost/37371] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@e6a4f6e{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:23:12,252 [Listener at localhost/37371] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@38bac53e{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:23:12,254 [Listener at localhost/37371] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 44735
2020-12-03 07:23:12,257 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:23:12,258 [BP-279225940-172.17.0.11-1606980167551 heartbeating to localhost/127.0.0.1:37371] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:23:12,259 [BP-279225940-172.17.0.11-1606980167551 heartbeating to localhost/127.0.0.1:37371] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-279225940-172.17.0.11-1606980167551 (Datanode Uuid a9f6fc87-7b25-48f4-8425-d3620813d33e) service to localhost/127.0.0.1:37371
2020-12-03 07:23:12,259 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:23:12,259 [BP-279225940-172.17.0.11-1606980167551 heartbeating to localhost/127.0.0.1:37371] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-279225940-172.17.0.11-1606980167551 (Datanode Uuid a9f6fc87-7b25-48f4-8425-d3620813d33e)
2020-12-03 07:23:12,260 [BP-279225940-172.17.0.11-1606980167551 heartbeating to localhost/127.0.0.1:37371] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-279225940-172.17.0.11-1606980167551
2020-12-03 07:23:12,262 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-279225940-172.17.0.11-1606980167551] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:23:12,262 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-279225940-172.17.0.11-1606980167551] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:23:12,268 [Listener at localhost/37371] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:23:12,268 [Listener at localhost/37371] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:23:12,269 [Listener at localhost/37371] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:23:12,269 [Listener at localhost/37371] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:23:12,272 [Listener at localhost/37371] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:23:12,272 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 0
2020-12-03 07:23:12,273 [Listener at localhost/37371] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:23:12,273 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@63b26a0a] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:23:12,274 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-4d64cd8d-286f-4d38-9cd8-eaa887a5ffa1) exiting.
2020-12-03 07:23:12,274 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-2ce793de-7ca9-4eff-a497-239e428e643d) exiting.
2020-12-03 07:23:12,318 [Listener at localhost/37371] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@5a1ec39c{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:23:12,320 [Listener at localhost/37371] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@116522b2{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:23:12,320 [Listener at localhost/37371] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@233ca3ea{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:23:12,321 [Listener at localhost/37371] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@2ea7b934{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:23:12,328 [Listener at localhost/37371] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 46834
2020-12-03 07:23:12,338 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:23:12,364 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:23:12,366 [BP-279225940-172.17.0.11-1606980167551 heartbeating to localhost/127.0.0.1:37371] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:23:12,367 [BP-279225940-172.17.0.11-1606980167551 heartbeating to localhost/127.0.0.1:37371] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-279225940-172.17.0.11-1606980167551 (Datanode Uuid 06f17149-3c92-4fc6-af33-7ac1a1270c19) service to localhost/127.0.0.1:37371
2020-12-03 07:23:12,367 [BP-279225940-172.17.0.11-1606980167551 heartbeating to localhost/127.0.0.1:37371] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-279225940-172.17.0.11-1606980167551 (Datanode Uuid 06f17149-3c92-4fc6-af33-7ac1a1270c19)
2020-12-03 07:23:12,367 [BP-279225940-172.17.0.11-1606980167551 heartbeating to localhost/127.0.0.1:37371] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-279225940-172.17.0.11-1606980167551
2020-12-03 07:23:12,369 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-279225940-172.17.0.11-1606980167551] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:23:12,369 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-279225940-172.17.0.11-1606980167551] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:23:12,381 [Listener at localhost/37371] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:23:12,381 [Listener at localhost/37371] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:23:12,383 [Listener at localhost/37371] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:23:12,383 [Listener at localhost/37371] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:23:12,387 [Listener at localhost/37371] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:23:12,387 [Listener at localhost/37371] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:stopAndJoinNameNode(2130)) - Shutting down the namenode
2020-12-03 07:23:12,387 [Listener at localhost/37371] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1334)) - Stopping services started for active state
2020-12-03 07:23:12,400 [Listener at localhost/37371] INFO  namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1410)) - Ending log segment 49, 49
2020-12-03 07:23:12,400 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@52329092] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4198)) - LazyPersistFileScrubber was interrupted, exiting
2020-12-03 07:23:12,400 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@2da0f1e1] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4107)) - NameNodeEditLogRoller was interrupted, exiting
2020-12-03 07:23:12,429 [Listener at localhost/37371] INFO  namenode.FSEditLog (FSEditLog.java:printStatistics(778)) - Number of transactions: 2 Total time for transactions(ms): 13 Number of transactions batched in Syncs: 48 Number of syncs: 3 SyncTimes(ms): 3 0 
2020-12-03 07:23:12,431 [Listener at localhost/37371] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(145)) - Finalizing edits file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_inprogress_0000000000000000049 -> /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000049-0000000000000000050
2020-12-03 07:23:12,432 [Listener at localhost/37371] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(145)) - Finalizing edits file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_inprogress_0000000000000000049 -> /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000049-0000000000000000050
2020-12-03 07:23:12,432 [FSEditLogAsync] INFO  namenode.FSEditLog (FSEditLogAsync.java:run(253)) - FSEditLogAsync was interrupted, exiting
2020-12-03 07:23:12,436 [CacheReplicationMonitor(2034990691)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2020-12-03 07:23:12,451 [Listener at localhost/37371] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 37371
2020-12-03 07:23:12,455 [IPC Server listener on 37371] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 37371
2020-12-03 07:23:12,455 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:23:12,458 [RedundancyMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4687)) - Stopping RedundancyMonitor.
2020-12-03 07:23:12,459 [StorageInfoMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4722)) - Stopping thread.
2020-12-03 07:23:12,479 [Listener at localhost/37371] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1334)) - Stopping services started for active state
2020-12-03 07:23:12,479 [Listener at localhost/37371] INFO  namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1434)) - Stopping services started for standby state
2020-12-03 07:23:12,481 [Listener at localhost/37371] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@617fd96c{/,null,UNAVAILABLE}{/hdfs}
2020-12-03 07:23:12,483 [Listener at localhost/37371] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@75a30575{HTTP/1.1,[http/1.1]}{localhost:41335}
2020-12-03 07:23:12,492 [Listener at localhost/37371] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@796de4dd{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:23:12,493 [Listener at localhost/37371] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@d0e0821{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:23:12,501 [Listener at localhost/37371] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping NameNode metrics system...
2020-12-03 07:23:12,511 [Listener at localhost/37371] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - NameNode metrics system stopped.
2020-12-03 07:23:12,513 [Listener at localhost/37371] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - NameNode metrics system shutdown complete.
msx-rc 0
