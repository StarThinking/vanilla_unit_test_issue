2020-12-03 07:24:00,560 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(493)) - starting cluster: numNameNodes=1, numDataNodes=11
Formatting using clusterid: testClusterID
2020-12-03 07:24:01,551 [main] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(229)) - Edit logging is async:true
2020-12-03 07:24:01,566 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(755)) - KeyProvider: null
2020-12-03 07:24:01,568 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(123)) - fsLock is fair: true
2020-12-03 07:24:01,569 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(141)) - Detailed lock hold time metrics enabled: false
2020-12-03 07:24:01,600 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(780)) - fsOwner             = root (auth:SIMPLE)
2020-12-03 07:24:01,601 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(781)) - supergroup          = supergroup
2020-12-03 07:24:01,601 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(782)) - isPermissionEnabled = true
2020-12-03 07:24:01,602 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(793)) - HA Enabled: false
2020-12-03 07:24:01,671 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:24:01,679 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - hadoop.configured.node.mapping is deprecated. Instead, use net.topology.configured.node.mapping
2020-12-03 07:24:01,680 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(303)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-12-03 07:24:01,680 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(311)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-12-03 07:24:01,688 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-12-03 07:24:01,689 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Dec 03 07:24:01
2020-12-03 07:24:01,693 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-12-03 07:24:01,695 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:24:01,698 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.9 GB = 39.3 MB
2020-12-03 07:24:01,698 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-12-03 07:24:01,722 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createSPSManager(5183)) - Storage policy satisfier is disabled
2020-12-03 07:24:01,722 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(595)) - dfs.block.access.token.enable = true
2020-12-03 07:24:01,723 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(617)) - dfs.block.access.key.update.interval=600 min(s), dfs.block.access.token.lifetime=600 min(s), dfs.encrypt.data.transfer.algorithm=null
2020-12-03 07:24:01,764 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-12-03 07:24:01,766 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-12-03 07:24:01,766 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-12-03 07:24:01,767 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-12-03 07:24:01,768 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(581)) - defaultReplication         = 3
2020-12-03 07:24:01,769 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(582)) - maxReplication             = 512
2020-12-03 07:24:01,769 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(583)) - minReplication             = 1
2020-12-03 07:24:01,769 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(584)) - maxReplicationStreams      = 0
2020-12-03 07:24:01,770 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(585)) - redundancyRecheckInterval  = 3000ms
2020-12-03 07:24:01,770 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(586)) - encryptDataTransfer        = false
2020-12-03 07:24:01,771 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(587)) - maxNumBlocksToLog          = 1000
2020-12-03 07:24:01,809 [main] INFO  namenode.FSDirectory (SerialNumberManager.java:<clinit>(51)) - GLOBAL serial map: bits=29 maxEntries=536870911
2020-12-03 07:24:01,809 [main] INFO  namenode.FSDirectory (SerialNumberManager.java:<clinit>(51)) - USER serial map: bits=24 maxEntries=16777215
2020-12-03 07:24:01,810 [main] INFO  namenode.FSDirectory (SerialNumberManager.java:<clinit>(51)) - GROUP serial map: bits=24 maxEntries=16777215
2020-12-03 07:24:01,810 [main] INFO  namenode.FSDirectory (SerialNumberManager.java:<clinit>(51)) - XATTR serial map: bits=24 maxEntries=16777215
2020-12-03 07:24:01,832 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-12-03 07:24:01,833 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:24:01,834 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.9 GB = 19.6 MB
2020-12-03 07:24:01,834 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-12-03 07:24:01,842 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(290)) - ACLs enabled? false
2020-12-03 07:24:01,843 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(294)) - POSIX ACL inheritance enabled? true
2020-12-03 07:24:01,843 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(298)) - XAttrs enabled? true
2020-12-03 07:24:01,844 [main] INFO  namenode.NameNode (FSDirectory.java:<init>(362)) - Caching file names occurring more than 10 times
2020-12-03 07:24:01,853 [main] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-12-03 07:24:01,857 [main] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-12-03 07:24:01,866 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-12-03 07:24:01,867 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:24:01,867 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.9 GB = 4.9 MB
2020-12-03 07:24:01,867 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-12-03 07:24:01,877 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-12-03 07:24:01,877 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-12-03 07:24:01,878 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-12-03 07:24:01,886 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(996)) - Retry cache on namenode is enabled
2020-12-03 07:24:01,886 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(1004)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-12-03 07:24:01,889 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-12-03 07:24:01,890 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:24:01,890 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.9 GB = 603.0 KB
2020-12-03 07:24:01,890 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-12-03 07:24:01,944 [main] INFO  namenode.FSImage (FSImage.java:format(185)) - Allocated new BlockPoolId: BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:02,038 [main] INFO  common.Storage (NNStorage.java:format(595)) - Storage directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1 has been successfully formatted.
2020-12-03 07:24:02,098 [main] INFO  common.Storage (NNStorage.java:format(595)) - Storage directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2 has been successfully formatted.
2020-12-03 07:24:02,136 [FSImageSaver for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(512)) - Saving image file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/fsimage.ckpt_0000000000000000000 using no compression
2020-12-03 07:24:02,136 [FSImageSaver for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(512)) - Saving image file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage.ckpt_0000000000000000000 using no compression
2020-12-03 07:24:02,306 [FSImageSaver for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(516)) - Image file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage.ckpt_0000000000000000000 of size 396 bytes saved in 0 seconds .
2020-12-03 07:24:02,306 [FSImageSaver for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(516)) - Image file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/fsimage.ckpt_0000000000000000000 of size 396 bytes saved in 0 seconds .
2020-12-03 07:24:02,372 [main] INFO  namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:getImageTxIdToRetain(203)) - Going to retain 1 images with txid >= 0
2020-12-03 07:24:02,377 [main] INFO  namenode.NameNode (NameNode.java:createNameNode(1632)) - createNameNode []
2020-12-03 07:24:02,775 [main] INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(118)) - Loaded properties from hadoop-metrics2.properties
2020-12-03 07:24:02,879 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 0 second(s).
2020-12-03 07:24:02,879 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2020-12-03 07:24:02,913 [main] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://127.0.0.1:0
2020-12-03 07:24:03,047 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@437da279] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:24:03,065 [main] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1641)) - Starting Web-server for hdfs at: http://localhost:0
2020-12-03 07:24:03,072 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:24:03,088 [main] INFO  util.log (Log.java:initialized(192)) - Logging initialized @3856ms
2020-12-03 07:24:03,225 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:24:03,229 [main] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-12-03 07:24:03,230 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:24:03,242 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:24:03,245 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-12-03 07:24:03,245 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:24:03,245 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:24:03,281 [main] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(102)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-12-03 07:24:03,282 [main] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(806)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-12-03 07:24:03,294 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 32878
2020-12-03 07:24:03,297 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:24:03,366 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@3e0e1046{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:24:03,367 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@7dc19a70{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:24:03,429 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@1c852c0f{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-12-03 07:24:03,439 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@3b79fd76{HTTP/1.1,[http/1.1]}{localhost:32878}
2020-12-03 07:24:03,440 [main] INFO  server.Server (Server.java:doStart(419)) - Started @4208ms
2020-12-03 07:24:03,451 [main] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(229)) - Edit logging is async:true
2020-12-03 07:24:03,452 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(755)) - KeyProvider: null
2020-12-03 07:24:03,452 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(123)) - fsLock is fair: true
2020-12-03 07:24:03,452 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(141)) - Detailed lock hold time metrics enabled: false
2020-12-03 07:24:03,452 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(780)) - fsOwner             = root (auth:SIMPLE)
2020-12-03 07:24:03,453 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(781)) - supergroup          = supergroup
2020-12-03 07:24:03,453 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(782)) - isPermissionEnabled = true
2020-12-03 07:24:03,453 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(793)) - HA Enabled: false
2020-12-03 07:24:03,454 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:24:03,455 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(303)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-12-03 07:24:03,455 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(311)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-12-03 07:24:03,455 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-12-03 07:24:03,456 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Dec 03 07:24:03
2020-12-03 07:24:03,456 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-12-03 07:24:03,456 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:24:03,457 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.8 GB = 36.4 MB
2020-12-03 07:24:03,457 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-12-03 07:24:03,463 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createSPSManager(5183)) - Storage policy satisfier is disabled
2020-12-03 07:24:03,463 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(595)) - dfs.block.access.token.enable = true
2020-12-03 07:24:03,464 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(617)) - dfs.block.access.key.update.interval=600 min(s), dfs.block.access.token.lifetime=600 min(s), dfs.encrypt.data.transfer.algorithm=null
2020-12-03 07:24:03,465 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-12-03 07:24:03,466 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-12-03 07:24:03,466 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-12-03 07:24:03,466 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-12-03 07:24:03,467 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(581)) - defaultReplication         = 3
2020-12-03 07:24:03,467 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(582)) - maxReplication             = 512
2020-12-03 07:24:03,467 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(583)) - minReplication             = 1
2020-12-03 07:24:03,468 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(584)) - maxReplicationStreams      = 0
2020-12-03 07:24:03,468 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(585)) - redundancyRecheckInterval  = 3000ms
2020-12-03 07:24:03,468 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(586)) - encryptDataTransfer        = false
2020-12-03 07:24:03,468 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(587)) - maxNumBlocksToLog          = 1000
2020-12-03 07:24:03,469 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-12-03 07:24:03,470 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:24:03,470 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.8 GB = 18.2 MB
2020-12-03 07:24:03,470 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-12-03 07:24:03,473 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(290)) - ACLs enabled? false
2020-12-03 07:24:03,474 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(294)) - POSIX ACL inheritance enabled? true
2020-12-03 07:24:03,474 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(298)) - XAttrs enabled? true
2020-12-03 07:24:03,474 [main] INFO  namenode.NameNode (FSDirectory.java:<init>(362)) - Caching file names occurring more than 10 times
2020-12-03 07:24:03,474 [main] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-12-03 07:24:03,474 [main] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-12-03 07:24:03,475 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-12-03 07:24:03,475 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:24:03,475 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.8 GB = 4.6 MB
2020-12-03 07:24:03,476 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-12-03 07:24:03,477 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-12-03 07:24:03,477 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-12-03 07:24:03,477 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-12-03 07:24:03,477 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(996)) - Retry cache on namenode is enabled
2020-12-03 07:24:03,477 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(1004)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-12-03 07:24:03,478 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-12-03 07:24:03,478 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:24:03,478 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.8 GB = 559.3 KB
2020-12-03 07:24:03,478 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-12-03 07:24:03,568 [main] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/in_use.lock acquired by nodename 12367@274d470f39cc
2020-12-03 07:24:04,013 [main] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/in_use.lock acquired by nodename 12367@274d470f39cc
2020-12-03 07:24:04,028 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(428)) - Recovering unfinalized segments in /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current
2020-12-03 07:24:04,028 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(428)) - Recovering unfinalized segments in /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current
2020-12-03 07:24:04,029 [main] INFO  namenode.FSImage (FSImage.java:loadFSImage(733)) - No edit log streams selected.
2020-12-03 07:24:04,029 [main] INFO  namenode.FSImage (FSImage.java:loadFSImageFile(797)) - Planning to load image: FSImageFile(file=/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2020-12-03 07:24:04,171 [main] INFO  namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(234)) - Loading 1 INodes.
2020-12-03 07:24:04,181 [main] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(246)) - Loaded FSImage in 0 seconds.
2020-12-03 07:24:04,181 [main] INFO  namenode.FSImage (FSImage.java:loadFSImage(978)) - Loaded image for txid 0 from /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000000
2020-12-03 07:24:04,188 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1110)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-12-03 07:24:04,190 [main] INFO  namenode.FSEditLog (FSEditLog.java:startLogSegment(1365)) - Starting log segment at 1
2020-12-03 07:24:05,370 [main] INFO  namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2020-12-03 07:24:05,998 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(727)) - Finished loading FSImage in 2517 msecs
2020-12-03 07:24:06,722 [main] INFO  namenode.NameNode (NameNodeRpcServer.java:<init>(448)) - RPC server is binding to localhost:0
2020-12-03 07:24:07,133 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:24:07,528 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:24:09,075 [Listener at localhost/38816] INFO  namenode.NameNode (NameNode.java:initialize(722)) - Clients are to use localhost:38816 to access this namenode/service.
2020-12-03 07:24:09,227 [Listener at localhost/38816] INFO  namenode.FSNamesystem (FSNamesystem.java:registerMBean(5090)) - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2020-12-03 07:24:09,453 [Listener at localhost/38816] INFO  namenode.LeaseManager (LeaseManager.java:getNumUnderConstructionBlocks(171)) - Number of blocks under construction: 0
2020-12-03 07:24:09,467 [org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager$Monitor@79c97cb] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:updateKeys(263)) - Updating block keys
2020-12-03 07:24:09,470 [Listener at localhost/38816] INFO  blockmanagement.BlockManager (BlockManager.java:initializeReplQueues(4922)) - initializing replication queues
2020-12-03 07:24:09,471 [Listener at localhost/38816] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(400)) - STATE* Leaving safe mode after 0 secs
2020-12-03 07:24:09,471 [Listener at localhost/38816] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(406)) - STATE* Network topology has 0 racks and 0 datanodes
2020-12-03 07:24:09,471 [Listener at localhost/38816] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(408)) - STATE* UnderReplicatedBlocks has 0 blocks
2020-12-03 07:24:09,475 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3585)) - Total number of blocks            = 0
2020-12-03 07:24:09,475 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3586)) - Number of invalid blocks          = 0
2020-12-03 07:24:09,476 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3587)) - Number of under-replicated blocks = 0
2020-12-03 07:24:09,476 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3588)) - Number of  over-replicated blocks = 0
2020-12-03 07:24:09,476 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3590)) - Number of blocks being written    = 0
2020-12-03 07:24:09,476 [Reconstruction Queue Initializer] INFO  hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(3593)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 5 msec
2020-12-03 07:24:09,572 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:24:09,572 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:24:09,576 [Listener at localhost/38816] INFO  namenode.NameNode (NameNode.java:startCommonServices(828)) - NameNode RPC up at: localhost/127.0.0.1:38816
2020-12-03 07:24:09,689 [Listener at localhost/38816] INFO  namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1222)) - Starting services required for active state
2020-12-03 07:24:09,690 [Listener at localhost/38816] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(777)) - Initializing quota with 4 thread(s)
2020-12-03 07:24:09,718 [Listener at localhost/38816] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(786)) - Quota initialization completed in 27 milliseconds
name space=1
storage space=0
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2020-12-03 07:24:09,723 [CacheReplicationMonitor(25655767)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-12-03 07:24:09,728 [Listener at localhost/38816] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1,[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:24:09,851 [Listener at localhost/38816] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:24:09,935 [Listener at localhost/38816] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:24:09,977 [Listener at localhost/38816] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:24:09,983 [Listener at localhost/38816] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:24:09,986 [Listener at localhost/38816] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:24:09,991 [Listener at localhost/38816] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:24:09,992 [Listener at localhost/38816] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:24:09,997 [Listener at localhost/38816] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:24:10,004 [Listener at localhost/38816] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:36140
2020-12-03 07:24:10,019 [Listener at localhost/38816] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:24:10,019 [Listener at localhost/38816] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:24:10,326 [Listener at localhost/38816] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:24:10,336 [Listener at localhost/38816] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:24:10,338 [Listener at localhost/38816] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:24:10,338 [Listener at localhost/38816] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:24:10,341 [Listener at localhost/38816] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:24:10,342 [Listener at localhost/38816] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:24:10,342 [Listener at localhost/38816] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:24:10,343 [Listener at localhost/38816] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:24:10,346 [Listener at localhost/38816] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 34080
2020-12-03 07:24:10,346 [Listener at localhost/38816] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:24:10,348 [Listener at localhost/38816] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@61526469{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:24:10,349 [Listener at localhost/38816] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@76ba13c{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:24:10,427 [Listener at localhost/38816] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@235f4c10{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:24:10,428 [Listener at localhost/38816] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@743cb8e0{HTTP/1.1,[http/1.1]}{localhost:34080}
2020-12-03 07:24:10,428 [Listener at localhost/38816] INFO  server.Server (Server.java:doStart(419)) - Started @11196ms
2020-12-03 07:24:11,265 [Listener at localhost/38816] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:40038
2020-12-03 07:24:11,266 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@76318a7d] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:24:11,266 [Listener at localhost/38816] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:24:11,266 [Listener at localhost/38816] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:24:11,287 [Listener at localhost/38816] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:24:11,288 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:24:11,295 [Listener at localhost/43238] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:43238
2020-12-03 07:24:11,324 [Listener at localhost/43238] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:24:11,326 [Listener at localhost/43238] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:24:11,400 [Thread-59] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:38816 starting to offer service
2020-12-03 07:24:11,407 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:24:11,407 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:24:11,412 [Listener at localhost/43238] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 1 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3,[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:24:11,415 [Listener at localhost/43238] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
2020-12-03 07:24:11,416 [Listener at localhost/43238] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:24:11,419 [Listener at localhost/43238] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:24:11,420 [Listener at localhost/43238] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:24:11,420 [Listener at localhost/43238] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:24:11,421 [Listener at localhost/43238] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:24:11,421 [Listener at localhost/43238] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:24:11,422 [Listener at localhost/43238] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:24:11,422 [Listener at localhost/43238] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:38680
2020-12-03 07:24:11,423 [Listener at localhost/43238] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:24:11,423 [Listener at localhost/43238] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:24:11,425 [Listener at localhost/43238] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:24:11,427 [Listener at localhost/43238] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:24:11,428 [Listener at localhost/43238] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:24:11,428 [Listener at localhost/43238] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:24:11,431 [Listener at localhost/43238] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:24:11,432 [Listener at localhost/43238] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:24:11,432 [Listener at localhost/43238] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:24:11,432 [Listener at localhost/43238] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:24:11,433 [Listener at localhost/43238] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 40098
2020-12-03 07:24:11,433 [Listener at localhost/43238] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:24:11,436 [Listener at localhost/43238] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@6b9267b{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:24:11,436 [Listener at localhost/43238] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@29ad44e3{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:24:11,444 [Listener at localhost/43238] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@285f09de{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:24:11,445 [Listener at localhost/43238] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@73393584{HTTP/1.1,[http/1.1]}{localhost:40098}
2020-12-03 07:24:11,446 [Listener at localhost/43238] INFO  server.Server (Server.java:doStart(419)) - Started @12214ms
2020-12-03 07:24:11,515 [Listener at localhost/43238] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:35056
2020-12-03 07:24:11,516 [Listener at localhost/43238] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:24:11,516 [Listener at localhost/43238] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:24:11,516 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@1827a871] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:24:11,517 [Listener at localhost/43238] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:24:11,519 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:24:11,532 [Listener at localhost/33077] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:33077
2020-12-03 07:24:11,537 [Listener at localhost/33077] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:24:11,538 [Listener at localhost/33077] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:24:11,539 [Thread-82] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:38816 starting to offer service
2020-12-03 07:24:11,541 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:24:11,541 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:24:11,545 [Listener at localhost/33077] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 2 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5,[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6
2020-12-03 07:24:11,548 [Listener at localhost/33077] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5
2020-12-03 07:24:11,548 [Listener at localhost/33077] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6
2020-12-03 07:24:11,550 [Listener at localhost/33077] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:24:11,551 [Listener at localhost/33077] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:24:11,551 [Listener at localhost/33077] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:24:11,552 [Listener at localhost/33077] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:24:11,552 [Listener at localhost/33077] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:24:11,553 [Listener at localhost/33077] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:24:11,553 [Listener at localhost/33077] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:40243
2020-12-03 07:24:11,554 [Listener at localhost/33077] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:24:11,554 [Listener at localhost/33077] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:24:11,556 [Listener at localhost/33077] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:24:11,558 [Listener at localhost/33077] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:24:11,559 [Listener at localhost/33077] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:24:11,559 [Listener at localhost/33077] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:24:11,562 [Listener at localhost/33077] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:24:11,563 [Listener at localhost/33077] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:24:11,563 [Listener at localhost/33077] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:24:11,563 [Listener at localhost/33077] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:24:11,564 [Listener at localhost/33077] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 39448
2020-12-03 07:24:11,565 [Listener at localhost/33077] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:24:11,567 [Listener at localhost/33077] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@7dac3fd8{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:24:11,568 [Listener at localhost/33077] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@2102a4d5{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:24:11,575 [Listener at localhost/33077] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@52eacb4b{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:24:11,576 [Listener at localhost/33077] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@5528a42c{HTTP/1.1,[http/1.1]}{localhost:39448}
2020-12-03 07:24:11,577 [Listener at localhost/33077] INFO  server.Server (Server.java:doStart(419)) - Started @12345ms
2020-12-03 07:24:11,599 [Listener at localhost/33077] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:46239
2020-12-03 07:24:11,600 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@1a6f5124] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:24:11,600 [Listener at localhost/33077] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:24:11,600 [Listener at localhost/33077] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:24:11,600 [Listener at localhost/33077] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:24:11,601 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:24:11,606 [Listener at localhost/41403] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:41403
2020-12-03 07:24:11,612 [Listener at localhost/41403] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:24:11,613 [Listener at localhost/41403] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:24:11,614 [Thread-106] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:38816 starting to offer service
2020-12-03 07:24:11,617 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:24:11,617 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:24:11,620 [Listener at localhost/41403] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 3 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7,[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8
2020-12-03 07:24:11,622 [Listener at localhost/41403] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7
2020-12-03 07:24:11,623 [Listener at localhost/41403] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8
2020-12-03 07:24:11,624 [Listener at localhost/41403] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:24:11,625 [Listener at localhost/41403] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:24:11,625 [Listener at localhost/41403] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:24:11,626 [Listener at localhost/41403] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:24:11,626 [Listener at localhost/41403] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:24:11,626 [Listener at localhost/41403] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:24:11,627 [Listener at localhost/41403] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:36526
2020-12-03 07:24:11,627 [Listener at localhost/41403] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:24:11,627 [Listener at localhost/41403] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:24:11,629 [Listener at localhost/41403] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:24:11,630 [Listener at localhost/41403] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:24:11,631 [Listener at localhost/41403] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:24:11,631 [Listener at localhost/41403] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:24:11,634 [Listener at localhost/41403] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:24:11,635 [Listener at localhost/41403] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:24:11,635 [Listener at localhost/41403] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:24:11,635 [Listener at localhost/41403] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:24:11,636 [Listener at localhost/41403] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 39981
2020-12-03 07:24:11,636 [Listener at localhost/41403] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:24:11,639 [Listener at localhost/41403] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@329a1243{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:24:11,646 [Listener at localhost/41403] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@2d35442b{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:24:11,659 [Listener at localhost/41403] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@21680803{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:24:11,660 [Listener at localhost/41403] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@588ab592{HTTP/1.1,[http/1.1]}{localhost:39981}
2020-12-03 07:24:11,661 [Listener at localhost/41403] INFO  server.Server (Server.java:doStart(419)) - Started @12429ms
2020-12-03 07:24:11,718 [Listener at localhost/41403] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:43859
2020-12-03 07:24:11,718 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@4cc61eb1] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:24:11,718 [Listener at localhost/41403] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:24:11,719 [Listener at localhost/41403] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:24:11,719 [Listener at localhost/41403] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:24:11,720 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:24:11,724 [Listener at localhost/36553] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:36553
2020-12-03 07:24:11,729 [Listener at localhost/36553] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:24:11,729 [Listener at localhost/36553] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:24:11,730 [Thread-128] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:38816 starting to offer service
2020-12-03 07:24:11,731 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:24:11,731 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:24:11,735 [Listener at localhost/36553] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 4 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9,[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10
2020-12-03 07:24:11,737 [Listener at localhost/36553] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9
2020-12-03 07:24:11,737 [Listener at localhost/36553] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10
2020-12-03 07:24:11,738 [Listener at localhost/36553] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:24:11,740 [Listener at localhost/36553] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:24:11,740 [Listener at localhost/36553] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:24:11,741 [Listener at localhost/36553] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:24:11,741 [Listener at localhost/36553] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:24:11,741 [Listener at localhost/36553] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:24:11,742 [Listener at localhost/36553] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:44312
2020-12-03 07:24:11,742 [Listener at localhost/36553] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:24:11,743 [Listener at localhost/36553] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:24:11,744 [Listener at localhost/36553] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:24:11,746 [Listener at localhost/36553] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:24:11,746 [Listener at localhost/36553] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:24:11,747 [Listener at localhost/36553] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:24:11,748 [Listener at localhost/36553] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:24:11,749 [Listener at localhost/36553] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:24:11,749 [Listener at localhost/36553] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:24:11,750 [Listener at localhost/36553] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:24:11,751 [Listener at localhost/36553] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 35831
2020-12-03 07:24:11,751 [Listener at localhost/36553] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:24:11,754 [Listener at localhost/36553] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@62c5bbdc{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:24:11,755 [Listener at localhost/36553] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@1bc53649{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:24:11,761 [Listener at localhost/36553] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@3abd581e{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:24:11,762 [Listener at localhost/36553] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@4d4d8fcf{HTTP/1.1,[http/1.1]}{localhost:35831}
2020-12-03 07:24:11,769 [Listener at localhost/36553] INFO  server.Server (Server.java:doStart(419)) - Started @12537ms
2020-12-03 07:24:11,792 [Listener at localhost/36553] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:43524
2020-12-03 07:24:11,793 [Listener at localhost/36553] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:24:11,793 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@6f0628de] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:24:11,793 [Listener at localhost/36553] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:24:11,794 [Listener at localhost/36553] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:24:11,795 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:24:11,800 [Listener at localhost/41810] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:41810
2020-12-03 07:24:11,805 [Listener at localhost/41810] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:24:11,806 [Listener at localhost/41810] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:24:11,807 [Thread-150] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:38816 starting to offer service
2020-12-03 07:24:11,808 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:24:11,808 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:24:11,812 [Listener at localhost/41810] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 5 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11,[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12
2020-12-03 07:24:11,814 [Listener at localhost/41810] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11
2020-12-03 07:24:11,815 [Listener at localhost/41810] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12
2020-12-03 07:24:11,816 [Listener at localhost/41810] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:24:11,828 [Listener at localhost/41810] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:24:11,829 [Listener at localhost/41810] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:24:11,830 [Listener at localhost/41810] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:24:11,831 [Listener at localhost/41810] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:24:11,831 [Listener at localhost/41810] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:24:11,832 [Listener at localhost/41810] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:37772
2020-12-03 07:24:11,832 [Listener at localhost/41810] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:24:11,832 [Listener at localhost/41810] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:24:11,835 [Listener at localhost/41810] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:24:11,838 [Listener at localhost/41810] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:24:11,839 [Listener at localhost/41810] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:24:11,839 [Listener at localhost/41810] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:24:11,842 [Listener at localhost/41810] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:24:11,843 [Listener at localhost/41810] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:24:11,844 [Listener at localhost/41810] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:24:11,844 [Listener at localhost/41810] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:24:11,845 [Listener at localhost/41810] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 38547
2020-12-03 07:24:11,845 [Listener at localhost/41810] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:24:11,847 [Listener at localhost/41810] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@619bd14c{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:24:11,848 [Listener at localhost/41810] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@a23a01d{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:24:11,856 [Listener at localhost/41810] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@22ee2d0{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:24:11,858 [Listener at localhost/41810] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@7bfc3126{HTTP/1.1,[http/1.1]}{localhost:38547}
2020-12-03 07:24:11,859 [Listener at localhost/41810] INFO  server.Server (Server.java:doStart(419)) - Started @12627ms
2020-12-03 07:24:11,888 [Listener at localhost/41810] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:41739
2020-12-03 07:24:11,888 [Listener at localhost/41810] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:24:11,888 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@53bc1328] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:24:11,888 [Listener at localhost/41810] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:24:11,889 [Listener at localhost/41810] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:24:11,890 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:24:11,895 [Listener at localhost/36035] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:36035
2020-12-03 07:24:11,900 [Listener at localhost/36035] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:24:11,901 [Listener at localhost/36035] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:24:11,901 [Thread-172] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:38816 starting to offer service
2020-12-03 07:24:11,904 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:24:11,904 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:24:11,908 [Listener at localhost/36035] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 6 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13,[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14
2020-12-03 07:24:11,910 [Listener at localhost/36035] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13
2020-12-03 07:24:11,911 [Listener at localhost/36035] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14
2020-12-03 07:24:11,912 [Listener at localhost/36035] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:24:11,913 [Listener at localhost/36035] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:24:11,914 [Listener at localhost/36035] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:24:11,915 [Listener at localhost/36035] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:24:11,915 [Listener at localhost/36035] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:24:11,916 [Listener at localhost/36035] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:24:11,917 [Listener at localhost/36035] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:41069
2020-12-03 07:24:11,917 [Listener at localhost/36035] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:24:11,917 [Listener at localhost/36035] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:24:11,918 [Listener at localhost/36035] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:24:11,920 [Listener at localhost/36035] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:24:11,921 [Listener at localhost/36035] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:24:11,921 [Listener at localhost/36035] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:24:11,925 [Listener at localhost/36035] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:24:11,926 [Listener at localhost/36035] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:24:11,926 [Listener at localhost/36035] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:24:11,927 [Listener at localhost/36035] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:24:11,928 [Listener at localhost/36035] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 44858
2020-12-03 07:24:11,928 [Listener at localhost/36035] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:24:11,930 [Listener at localhost/36035] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@49ef32e0{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:24:11,931 [Listener at localhost/36035] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@6bd51ed8{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:24:11,941 [Listener at localhost/36035] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@bae47a0{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:24:11,943 [Listener at localhost/36035] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@74a9c4b0{HTTP/1.1,[http/1.1]}{localhost:44858}
2020-12-03 07:24:11,943 [Listener at localhost/36035] INFO  server.Server (Server.java:doStart(419)) - Started @12711ms
2020-12-03 07:24:11,979 [Listener at localhost/36035] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:41537
2020-12-03 07:24:11,980 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@1c05a54d] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:24:11,980 [Listener at localhost/36035] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:24:11,981 [Listener at localhost/36035] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:24:11,981 [Listener at localhost/36035] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:24:11,988 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:24:12,009 [Listener at localhost/37986] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:37986
2020-12-03 07:24:12,015 [Listener at localhost/37986] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:24:12,023 [Listener at localhost/37986] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:24:12,024 [Thread-194] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:38816 starting to offer service
2020-12-03 07:24:12,027 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:24:12,036 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:24:12,136 [Thread-59] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:38816
2020-12-03 07:24:12,141 [Thread-82] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:38816
2020-12-03 07:24:12,141 [Thread-128] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:38816
2020-12-03 07:24:12,141 [Thread-172] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:38816
2020-12-03 07:24:12,142 [Thread-106] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:38816
2020-12-03 07:24:12,142 [Thread-150] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:38816
2020-12-03 07:24:12,142 [Thread-59] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:24:12,145 [Thread-194] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:38816
2020-12-03 07:24:12,147 [Listener at localhost/37986] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 7 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15,[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16
2020-12-03 07:24:12,149 [Thread-82] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:24:12,150 [Thread-128] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:24:12,150 [Thread-172] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:24:12,151 [Thread-150] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:24:12,151 [Thread-194] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:24:12,152 [Listener at localhost/37986] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15
2020-12-03 07:24:12,153 [Listener at localhost/37986] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16
2020-12-03 07:24:12,154 [Thread-106] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:24:12,155 [Listener at localhost/37986] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:24:12,156 [Listener at localhost/37986] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:24:12,156 [Listener at localhost/37986] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:24:12,156 [Listener at localhost/37986] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:24:12,157 [Listener at localhost/37986] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:24:12,157 [Listener at localhost/37986] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:24:12,158 [Listener at localhost/37986] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:44445
2020-12-03 07:24:12,158 [Listener at localhost/37986] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:24:12,158 [Listener at localhost/37986] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:24:12,160 [Listener at localhost/37986] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:24:12,162 [Listener at localhost/37986] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:24:12,163 [Listener at localhost/37986] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:24:12,163 [Listener at localhost/37986] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:24:12,167 [Listener at localhost/37986] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:24:12,167 [Listener at localhost/37986] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:24:12,167 [Listener at localhost/37986] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:24:12,168 [Listener at localhost/37986] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:24:12,169 [Listener at localhost/37986] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 43219
2020-12-03 07:24:12,169 [Listener at localhost/37986] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:24:12,172 [Listener at localhost/37986] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@24528a25{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:24:12,173 [Listener at localhost/37986] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@59221b97{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:24:12,206 [Listener at localhost/37986] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@530a8454{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:24:12,207 [Listener at localhost/37986] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@2b750e36{HTTP/1.1,[http/1.1]}{localhost:43219}
2020-12-03 07:24:12,208 [Listener at localhost/37986] INFO  server.Server (Server.java:doStart(419)) - Started @12976ms
2020-12-03 07:24:12,314 [Listener at localhost/37986] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:40126
2020-12-03 07:24:12,315 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@5215cd9a] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:24:12,315 [Listener at localhost/37986] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:24:12,315 [Listener at localhost/37986] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:24:12,316 [Listener at localhost/37986] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:24:12,317 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:24:12,322 [Listener at localhost/35715] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:35715
2020-12-03 07:24:12,328 [Listener at localhost/35715] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:24:12,329 [Listener at localhost/35715] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:24:12,330 [Thread-216] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:38816 starting to offer service
2020-12-03 07:24:12,350 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:24:12,350 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:24:12,351 [Thread-59] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/in_use.lock acquired by nodename 12367@274d470f39cc
2020-12-03 07:24:12,353 [Thread-59] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1 is not formatted for namespace 110378089. Formatting...
2020-12-03 07:24:12,357 [Thread-216] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:38816
2020-12-03 07:24:12,358 [Thread-216] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:24:12,360 [Thread-59] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-b5e98384-c299-4c9a-bc26-b30d23c7aa11 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1 
2020-12-03 07:24:12,362 [Listener at localhost/35715] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 8 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17,[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18
2020-12-03 07:24:12,364 [Listener at localhost/35715] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17
2020-12-03 07:24:12,364 [Listener at localhost/35715] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18
2020-12-03 07:24:12,365 [Listener at localhost/35715] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:24:12,366 [Listener at localhost/35715] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:24:12,366 [Listener at localhost/35715] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:24:12,367 [Listener at localhost/35715] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:24:12,367 [Listener at localhost/35715] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:24:12,368 [Listener at localhost/35715] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:24:12,368 [Listener at localhost/35715] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:38730
2020-12-03 07:24:12,369 [Listener at localhost/35715] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:24:12,369 [Listener at localhost/35715] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:24:12,370 [Listener at localhost/35715] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:24:12,373 [Listener at localhost/35715] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:24:12,385 [Listener at localhost/35715] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:24:12,386 [Listener at localhost/35715] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:24:12,388 [Listener at localhost/35715] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:24:12,389 [Listener at localhost/35715] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:24:12,389 [Listener at localhost/35715] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:24:12,389 [Listener at localhost/35715] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:24:12,390 [Listener at localhost/35715] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 36248
2020-12-03 07:24:12,391 [Listener at localhost/35715] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:24:12,393 [Listener at localhost/35715] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@50cf5a23{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:24:12,393 [Listener at localhost/35715] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@273c947f{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:24:12,400 [Listener at localhost/35715] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@2c1dc8e{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:24:12,402 [Listener at localhost/35715] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@b273a59{HTTP/1.1,[http/1.1]}{localhost:36248}
2020-12-03 07:24:12,402 [Listener at localhost/35715] INFO  server.Server (Server.java:doStart(419)) - Started @13170ms
2020-12-03 07:24:12,430 [Listener at localhost/35715] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:37138
2020-12-03 07:24:12,431 [Listener at localhost/35715] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:24:12,431 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@251ebf23] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:24:12,431 [Listener at localhost/35715] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:24:12,432 [Listener at localhost/35715] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:24:12,433 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:24:12,436 [Listener at localhost/36951] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:36951
2020-12-03 07:24:12,441 [Thread-106] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/in_use.lock acquired by nodename 12367@274d470f39cc
2020-12-03 07:24:12,441 [Thread-194] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13/in_use.lock acquired by nodename 12367@274d470f39cc
2020-12-03 07:24:12,442 [Thread-150] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9/in_use.lock acquired by nodename 12367@274d470f39cc
2020-12-03 07:24:12,441 [Thread-128] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7/in_use.lock acquired by nodename 12367@274d470f39cc
2020-12-03 07:24:12,441 [Thread-172] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11/in_use.lock acquired by nodename 12367@274d470f39cc
2020-12-03 07:24:12,442 [Thread-128] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7 is not formatted for namespace 110378089. Formatting...
2020-12-03 07:24:12,441 [Thread-82] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/in_use.lock acquired by nodename 12367@274d470f39cc
2020-12-03 07:24:12,442 [Thread-172] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11 is not formatted for namespace 110378089. Formatting...
2020-12-03 07:24:12,442 [Thread-150] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9 is not formatted for namespace 110378089. Formatting...
2020-12-03 07:24:12,442 [Thread-194] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13 is not formatted for namespace 110378089. Formatting...
2020-12-03 07:24:12,442 [Thread-106] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5 is not formatted for namespace 110378089. Formatting...
2020-12-03 07:24:12,442 [Thread-82] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3 is not formatted for namespace 110378089. Formatting...
2020-12-03 07:24:12,443 [Listener at localhost/36951] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:24:12,443 [Listener at localhost/36951] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:24:12,444 [Thread-238] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:38816 starting to offer service
2020-12-03 07:24:12,447 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:24:12,447 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:24:12,451 [Thread-238] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:38816
2020-12-03 07:24:12,452 [Thread-238] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:24:12,488 [Thread-128] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-44a63aad-efff-4da7-8035-f12475e12191 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7 
2020-12-03 07:24:12,488 [Thread-194] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-8517971e-c5a6-4cda-8698-97e4d8202f94 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13 
2020-12-03 07:24:12,489 [Listener at localhost/36951] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 9 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19,[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20
2020-12-03 07:24:12,489 [Thread-106] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-02609b7a-10b1-4d2f-a95f-0fbf402c5ad9 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5 
2020-12-03 07:24:12,488 [Thread-150] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-adf39776-9f4e-495e-9c0f-d1401f599447 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9 
2020-12-03 07:24:12,488 [Thread-82] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-4ed0dfcb-1823-4feb-a78e-c34190aa43b1 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3 
2020-12-03 07:24:12,488 [Thread-172] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-c57dae7c-594d-4814-81e5-c9bf204f41ae for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11 
2020-12-03 07:24:12,490 [Listener at localhost/36951] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19
2020-12-03 07:24:12,491 [Listener at localhost/36951] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20
2020-12-03 07:24:12,492 [Listener at localhost/36951] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:24:12,492 [Listener at localhost/36951] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:24:12,492 [Listener at localhost/36951] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:24:12,493 [Listener at localhost/36951] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:24:12,493 [Listener at localhost/36951] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:24:12,493 [Listener at localhost/36951] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:24:12,494 [Listener at localhost/36951] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:36287
2020-12-03 07:24:12,494 [Listener at localhost/36951] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:24:12,494 [Listener at localhost/36951] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:24:12,495 [Listener at localhost/36951] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:24:12,498 [Listener at localhost/36951] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:24:12,499 [Listener at localhost/36951] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:24:12,499 [Listener at localhost/36951] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:24:12,501 [Listener at localhost/36951] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:24:12,502 [Listener at localhost/36951] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:24:12,502 [Listener at localhost/36951] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:24:12,502 [Listener at localhost/36951] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:24:12,506 [Listener at localhost/36951] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 40250
2020-12-03 07:24:12,507 [Listener at localhost/36951] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:24:12,511 [Listener at localhost/36951] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@63034ed1{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:24:12,511 [Listener at localhost/36951] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@55a8dc49{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:24:12,518 [Listener at localhost/36951] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@669d2b1b{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:24:12,520 [Listener at localhost/36951] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@721eb7df{HTTP/1.1,[http/1.1]}{localhost:40250}
2020-12-03 07:24:12,520 [Listener at localhost/36951] INFO  server.Server (Server.java:doStart(419)) - Started @13288ms
2020-12-03 07:24:12,536 [Listener at localhost/36951] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:35781
2020-12-03 07:24:12,537 [Listener at localhost/36951] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:24:12,537 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@5d52e3ef] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:24:12,537 [Listener at localhost/36951] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:24:12,538 [Listener at localhost/36951] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:24:12,538 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:24:12,542 [Listener at localhost/38720] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:38720
2020-12-03 07:24:12,547 [Listener at localhost/38720] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:24:12,547 [Listener at localhost/38720] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:24:12,548 [Thread-260] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:38816 starting to offer service
2020-12-03 07:24:12,551 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:24:12,551 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:24:12,555 [Thread-260] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:38816
2020-12-03 07:24:12,555 [Thread-260] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:24:12,556 [Listener at localhost/38720] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 10 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21,[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22
2020-12-03 07:24:12,558 [Listener at localhost/38720] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21
2020-12-03 07:24:12,559 [Listener at localhost/38720] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22
2020-12-03 07:24:12,560 [Listener at localhost/38720] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:24:12,561 [Listener at localhost/38720] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:24:12,561 [Listener at localhost/38720] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:24:12,562 [Listener at localhost/38720] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:24:12,562 [Listener at localhost/38720] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:24:12,562 [Listener at localhost/38720] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:24:12,563 [Listener at localhost/38720] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:43902
2020-12-03 07:24:12,563 [Listener at localhost/38720] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:24:12,563 [Listener at localhost/38720] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:24:12,564 [Listener at localhost/38720] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:24:12,566 [Listener at localhost/38720] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:24:12,566 [Listener at localhost/38720] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:24:12,567 [Listener at localhost/38720] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:24:12,569 [Listener at localhost/38720] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:24:12,569 [Listener at localhost/38720] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:24:12,569 [Listener at localhost/38720] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:24:12,569 [Listener at localhost/38720] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:24:12,570 [Listener at localhost/38720] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 35848
2020-12-03 07:24:12,571 [Listener at localhost/38720] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:24:12,572 [Listener at localhost/38720] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@680bddf5{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:24:12,573 [Listener at localhost/38720] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@2d83c5a5{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:24:12,581 [Listener at localhost/38720] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@894858{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:24:12,582 [Listener at localhost/38720] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@7af707e0{HTTP/1.1,[http/1.1]}{localhost:35848}
2020-12-03 07:24:12,582 [Listener at localhost/38720] INFO  server.Server (Server.java:doStart(419)) - Started @13351ms
2020-12-03 07:24:12,609 [Listener at localhost/38720] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:34956
2020-12-03 07:24:12,609 [Listener at localhost/38720] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:24:12,609 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@3ecedf21] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:24:12,610 [Listener at localhost/38720] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:24:12,610 [Listener at localhost/38720] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:24:12,611 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:24:12,618 [Listener at localhost/46163] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:46163
2020-12-03 07:24:12,620 [Thread-216] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15/in_use.lock acquired by nodename 12367@274d470f39cc
2020-12-03 07:24:12,620 [Thread-216] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15 is not formatted for namespace 110378089. Formatting...
2020-12-03 07:24:12,625 [Listener at localhost/46163] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:24:12,625 [Listener at localhost/46163] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:24:12,626 [Thread-282] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:38816 starting to offer service
2020-12-03 07:24:12,630 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:24:12,630 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:24:12,656 [Thread-282] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:38816
2020-12-03 07:24:12,657 [Thread-282] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:24:12,659 [Thread-216] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-a51c32af-aab6-4fd9-b287-03a51134ce85 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15 
2020-12-03 07:24:12,782 [Thread-238] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17/in_use.lock acquired by nodename 12367@274d470f39cc
2020-12-03 07:24:12,782 [Thread-260] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19/in_use.lock acquired by nodename 12367@274d470f39cc
2020-12-03 07:24:12,782 [Thread-238] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17 is not formatted for namespace 110378089. Formatting...
2020-12-03 07:24:12,783 [Thread-260] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19 is not formatted for namespace 110378089. Formatting...
2020-12-03 07:24:12,785 [Thread-238] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-f5960db6-7b54-4094-bc6c-6bca60faedad for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17 
2020-12-03 07:24:12,785 [Thread-260] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-f0f6df85-a46f-4d89-b667-125138a11cef for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19 
2020-12-03 07:24:12,926 [Thread-282] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21/in_use.lock acquired by nodename 12367@274d470f39cc
2020-12-03 07:24:12,926 [Thread-282] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21 is not formatted for namespace 110378089. Formatting...
2020-12-03 07:24:12,964 [Thread-282] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-b7b6573e-a1ea-4bd1-89c8-d3f7aef4bee9 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21 
2020-12-03 07:24:13,157 [Thread-59] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/in_use.lock acquired by nodename 12367@274d470f39cc
2020-12-03 07:24:13,158 [Thread-59] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2 is not formatted for namespace 110378089. Formatting...
2020-12-03 07:24:13,268 [IPC Server handler 1 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:24:13,277 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:24:13,278 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:24:13,367 [Thread-128] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/in_use.lock acquired by nodename 12367@274d470f39cc
2020-12-03 07:24:13,367 [Thread-172] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12/in_use.lock acquired by nodename 12367@274d470f39cc
2020-12-03 07:24:13,367 [Thread-128] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8 is not formatted for namespace 110378089. Formatting...
2020-12-03 07:24:13,367 [Thread-194] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14/in_use.lock acquired by nodename 12367@274d470f39cc
2020-12-03 07:24:13,367 [Thread-106] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/in_use.lock acquired by nodename 12367@274d470f39cc
2020-12-03 07:24:13,367 [Thread-194] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14 is not formatted for namespace 110378089. Formatting...
2020-12-03 07:24:13,367 [Thread-150] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10/in_use.lock acquired by nodename 12367@274d470f39cc
2020-12-03 07:24:13,367 [Thread-82] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/in_use.lock acquired by nodename 12367@274d470f39cc
2020-12-03 07:24:13,368 [Thread-150] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10 is not formatted for namespace 110378089. Formatting...
2020-12-03 07:24:13,367 [Thread-106] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6 is not formatted for namespace 110378089. Formatting...
2020-12-03 07:24:13,367 [Thread-172] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12 is not formatted for namespace 110378089. Formatting...
2020-12-03 07:24:13,368 [Thread-82] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4 is not formatted for namespace 110378089. Formatting...
2020-12-03 07:24:13,383 [IPC Server handler 9 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:24:13,384 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:24:13,384 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:24:13,447 [Thread-59] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-65fbe59b-ff8b-4331-bff0-e44b48174987 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2 
2020-12-03 07:24:13,447 [Thread-82] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-9861837d-0d69-4f2f-b2aa-8b94418cf3c9 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4 
2020-12-03 07:24:13,447 [Thread-172] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-fb82c216-d71b-4654-9ad9-21ab3a7512a6 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12 
2020-12-03 07:24:13,448 [Thread-128] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-bc4245b5-06be-49c3-84ee-8bb52e5db51d for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8 
2020-12-03 07:24:13,448 [Thread-106] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-35c3127f-e8ca-452b-bcb1-392069cbc853 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6 
2020-12-03 07:24:13,449 [Thread-150] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-8cd30729-7874-4e19-9225-c6c7a923c355 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10 
2020-12-03 07:24:13,449 [Thread-194] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-95103ad2-7662-4492-ba1b-5cecf313b9e8 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14 
2020-12-03 07:24:13,486 [IPC Server handler 7 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:24:13,487 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:24:13,487 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:24:13,589 [IPC Server handler 3 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:24:13,590 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:24:13,590 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:24:13,693 [IPC Server handler 5 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:24:13,694 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:24:13,694 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:24:13,797 [IPC Server handler 4 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:24:13,798 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:24:13,798 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:24:13,879 [Thread-216] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16/in_use.lock acquired by nodename 12367@274d470f39cc
2020-12-03 07:24:13,879 [Thread-216] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16 is not formatted for namespace 110378089. Formatting...
2020-12-03 07:24:13,901 [IPC Server handler 0 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:24:13,902 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:24:13,902 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:24:14,003 [Thread-216] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-350dbe39-4a8c-4163-8d64-f89d775392cf for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16 
2020-12-03 07:24:14,004 [IPC Server handler 6 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:24:14,005 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:24:14,005 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:24:14,037 [Thread-238] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18/in_use.lock acquired by nodename 12367@274d470f39cc
2020-12-03 07:24:14,037 [Thread-260] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20/in_use.lock acquired by nodename 12367@274d470f39cc
2020-12-03 07:24:14,038 [Thread-238] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18 is not formatted for namespace 110378089. Formatting...
2020-12-03 07:24:14,038 [Thread-260] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20 is not formatted for namespace 110378089. Formatting...
2020-12-03 07:24:14,114 [IPC Server handler 2 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:24:14,115 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:24:14,115 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:24:14,133 [Thread-260] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-c8ee244d-58c6-400b-8261-c3c19df916dd for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20 
2020-12-03 07:24:14,133 [Thread-238] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-85392bd5-b857-478f-99fd-a6820ffff252 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18 
2020-12-03 07:24:14,191 [Thread-282] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22/in_use.lock acquired by nodename 12367@274d470f39cc
2020-12-03 07:24:14,191 [Thread-282] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22 is not formatted for namespace 110378089. Formatting...
2020-12-03 07:24:14,218 [IPC Server handler 1 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:24:14,219 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:24:14,219 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:24:14,221 [Thread-282] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-6d3e897d-ec54-4b20-9f96-ccf88af97eaa for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22 
2020-12-03 07:24:14,322 [IPC Server handler 8 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:24:14,323 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:24:14,324 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:24:14,334 [Thread-128] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:14,335 [Thread-82] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:14,335 [Thread-172] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:14,335 [Thread-194] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:14,335 [Thread-106] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:14,335 [Thread-82] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:14,335 [Thread-150] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:14,335 [Thread-59] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:14,335 [Thread-128] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7/current/BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:14,336 [Thread-82] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3 and block pool id BP-1088075852-172.17.0.6-1606980241923 is not formatted. Formatting ...
2020-12-03 07:24:14,336 [Thread-82] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1088075852-172.17.0.6-1606980241923 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1088075852-172.17.0.6-1606980241923/current
2020-12-03 07:24:14,336 [Thread-59] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:14,336 [Thread-150] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9/current/BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:14,335 [Thread-106] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:14,335 [Thread-194] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13/current/BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:14,335 [Thread-172] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11/current/BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:14,338 [Thread-194] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13 and block pool id BP-1088075852-172.17.0.6-1606980241923 is not formatted. Formatting ...
2020-12-03 07:24:14,338 [Thread-194] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1088075852-172.17.0.6-1606980241923 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13/current/BP-1088075852-172.17.0.6-1606980241923/current
2020-12-03 07:24:14,337 [Thread-106] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5 and block pool id BP-1088075852-172.17.0.6-1606980241923 is not formatted. Formatting ...
2020-12-03 07:24:14,337 [Thread-150] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9 and block pool id BP-1088075852-172.17.0.6-1606980241923 is not formatted. Formatting ...
2020-12-03 07:24:14,338 [Thread-150] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1088075852-172.17.0.6-1606980241923 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9/current/BP-1088075852-172.17.0.6-1606980241923/current
2020-12-03 07:24:14,337 [Thread-59] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1 and block pool id BP-1088075852-172.17.0.6-1606980241923 is not formatted. Formatting ...
2020-12-03 07:24:14,343 [Thread-59] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1088075852-172.17.0.6-1606980241923 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1088075852-172.17.0.6-1606980241923/current
2020-12-03 07:24:14,336 [Thread-128] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7 and block pool id BP-1088075852-172.17.0.6-1606980241923 is not formatted. Formatting ...
2020-12-03 07:24:14,343 [Thread-128] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1088075852-172.17.0.6-1606980241923 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7/current/BP-1088075852-172.17.0.6-1606980241923/current
2020-12-03 07:24:14,338 [Thread-106] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1088075852-172.17.0.6-1606980241923 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-1088075852-172.17.0.6-1606980241923/current
2020-12-03 07:24:14,338 [Thread-172] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11 and block pool id BP-1088075852-172.17.0.6-1606980241923 is not formatted. Formatting ...
2020-12-03 07:24:14,344 [Thread-172] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1088075852-172.17.0.6-1606980241923 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11/current/BP-1088075852-172.17.0.6-1606980241923/current
2020-12-03 07:24:14,426 [IPC Server handler 7 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:24:14,427 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:24:14,428 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:24:14,477 [Thread-216] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:14,478 [Thread-216] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15/current/BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:14,478 [Thread-216] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15 and block pool id BP-1088075852-172.17.0.6-1606980241923 is not formatted. Formatting ...
2020-12-03 07:24:14,478 [Thread-216] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1088075852-172.17.0.6-1606980241923 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15/current/BP-1088075852-172.17.0.6-1606980241923/current
2020-12-03 07:24:14,530 [IPC Server handler 3 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:24:14,531 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:24:14,531 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:24:14,564 [Thread-260] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:14,565 [Thread-238] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:14,565 [Thread-260] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19/current/BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:14,565 [Thread-238] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17/current/BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:14,565 [Thread-260] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19 and block pool id BP-1088075852-172.17.0.6-1606980241923 is not formatted. Formatting ...
2020-12-03 07:24:14,565 [Thread-238] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17 and block pool id BP-1088075852-172.17.0.6-1606980241923 is not formatted. Formatting ...
2020-12-03 07:24:14,565 [Thread-260] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1088075852-172.17.0.6-1606980241923 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19/current/BP-1088075852-172.17.0.6-1606980241923/current
2020-12-03 07:24:14,566 [Thread-238] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1088075852-172.17.0.6-1606980241923 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17/current/BP-1088075852-172.17.0.6-1606980241923/current
2020-12-03 07:24:14,633 [IPC Server handler 5 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:24:14,634 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:24:14,634 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:24:14,703 [Thread-282] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:14,703 [Thread-282] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21/current/BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:14,703 [Thread-282] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21 and block pool id BP-1088075852-172.17.0.6-1606980241923 is not formatted. Formatting ...
2020-12-03 07:24:14,703 [Thread-282] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1088075852-172.17.0.6-1606980241923 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21/current/BP-1088075852-172.17.0.6-1606980241923/current
2020-12-03 07:24:14,737 [IPC Server handler 4 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:24:14,737 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:24:14,738 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:24:14,802 [Thread-82] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:14,802 [Thread-128] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:14,802 [Thread-82] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:14,802 [Thread-128] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/current/BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:14,802 [Thread-82] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4 and block pool id BP-1088075852-172.17.0.6-1606980241923 is not formatted. Formatting ...
2020-12-03 07:24:14,802 [Thread-150] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:14,802 [Thread-128] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8 and block pool id BP-1088075852-172.17.0.6-1606980241923 is not formatted. Formatting ...
2020-12-03 07:24:14,802 [Thread-82] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1088075852-172.17.0.6-1606980241923 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1088075852-172.17.0.6-1606980241923/current
2020-12-03 07:24:14,803 [Thread-150] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10/current/BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:14,803 [Thread-128] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1088075852-172.17.0.6-1606980241923 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/current/BP-1088075852-172.17.0.6-1606980241923/current
2020-12-03 07:24:14,803 [Thread-150] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10 and block pool id BP-1088075852-172.17.0.6-1606980241923 is not formatted. Formatting ...
2020-12-03 07:24:14,803 [Thread-106] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:14,803 [Thread-150] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1088075852-172.17.0.6-1606980241923 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10/current/BP-1088075852-172.17.0.6-1606980241923/current
2020-12-03 07:24:14,804 [Thread-106] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/current/BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:14,804 [Thread-106] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6 and block pool id BP-1088075852-172.17.0.6-1606980241923 is not formatted. Formatting ...
2020-12-03 07:24:14,804 [Thread-106] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1088075852-172.17.0.6-1606980241923 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/current/BP-1088075852-172.17.0.6-1606980241923/current
2020-12-03 07:24:14,804 [Thread-172] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:14,805 [Thread-172] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12/current/BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:14,805 [Thread-172] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12 and block pool id BP-1088075852-172.17.0.6-1606980241923 is not formatted. Formatting ...
2020-12-03 07:24:14,805 [Thread-172] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1088075852-172.17.0.6-1606980241923 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12/current/BP-1088075852-172.17.0.6-1606980241923/current
2020-12-03 07:24:14,805 [Thread-194] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:14,806 [Thread-194] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14/current/BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:14,806 [Thread-194] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14 and block pool id BP-1088075852-172.17.0.6-1606980241923 is not formatted. Formatting ...
2020-12-03 07:24:14,806 [Thread-194] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1088075852-172.17.0.6-1606980241923 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14/current/BP-1088075852-172.17.0.6-1606980241923/current
2020-12-03 07:24:14,809 [Thread-59] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:14,810 [Thread-59] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:14,810 [Thread-59] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2 and block pool id BP-1088075852-172.17.0.6-1606980241923 is not formatted. Formatting ...
2020-12-03 07:24:14,810 [Thread-59] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1088075852-172.17.0.6-1606980241923 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1088075852-172.17.0.6-1606980241923/current
2020-12-03 07:24:14,841 [IPC Server handler 0 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:24:14,841 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:24:14,842 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:24:14,944 [IPC Server handler 6 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:24:14,945 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:24:14,945 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:24:14,957 [Thread-216] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:14,958 [Thread-216] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16/current/BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:14,958 [Thread-216] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16 and block pool id BP-1088075852-172.17.0.6-1606980241923 is not formatted. Formatting ...
2020-12-03 07:24:14,958 [Thread-216] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1088075852-172.17.0.6-1606980241923 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16/current/BP-1088075852-172.17.0.6-1606980241923/current
2020-12-03 07:24:15,047 [IPC Server handler 2 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:24:15,048 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:24:15,048 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:24:15,126 [Thread-260] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:15,126 [Thread-238] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:15,126 [Thread-260] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20/current/BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:15,126 [Thread-238] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18/current/BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:15,127 [Thread-260] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20 and block pool id BP-1088075852-172.17.0.6-1606980241923 is not formatted. Formatting ...
2020-12-03 07:24:15,127 [Thread-238] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18 and block pool id BP-1088075852-172.17.0.6-1606980241923 is not formatted. Formatting ...
2020-12-03 07:24:15,127 [Thread-260] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1088075852-172.17.0.6-1606980241923 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20/current/BP-1088075852-172.17.0.6-1606980241923/current
2020-12-03 07:24:15,127 [Thread-238] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1088075852-172.17.0.6-1606980241923 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18/current/BP-1088075852-172.17.0.6-1606980241923/current
2020-12-03 07:24:15,150 [IPC Server handler 1 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:24:15,151 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:24:15,151 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:24:15,253 [IPC Server handler 8 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:24:15,254 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:24:15,255 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:24:15,308 [Thread-282] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:15,308 [Thread-282] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22/current/BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:15,309 [Thread-282] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22 and block pool id BP-1088075852-172.17.0.6-1606980241923 is not formatted. Formatting ...
2020-12-03 07:24:15,309 [Thread-282] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1088075852-172.17.0.6-1606980241923 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22/current/BP-1088075852-172.17.0.6-1606980241923/current
2020-12-03 07:24:15,357 [IPC Server handler 9 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:24:15,358 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:24:15,358 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:24:15,396 [Thread-106] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=110378089;bpid=BP-1088075852-172.17.0.6-1606980241923;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=110378089;c=1606980241923;bpid=BP-1088075852-172.17.0.6-1606980241923;dnuuid=null
2020-12-03 07:24:15,396 [Thread-128] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=110378089;bpid=BP-1088075852-172.17.0.6-1606980241923;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=110378089;c=1606980241923;bpid=BP-1088075852-172.17.0.6-1606980241923;dnuuid=null
2020-12-03 07:24:15,396 [Thread-82] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=110378089;bpid=BP-1088075852-172.17.0.6-1606980241923;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=110378089;c=1606980241923;bpid=BP-1088075852-172.17.0.6-1606980241923;dnuuid=null
2020-12-03 07:24:15,396 [Thread-172] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=110378089;bpid=BP-1088075852-172.17.0.6-1606980241923;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=110378089;c=1606980241923;bpid=BP-1088075852-172.17.0.6-1606980241923;dnuuid=null
2020-12-03 07:24:15,396 [Thread-150] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=110378089;bpid=BP-1088075852-172.17.0.6-1606980241923;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=110378089;c=1606980241923;bpid=BP-1088075852-172.17.0.6-1606980241923;dnuuid=null
2020-12-03 07:24:15,396 [Thread-59] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=110378089;bpid=BP-1088075852-172.17.0.6-1606980241923;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=110378089;c=1606980241923;bpid=BP-1088075852-172.17.0.6-1606980241923;dnuuid=null
2020-12-03 07:24:15,396 [Thread-194] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=110378089;bpid=BP-1088075852-172.17.0.6-1606980241923;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=110378089;c=1606980241923;bpid=BP-1088075852-172.17.0.6-1606980241923;dnuuid=null
2020-12-03 07:24:15,460 [IPC Server handler 3 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:24:15,461 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:24:15,461 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:24:15,527 [Thread-216] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=110378089;bpid=BP-1088075852-172.17.0.6-1606980241923;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=110378089;c=1606980241923;bpid=BP-1088075852-172.17.0.6-1606980241923;dnuuid=null
2020-12-03 07:24:15,563 [IPC Server handler 5 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:24:15,564 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:24:15,564 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:24:15,667 [IPC Server handler 4 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:24:15,668 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:24:15,668 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:24:15,670 [Thread-260] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=110378089;bpid=BP-1088075852-172.17.0.6-1606980241923;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=110378089;c=1606980241923;bpid=BP-1088075852-172.17.0.6-1606980241923;dnuuid=null
2020-12-03 07:24:15,670 [Thread-238] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=110378089;bpid=BP-1088075852-172.17.0.6-1606980241923;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=110378089;c=1606980241923;bpid=BP-1088075852-172.17.0.6-1606980241923;dnuuid=null
2020-12-03 07:24:15,753 [Thread-282] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=110378089;bpid=BP-1088075852-172.17.0.6-1606980241923;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=110378089;c=1606980241923;bpid=BP-1088075852-172.17.0.6-1606980241923;dnuuid=null
2020-12-03 07:24:15,770 [IPC Server handler 0 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:24:15,771 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:24:15,771 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:24:15,874 [IPC Server handler 6 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:24:15,875 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:24:15,875 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:24:15,885 [Thread-150] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID 941777f1-35bc-48df-a860-96f904e7fd34
2020-12-03 07:24:15,885 [Thread-194] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID 31eff9f5-fd87-487e-8b1a-43cc24be4844
2020-12-03 07:24:15,885 [Thread-172] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID e6a510c0-efbe-44e4-b3ca-915180c69d66
2020-12-03 07:24:15,885 [Thread-59] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID 31491a6f-508d-45fd-8589-10a6a9373e21
2020-12-03 07:24:15,885 [Thread-106] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID 028e287b-d054-4bef-b640-ae581093b777
2020-12-03 07:24:15,885 [Thread-82] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID 61b60f2f-795d-44aa-82c3-0ac238b8a077
2020-12-03 07:24:15,885 [Thread-128] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID 53d1c8e9-e11d-4d1a-9a77-35621ba2c419
2020-12-03 07:24:15,977 [IPC Server handler 2 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:24:15,978 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:24:15,978 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:24:16,011 [Thread-216] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID ad0684d8-6863-4e19-b2b3-d95e7d2668d9
2020-12-03 07:24:16,081 [IPC Server handler 1 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:24:16,081 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:24:16,082 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:24:16,145 [Thread-238] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID e6d8c096-523b-4539-b874-b517e981fab6
2020-12-03 07:24:16,145 [Thread-260] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID a8821b81-1e27-4139-97ff-1efdadfd6b4f
2020-12-03 07:24:16,188 [IPC Server handler 8 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:24:16,189 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:24:16,189 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:24:16,230 [Thread-282] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID 8797193e-f01a-4f9a-9a17-eb37f9f1de79
2020-12-03 07:24:16,291 [IPC Server handler 9 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:24:16,292 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:24:16,292 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:24:16,393 [IPC Server handler 7 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:24:16,394 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:24:16,394 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:24:16,432 [Thread-172] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-c57dae7c-594d-4814-81e5-c9bf204f41ae
2020-12-03 07:24:16,432 [Thread-172] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11, StorageType: DISK
2020-12-03 07:24:16,432 [Thread-194] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-8517971e-c5a6-4cda-8698-97e4d8202f94
2020-12-03 07:24:16,432 [Thread-128] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-44a63aad-efff-4da7-8035-f12475e12191
2020-12-03 07:24:16,435 [Thread-216] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-a51c32af-aab6-4fd9-b287-03a51134ce85
2020-12-03 07:24:16,433 [Thread-59] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-b5e98384-c299-4c9a-bc26-b30d23c7aa11
2020-12-03 07:24:16,441 [Thread-59] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, StorageType: DISK
2020-12-03 07:24:16,434 [Thread-194] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13, StorageType: DISK
2020-12-03 07:24:16,432 [Thread-82] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-4ed0dfcb-1823-4feb-a78e-c34190aa43b1
2020-12-03 07:24:16,442 [Thread-82] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3, StorageType: DISK
2020-12-03 07:24:16,432 [Thread-282] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-b7b6573e-a1ea-4bd1-89c8-d3f7aef4bee9
2020-12-03 07:24:16,434 [Thread-106] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-02609b7a-10b1-4d2f-a95f-0fbf402c5ad9
2020-12-03 07:24:16,432 [Thread-238] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-f5960db6-7b54-4094-bc6c-6bca60faedad
2020-12-03 07:24:16,432 [Thread-150] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-adf39776-9f4e-495e-9c0f-d1401f599447
2020-12-03 07:24:16,443 [Thread-172] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-fb82c216-d71b-4654-9ad9-21ab3a7512a6
2020-12-03 07:24:16,443 [Thread-238] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17, StorageType: DISK
2020-12-03 07:24:16,443 [Thread-106] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5, StorageType: DISK
2020-12-03 07:24:16,443 [Thread-282] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21, StorageType: DISK
2020-12-03 07:24:16,441 [Thread-216] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15, StorageType: DISK
2020-12-03 07:24:16,433 [Thread-260] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-f0f6df85-a46f-4d89-b667-125138a11cef
2020-12-03 07:24:16,435 [Thread-128] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7, StorageType: DISK
2020-12-03 07:24:16,446 [Thread-260] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19, StorageType: DISK
2020-12-03 07:24:16,445 [Thread-172] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12, StorageType: DISK
2020-12-03 07:24:16,443 [Thread-150] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9, StorageType: DISK
2020-12-03 07:24:16,446 [Thread-194] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-95103ad2-7662-4492-ba1b-5cecf313b9e8
2020-12-03 07:24:16,447 [Thread-106] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-35c3127f-e8ca-452b-bcb1-392069cbc853
2020-12-03 07:24:16,449 [Thread-194] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14, StorageType: DISK
2020-12-03 07:24:16,449 [Thread-106] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6, StorageType: DISK
2020-12-03 07:24:16,451 [Thread-59] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-65fbe59b-ff8b-4331-bff0-e44b48174987
2020-12-03 07:24:16,452 [Thread-59] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, StorageType: DISK
2020-12-03 07:24:16,453 [Thread-150] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-8cd30729-7874-4e19-9225-c6c7a923c355
2020-12-03 07:24:16,454 [Thread-150] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10, StorageType: DISK
2020-12-03 07:24:16,455 [Thread-106] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:24:16,455 [Thread-150] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:24:16,455 [Thread-172] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:24:16,455 [Thread-194] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:24:16,455 [Thread-59] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:24:16,456 [Thread-260] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-c8ee244d-58c6-400b-8261-c3c19df916dd
2020-12-03 07:24:16,458 [Thread-260] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20, StorageType: DISK
2020-12-03 07:24:16,458 [Thread-260] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:24:16,459 [Thread-128] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-bc4245b5-06be-49c3-84ee-8bb52e5db51d
2020-12-03 07:24:16,460 [Thread-128] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8, StorageType: DISK
2020-12-03 07:24:16,460 [Thread-216] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-350dbe39-4a8c-4163-8d64-f89d775392cf
2020-12-03 07:24:16,460 [Thread-128] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:24:16,460 [Thread-216] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16, StorageType: DISK
2020-12-03 07:24:16,462 [Thread-216] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:24:16,462 [Thread-282] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-6d3e897d-ec54-4b20-9f96-ccf88af97eaa
2020-12-03 07:24:16,462 [Thread-282] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22, StorageType: DISK
2020-12-03 07:24:16,469 [Thread-282] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:24:16,470 [Thread-238] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-85392bd5-b857-478f-99fd-a6820ffff252
2020-12-03 07:24:16,470 [Thread-238] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18, StorageType: DISK
2020-12-03 07:24:16,474 [Thread-238] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:24:16,509 [IPC Server handler 5 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:24:16,513 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:24:16,513 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:24:16,530 [Thread-282] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21
2020-12-03 07:24:16,537 [Thread-82] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-9861837d-0d69-4f2f-b2aa-8b94418cf3c9
2020-12-03 07:24:16,537 [Thread-82] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, StorageType: DISK
2020-12-03 07:24:16,538 [Thread-82] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:24:16,538 [Thread-216] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15
2020-12-03 07:24:16,540 [Thread-82] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
2020-12-03 07:24:16,549 [Thread-238] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17
2020-12-03 07:24:16,551 [Thread-128] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7
2020-12-03 07:24:16,560 [Thread-282] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21
2020-12-03 07:24:16,566 [Thread-282] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22
2020-12-03 07:24:16,568 [Thread-194] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13
2020-12-03 07:24:16,567 [Thread-59] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:24:16,566 [Thread-260] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19
2020-12-03 07:24:16,569 [Thread-172] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11
2020-12-03 07:24:16,569 [Thread-282] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22
2020-12-03 07:24:16,570 [Thread-150] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9
2020-12-03 07:24:16,571 [Thread-282] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:16,560 [Thread-216] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15
2020-12-03 07:24:16,567 [Thread-238] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17
2020-12-03 07:24:16,575 [Thread-238] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18
2020-12-03 07:24:16,567 [Thread-82] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
2020-12-03 07:24:16,575 [Thread-216] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16
2020-12-03 07:24:16,567 [Thread-128] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7
2020-12-03 07:24:16,577 [Thread-82] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:24:16,577 [Thread-194] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13
2020-12-03 07:24:16,575 [Thread-238] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18
2020-12-03 07:24:16,578 [Thread-82] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:24:16,578 [Thread-194] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14
2020-12-03 07:24:16,578 [Thread-318] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21...
2020-12-03 07:24:16,578 [Thread-128] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8
2020-12-03 07:24:16,581 [Thread-319] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22...
2020-12-03 07:24:16,582 [Thread-82] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:16,580 [Thread-216] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16
2020-12-03 07:24:16,580 [Thread-194] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14
2020-12-03 07:24:16,580 [Thread-260] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19
2020-12-03 07:24:16,583 [Thread-260] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20
2020-12-03 07:24:16,583 [Thread-321] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4...
2020-12-03 07:24:16,578 [Thread-172] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11
2020-12-03 07:24:16,578 [Thread-238] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:16,584 [Thread-172] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12
2020-12-03 07:24:16,583 [Thread-260] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20
2020-12-03 07:24:16,584 [Thread-260] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:16,584 [Thread-322] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17...
2020-12-03 07:24:16,588 [Thread-324] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19...
2020-12-03 07:24:16,583 [Thread-194] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:16,583 [Thread-320] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3...
2020-12-03 07:24:16,583 [Thread-216] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:16,589 [Thread-326] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13...
2020-12-03 07:24:16,589 [Thread-327] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14...
2020-12-03 07:24:16,589 [Thread-328] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15...
2020-12-03 07:24:16,589 [Thread-329] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16...
2020-12-03 07:24:16,582 [Thread-106] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5
2020-12-03 07:24:16,582 [Thread-59] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:24:16,591 [Thread-59] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:24:16,591 [Thread-59] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:24:16,582 [Thread-128] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8
2020-12-03 07:24:16,596 [Thread-128] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:16,582 [Thread-150] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9
2020-12-03 07:24:16,597 [Thread-150] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10
2020-12-03 07:24:16,596 [Thread-106] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5
2020-12-03 07:24:16,596 [Thread-59] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:16,589 [Thread-325] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20...
2020-12-03 07:24:16,588 [Thread-172] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12
2020-12-03 07:24:16,584 [Thread-323] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18...
2020-12-03 07:24:16,598 [Thread-172] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:16,598 [Thread-150] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10
2020-12-03 07:24:16,597 [Thread-330] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7...
2020-12-03 07:24:16,597 [Thread-106] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6
2020-12-03 07:24:16,599 [Thread-334] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11...
2020-12-03 07:24:16,598 [Thread-333] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2020-12-03 07:24:16,598 [Thread-331] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8...
2020-12-03 07:24:16,598 [Thread-332] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2020-12-03 07:24:16,600 [Thread-335] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12...
2020-12-03 07:24:16,599 [Thread-106] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6
2020-12-03 07:24:16,599 [Thread-150] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:16,601 [Thread-106] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:16,602 [Thread-336] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9...
2020-12-03 07:24:16,602 [Thread-337] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5...
2020-12-03 07:24:16,602 [Thread-338] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10...
2020-12-03 07:24:16,602 [Thread-339] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6...
2020-12-03 07:24:16,618 [IPC Server handler 4 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:24:16,619 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:24:16,619 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:24:16,722 [IPC Server handler 0 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:24:16,723 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:24:16,723 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:24:16,740 [Thread-327] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1088075852-172.17.0.6-1606980241923 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14: 151ms
2020-12-03 07:24:16,826 [IPC Server handler 6 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:24:16,829 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:24:16,829 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:24:16,931 [IPC Server handler 2 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:24:16,932 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:24:16,932 [Thread-322] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1088075852-172.17.0.6-1606980241923 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17: 344ms
2020-12-03 07:24:16,932 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:24:16,936 [Thread-331] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1088075852-172.17.0.6-1606980241923 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8: 335ms
2020-12-03 07:24:16,936 [Thread-328] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1088075852-172.17.0.6-1606980241923 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15: 347ms
2020-12-03 07:24:16,945 [Thread-325] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1088075852-172.17.0.6-1606980241923 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20: 348ms
2020-12-03 07:24:16,945 [Thread-323] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1088075852-172.17.0.6-1606980241923 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18: 347ms
2020-12-03 07:24:16,950 [Thread-335] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1088075852-172.17.0.6-1606980241923 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12: 345ms
2020-12-03 07:24:16,950 [Thread-336] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1088075852-172.17.0.6-1606980241923 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9: 344ms
2020-12-03 07:24:16,959 [Thread-238] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1088075852-172.17.0.6-1606980241923: 367ms
2020-12-03 07:24:16,960 [Thread-329] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1088075852-172.17.0.6-1606980241923 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16: 371ms
2020-12-03 07:24:16,960 [Thread-337] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1088075852-172.17.0.6-1606980241923 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5: 358ms
2020-12-03 07:24:16,960 [Thread-216] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1088075852-172.17.0.6-1606980241923: 371ms
2020-12-03 07:24:16,962 [Thread-339] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1088075852-172.17.0.6-1606980241923 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6: 359ms
2020-12-03 07:24:16,963 [Thread-364] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18...
2020-12-03 07:24:16,972 [Thread-318] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1088075852-172.17.0.6-1606980241923 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21: 392ms
2020-12-03 07:24:16,972 [Thread-362] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15...
2020-12-03 07:24:16,971 [Thread-319] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1088075852-172.17.0.6-1606980241923 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22: 389ms
2020-12-03 07:24:16,970 [Thread-334] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1088075852-172.17.0.6-1606980241923 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11: 371ms
2020-12-03 07:24:16,970 [Thread-320] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1088075852-172.17.0.6-1606980241923 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3: 381ms
2020-12-03 07:24:16,970 [Thread-333] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1088075852-172.17.0.6-1606980241923 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 370ms
2020-12-03 07:24:16,969 [Thread-321] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1088075852-172.17.0.6-1606980241923 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4: 386ms
2020-12-03 07:24:16,967 [Thread-332] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1088075852-172.17.0.6-1606980241923 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 366ms
2020-12-03 07:24:16,985 [Thread-82] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1088075852-172.17.0.6-1606980241923: 403ms
2020-12-03 07:24:16,986 [Thread-330] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1088075852-172.17.0.6-1606980241923 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7: 387ms
2020-12-03 07:24:16,966 [Thread-365] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16...
2020-12-03 07:24:16,966 [Thread-363] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17...
2020-12-03 07:24:16,966 [Thread-106] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1088075852-172.17.0.6-1606980241923: 362ms
2020-12-03 07:24:16,986 [Thread-363] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17/current/BP-1088075852-172.17.0.6-1606980241923/current/replicas doesn't exist 
2020-12-03 07:24:16,986 [Thread-367] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4...
2020-12-03 07:24:16,986 [Thread-365] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16/current/BP-1088075852-172.17.0.6-1606980241923/current/replicas doesn't exist 
2020-12-03 07:24:16,986 [Thread-324] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1088075852-172.17.0.6-1606980241923 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19: 398ms
2020-12-03 07:24:16,986 [Thread-128] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1088075852-172.17.0.6-1606980241923: 390ms
2020-12-03 07:24:16,987 [Thread-260] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1088075852-172.17.0.6-1606980241923: 403ms
2020-12-03 07:24:16,986 [Thread-59] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1088075852-172.17.0.6-1606980241923: 389ms
2020-12-03 07:24:16,985 [Thread-172] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1088075852-172.17.0.6-1606980241923: 387ms
2020-12-03 07:24:16,985 [Thread-282] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1088075852-172.17.0.6-1606980241923: 414ms
2020-12-03 07:24:16,984 [Thread-326] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1088075852-172.17.0.6-1606980241923 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13: 395ms
2020-12-03 07:24:16,987 [Thread-373] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2020-12-03 07:24:16,988 [Thread-194] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1088075852-172.17.0.6-1606980241923: 398ms
2020-12-03 07:24:16,988 [Thread-373] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1088075852-172.17.0.6-1606980241923/current/replicas doesn't exist 
2020-12-03 07:24:16,980 [Thread-362] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15/current/BP-1088075852-172.17.0.6-1606980241923/current/replicas doesn't exist 
2020-12-03 07:24:16,988 [Thread-371] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19...
2020-12-03 07:24:16,973 [Thread-364] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18/current/BP-1088075852-172.17.0.6-1606980241923/current/replicas doesn't exist 
2020-12-03 07:24:16,988 [Thread-365] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16: 2ms
2020-12-03 07:24:16,988 [Thread-363] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17: 2ms
2020-12-03 07:24:16,988 [Thread-375] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2020-12-03 07:24:16,987 [Thread-370] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7...
2020-12-03 07:24:16,987 [Thread-369] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6...
2020-12-03 07:24:16,987 [Thread-338] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1088075852-172.17.0.6-1606980241923 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10: 385ms
2020-12-03 07:24:16,986 [Thread-367] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1088075852-172.17.0.6-1606980241923/current/replicas doesn't exist 
2020-12-03 07:24:16,986 [Thread-368] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5...
2020-12-03 07:24:16,991 [Thread-368] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-1088075852-172.17.0.6-1606980241923/current/replicas doesn't exist 
2020-12-03 07:24:16,991 [Thread-368] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5: 0ms
2020-12-03 07:24:16,986 [Thread-366] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3...
2020-12-03 07:24:16,994 [Thread-366] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1088075852-172.17.0.6-1606980241923/current/replicas doesn't exist 
2020-12-03 07:24:16,991 [Thread-367] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4: 5ms
2020-12-03 07:24:16,991 [Thread-150] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1088075852-172.17.0.6-1606980241923: 390ms
2020-12-03 07:24:16,991 [Thread-369] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/current/BP-1088075852-172.17.0.6-1606980241923/current/replicas doesn't exist 
2020-12-03 07:24:16,991 [Thread-379] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12...
2020-12-03 07:24:16,996 [Thread-382] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9...
2020-12-03 07:24:16,996 [Thread-379] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12/current/BP-1088075852-172.17.0.6-1606980241923/current/replicas doesn't exist 
2020-12-03 07:24:16,991 [Thread-370] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7/current/BP-1088075852-172.17.0.6-1606980241923/current/replicas doesn't exist 
2020-12-03 07:24:16,990 [Thread-375] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1088075852-172.17.0.6-1606980241923/current/replicas doesn't exist 
2020-12-03 07:24:16,990 [Thread-362] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15: 11ms
2020-12-03 07:24:16,990 [Thread-364] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18: 17ms
2020-12-03 07:24:16,997 [Thread-216] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923: 37ms
2020-12-03 07:24:16,997 [Thread-238] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923: 37ms
2020-12-03 07:24:16,990 [Thread-381] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14...
2020-12-03 07:24:16,989 [Thread-380] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22...
2020-12-03 07:24:16,989 [Thread-376] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21...
2020-12-03 07:24:16,988 [Thread-378] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13...
2020-12-03 07:24:16,988 [Thread-371] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19/current/BP-1088075852-172.17.0.6-1606980241923/current/replicas doesn't exist 
2020-12-03 07:24:16,988 [Thread-373] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 1ms
2020-12-03 07:24:16,988 [Thread-377] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20...
2020-12-03 07:24:16,999 [Thread-371] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19: 10ms
2020-12-03 07:24:16,988 [Thread-374] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11...
2020-12-03 07:24:16,988 [Thread-372] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8...
2020-12-03 07:24:16,999 [Thread-377] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20/current/BP-1088075852-172.17.0.6-1606980241923/current/replicas doesn't exist 
2020-12-03 07:24:16,998 [Thread-369] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6: 8ms
2020-12-03 07:24:16,999 [Thread-106] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923: 13ms
2020-12-03 07:24:16,999 [Thread-377] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20: 1ms
2020-12-03 07:24:16,998 [Thread-378] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13/current/BP-1088075852-172.17.0.6-1606980241923/current/replicas doesn't exist 
2020-12-03 07:24:16,998 [Thread-376] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21/current/BP-1088075852-172.17.0.6-1606980241923/current/replicas doesn't exist 
2020-12-03 07:24:16,998 [Thread-380] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22/current/BP-1088075852-172.17.0.6-1606980241923/current/replicas doesn't exist 
2020-12-03 07:24:17,000 [Thread-378] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13: 2ms
2020-12-03 07:24:17,000 [Thread-380] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22: 2ms
2020-12-03 07:24:17,000 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16
2020-12-03 07:24:17,003 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5
2020-12-03 07:24:16,998 [Thread-381] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14/current/BP-1088075852-172.17.0.6-1606980241923/current/replicas doesn't exist 
2020-12-03 07:24:16,997 [Thread-375] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 8ms
2020-12-03 07:24:16,997 [Thread-379] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12: 1ms
2020-12-03 07:24:16,997 [Thread-370] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7: 7ms
2020-12-03 07:24:16,997 [Thread-382] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9/current/BP-1088075852-172.17.0.6-1606980241923/current/replicas doesn't exist 
2020-12-03 07:24:16,996 [Thread-383] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10...
2020-12-03 07:24:16,996 [Thread-366] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3: 1ms
2020-12-03 07:24:17,007 [Thread-381] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14: 9ms
2020-12-03 07:24:17,007 [Thread-59] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923: 20ms
2020-12-03 07:24:17,003 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17
2020-12-03 07:24:17,003 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6
2020-12-03 07:24:17,003 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15
2020-12-03 07:24:17,011 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17, DS-f5960db6-7b54-4094-bc6c-6bca60faedad): finished scanning block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:17,012 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:24:17,002 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18
2020-12-03 07:24:17,012 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-b5e98384-c299-4c9a-bc26-b30d23c7aa11): finished scanning block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:17,002 [Thread-376] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21: 3ms
2020-12-03 07:24:17,012 [Thread-282] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923: 25ms
2020-12-03 07:24:16,999 [Thread-260] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923: 12ms
2020-12-03 07:24:16,999 [Thread-372] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/current/BP-1088075852-172.17.0.6-1606980241923/current/replicas doesn't exist 
2020-12-03 07:24:16,999 [Thread-374] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11/current/BP-1088075852-172.17.0.6-1606980241923/current/replicas doesn't exist 
2020-12-03 07:24:17,014 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21
2020-12-03 07:24:17,014 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20
2020-12-03 07:24:17,014 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22
2020-12-03 07:24:17,012 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18, DS-85392bd5-b857-478f-99fd-a6820ffff252): finished scanning block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:17,012 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15, DS-a51c32af-aab6-4fd9-b287-03a51134ce85): finished scanning block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:17,011 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6, DS-35c3127f-e8ca-452b-bcb1-392069cbc853): finished scanning block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:17,011 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:24:17,011 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5, DS-02609b7a-10b1-4d2f-a95f-0fbf402c5ad9): finished scanning block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:17,011 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16, DS-350dbe39-4a8c-4163-8d64-f89d775392cf): finished scanning block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:17,015 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-65fbe59b-ff8b-4331-bff0-e44b48174987): finished scanning block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:17,011 [Thread-383] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10/current/BP-1088075852-172.17.0.6-1606980241923/current/replicas doesn't exist 
2020-12-03 07:24:17,011 [Thread-194] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923: 23ms
2020-12-03 07:24:17,011 [Thread-82] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923: 26ms
2020-12-03 07:24:17,011 [Thread-382] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9: 15ms
2020-12-03 07:24:17,015 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19
2020-12-03 07:24:17,016 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:24:17,014 [Thread-374] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11: 15ms
2020-12-03 07:24:17,016 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, DS-9861837d-0d69-4f2f-b2aa-8b94418cf3c9): finished scanning block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:17,017 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21, DS-b7b6573e-a1ea-4bd1-89c8-d3f7aef4bee9): finished scanning block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:17,017 [Thread-383] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10: 6ms
2020-12-03 07:24:17,014 [Thread-372] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8: 15ms
2020-12-03 07:24:17,017 [Thread-150] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923: 21ms
2020-12-03 07:24:17,018 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10
2020-12-03 07:24:17,018 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9
2020-12-03 07:24:17,018 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10, DS-8cd30729-7874-4e19-9225-c6c7a923c355): finished scanning block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:17,017 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20, DS-c8ee244d-58c6-400b-8261-c3c19df916dd): finished scanning block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:17,018 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9, DS-adf39776-9f4e-495e-9c0f-d1401f599447): finished scanning block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:17,016 [Thread-172] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923: 29ms
2020-12-03 07:24:17,016 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14
2020-12-03 07:24:17,016 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
2020-12-03 07:24:17,016 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13
2020-12-03 07:24:17,019 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14, DS-95103ad2-7662-4492-ba1b-5cecf313b9e8): finished scanning block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:17,019 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3, DS-4ed0dfcb-1823-4feb-a78e-c34190aa43b1): finished scanning block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:17,019 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12
2020-12-03 07:24:17,019 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12, DS-fb82c216-d71b-4654-9ad9-21ab3a7512a6): finished scanning block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:17,017 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22, DS-6d3e897d-ec54-4b20-9f96-ccf88af97eaa): finished scanning block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:17,017 [Thread-128] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1088075852-172.17.0.6-1606980241923: 30ms
2020-12-03 07:24:17,020 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19, DS-f0f6df85-a46f-4d89-b667-125138a11cef): finished scanning block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:17,019 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11
2020-12-03 07:24:17,019 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13, DS-8517971e-c5a6-4cda-8698-97e4d8202f94): finished scanning block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:17,022 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7
2020-12-03 07:24:17,022 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1088075852-172.17.0.6-1606980241923 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8
2020-12-03 07:24:17,022 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11, DS-c57dae7c-594d-4814-81e5-c9bf204f41ae): finished scanning block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:17,023 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7, DS-44a63aad-efff-4da7-8035-f12475e12191): finished scanning block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:17,023 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8, DS-bc4245b5-06be-49c3-84ee-8bb52e5db51d): finished scanning block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:17,034 [IPC Server handler 1 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:24:17,035 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:24:17,035 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:24:17,039 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7, DS-44a63aad-efff-4da7-8035-f12475e12191): no suitable block pools found to scan.  Waiting 1814399983 ms.
2020-12-03 07:24:17,039 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22, DS-6d3e897d-ec54-4b20-9f96-ccf88af97eaa): no suitable block pools found to scan.  Waiting 1814399975 ms.
2020-12-03 07:24:17,039 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15, DS-a51c32af-aab6-4fd9-b287-03a51134ce85): no suitable block pools found to scan.  Waiting 1814399961 ms.
2020-12-03 07:24:17,039 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21, DS-b7b6573e-a1ea-4bd1-89c8-d3f7aef4bee9): no suitable block pools found to scan.  Waiting 1814399975 ms.
2020-12-03 07:24:17,039 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19, DS-f0f6df85-a46f-4d89-b667-125138a11cef): no suitable block pools found to scan.  Waiting 1814399975 ms.
2020-12-03 07:24:17,039 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, DS-9861837d-0d69-4f2f-b2aa-8b94418cf3c9): no suitable block pools found to scan.  Waiting 1814399977 ms.
2020-12-03 07:24:17,039 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17, DS-f5960db6-7b54-4094-bc6c-6bca60faedad): no suitable block pools found to scan.  Waiting 1814399961 ms.
2020-12-03 07:24:17,039 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5, DS-02609b7a-10b1-4d2f-a95f-0fbf402c5ad9): no suitable block pools found to scan.  Waiting 1814399961 ms.
2020-12-03 07:24:17,039 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9, DS-adf39776-9f4e-495e-9c0f-d1401f599447): no suitable block pools found to scan.  Waiting 1814399979 ms.
2020-12-03 07:24:17,039 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20, DS-c8ee244d-58c6-400b-8261-c3c19df916dd): no suitable block pools found to scan.  Waiting 1814399975 ms.
2020-12-03 07:24:17,039 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18, DS-85392bd5-b857-478f-99fd-a6820ffff252): no suitable block pools found to scan.  Waiting 1814399961 ms.
2020-12-03 07:24:17,039 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8, DS-bc4245b5-06be-49c3-84ee-8bb52e5db51d): no suitable block pools found to scan.  Waiting 1814399983 ms.
2020-12-03 07:24:17,039 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13, DS-8517971e-c5a6-4cda-8698-97e4d8202f94): no suitable block pools found to scan.  Waiting 1814399977 ms.
2020-12-03 07:24:17,039 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14, DS-95103ad2-7662-4492-ba1b-5cecf313b9e8): no suitable block pools found to scan.  Waiting 1814399977 ms.
2020-12-03 07:24:17,039 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3, DS-4ed0dfcb-1823-4feb-a78e-c34190aa43b1): no suitable block pools found to scan.  Waiting 1814399977 ms.
2020-12-03 07:24:17,039 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-65fbe59b-ff8b-4331-bff0-e44b48174987): no suitable block pools found to scan.  Waiting 1814399972 ms.
2020-12-03 07:24:17,039 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12, DS-fb82c216-d71b-4654-9ad9-21ab3a7512a6): no suitable block pools found to scan.  Waiting 1814399980 ms.
2020-12-03 07:24:17,039 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10, DS-8cd30729-7874-4e19-9225-c6c7a923c355): no suitable block pools found to scan.  Waiting 1814399979 ms.
2020-12-03 07:24:17,039 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16, DS-350dbe39-4a8c-4163-8d64-f89d775392cf): no suitable block pools found to scan.  Waiting 1814399961 ms.
2020-12-03 07:24:17,039 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6, DS-35c3127f-e8ca-452b-bcb1-392069cbc853): no suitable block pools found to scan.  Waiting 1814399961 ms.
2020-12-03 07:24:17,039 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-b5e98384-c299-4c9a-bc26-b30d23c7aa11): no suitable block pools found to scan.  Waiting 1814399972 ms.
2020-12-03 07:24:17,039 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11, DS-c57dae7c-594d-4814-81e5-c9bf204f41ae): no suitable block pools found to scan.  Waiting 1814399980 ms.
2020-12-03 07:24:17,042 [Thread-216] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 7:43 AM with interval of 21600000ms
2020-12-03 07:24:17,042 [Thread-172] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 8:24 AM with interval of 21600000ms
2020-12-03 07:24:17,042 [Thread-260] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 11:34 AM with interval of 21600000ms
2020-12-03 07:24:17,042 [Thread-59] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 8:26 AM with interval of 21600000ms
2020-12-03 07:24:17,042 [Thread-128] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 1:14 PM with interval of 21600000ms
2020-12-03 07:24:17,042 [Thread-238] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 7:53 AM with interval of 21600000ms
2020-12-03 07:24:17,042 [Thread-106] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 10:16 AM with interval of 21600000ms
2020-12-03 07:24:17,042 [Thread-82] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 8:04 AM with interval of 21600000ms
2020-12-03 07:24:17,042 [Thread-282] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 11:39 AM with interval of 21600000ms
2020-12-03 07:24:17,047 [Thread-194] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 8:16 AM with interval of 21600000ms
2020-12-03 07:24:17,047 [Thread-150] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 8:42 AM with interval of 21600000ms
2020-12-03 07:24:17,066 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid 61b60f2f-795d-44aa-82c3-0ac238b8a077) service to localhost/127.0.0.1:38816 beginning handshake with NN
2020-12-03 07:24:17,066 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid ad0684d8-6863-4e19-b2b3-d95e7d2668d9) service to localhost/127.0.0.1:38816 beginning handshake with NN
2020-12-03 07:24:17,066 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid a8821b81-1e27-4139-97ff-1efdadfd6b4f) service to localhost/127.0.0.1:38816 beginning handshake with NN
2020-12-03 07:24:17,066 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid e6a510c0-efbe-44e4-b3ca-915180c69d66) service to localhost/127.0.0.1:38816 beginning handshake with NN
2020-12-03 07:24:17,066 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid e6d8c096-523b-4539-b874-b517e981fab6) service to localhost/127.0.0.1:38816 beginning handshake with NN
2020-12-03 07:24:17,069 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid 028e287b-d054-4bef-b640-ae581093b777) service to localhost/127.0.0.1:38816 beginning handshake with NN
2020-12-03 07:24:17,069 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid 31eff9f5-fd87-487e-8b1a-43cc24be4844) service to localhost/127.0.0.1:38816 beginning handshake with NN
2020-12-03 07:24:17,076 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid 53d1c8e9-e11d-4d1a-9a77-35621ba2c419) service to localhost/127.0.0.1:38816 beginning handshake with NN
2020-12-03 07:24:17,074 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid 31491a6f-508d-45fd-8589-10a6a9373e21) service to localhost/127.0.0.1:38816 beginning handshake with NN
2020-12-03 07:24:17,076 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid 941777f1-35bc-48df-a860-96f904e7fd34) service to localhost/127.0.0.1:38816 beginning handshake with NN
2020-12-03 07:24:17,077 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid 8797193e-f01a-4f9a-9a17-eb37f9f1de79) service to localhost/127.0.0.1:38816 beginning handshake with NN
2020-12-03 07:24:17,088 [IPC Server handler 8 on default port 38816] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:38730, datanodeUuid=e6d8c096-523b-4539-b874-b517e981fab6, infoPort=37138, infoSecurePort=0, ipcPort=36951, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923) storage e6d8c096-523b-4539-b874-b517e981fab6
2020-12-03 07:24:17,090 [IPC Server handler 8 on default port 38816] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:38730
2020-12-03 07:24:17,091 [IPC Server handler 8 on default port 38816] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN e6d8c096-523b-4539-b874-b517e981fab6 (127.0.0.1:38730).
2020-12-03 07:24:17,094 [IPC Server handler 7 on default port 38816] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:36287, datanodeUuid=a8821b81-1e27-4139-97ff-1efdadfd6b4f, infoPort=35781, infoSecurePort=0, ipcPort=38720, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923) storage a8821b81-1e27-4139-97ff-1efdadfd6b4f
2020-12-03 07:24:17,094 [IPC Server handler 7 on default port 38816] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:36287
2020-12-03 07:24:17,095 [IPC Server handler 7 on default port 38816] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN a8821b81-1e27-4139-97ff-1efdadfd6b4f (127.0.0.1:36287).
2020-12-03 07:24:17,096 [IPC Server handler 9 on default port 38816] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:44445, datanodeUuid=ad0684d8-6863-4e19-b2b3-d95e7d2668d9, infoPort=40126, infoSecurePort=0, ipcPort=35715, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923) storage ad0684d8-6863-4e19-b2b3-d95e7d2668d9
2020-12-03 07:24:17,096 [IPC Server handler 9 on default port 38816] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:44445
2020-12-03 07:24:17,096 [IPC Server handler 9 on default port 38816] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN ad0684d8-6863-4e19-b2b3-d95e7d2668d9 (127.0.0.1:44445).
2020-12-03 07:24:17,096 [IPC Server handler 5 on default port 38816] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:41069, datanodeUuid=31eff9f5-fd87-487e-8b1a-43cc24be4844, infoPort=41537, infoSecurePort=0, ipcPort=37986, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923) storage 31eff9f5-fd87-487e-8b1a-43cc24be4844
2020-12-03 07:24:17,097 [IPC Server handler 5 on default port 38816] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:41069
2020-12-03 07:24:17,097 [IPC Server handler 5 on default port 38816] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 31eff9f5-fd87-487e-8b1a-43cc24be4844 (127.0.0.1:41069).
2020-12-03 07:24:17,097 [IPC Server handler 3 on default port 38816] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:37772, datanodeUuid=e6a510c0-efbe-44e4-b3ca-915180c69d66, infoPort=41739, infoSecurePort=0, ipcPort=36035, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923) storage e6a510c0-efbe-44e4-b3ca-915180c69d66
2020-12-03 07:24:17,097 [IPC Server handler 3 on default port 38816] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:37772
2020-12-03 07:24:17,098 [IPC Server handler 3 on default port 38816] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN e6a510c0-efbe-44e4-b3ca-915180c69d66 (127.0.0.1:37772).
2020-12-03 07:24:17,098 [IPC Server handler 4 on default port 38816] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:44312, datanodeUuid=941777f1-35bc-48df-a860-96f904e7fd34, infoPort=43524, infoSecurePort=0, ipcPort=41810, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923) storage 941777f1-35bc-48df-a860-96f904e7fd34
2020-12-03 07:24:17,098 [IPC Server handler 4 on default port 38816] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:44312
2020-12-03 07:24:17,098 [IPC Server handler 4 on default port 38816] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 941777f1-35bc-48df-a860-96f904e7fd34 (127.0.0.1:44312).
2020-12-03 07:24:17,099 [IPC Server handler 0 on default port 38816] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:38680, datanodeUuid=61b60f2f-795d-44aa-82c3-0ac238b8a077, infoPort=35056, infoSecurePort=0, ipcPort=33077, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923) storage 61b60f2f-795d-44aa-82c3-0ac238b8a077
2020-12-03 07:24:17,099 [IPC Server handler 0 on default port 38816] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:38680
2020-12-03 07:24:17,099 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid e6d8c096-523b-4539-b874-b517e981fab6) service to localhost/127.0.0.1:38816 successfully registered with NN
2020-12-03 07:24:17,099 [IPC Server handler 0 on default port 38816] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 61b60f2f-795d-44aa-82c3-0ac238b8a077 (127.0.0.1:38680).
2020-12-03 07:24:17,099 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (DataNode.java:registerBlockPoolWithSecretManager(1615)) - Block token params received from NN: for block pool BP-1088075852-172.17.0.6-1606980241923 keyUpdateInterval=600 min(s), tokenLifetime=600 min(s)
2020-12-03 07:24:17,099 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid a8821b81-1e27-4139-97ff-1efdadfd6b4f) service to localhost/127.0.0.1:38816 successfully registered with NN
2020-12-03 07:24:17,099 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid 941777f1-35bc-48df-a860-96f904e7fd34) service to localhost/127.0.0.1:38816 successfully registered with NN
2020-12-03 07:24:17,100 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (DataNode.java:registerBlockPoolWithSecretManager(1615)) - Block token params received from NN: for block pool BP-1088075852-172.17.0.6-1606980241923 keyUpdateInterval=600 min(s), tokenLifetime=600 min(s)
2020-12-03 07:24:17,100 [IPC Server handler 6 on default port 38816] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:36526, datanodeUuid=53d1c8e9-e11d-4d1a-9a77-35621ba2c419, infoPort=43859, infoSecurePort=0, ipcPort=36553, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923) storage 53d1c8e9-e11d-4d1a-9a77-35621ba2c419
2020-12-03 07:24:17,100 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid e6a510c0-efbe-44e4-b3ca-915180c69d66) service to localhost/127.0.0.1:38816 successfully registered with NN
2020-12-03 07:24:17,100 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (DataNode.java:registerBlockPoolWithSecretManager(1615)) - Block token params received from NN: for block pool BP-1088075852-172.17.0.6-1606980241923 keyUpdateInterval=600 min(s), tokenLifetime=600 min(s)
2020-12-03 07:24:17,101 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:24:17,100 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid 61b60f2f-795d-44aa-82c3-0ac238b8a077) service to localhost/127.0.0.1:38816 successfully registered with NN
2020-12-03 07:24:17,100 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (DataNode.java:registerBlockPoolWithSecretManager(1615)) - Block token params received from NN: for block pool BP-1088075852-172.17.0.6-1606980241923 keyUpdateInterval=600 min(s), tokenLifetime=600 min(s)
2020-12-03 07:24:17,100 [IPC Server handler 6 on default port 38816] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:36526
2020-12-03 07:24:17,101 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:24:17,101 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (DataNode.java:registerBlockPoolWithSecretManager(1615)) - Block token params received from NN: for block pool BP-1088075852-172.17.0.6-1606980241923 keyUpdateInterval=600 min(s), tokenLifetime=600 min(s)
2020-12-03 07:24:17,101 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:38816 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:24:17,101 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:24:17,101 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:24:17,101 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:38816 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:24:17,101 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid ad0684d8-6863-4e19-b2b3-d95e7d2668d9) service to localhost/127.0.0.1:38816 successfully registered with NN
2020-12-03 07:24:17,101 [IPC Server handler 6 on default port 38816] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 53d1c8e9-e11d-4d1a-9a77-35621ba2c419 (127.0.0.1:36526).
2020-12-03 07:24:17,101 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:24:17,102 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (DataNode.java:registerBlockPoolWithSecretManager(1615)) - Block token params received from NN: for block pool BP-1088075852-172.17.0.6-1606980241923 keyUpdateInterval=600 min(s), tokenLifetime=600 min(s)
2020-12-03 07:24:17,102 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:38816 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:24:17,102 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:38816 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:24:17,102 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:24:17,102 [IPC Server handler 2 on default port 38816] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:36140, datanodeUuid=31491a6f-508d-45fd-8589-10a6a9373e21, infoPort=40038, infoSecurePort=0, ipcPort=43238, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923) storage 31491a6f-508d-45fd-8589-10a6a9373e21
2020-12-03 07:24:17,102 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:38816 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:24:17,103 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid 53d1c8e9-e11d-4d1a-9a77-35621ba2c419) service to localhost/127.0.0.1:38816 successfully registered with NN
2020-12-03 07:24:17,103 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:38816 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:24:17,103 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (DataNode.java:registerBlockPoolWithSecretManager(1615)) - Block token params received from NN: for block pool BP-1088075852-172.17.0.6-1606980241923 keyUpdateInterval=600 min(s), tokenLifetime=600 min(s)
2020-12-03 07:24:17,103 [IPC Server handler 2 on default port 38816] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:36140
2020-12-03 07:24:17,108 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:24:17,109 [IPC Server handler 2 on default port 38816] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 31491a6f-508d-45fd-8589-10a6a9373e21 (127.0.0.1:36140).
2020-12-03 07:24:17,109 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:38816 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:24:17,109 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid 31eff9f5-fd87-487e-8b1a-43cc24be4844) service to localhost/127.0.0.1:38816 successfully registered with NN
2020-12-03 07:24:17,109 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (DataNode.java:registerBlockPoolWithSecretManager(1615)) - Block token params received from NN: for block pool BP-1088075852-172.17.0.6-1606980241923 keyUpdateInterval=600 min(s), tokenLifetime=600 min(s)
2020-12-03 07:24:17,109 [IPC Server handler 1 on default port 38816] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:40243, datanodeUuid=028e287b-d054-4bef-b640-ae581093b777, infoPort=46239, infoSecurePort=0, ipcPort=41403, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923) storage 028e287b-d054-4bef-b640-ae581093b777
2020-12-03 07:24:17,109 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:24:17,111 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:38816 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:24:17,111 [IPC Server handler 1 on default port 38816] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:40243
2020-12-03 07:24:17,111 [IPC Server handler 1 on default port 38816] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 028e287b-d054-4bef-b640-ae581093b777 (127.0.0.1:40243).
2020-12-03 07:24:17,112 [IPC Server handler 7 on default port 38816] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:43902, datanodeUuid=8797193e-f01a-4f9a-9a17-eb37f9f1de79, infoPort=34956, infoSecurePort=0, ipcPort=46163, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923) storage 8797193e-f01a-4f9a-9a17-eb37f9f1de79
2020-12-03 07:24:17,112 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid 31491a6f-508d-45fd-8589-10a6a9373e21) service to localhost/127.0.0.1:38816 successfully registered with NN
2020-12-03 07:24:17,112 [IPC Server handler 7 on default port 38816] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:43902
2020-12-03 07:24:17,112 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (DataNode.java:registerBlockPoolWithSecretManager(1615)) - Block token params received from NN: for block pool BP-1088075852-172.17.0.6-1606980241923 keyUpdateInterval=600 min(s), tokenLifetime=600 min(s)
2020-12-03 07:24:17,112 [IPC Server handler 7 on default port 38816] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 8797193e-f01a-4f9a-9a17-eb37f9f1de79 (127.0.0.1:43902).
2020-12-03 07:24:17,113 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:24:17,113 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid 028e287b-d054-4bef-b640-ae581093b777) service to localhost/127.0.0.1:38816 successfully registered with NN
2020-12-03 07:24:17,113 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:38816 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:24:17,113 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (DataNode.java:registerBlockPoolWithSecretManager(1615)) - Block token params received from NN: for block pool BP-1088075852-172.17.0.6-1606980241923 keyUpdateInterval=600 min(s), tokenLifetime=600 min(s)
2020-12-03 07:24:17,113 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid 8797193e-f01a-4f9a-9a17-eb37f9f1de79) service to localhost/127.0.0.1:38816 successfully registered with NN
2020-12-03 07:24:17,114 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:24:17,114 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (DataNode.java:registerBlockPoolWithSecretManager(1615)) - Block token params received from NN: for block pool BP-1088075852-172.17.0.6-1606980241923 keyUpdateInterval=600 min(s), tokenLifetime=600 min(s)
2020-12-03 07:24:17,114 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:38816 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:24:17,115 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:24:17,115 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:38816 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:24:17,133 [IPC Server handler 8 on default port 38816] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-c57dae7c-594d-4814-81e5-c9bf204f41ae for DN 127.0.0.1:37772
2020-12-03 07:24:17,133 [IPC Server handler 8 on default port 38816] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-fb82c216-d71b-4654-9ad9-21ab3a7512a6 for DN 127.0.0.1:37772
2020-12-03 07:24:17,135 [IPC Server handler 1 on default port 38816] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-b5e98384-c299-4c9a-bc26-b30d23c7aa11 for DN 127.0.0.1:36140
2020-12-03 07:24:17,138 [IPC Server handler 1 on default port 38816] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-65fbe59b-ff8b-4331-bff0-e44b48174987 for DN 127.0.0.1:36140
2020-12-03 07:24:17,139 [IPC Server handler 6 on default port 38816] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-8517971e-c5a6-4cda-8698-97e4d8202f94 for DN 127.0.0.1:41069
2020-12-03 07:24:17,139 [IPC Server handler 6 on default port 38816] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-95103ad2-7662-4492-ba1b-5cecf313b9e8 for DN 127.0.0.1:41069
2020-12-03 07:24:17,142 [IPC Server handler 2 on default port 38816] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-adf39776-9f4e-495e-9c0f-d1401f599447 for DN 127.0.0.1:44312
2020-12-03 07:24:17,142 [IPC Server handler 2 on default port 38816] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-8cd30729-7874-4e19-9225-c6c7a923c355 for DN 127.0.0.1:44312
2020-12-03 07:24:17,143 [IPC Server handler 0 on default port 38816] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-02609b7a-10b1-4d2f-a95f-0fbf402c5ad9 for DN 127.0.0.1:40243
2020-12-03 07:24:17,143 [IPC Server handler 0 on default port 38816] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-35c3127f-e8ca-452b-bcb1-392069cbc853 for DN 127.0.0.1:40243
2020-12-03 07:24:17,144 [IPC Server handler 4 on default port 38816] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-a51c32af-aab6-4fd9-b287-03a51134ce85 for DN 127.0.0.1:44445
2020-12-03 07:24:17,145 [IPC Server handler 4 on default port 38816] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-350dbe39-4a8c-4163-8d64-f89d775392cf for DN 127.0.0.1:44445
2020-12-03 07:24:17,146 [IPC Server handler 3 on default port 38816] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-f5960db6-7b54-4094-bc6c-6bca60faedad for DN 127.0.0.1:38730
2020-12-03 07:24:17,147 [IPC Server handler 3 on default port 38816] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-85392bd5-b857-478f-99fd-a6820ffff252 for DN 127.0.0.1:38730
2020-12-03 07:24:17,151 [IPC Server handler 9 on default port 38816] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-b7b6573e-a1ea-4bd1-89c8-d3f7aef4bee9 for DN 127.0.0.1:43902
2020-12-03 07:24:17,151 [IPC Server handler 7 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:24:17,151 [IPC Server handler 9 on default port 38816] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-6d3e897d-ec54-4b20-9f96-ccf88af97eaa for DN 127.0.0.1:43902
2020-12-03 07:24:17,152 [IPC Server handler 5 on default port 38816] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-4ed0dfcb-1823-4feb-a78e-c34190aa43b1 for DN 127.0.0.1:38680
2020-12-03 07:24:17,152 [IPC Server handler 5 on default port 38816] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-9861837d-0d69-4f2f-b2aa-8b94418cf3c9 for DN 127.0.0.1:38680
2020-12-03 07:24:17,158 [IPC Server handler 7 on default port 38816] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-f0f6df85-a46f-4d89-b667-125138a11cef for DN 127.0.0.1:36287
2020-12-03 07:24:17,158 [IPC Server handler 7 on default port 38816] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-c8ee244d-58c6-400b-8261-c3c19df916dd for DN 127.0.0.1:36287
2020-12-03 07:24:17,159 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2798)) - No heartbeat from DataNode: 127.0.0.1:36287
2020-12-03 07:24:17,160 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:24:17,160 [IPC Server handler 5 on default port 38816] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-44a63aad-efff-4da7-8035-f12475e12191 for DN 127.0.0.1:36526
2020-12-03 07:24:17,160 [IPC Server handler 5 on default port 38816] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-bc4245b5-06be-49c3-84ee-8bb52e5db51d for DN 127.0.0.1:36526
2020-12-03 07:24:17,187 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x946acad10e64be68: Processing first storage report for DS-c57dae7c-594d-4814-81e5-c9bf204f41ae from datanode e6a510c0-efbe-44e4-b3ca-915180c69d66
2020-12-03 07:24:17,189 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x946acad10e64be68: from storage DS-c57dae7c-594d-4814-81e5-c9bf204f41ae node DatanodeRegistration(127.0.0.1:37772, datanodeUuid=e6a510c0-efbe-44e4-b3ca-915180c69d66, infoPort=41739, infoSecurePort=0, ipcPort=36035, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923), blocks: 0, hasStaleStorage: true, processing time: 2 msecs, invalidatedBlocks: 0
2020-12-03 07:24:17,189 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x6ea85b7a57a433e3: Processing first storage report for DS-b5e98384-c299-4c9a-bc26-b30d23c7aa11 from datanode 31491a6f-508d-45fd-8589-10a6a9373e21
2020-12-03 07:24:17,189 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x6ea85b7a57a433e3: from storage DS-b5e98384-c299-4c9a-bc26-b30d23c7aa11 node DatanodeRegistration(127.0.0.1:36140, datanodeUuid=31491a6f-508d-45fd-8589-10a6a9373e21, infoPort=40038, infoSecurePort=0, ipcPort=43238, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:24:17,189 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0xecdd8e93a5f2afb4: Processing first storage report for DS-a51c32af-aab6-4fd9-b287-03a51134ce85 from datanode ad0684d8-6863-4e19-b2b3-d95e7d2668d9
2020-12-03 07:24:17,189 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0xecdd8e93a5f2afb4: from storage DS-a51c32af-aab6-4fd9-b287-03a51134ce85 node DatanodeRegistration(127.0.0.1:44445, datanodeUuid=ad0684d8-6863-4e19-b2b3-d95e7d2668d9, infoPort=40126, infoSecurePort=0, ipcPort=35715, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:24:17,190 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x715d2450d80e1e23: Processing first storage report for DS-adf39776-9f4e-495e-9c0f-d1401f599447 from datanode 941777f1-35bc-48df-a860-96f904e7fd34
2020-12-03 07:24:17,190 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x715d2450d80e1e23: from storage DS-adf39776-9f4e-495e-9c0f-d1401f599447 node DatanodeRegistration(127.0.0.1:44312, datanodeUuid=941777f1-35bc-48df-a860-96f904e7fd34, infoPort=43524, infoSecurePort=0, ipcPort=41810, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923), blocks: 0, hasStaleStorage: true, processing time: 1 msecs, invalidatedBlocks: 0
2020-12-03 07:24:17,190 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0xc873e85899113f2: Processing first storage report for DS-95103ad2-7662-4492-ba1b-5cecf313b9e8 from datanode 31eff9f5-fd87-487e-8b1a-43cc24be4844
2020-12-03 07:24:17,190 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0xc873e85899113f2: from storage DS-95103ad2-7662-4492-ba1b-5cecf313b9e8 node DatanodeRegistration(127.0.0.1:41069, datanodeUuid=31eff9f5-fd87-487e-8b1a-43cc24be4844, infoPort=41537, infoSecurePort=0, ipcPort=37986, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:24:17,190 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x65124fe5f65f8952: Processing first storage report for DS-35c3127f-e8ca-452b-bcb1-392069cbc853 from datanode 028e287b-d054-4bef-b640-ae581093b777
2020-12-03 07:24:17,190 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x65124fe5f65f8952: from storage DS-35c3127f-e8ca-452b-bcb1-392069cbc853 node DatanodeRegistration(127.0.0.1:40243, datanodeUuid=028e287b-d054-4bef-b640-ae581093b777, infoPort=46239, infoSecurePort=0, ipcPort=41403, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:24:17,190 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x946acad10e64be68: Processing first storage report for DS-fb82c216-d71b-4654-9ad9-21ab3a7512a6 from datanode e6a510c0-efbe-44e4-b3ca-915180c69d66
2020-12-03 07:24:17,191 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x946acad10e64be68: from storage DS-fb82c216-d71b-4654-9ad9-21ab3a7512a6 node DatanodeRegistration(127.0.0.1:37772, datanodeUuid=e6a510c0-efbe-44e4-b3ca-915180c69d66, infoPort=41739, infoSecurePort=0, ipcPort=36035, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:24:17,191 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x6ea85b7a57a433e3: Processing first storage report for DS-65fbe59b-ff8b-4331-bff0-e44b48174987 from datanode 31491a6f-508d-45fd-8589-10a6a9373e21
2020-12-03 07:24:17,191 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x6ea85b7a57a433e3: from storage DS-65fbe59b-ff8b-4331-bff0-e44b48174987 node DatanodeRegistration(127.0.0.1:36140, datanodeUuid=31491a6f-508d-45fd-8589-10a6a9373e21, infoPort=40038, infoSecurePort=0, ipcPort=43238, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:24:17,191 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0xecdd8e93a5f2afb4: Processing first storage report for DS-350dbe39-4a8c-4163-8d64-f89d775392cf from datanode ad0684d8-6863-4e19-b2b3-d95e7d2668d9
2020-12-03 07:24:17,191 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0xecdd8e93a5f2afb4: from storage DS-350dbe39-4a8c-4163-8d64-f89d775392cf node DatanodeRegistration(127.0.0.1:44445, datanodeUuid=ad0684d8-6863-4e19-b2b3-d95e7d2668d9, infoPort=40126, infoSecurePort=0, ipcPort=35715, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:24:17,191 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x715d2450d80e1e23: Processing first storage report for DS-8cd30729-7874-4e19-9225-c6c7a923c355 from datanode 941777f1-35bc-48df-a860-96f904e7fd34
2020-12-03 07:24:17,191 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x715d2450d80e1e23: from storage DS-8cd30729-7874-4e19-9225-c6c7a923c355 node DatanodeRegistration(127.0.0.1:44312, datanodeUuid=941777f1-35bc-48df-a860-96f904e7fd34, infoPort=43524, infoSecurePort=0, ipcPort=41810, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:24:17,191 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0xc873e85899113f2: Processing first storage report for DS-8517971e-c5a6-4cda-8698-97e4d8202f94 from datanode 31eff9f5-fd87-487e-8b1a-43cc24be4844
2020-12-03 07:24:17,192 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0xc873e85899113f2: from storage DS-8517971e-c5a6-4cda-8698-97e4d8202f94 node DatanodeRegistration(127.0.0.1:41069, datanodeUuid=31eff9f5-fd87-487e-8b1a-43cc24be4844, infoPort=41537, infoSecurePort=0, ipcPort=37986, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:24:17,192 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x65124fe5f65f8952: Processing first storage report for DS-02609b7a-10b1-4d2f-a95f-0fbf402c5ad9 from datanode 028e287b-d054-4bef-b640-ae581093b777
2020-12-03 07:24:17,192 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x65124fe5f65f8952: from storage DS-02609b7a-10b1-4d2f-a95f-0fbf402c5ad9 node DatanodeRegistration(127.0.0.1:40243, datanodeUuid=028e287b-d054-4bef-b640-ae581093b777, infoPort=46239, infoSecurePort=0, ipcPort=41403, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:24:17,214 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x715d2450d80e1e23,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 43 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:24:17,214 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0xecdd8e93a5f2afb4,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 43 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:24:17,214 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x6ea85b7a57a433e3,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 43 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:24:17,214 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0xc873e85899113f2,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 43 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:24:17,214 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x946acad10e64be68,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 43 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:24:17,215 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:17,214 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x65124fe5f65f8952,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 43 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:24:17,215 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:17,215 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:17,215 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:17,215 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:17,215 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:17,263 [IPC Server handler 8 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:24:17,267 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2758)) - Cluster is active
2020-12-03 07:24:17,296 [IPC Server handler 4 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/striped	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-12-03 07:24:17,323 [IPC Server handler 5 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setErasureCodingPolicy	src=/striped	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-12-03 07:24:17,333 [IPC Server handler 7 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=enableErasureCodingPolicy	src=RS-6-3-1024k	dst=null	perm=null	proto=rpc
2020-12-03 07:24:17,338 [Thread-417] INFO  hdfs.TestFileChecksum (TestFileChecksum.java:testStripedFileChecksumWithMissedDataBlocksRangeQuery(290)) - Checksum file:/striped/stripedFileChecksum1, requested length:754974720
2020-12-03 07:24:18,437 [IPC Server handler 2 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/striped/stripedFileChecksum1	dst=null	perm=null	proto=rpc
2020-12-03 07:24:18,470 [IPC Server handler 1 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/striped/stripedFileChecksum1	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2020-12-03 07:24:18,896 [IPC Server handler 3 on default port 38816] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_-9223372036854775792_1001, replicas=127.0.0.1:36526, 127.0.0.1:38730, 127.0.0.1:43902, 127.0.0.1:44445, 127.0.0.1:37772, 127.0.0.1:44312, 127.0.0.1:40243, 127.0.0.1:36287, 127.0.0.1:36140 for /striped/stripedFileChecksum1
2020-12-03 07:24:18,948 [Thread-418] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:18,972 [Thread-419] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:18,980 [Thread-420] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:18,986 [Thread-421] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:18,995 [Thread-422] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:19,003 [Thread-423] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:20,042 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@1827a871] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(205)) - Detected pause in JVM or host machine (eg GC): pause of approximately 1019ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=400ms
GC pool 'PS Scavenge' had collection(s): count=1 time=615ms
2020-12-03 07:24:20,042 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@5d52e3ef] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(205)) - Detected pause in JVM or host machine (eg GC): pause of approximately 1001ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=400ms
GC pool 'PS Scavenge' had collection(s): count=1 time=615ms
2020-12-03 07:24:20,078 [Thread-424] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:20,131 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x16866707ee0a9331: Processing first storage report for DS-c8ee244d-58c6-400b-8261-c3c19df916dd from datanode a8821b81-1e27-4139-97ff-1efdadfd6b4f
2020-12-03 07:24:20,131 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x16866707ee0a9331: from storage DS-c8ee244d-58c6-400b-8261-c3c19df916dd node DatanodeRegistration(127.0.0.1:36287, datanodeUuid=a8821b81-1e27-4139-97ff-1efdadfd6b4f, infoPort=35781, infoSecurePort=0, ipcPort=38720, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:24:20,136 [Thread-425] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:20,136 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x95590c900e00adbb: Processing first storage report for DS-f5960db6-7b54-4094-bc6c-6bca60faedad from datanode e6d8c096-523b-4539-b874-b517e981fab6
2020-12-03 07:24:20,137 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x95590c900e00adbb: from storage DS-f5960db6-7b54-4094-bc6c-6bca60faedad node DatanodeRegistration(127.0.0.1:38730, datanodeUuid=e6d8c096-523b-4539-b874-b517e981fab6, infoPort=37138, infoSecurePort=0, ipcPort=36951, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923), blocks: 0, hasStaleStorage: true, processing time: 1 msecs, invalidatedBlocks: 0
2020-12-03 07:24:20,140 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0xe23cc46f1ab8eed0: Processing first storage report for DS-44a63aad-efff-4da7-8035-f12475e12191 from datanode 53d1c8e9-e11d-4d1a-9a77-35621ba2c419
2020-12-03 07:24:20,140 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0xe23cc46f1ab8eed0: from storage DS-44a63aad-efff-4da7-8035-f12475e12191 node DatanodeRegistration(127.0.0.1:36526, datanodeUuid=53d1c8e9-e11d-4d1a-9a77-35621ba2c419, infoPort=43859, infoSecurePort=0, ipcPort=36553, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:24:20,141 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x16866707ee0a9331: Processing first storage report for DS-f0f6df85-a46f-4d89-b667-125138a11cef from datanode a8821b81-1e27-4139-97ff-1efdadfd6b4f
2020-12-03 07:24:20,141 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x16866707ee0a9331: from storage DS-f0f6df85-a46f-4d89-b667-125138a11cef node DatanodeRegistration(127.0.0.1:36287, datanodeUuid=a8821b81-1e27-4139-97ff-1efdadfd6b4f, infoPort=35781, infoSecurePort=0, ipcPort=38720, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:24:20,141 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x1f9726fe12158809: Processing first storage report for DS-4ed0dfcb-1823-4feb-a78e-c34190aa43b1 from datanode 61b60f2f-795d-44aa-82c3-0ac238b8a077
2020-12-03 07:24:20,141 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x1f9726fe12158809: from storage DS-4ed0dfcb-1823-4feb-a78e-c34190aa43b1 node DatanodeRegistration(127.0.0.1:38680, datanodeUuid=61b60f2f-795d-44aa-82c3-0ac238b8a077, infoPort=35056, infoSecurePort=0, ipcPort=33077, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:24:20,141 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x95590c900e00adbb: Processing first storage report for DS-85392bd5-b857-478f-99fd-a6820ffff252 from datanode e6d8c096-523b-4539-b874-b517e981fab6
2020-12-03 07:24:20,141 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x95590c900e00adbb: from storage DS-85392bd5-b857-478f-99fd-a6820ffff252 node DatanodeRegistration(127.0.0.1:38730, datanodeUuid=e6d8c096-523b-4539-b874-b517e981fab6, infoPort=37138, infoSecurePort=0, ipcPort=36951, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:24:20,142 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0xbd0349add375d843: Processing first storage report for DS-6d3e897d-ec54-4b20-9f96-ccf88af97eaa from datanode 8797193e-f01a-4f9a-9a17-eb37f9f1de79
2020-12-03 07:24:20,142 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0xbd0349add375d843: from storage DS-6d3e897d-ec54-4b20-9f96-ccf88af97eaa node DatanodeRegistration(127.0.0.1:43902, datanodeUuid=8797193e-f01a-4f9a-9a17-eb37f9f1de79, infoPort=34956, infoSecurePort=0, ipcPort=46163, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923), blocks: 0, hasStaleStorage: true, processing time: 1 msecs, invalidatedBlocks: 0
2020-12-03 07:24:20,142 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0xe23cc46f1ab8eed0: Processing first storage report for DS-bc4245b5-06be-49c3-84ee-8bb52e5db51d from datanode 53d1c8e9-e11d-4d1a-9a77-35621ba2c419
2020-12-03 07:24:20,142 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0xe23cc46f1ab8eed0: from storage DS-bc4245b5-06be-49c3-84ee-8bb52e5db51d node DatanodeRegistration(127.0.0.1:36526, datanodeUuid=53d1c8e9-e11d-4d1a-9a77-35621ba2c419, infoPort=43859, infoSecurePort=0, ipcPort=36553, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:24:20,142 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x1f9726fe12158809: Processing first storage report for DS-9861837d-0d69-4f2f-b2aa-8b94418cf3c9 from datanode 61b60f2f-795d-44aa-82c3-0ac238b8a077
2020-12-03 07:24:20,142 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x1f9726fe12158809: from storage DS-9861837d-0d69-4f2f-b2aa-8b94418cf3c9 node DatanodeRegistration(127.0.0.1:38680, datanodeUuid=61b60f2f-795d-44aa-82c3-0ac238b8a077, infoPort=35056, infoSecurePort=0, ipcPort=33077, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:24:20,142 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0xbd0349add375d843: Processing first storage report for DS-b7b6573e-a1ea-4bd1-89c8-d3f7aef4bee9 from datanode 8797193e-f01a-4f9a-9a17-eb37f9f1de79
2020-12-03 07:24:20,143 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0xbd0349add375d843: from storage DS-b7b6573e-a1ea-4bd1-89c8-d3f7aef4bee9 node DatanodeRegistration(127.0.0.1:43902, datanodeUuid=8797193e-f01a-4f9a-9a17-eb37f9f1de79, infoPort=34956, infoSecurePort=0, ipcPort=46163, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:24:20,145 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x95590c900e00adbb,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 31 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:24:20,145 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x16866707ee0a9331,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 32 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:24:20,145 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:20,145 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0xbd0349add375d843,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 10 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:24:20,145 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:20,145 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x1f9726fe12158809,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 1 msec to generate and 21 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:24:20,146 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:20,145 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0xe23cc46f1ab8eed0,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 17 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:24:20,145 [Thread-426] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:20,145 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:20,146 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:20,199 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:34642 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775787_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775787_1001 src: /127.0.0.1:34642 dest: /127.0.0.1:44312
2020-12-03 07:24:20,199 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:34786 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775784_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775784_1001 src: /127.0.0.1:34786 dest: /127.0.0.1:36140
2020-12-03 07:24:20,199 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:43622 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775790_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775790_1001 src: /127.0.0.1:43622 dest: /127.0.0.1:43902
2020-12-03 07:24:20,199 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:49938 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775791_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775791_1001 src: /127.0.0.1:49938 dest: /127.0.0.1:38730
2020-12-03 07:24:20,199 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:58306 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775786_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775786_1001 src: /127.0.0.1:58306 dest: /127.0.0.1:40243
2020-12-03 07:24:20,199 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:44534 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775792_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775792_1001 src: /127.0.0.1:44534 dest: /127.0.0.1:36526
2020-12-03 07:24:20,199 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:36740 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775789_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775789_1001 src: /127.0.0.1:36740 dest: /127.0.0.1:44445
2020-12-03 07:24:20,199 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:44126 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775788_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775788_1001 src: /127.0.0.1:44126 dest: /127.0.0.1:37772
2020-12-03 07:24:20,199 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:48852 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775785_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775785_1001 src: /127.0.0.1:48852 dest: /127.0.0.1:36287
2020-12-03 07:24:20,464 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775789_1001, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:36740, dest: /127.0.0.1:44445, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: ad0684d8-6863-4e19-b2b3-d95e7d2668d9, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775789_1001, duration(ns): 216269068
2020-12-03 07:24:20,466 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775785_1001, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:48852, dest: /127.0.0.1:36287, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: a8821b81-1e27-4139-97ff-1efdadfd6b4f, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775785_1001, duration(ns): 215826719
2020-12-03 07:24:20,466 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775784_1001, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:34786, dest: /127.0.0.1:36140, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 31491a6f-508d-45fd-8589-10a6a9373e21, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775784_1001, duration(ns): 215097813
2020-12-03 07:24:20,465 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775792_1001, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:44534, dest: /127.0.0.1:36526, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 53d1c8e9-e11d-4d1a-9a77-35621ba2c419, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775792_1001, duration(ns): 205073292
2020-12-03 07:24:20,465 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775786_1001, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:58306, dest: /127.0.0.1:40243, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 028e287b-d054-4bef-b640-ae581093b777, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775786_1001, duration(ns): 214810519
2020-12-03 07:24:20,464 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775787_1001, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:34642, dest: /127.0.0.1:44312, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 941777f1-35bc-48df-a860-96f904e7fd34, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775787_1001, duration(ns): 216395401
2020-12-03 07:24:20,464 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775790_1001, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:43622, dest: /127.0.0.1:43902, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 8797193e-f01a-4f9a-9a17-eb37f9f1de79, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775790_1001, duration(ns): 215559395
2020-12-03 07:24:20,466 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775787_1001, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775787_1001, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:20,466 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775786_1001, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775786_1001, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:20,466 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775792_1001, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775792_1001, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:20,466 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775784_1001, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775784_1001, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:20,466 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775789_1001, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775789_1001, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:20,466 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775785_1001, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775785_1001, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:20,466 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775788_1001, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:44126, dest: /127.0.0.1:37772, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: e6a510c0-efbe-44e4-b3ca-915180c69d66, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775788_1001, duration(ns): 214573054
2020-12-03 07:24:20,466 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775791_1001, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:49938, dest: /127.0.0.1:38730, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: e6d8c096-523b-4539-b874-b517e981fab6, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775791_1001, duration(ns): 215289050
2020-12-03 07:24:20,467 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775790_1001, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775790_1001, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:20,467 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775791_1001, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775791_1001, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:20,467 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775788_1001, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775788_1001, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:20,476 [IPC Server handler 9 on default port 38816] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_-9223372036854775776_1002, replicas=127.0.0.1:36287, 127.0.0.1:38680, 127.0.0.1:38730, 127.0.0.1:44445, 127.0.0.1:37772, 127.0.0.1:36526, 127.0.0.1:40243, 127.0.0.1:36140, 127.0.0.1:41069 for /striped/stripedFileChecksum1
2020-12-03 07:24:20,482 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:20,487 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:48862 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775776_1002]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775776_1002 src: /127.0.0.1:48862 dest: /127.0.0.1:36287
2020-12-03 07:24:20,489 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:20,498 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:49836 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775775_1002]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775775_1002 src: /127.0.0.1:49836 dest: /127.0.0.1:38680
2020-12-03 07:24:20,499 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:20,503 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:49988 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775774_1002]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775774_1002 src: /127.0.0.1:49988 dest: /127.0.0.1:38730
2020-12-03 07:24:20,504 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:20,509 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:36788 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775773_1002]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775773_1002 src: /127.0.0.1:36788 dest: /127.0.0.1:44445
2020-12-03 07:24:20,513 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:20,515 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:44176 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775772_1002]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775772_1002 src: /127.0.0.1:44176 dest: /127.0.0.1:37772
2020-12-03 07:24:20,516 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:20,521 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:44596 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775771_1002]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775771_1002 src: /127.0.0.1:44596 dest: /127.0.0.1:36526
2020-12-03 07:24:20,525 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:20,527 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:58356 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775770_1002]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775770_1002 src: /127.0.0.1:58356 dest: /127.0.0.1:40243
2020-12-03 07:24:20,531 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:20,533 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:34824 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775769_1002]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775769_1002 src: /127.0.0.1:34824 dest: /127.0.0.1:36140
2020-12-03 07:24:20,538 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:20,541 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:35670 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775768_1002]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775768_1002 src: /127.0.0.1:35670 dest: /127.0.0.1:41069
2020-12-03 07:24:20,680 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775769_1002, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:34824, dest: /127.0.0.1:36140, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 31491a6f-508d-45fd-8589-10a6a9373e21, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775769_1002, duration(ns): 141425835
2020-12-03 07:24:20,680 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775770_1002, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:58356, dest: /127.0.0.1:40243, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 028e287b-d054-4bef-b640-ae581093b777, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775770_1002, duration(ns): 148029711
2020-12-03 07:24:20,682 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775769_1002, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775769_1002, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:20,682 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775770_1002, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775770_1002, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:20,680 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775768_1002, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:35670, dest: /127.0.0.1:41069, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 31eff9f5-fd87-487e-8b1a-43cc24be4844, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775768_1002, duration(ns): 134452652
2020-12-03 07:24:20,680 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775771_1002, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:44596, dest: /127.0.0.1:36526, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 53d1c8e9-e11d-4d1a-9a77-35621ba2c419, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775771_1002, duration(ns): 154052069
2020-12-03 07:24:20,683 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775771_1002, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775771_1002, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:20,684 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775768_1002, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775768_1002, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:20,680 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775772_1002, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:44176, dest: /127.0.0.1:37772, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: e6a510c0-efbe-44e4-b3ca-915180c69d66, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775772_1002, duration(ns): 158782235
2020-12-03 07:24:20,680 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775776_1002, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:48862, dest: /127.0.0.1:36287, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: a8821b81-1e27-4139-97ff-1efdadfd6b4f, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775776_1002, duration(ns): 186872836
2020-12-03 07:24:20,680 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775773_1002, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:36788, dest: /127.0.0.1:44445, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: ad0684d8-6863-4e19-b2b3-d95e7d2668d9, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775773_1002, duration(ns): 165965200
2020-12-03 07:24:20,680 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775774_1002, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:49988, dest: /127.0.0.1:38730, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: e6d8c096-523b-4539-b874-b517e981fab6, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775774_1002, duration(ns): 171390310
2020-12-03 07:24:20,687 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775772_1002, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775772_1002, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:20,682 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775775_1002, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:49836, dest: /127.0.0.1:38680, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 61b60f2f-795d-44aa-82c3-0ac238b8a077, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775775_1002, duration(ns): 179403791
2020-12-03 07:24:20,687 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775774_1002, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775774_1002, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:20,687 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775775_1002, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775775_1002, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:20,687 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775773_1002, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775773_1002, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:20,690 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775776_1002, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775776_1002, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:20,695 [IPC Server handler 2 on default port 38816] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_-9223372036854775760_1003, replicas=127.0.0.1:36526, 127.0.0.1:37772, 127.0.0.1:41069, 127.0.0.1:38680, 127.0.0.1:44445, 127.0.0.1:36287, 127.0.0.1:36140, 127.0.0.1:43902, 127.0.0.1:38730 for /striped/stripedFileChecksum1
2020-12-03 07:24:20,700 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:20,704 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:20,704 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:44614 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775760_1003]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775760_1003 src: /127.0.0.1:44614 dest: /127.0.0.1:36526
2020-12-03 07:24:20,705 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:44200 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775759_1003]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775759_1003 src: /127.0.0.1:44200 dest: /127.0.0.1:37772
2020-12-03 07:24:20,707 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:20,717 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:20,717 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:35680 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775758_1003]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775758_1003 src: /127.0.0.1:35680 dest: /127.0.0.1:41069
2020-12-03 07:24:20,718 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:49874 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775757_1003]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775757_1003 src: /127.0.0.1:49874 dest: /127.0.0.1:38680
2020-12-03 07:24:20,718 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:20,722 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:36824 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775756_1003]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775756_1003 src: /127.0.0.1:36824 dest: /127.0.0.1:44445
2020-12-03 07:24:20,724 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:20,726 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:48920 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775755_1003]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775755_1003 src: /127.0.0.1:48920 dest: /127.0.0.1:36287
2020-12-03 07:24:20,730 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:20,732 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:34852 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775754_1003]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775754_1003 src: /127.0.0.1:34852 dest: /127.0.0.1:36140
2020-12-03 07:24:20,738 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:20,740 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:43720 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775753_1003]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775753_1003 src: /127.0.0.1:43720 dest: /127.0.0.1:43902
2020-12-03 07:24:20,740 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:20,743 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:50040 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775752_1003]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775752_1003 src: /127.0.0.1:50040 dest: /127.0.0.1:38730
2020-12-03 07:24:20,890 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775759_1003, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:44200, dest: /127.0.0.1:37772, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: e6a510c0-efbe-44e4-b3ca-915180c69d66, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775759_1003, duration(ns): 159145716
2020-12-03 07:24:20,890 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775759_1003, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775759_1003, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:20,891 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775757_1003, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:49874, dest: /127.0.0.1:38680, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 61b60f2f-795d-44aa-82c3-0ac238b8a077, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775757_1003, duration(ns): 151631179
2020-12-03 07:24:20,891 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775752_1003, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:50040, dest: /127.0.0.1:38730, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: e6d8c096-523b-4539-b874-b517e981fab6, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775752_1003, duration(ns): 146259779
2020-12-03 07:24:20,891 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775757_1003, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775757_1003, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:20,891 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775752_1003, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775752_1003, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:20,891 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775755_1003, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:48920, dest: /127.0.0.1:36287, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: a8821b81-1e27-4139-97ff-1efdadfd6b4f, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775755_1003, duration(ns): 144824561
2020-12-03 07:24:20,900 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775755_1003, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775755_1003, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:20,902 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775756_1003, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:36824, dest: /127.0.0.1:44445, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: ad0684d8-6863-4e19-b2b3-d95e7d2668d9, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775756_1003, duration(ns): 165153502
2020-12-03 07:24:20,902 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775756_1003, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775756_1003, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:20,902 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775754_1003, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:34852, dest: /127.0.0.1:36140, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 31491a6f-508d-45fd-8589-10a6a9373e21, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775754_1003, duration(ns): 158325823
2020-12-03 07:24:20,903 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775754_1003, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775754_1003, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:20,903 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775758_1003, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:35680, dest: /127.0.0.1:41069, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 31eff9f5-fd87-487e-8b1a-43cc24be4844, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775758_1003, duration(ns): 148968617
2020-12-03 07:24:20,903 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775758_1003, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775758_1003, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:20,905 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775760_1003, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:44614, dest: /127.0.0.1:36526, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 53d1c8e9-e11d-4d1a-9a77-35621ba2c419, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775760_1003, duration(ns): 183763981
2020-12-03 07:24:20,905 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775760_1003, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775760_1003, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:20,906 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775753_1003, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:43720, dest: /127.0.0.1:43902, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 8797193e-f01a-4f9a-9a17-eb37f9f1de79, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775753_1003, duration(ns): 149928981
2020-12-03 07:24:20,906 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775753_1003, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775753_1003, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:20,914 [IPC Server handler 1 on default port 38816] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_-9223372036854775744_1004, replicas=127.0.0.1:43902, 127.0.0.1:44445, 127.0.0.1:41069, 127.0.0.1:37772, 127.0.0.1:40243, 127.0.0.1:36526, 127.0.0.1:36287, 127.0.0.1:38730, 127.0.0.1:36140 for /striped/stripedFileChecksum1
2020-12-03 07:24:20,918 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:20,919 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:20,920 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:43734 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775744_1004]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775744_1004 src: /127.0.0.1:43734 dest: /127.0.0.1:43902
2020-12-03 07:24:20,920 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:36854 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775743_1004]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775743_1004 src: /127.0.0.1:36854 dest: /127.0.0.1:44445
2020-12-03 07:24:20,922 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:20,936 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:20,939 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:44244 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775741_1004]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775741_1004 src: /127.0.0.1:44244 dest: /127.0.0.1:37772
2020-12-03 07:24:20,939 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:20,939 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:35720 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775742_1004]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775742_1004 src: /127.0.0.1:35720 dest: /127.0.0.1:41069
2020-12-03 07:24:20,940 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:58426 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775740_1004]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775740_1004 src: /127.0.0.1:58426 dest: /127.0.0.1:40243
2020-12-03 07:24:20,942 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:20,954 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:44676 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775739_1004]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775739_1004 src: /127.0.0.1:44676 dest: /127.0.0.1:36526
2020-12-03 07:24:20,969 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:20,970 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:48970 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775738_1004]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775738_1004 src: /127.0.0.1:48970 dest: /127.0.0.1:36287
2020-12-03 07:24:20,981 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:20,985 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:20,990 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:34904 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775736_1004]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775736_1004 src: /127.0.0.1:34904 dest: /127.0.0.1:36140
2020-12-03 07:24:20,990 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:50084 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775737_1004]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775737_1004 src: /127.0.0.1:50084 dest: /127.0.0.1:38730
2020-12-03 07:24:21,174 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775740_1004, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:58426, dest: /127.0.0.1:40243, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 028e287b-d054-4bef-b640-ae581093b777, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775740_1004, duration(ns): 229250086
2020-12-03 07:24:21,176 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775740_1004, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775740_1004, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:21,174 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775742_1004, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:35720, dest: /127.0.0.1:41069, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 31eff9f5-fd87-487e-8b1a-43cc24be4844, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775742_1004, duration(ns): 209716059
2020-12-03 07:24:21,176 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775742_1004, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775742_1004, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:21,175 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775744_1004, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:43734, dest: /127.0.0.1:43902, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 8797193e-f01a-4f9a-9a17-eb37f9f1de79, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775744_1004, duration(ns): 230589506
2020-12-03 07:24:21,177 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775744_1004, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775744_1004, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:21,175 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775739_1004, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:44676, dest: /127.0.0.1:36526, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 53d1c8e9-e11d-4d1a-9a77-35621ba2c419, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775739_1004, duration(ns): 216556224
2020-12-03 07:24:21,175 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775741_1004, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:44244, dest: /127.0.0.1:37772, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: e6a510c0-efbe-44e4-b3ca-915180c69d66, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775741_1004, duration(ns): 211632026
2020-12-03 07:24:21,177 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775739_1004, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775739_1004, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:21,177 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775741_1004, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775741_1004, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:21,177 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775737_1004, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:50084, dest: /127.0.0.1:38730, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: e6d8c096-523b-4539-b874-b517e981fab6, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775737_1004, duration(ns): 157984428
2020-12-03 07:24:21,178 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775737_1004, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775737_1004, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:21,178 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775738_1004, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:48970, dest: /127.0.0.1:36287, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: a8821b81-1e27-4139-97ff-1efdadfd6b4f, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775738_1004, duration(ns): 176200405
2020-12-03 07:24:21,180 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775738_1004, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775738_1004, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:21,180 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775743_1004, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:36854, dest: /127.0.0.1:44445, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: ad0684d8-6863-4e19-b2b3-d95e7d2668d9, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775743_1004, duration(ns): 250944936
2020-12-03 07:24:21,181 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775743_1004, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775743_1004, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:21,181 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775736_1004, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:34904, dest: /127.0.0.1:36140, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 31491a6f-508d-45fd-8589-10a6a9373e21, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775736_1004, duration(ns): 161109270
2020-12-03 07:24:21,181 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775736_1004, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775736_1004, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:21,188 [IPC Server handler 0 on default port 38816] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_-9223372036854775728_1005, replicas=127.0.0.1:36287, 127.0.0.1:37772, 127.0.0.1:41069, 127.0.0.1:44312, 127.0.0.1:44445, 127.0.0.1:36140, 127.0.0.1:40243, 127.0.0.1:38730, 127.0.0.1:43902 for /striped/stripedFileChecksum1
2020-12-03 07:24:21,195 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:21,198 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:21,200 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:21,203 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:48982 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005 src: /127.0.0.1:48982 dest: /127.0.0.1:36287
2020-12-03 07:24:21,203 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:21,204 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:21,205 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:35758 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005 src: /127.0.0.1:35758 dest: /127.0.0.1:41069
2020-12-03 07:24:21,209 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:21,214 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:36900 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005 src: /127.0.0.1:36900 dest: /127.0.0.1:44445
2020-12-03 07:24:21,214 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:34922 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775723_1005]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775723_1005 src: /127.0.0.1:34922 dest: /127.0.0.1:36140
2020-12-03 07:24:21,214 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:34796 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005 src: /127.0.0.1:34796 dest: /127.0.0.1:44312
2020-12-03 07:24:21,216 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:44278 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005 src: /127.0.0.1:44278 dest: /127.0.0.1:37772
2020-12-03 07:24:21,228 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:21,243 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:21,244 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:21,245 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:43800 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775720_1005]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775720_1005 src: /127.0.0.1:43800 dest: /127.0.0.1:43902
2020-12-03 07:24:21,245 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:58460 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005 src: /127.0.0.1:58460 dest: /127.0.0.1:40243
2020-12-03 07:24:21,250 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:50116 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775721_1005]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775721_1005 src: /127.0.0.1:50116 dest: /127.0.0.1:38730
2020-12-03 07:24:21,370 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775721_1005, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:50116, dest: /127.0.0.1:38730, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: e6d8c096-523b-4539-b874-b517e981fab6, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775721_1005, duration(ns): 114834301
2020-12-03 07:24:21,371 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:35758, dest: /127.0.0.1:41069, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 31eff9f5-fd87-487e-8b1a-43cc24be4844, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005, duration(ns): 151755061
2020-12-03 07:24:21,378 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775721_1005, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775721_1005, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:21,370 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:36900, dest: /127.0.0.1:44445, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: ad0684d8-6863-4e19-b2b3-d95e7d2668d9, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005, duration(ns): 139491345
2020-12-03 07:24:21,378 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:21,378 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:21,379 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:44278, dest: /127.0.0.1:37772, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: e6a510c0-efbe-44e4-b3ca-915180c69d66, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005, duration(ns): 129386282
2020-12-03 07:24:21,379 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:21,381 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:58460, dest: /127.0.0.1:40243, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 028e287b-d054-4bef-b640-ae581093b777, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005, duration(ns): 121406708
2020-12-03 07:24:21,381 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:21,381 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775720_1005, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:43800, dest: /127.0.0.1:43902, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 8797193e-f01a-4f9a-9a17-eb37f9f1de79, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775720_1005, duration(ns): 119789395
2020-12-03 07:24:21,381 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:48982, dest: /127.0.0.1:36287, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: a8821b81-1e27-4139-97ff-1efdadfd6b4f, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005, duration(ns): 162103295
2020-12-03 07:24:21,382 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:21,382 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775720_1005, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775720_1005, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:21,382 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:34796, dest: /127.0.0.1:44312, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 941777f1-35bc-48df-a860-96f904e7fd34, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005, duration(ns): 155271374
2020-12-03 07:24:21,382 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:21,382 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775723_1005, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:34922, dest: /127.0.0.1:36140, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 31491a6f-508d-45fd-8589-10a6a9373e21, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775723_1005, duration(ns): 152893643
2020-12-03 07:24:21,382 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775723_1005, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775723_1005, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:21,388 [IPC Server handler 0 on default port 38816] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_-9223372036854775712_1006, replicas=127.0.0.1:38730, 127.0.0.1:43902, 127.0.0.1:37772, 127.0.0.1:41069, 127.0.0.1:40243, 127.0.0.1:44445, 127.0.0.1:44312, 127.0.0.1:38680, 127.0.0.1:36140 for /striped/stripedFileChecksum1
2020-12-03 07:24:21,393 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:21,394 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:50126 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775712_1006]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775712_1006 src: /127.0.0.1:50126 dest: /127.0.0.1:38730
2020-12-03 07:24:21,394 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:21,395 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:43810 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775711_1006]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775711_1006 src: /127.0.0.1:43810 dest: /127.0.0.1:43902
2020-12-03 07:24:21,396 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:21,397 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:44313 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775710_1006]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775710_1006 src: /127.0.0.1:44313 dest: /127.0.0.1:37772
2020-12-03 07:24:21,398 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:21,399 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:35792 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775709_1006]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775709_1006 src: /127.0.0.1:35792 dest: /127.0.0.1:41069
2020-12-03 07:24:21,400 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:21,402 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:21,402 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:58486 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775708_1006]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775708_1006 src: /127.0.0.1:58486 dest: /127.0.0.1:40243
2020-12-03 07:24:21,403 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:36934 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775707_1006]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775707_1006 src: /127.0.0.1:36934 dest: /127.0.0.1:44445
2020-12-03 07:24:21,410 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:21,416 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:21,417 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:21,418 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:34834 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775706_1006]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775706_1006 src: /127.0.0.1:34834 dest: /127.0.0.1:44312
2020-12-03 07:24:21,418 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:34962 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775704_1006]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775704_1006 src: /127.0.0.1:34962 dest: /127.0.0.1:36140
2020-12-03 07:24:21,418 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:49992 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775705_1006]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775705_1006 src: /127.0.0.1:49992 dest: /127.0.0.1:38680
2020-12-03 07:24:21,553 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775704_1006, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:34962, dest: /127.0.0.1:36140, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 31491a6f-508d-45fd-8589-10a6a9373e21, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775704_1006, duration(ns): 130840244
2020-12-03 07:24:21,553 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775704_1006, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775704_1006, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:21,566 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775712_1006, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:50126, dest: /127.0.0.1:38730, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: e6d8c096-523b-4539-b874-b517e981fab6, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775712_1006, duration(ns): 155062247
2020-12-03 07:24:21,566 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775712_1006, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775712_1006, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:21,566 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775709_1006, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:35792, dest: /127.0.0.1:41069, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 31eff9f5-fd87-487e-8b1a-43cc24be4844, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775709_1006, duration(ns): 149768747
2020-12-03 07:24:21,567 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775709_1006, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775709_1006, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:21,566 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775710_1006, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:44313, dest: /127.0.0.1:37772, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: e6a510c0-efbe-44e4-b3ca-915180c69d66, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775710_1006, duration(ns): 152399127
2020-12-03 07:24:21,567 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775710_1006, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775710_1006, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:21,567 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775707_1006, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:36934, dest: /127.0.0.1:44445, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: ad0684d8-6863-4e19-b2b3-d95e7d2668d9, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775707_1006, duration(ns): 146037885
2020-12-03 07:24:21,567 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775706_1006, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:34834, dest: /127.0.0.1:44312, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 941777f1-35bc-48df-a860-96f904e7fd34, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775706_1006, duration(ns): 131363769
2020-12-03 07:24:21,566 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775708_1006, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:58486, dest: /127.0.0.1:40243, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 028e287b-d054-4bef-b640-ae581093b777, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775708_1006, duration(ns): 147067981
2020-12-03 07:24:21,568 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775706_1006, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775706_1006, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:21,567 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775707_1006, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775707_1006, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:21,567 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775705_1006, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:49992, dest: /127.0.0.1:38680, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 61b60f2f-795d-44aa-82c3-0ac238b8a077, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775705_1006, duration(ns): 131127006
2020-12-03 07:24:21,567 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775711_1006, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:43810, dest: /127.0.0.1:43902, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 8797193e-f01a-4f9a-9a17-eb37f9f1de79, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775711_1006, duration(ns): 154118541
2020-12-03 07:24:21,568 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775705_1006, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775705_1006, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:21,568 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775711_1006, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775711_1006, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:21,568 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775708_1006, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775708_1006, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:21,572 [IPC Server handler 0 on default port 38816] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_-9223372036854775696_1007, replicas=127.0.0.1:41069, 127.0.0.1:38680, 127.0.0.1:44445, 127.0.0.1:36140, 127.0.0.1:36287, 127.0.0.1:43902, 127.0.0.1:37772, 127.0.0.1:38730, 127.0.0.1:36526 for /striped/stripedFileChecksum1
2020-12-03 07:24:21,576 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:21,577 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:21,577 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:35822 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775696_1007]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775696_1007 src: /127.0.0.1:35822 dest: /127.0.0.1:41069
2020-12-03 07:24:21,578 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:50016 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775695_1007]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775695_1007 src: /127.0.0.1:50016 dest: /127.0.0.1:38680
2020-12-03 07:24:21,580 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:21,581 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:36964 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775694_1007]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775694_1007 src: /127.0.0.1:36964 dest: /127.0.0.1:44445
2020-12-03 07:24:21,582 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:21,583 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:34986 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775693_1007]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775693_1007 src: /127.0.0.1:34986 dest: /127.0.0.1:36140
2020-12-03 07:24:21,584 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:21,585 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:49058 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775692_1007]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775692_1007 src: /127.0.0.1:49058 dest: /127.0.0.1:36287
2020-12-03 07:24:21,586 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:21,587 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:43854 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775691_1007]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775691_1007 src: /127.0.0.1:43854 dest: /127.0.0.1:43902
2020-12-03 07:24:21,594 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:21,595 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:44358 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775690_1007]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775690_1007 src: /127.0.0.1:44358 dest: /127.0.0.1:37772
2020-12-03 07:24:21,597 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:21,598 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:50182 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775689_1007]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775689_1007 src: /127.0.0.1:50182 dest: /127.0.0.1:38730
2020-12-03 07:24:21,600 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:21,602 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:44786 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775688_1007]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775688_1007 src: /127.0.0.1:44786 dest: /127.0.0.1:36526
2020-12-03 07:24:21,977 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775694_1007, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:36964, dest: /127.0.0.1:44445, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: ad0684d8-6863-4e19-b2b3-d95e7d2668d9, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775694_1007, duration(ns): 394299599
2020-12-03 07:24:21,977 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775696_1007, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:35822, dest: /127.0.0.1:41069, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 31eff9f5-fd87-487e-8b1a-43cc24be4844, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775696_1007, duration(ns): 397720279
2020-12-03 07:24:21,977 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775694_1007, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775694_1007, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:21,977 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775696_1007, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775696_1007, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:21,977 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775690_1007, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:44358, dest: /127.0.0.1:37772, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: e6a510c0-efbe-44e4-b3ca-915180c69d66, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775690_1007, duration(ns): 378646592
2020-12-03 07:24:21,977 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775691_1007, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:43854, dest: /127.0.0.1:43902, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 8797193e-f01a-4f9a-9a17-eb37f9f1de79, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775691_1007, duration(ns): 388012782
2020-12-03 07:24:21,977 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775688_1007, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:44786, dest: /127.0.0.1:36526, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 53d1c8e9-e11d-4d1a-9a77-35621ba2c419, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775688_1007, duration(ns): 373258513
2020-12-03 07:24:21,978 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775691_1007, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775691_1007, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:21,978 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775688_1007, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775688_1007, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:21,977 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775692_1007, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:49058, dest: /127.0.0.1:36287, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: a8821b81-1e27-4139-97ff-1efdadfd6b4f, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775692_1007, duration(ns): 390035897
2020-12-03 07:24:21,978 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775689_1007, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:50182, dest: /127.0.0.1:38730, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: e6d8c096-523b-4539-b874-b517e981fab6, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775689_1007, duration(ns): 376690958
2020-12-03 07:24:21,978 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775692_1007, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775692_1007, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:21,978 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775689_1007, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775689_1007, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:21,977 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775693_1007, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:34986, dest: /127.0.0.1:36140, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 31491a6f-508d-45fd-8589-10a6a9373e21, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775693_1007, duration(ns): 391730895
2020-12-03 07:24:21,978 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775690_1007, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775690_1007, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:21,979 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775693_1007, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775693_1007, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:21,977 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775695_1007, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:50016, dest: /127.0.0.1:38680, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 61b60f2f-795d-44aa-82c3-0ac238b8a077, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775695_1007, duration(ns): 398267624
2020-12-03 07:24:21,979 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775695_1007, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775695_1007, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:21,982 [IPC Server handler 9 on default port 38816] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_-9223372036854775680_1008, replicas=127.0.0.1:36140, 127.0.0.1:36287, 127.0.0.1:41069, 127.0.0.1:37772, 127.0.0.1:43902, 127.0.0.1:38680, 127.0.0.1:38730, 127.0.0.1:40243, 127.0.0.1:44312 for /striped/stripedFileChecksum1
2020-12-03 07:24:21,988 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:21,989 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:21,989 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:35030 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775680_1008]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775680_1008 src: /127.0.0.1:35030 dest: /127.0.0.1:36140
2020-12-03 07:24:21,990 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:49104 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775679_1008]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775679_1008 src: /127.0.0.1:49104 dest: /127.0.0.1:36287
2020-12-03 07:24:21,991 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:21,991 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:35878 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775678_1008]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775678_1008 src: /127.0.0.1:35878 dest: /127.0.0.1:41069
2020-12-03 07:24:21,992 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:21,994 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:21,994 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:44402 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775677_1008]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775677_1008 src: /127.0.0.1:44402 dest: /127.0.0.1:37772
2020-12-03 07:24:21,995 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:43906 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775676_1008]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775676_1008 src: /127.0.0.1:43906 dest: /127.0.0.1:43902
2020-12-03 07:24:21,996 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:21,996 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:50078 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775675_1008]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775675_1008 src: /127.0.0.1:50078 dest: /127.0.0.1:38680
2020-12-03 07:24:22,002 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:22,003 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:50228 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775674_1008]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775674_1008 src: /127.0.0.1:50228 dest: /127.0.0.1:38730
2020-12-03 07:24:22,005 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:22,006 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:58582 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775673_1008]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775673_1008 src: /127.0.0.1:58582 dest: /127.0.0.1:40243
2020-12-03 07:24:22,008 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:22,009 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:34928 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775672_1008]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775672_1008 src: /127.0.0.1:34928 dest: /127.0.0.1:44312
2020-12-03 07:24:22,157 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775679_1008, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:49104, dest: /127.0.0.1:36287, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: a8821b81-1e27-4139-97ff-1efdadfd6b4f, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775679_1008, duration(ns): 165211914
2020-12-03 07:24:22,157 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775672_1008, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:34928, dest: /127.0.0.1:44312, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 941777f1-35bc-48df-a860-96f904e7fd34, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775672_1008, duration(ns): 143039246
2020-12-03 07:24:22,157 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775679_1008, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775679_1008, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:22,158 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775672_1008, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775672_1008, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:22,157 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775673_1008, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:58582, dest: /127.0.0.1:40243, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 028e287b-d054-4bef-b640-ae581093b777, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775673_1008, duration(ns): 149343203
2020-12-03 07:24:22,157 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775677_1008, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:44402, dest: /127.0.0.1:37772, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: e6a510c0-efbe-44e4-b3ca-915180c69d66, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775677_1008, duration(ns): 158051653
2020-12-03 07:24:22,157 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775676_1008, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:43906, dest: /127.0.0.1:43902, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 8797193e-f01a-4f9a-9a17-eb37f9f1de79, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775676_1008, duration(ns): 160752417
2020-12-03 07:24:22,158 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775677_1008, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775677_1008, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:22,157 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775680_1008, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:35030, dest: /127.0.0.1:36140, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 31491a6f-508d-45fd-8589-10a6a9373e21, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775680_1008, duration(ns): 165509076
2020-12-03 07:24:22,157 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775678_1008, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:35878, dest: /127.0.0.1:41069, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 31eff9f5-fd87-487e-8b1a-43cc24be4844, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775678_1008, duration(ns): 164003625
2020-12-03 07:24:22,157 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775674_1008, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:50228, dest: /127.0.0.1:38730, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: e6d8c096-523b-4539-b874-b517e981fab6, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775674_1008, duration(ns): 151958765
2020-12-03 07:24:22,158 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775678_1008, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775678_1008, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:22,157 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775675_1008, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:50078, dest: /127.0.0.1:38680, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 61b60f2f-795d-44aa-82c3-0ac238b8a077, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775675_1008, duration(ns): 159069055
2020-12-03 07:24:22,158 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775674_1008, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775674_1008, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:22,159 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775675_1008, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775675_1008, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:22,158 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775680_1008, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775680_1008, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:22,158 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775676_1008, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775676_1008, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:22,158 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775673_1008, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775673_1008, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:22,167 [IPC Server handler 9 on default port 38816] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_-9223372036854775664_1009, replicas=127.0.0.1:41069, 127.0.0.1:44312, 127.0.0.1:36140, 127.0.0.1:38680, 127.0.0.1:37772, 127.0.0.1:44445, 127.0.0.1:40243, 127.0.0.1:36287, 127.0.0.1:36526 for /striped/stripedFileChecksum1
2020-12-03 07:24:22,171 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:22,172 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:35894 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775664_1009]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775664_1009 src: /127.0.0.1:35894 dest: /127.0.0.1:41069
2020-12-03 07:24:22,172 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:22,173 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:34932 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775663_1009]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775663_1009 src: /127.0.0.1:34932 dest: /127.0.0.1:44312
2020-12-03 07:24:22,174 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:22,175 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:35058 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775662_1009]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775662_1009 src: /127.0.0.1:35058 dest: /127.0.0.1:36140
2020-12-03 07:24:22,176 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:22,177 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:50096 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775661_1009]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775661_1009 src: /127.0.0.1:50096 dest: /127.0.0.1:38680
2020-12-03 07:24:22,178 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:22,179 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:44430 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775660_1009]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775660_1009 src: /127.0.0.1:44430 dest: /127.0.0.1:37772
2020-12-03 07:24:22,179 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:22,180 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:37050 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775659_1009]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775659_1009 src: /127.0.0.1:37050 dest: /127.0.0.1:44445
2020-12-03 07:24:22,186 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:22,187 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:58610 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775658_1009]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775658_1009 src: /127.0.0.1:58610 dest: /127.0.0.1:40243
2020-12-03 07:24:22,188 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:22,190 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:49148 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775657_1009]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775657_1009 src: /127.0.0.1:49148 dest: /127.0.0.1:36287
2020-12-03 07:24:22,191 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:22,192 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:44862 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775656_1009]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775656_1009 src: /127.0.0.1:44862 dest: /127.0.0.1:36526
2020-12-03 07:24:22,317 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775657_1009, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:49148, dest: /127.0.0.1:36287, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: a8821b81-1e27-4139-97ff-1efdadfd6b4f, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775657_1009, duration(ns): 125624895
2020-12-03 07:24:22,317 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775660_1009, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:44430, dest: /127.0.0.1:37772, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: e6a510c0-efbe-44e4-b3ca-915180c69d66, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775660_1009, duration(ns): 136110369
2020-12-03 07:24:22,317 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775660_1009, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775660_1009, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:22,317 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775658_1009, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:58610, dest: /127.0.0.1:40243, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 028e287b-d054-4bef-b640-ae581093b777, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775658_1009, duration(ns): 127321615
2020-12-03 07:24:22,317 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775656_1009, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:44862, dest: /127.0.0.1:36526, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 53d1c8e9-e11d-4d1a-9a77-35621ba2c419, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775656_1009, duration(ns): 123222448
2020-12-03 07:24:22,317 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775659_1009, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:37050, dest: /127.0.0.1:44445, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: ad0684d8-6863-4e19-b2b3-d95e7d2668d9, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775659_1009, duration(ns): 134725870
2020-12-03 07:24:22,318 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775656_1009, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775656_1009, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:22,318 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775662_1009, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:35058, dest: /127.0.0.1:36140, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 31491a6f-508d-45fd-8589-10a6a9373e21, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775662_1009, duration(ns): 139741831
2020-12-03 07:24:22,318 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775659_1009, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775659_1009, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:22,318 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775662_1009, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775662_1009, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:22,318 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775658_1009, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775658_1009, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:22,317 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775661_1009, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:50096, dest: /127.0.0.1:38680, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 61b60f2f-795d-44aa-82c3-0ac238b8a077, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775661_1009, duration(ns): 137719544
2020-12-03 07:24:22,317 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775664_1009, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:35894, dest: /127.0.0.1:41069, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 31eff9f5-fd87-487e-8b1a-43cc24be4844, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775664_1009, duration(ns): 143337313
2020-12-03 07:24:22,319 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775661_1009, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775661_1009, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:22,317 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775663_1009, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:34932, dest: /127.0.0.1:44312, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 941777f1-35bc-48df-a860-96f904e7fd34, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775663_1009, duration(ns): 140041035
2020-12-03 07:24:22,317 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775657_1009, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775657_1009, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:22,319 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775663_1009, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775663_1009, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:22,319 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775664_1009, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775664_1009, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:22,322 [IPC Server handler 9 on default port 38816] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_-9223372036854775648_1010, replicas=127.0.0.1:38730, 127.0.0.1:36526, 127.0.0.1:44445, 127.0.0.1:38680, 127.0.0.1:40243, 127.0.0.1:36140, 127.0.0.1:37772, 127.0.0.1:44312, 127.0.0.1:36287 for /striped/stripedFileChecksum1
2020-12-03 07:24:22,326 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:22,327 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:50278 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775648_1010]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775648_1010 src: /127.0.0.1:50278 dest: /127.0.0.1:38730
2020-12-03 07:24:22,328 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:22,329 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:22,330 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:44878 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775647_1010]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775647_1010 src: /127.0.0.1:44878 dest: /127.0.0.1:36526
2020-12-03 07:24:22,331 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:22,332 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:37080 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775646_1010]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775646_1010 src: /127.0.0.1:37080 dest: /127.0.0.1:44445
2020-12-03 07:24:22,332 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:50136 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775645_1010]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775645_1010 src: /127.0.0.1:50136 dest: /127.0.0.1:38680
2020-12-03 07:24:22,333 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:22,335 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:22,337 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:35106 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775643_1010]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775643_1010 src: /127.0.0.1:35106 dest: /127.0.0.1:36140
2020-12-03 07:24:22,337 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:58638 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775644_1010]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775644_1010 src: /127.0.0.1:58638 dest: /127.0.0.1:40243
2020-12-03 07:24:22,343 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:22,345 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:44472 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775642_1010]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775642_1010 src: /127.0.0.1:44472 dest: /127.0.0.1:37772
2020-12-03 07:24:22,346 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:22,347 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:34990 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775641_1010]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775641_1010 src: /127.0.0.1:34990 dest: /127.0.0.1:44312
2020-12-03 07:24:22,348 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:22,350 [DataXceiver for client DFSClient_NONMAPREDUCE_41495814_1 at /127.0.0.1:49186 [Receiving block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775640_1010]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775640_1010 src: /127.0.0.1:49186 dest: /127.0.0.1:36287
2020-12-03 07:24:22,481 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775647_1010, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:44878, dest: /127.0.0.1:36526, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 53d1c8e9-e11d-4d1a-9a77-35621ba2c419, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775647_1010, duration(ns): 149001250
2020-12-03 07:24:22,481 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775643_1010, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:35106, dest: /127.0.0.1:36140, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 31491a6f-508d-45fd-8589-10a6a9373e21, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775643_1010, duration(ns): 141283036
2020-12-03 07:24:22,481 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775648_1010, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:50278, dest: /127.0.0.1:38730, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: e6d8c096-523b-4539-b874-b517e981fab6, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775648_1010, duration(ns): 151976587
2020-12-03 07:24:22,481 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775646_1010, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:37080, dest: /127.0.0.1:44445, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: ad0684d8-6863-4e19-b2b3-d95e7d2668d9, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775646_1010, duration(ns): 145148705
2020-12-03 07:24:22,481 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775644_1010, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:58638, dest: /127.0.0.1:40243, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 028e287b-d054-4bef-b640-ae581093b777, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775644_1010, duration(ns): 142455978
2020-12-03 07:24:22,481 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775641_1010, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:34990, dest: /127.0.0.1:44312, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 941777f1-35bc-48df-a860-96f904e7fd34, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775641_1010, duration(ns): 132217381
2020-12-03 07:24:22,481 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775645_1010, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:50136, dest: /127.0.0.1:38680, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: 61b60f2f-795d-44aa-82c3-0ac238b8a077, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775645_1010, duration(ns): 145124366
2020-12-03 07:24:22,481 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775642_1010, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:44472, dest: /127.0.0.1:37772, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: e6a510c0-efbe-44e4-b3ca-915180c69d66, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775642_1010, duration(ns): 134257590
2020-12-03 07:24:22,482 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775645_1010, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775645_1010, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:22,482 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775641_1010, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775641_1010, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:22,482 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775640_1010, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:49186, dest: /127.0.0.1:36287, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_41495814_1, offset: 0, srvID: a8821b81-1e27-4139-97ff-1efdadfd6b4f, blockid: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775640_1010, duration(ns): 128987121
2020-12-03 07:24:22,482 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775644_1010, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775644_1010, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:22,482 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775646_1010, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775646_1010, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:22,481 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775648_1010, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775648_1010, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:22,481 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775643_1010, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775643_1010, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:22,481 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775647_1010, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775647_1010, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:22,482 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775640_1010, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775640_1010, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:22,482 [PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775642_1010, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775642_1010, type=LAST_IN_PIPELINE terminating
2020-12-03 07:24:22,488 [IPC Server handler 9 on default port 38816] INFO  hdfs.StateChange (FSNamesystem.java:completeFile(2948)) - DIR* completeFile: /striped/stripedFileChecksum1 is closed by DFSClient_NONMAPREDUCE_41495814_1
2020-12-03 07:24:22,505 [IPC Server handler 5 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/striped/stripedFileChecksum1	dst=null	perm=null	proto=rpc
2020-12-03 07:24:22,529 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:36526,DS-44a63aad-efff-4da7-8035-f12475e12191,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36526,DS-44a63aad-efff-4da7-8035-f12475e12191,DISK], DatanodeInfoWithStorage[127.0.0.1:38730,DS-f5960db6-7b54-4094-bc6c-6bca60faedad,DISK], DatanodeInfoWithStorage[127.0.0.1:43902,DS-b7b6573e-a1ea-4bd1-89c8-d3f7aef4bee9,DISK], DatanodeInfoWithStorage[127.0.0.1:44445,DS-a51c32af-aab6-4fd9-b287-03a51134ce85,DISK], DatanodeInfoWithStorage[127.0.0.1:37772,DS-c57dae7c-594d-4814-81e5-c9bf204f41ae,DISK], DatanodeInfoWithStorage[127.0.0.1:44312,DS-adf39776-9f4e-495e-9c0f-d1401f599447,DISK], DatanodeInfoWithStorage[127.0.0.1:40243,DS-02609b7a-10b1-4d2f-a95f-0fbf402c5ad9,DISK], DatanodeInfoWithStorage[127.0.0.1:36287,DS-f0f6df85-a46f-4d89-b667-125138a11cef,DISK], DatanodeInfoWithStorage[127.0.0.1:36140,DS-b5e98384-c299-4c9a-bc26-b30d23c7aa11,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7, 8]}
2020-12-03 07:24:22,584 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:extractChecksumProperties(430)) - set bytesPerCRC=512, crcPerBlock=12288
2020-12-03 07:24:22,584 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(718)) - got reply from DatanodeInfoWithStorage[127.0.0.1:36526,DS-44a63aad-efff-4da7-8035-f12475e12191,DISK]: blockChecksum=0xb82cfa69, blockChecksumType=COMPOSITE_CRC
2020-12-03 07:24:22,585 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:36287,DS-c8ee244d-58c6-400b-8261-c3c19df916dd,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=37748736; locs=[DatanodeInfoWithStorage[127.0.0.1:36287,DS-c8ee244d-58c6-400b-8261-c3c19df916dd,DISK], DatanodeInfoWithStorage[127.0.0.1:38680,DS-4ed0dfcb-1823-4feb-a78e-c34190aa43b1,DISK], DatanodeInfoWithStorage[127.0.0.1:38730,DS-85392bd5-b857-478f-99fd-a6820ffff252,DISK], DatanodeInfoWithStorage[127.0.0.1:44445,DS-350dbe39-4a8c-4163-8d64-f89d775392cf,DISK], DatanodeInfoWithStorage[127.0.0.1:37772,DS-fb82c216-d71b-4654-9ad9-21ab3a7512a6,DISK], DatanodeInfoWithStorage[127.0.0.1:36526,DS-bc4245b5-06be-49c3-84ee-8bb52e5db51d,DISK], DatanodeInfoWithStorage[127.0.0.1:40243,DS-35c3127f-e8ca-452b-bcb1-392069cbc853,DISK], DatanodeInfoWithStorage[127.0.0.1:36140,DS-65fbe59b-ff8b-4331-bff0-e44b48174987,DISK], DatanodeInfoWithStorage[127.0.0.1:41069,DS-8517971e-c5a6-4cda-8698-97e4d8202f94,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7, 8]}
2020-12-03 07:24:22,606 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(718)) - got reply from DatanodeInfoWithStorage[127.0.0.1:36287,DS-c8ee244d-58c6-400b-8261-c3c19df916dd,DISK]: blockChecksum=0x9693fb79, blockChecksumType=COMPOSITE_CRC
2020-12-03 07:24:22,607 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:36526,DS-44a63aad-efff-4da7-8035-f12475e12191,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775760_1003; getBlockSize()=37748736; corrupt=false; offset=75497472; locs=[DatanodeInfoWithStorage[127.0.0.1:36526,DS-44a63aad-efff-4da7-8035-f12475e12191,DISK], DatanodeInfoWithStorage[127.0.0.1:37772,DS-c57dae7c-594d-4814-81e5-c9bf204f41ae,DISK], DatanodeInfoWithStorage[127.0.0.1:41069,DS-95103ad2-7662-4492-ba1b-5cecf313b9e8,DISK], DatanodeInfoWithStorage[127.0.0.1:38680,DS-9861837d-0d69-4f2f-b2aa-8b94418cf3c9,DISK], DatanodeInfoWithStorage[127.0.0.1:44445,DS-a51c32af-aab6-4fd9-b287-03a51134ce85,DISK], DatanodeInfoWithStorage[127.0.0.1:36287,DS-f0f6df85-a46f-4d89-b667-125138a11cef,DISK], DatanodeInfoWithStorage[127.0.0.1:36140,DS-b5e98384-c299-4c9a-bc26-b30d23c7aa11,DISK], DatanodeInfoWithStorage[127.0.0.1:43902,DS-6d3e897d-ec54-4b20-9f96-ccf88af97eaa,DISK], DatanodeInfoWithStorage[127.0.0.1:38730,DS-f5960db6-7b54-4094-bc6c-6bca60faedad,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7, 8]}
2020-12-03 07:24:22,627 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(718)) - got reply from DatanodeInfoWithStorage[127.0.0.1:36526,DS-44a63aad-efff-4da7-8035-f12475e12191,DISK]: blockChecksum=0xfbf3413d, blockChecksumType=COMPOSITE_CRC
2020-12-03 07:24:22,628 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:43902,DS-b7b6573e-a1ea-4bd1-89c8-d3f7aef4bee9,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775744_1004; getBlockSize()=37748736; corrupt=false; offset=113246208; locs=[DatanodeInfoWithStorage[127.0.0.1:43902,DS-b7b6573e-a1ea-4bd1-89c8-d3f7aef4bee9,DISK], DatanodeInfoWithStorage[127.0.0.1:44445,DS-350dbe39-4a8c-4163-8d64-f89d775392cf,DISK], DatanodeInfoWithStorage[127.0.0.1:41069,DS-8517971e-c5a6-4cda-8698-97e4d8202f94,DISK], DatanodeInfoWithStorage[127.0.0.1:37772,DS-fb82c216-d71b-4654-9ad9-21ab3a7512a6,DISK], DatanodeInfoWithStorage[127.0.0.1:40243,DS-02609b7a-10b1-4d2f-a95f-0fbf402c5ad9,DISK], DatanodeInfoWithStorage[127.0.0.1:36526,DS-bc4245b5-06be-49c3-84ee-8bb52e5db51d,DISK], DatanodeInfoWithStorage[127.0.0.1:36287,DS-c8ee244d-58c6-400b-8261-c3c19df916dd,DISK], DatanodeInfoWithStorage[127.0.0.1:38730,DS-85392bd5-b857-478f-99fd-a6820ffff252,DISK], DatanodeInfoWithStorage[127.0.0.1:36140,DS-65fbe59b-ff8b-4331-bff0-e44b48174987,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7, 8]}
2020-12-03 07:24:22,649 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(718)) - got reply from DatanodeInfoWithStorage[127.0.0.1:43902,DS-b7b6573e-a1ea-4bd1-89c8-d3f7aef4bee9,DISK]: blockChecksum=0x0c7ef6ff, blockChecksumType=COMPOSITE_CRC
2020-12-03 07:24:22,650 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:36287,DS-f0f6df85-a46f-4d89-b667-125138a11cef,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005; getBlockSize()=37748736; corrupt=false; offset=150994944; locs=[DatanodeInfoWithStorage[127.0.0.1:36287,DS-f0f6df85-a46f-4d89-b667-125138a11cef,DISK], DatanodeInfoWithStorage[127.0.0.1:37772,DS-c57dae7c-594d-4814-81e5-c9bf204f41ae,DISK], DatanodeInfoWithStorage[127.0.0.1:41069,DS-95103ad2-7662-4492-ba1b-5cecf313b9e8,DISK], DatanodeInfoWithStorage[127.0.0.1:44312,DS-8cd30729-7874-4e19-9225-c6c7a923c355,DISK], DatanodeInfoWithStorage[127.0.0.1:44445,DS-a51c32af-aab6-4fd9-b287-03a51134ce85,DISK], DatanodeInfoWithStorage[127.0.0.1:36140,DS-b5e98384-c299-4c9a-bc26-b30d23c7aa11,DISK], DatanodeInfoWithStorage[127.0.0.1:40243,DS-35c3127f-e8ca-452b-bcb1-392069cbc853,DISK], DatanodeInfoWithStorage[127.0.0.1:38730,DS-f5960db6-7b54-4094-bc6c-6bca60faedad,DISK], DatanodeInfoWithStorage[127.0.0.1:43902,DS-6d3e897d-ec54-4b20-9f96-ccf88af97eaa,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7, 8]}
2020-12-03 07:24:22,672 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(718)) - got reply from DatanodeInfoWithStorage[127.0.0.1:36287,DS-f0f6df85-a46f-4d89-b667-125138a11cef,DISK]: blockChecksum=0x462cc45a, blockChecksumType=COMPOSITE_CRC
2020-12-03 07:24:22,673 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:38730,DS-85392bd5-b857-478f-99fd-a6820ffff252,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775712_1006; getBlockSize()=37748736; corrupt=false; offset=188743680; locs=[DatanodeInfoWithStorage[127.0.0.1:38730,DS-85392bd5-b857-478f-99fd-a6820ffff252,DISK], DatanodeInfoWithStorage[127.0.0.1:43902,DS-b7b6573e-a1ea-4bd1-89c8-d3f7aef4bee9,DISK], DatanodeInfoWithStorage[127.0.0.1:37772,DS-fb82c216-d71b-4654-9ad9-21ab3a7512a6,DISK], DatanodeInfoWithStorage[127.0.0.1:41069,DS-8517971e-c5a6-4cda-8698-97e4d8202f94,DISK], DatanodeInfoWithStorage[127.0.0.1:40243,DS-02609b7a-10b1-4d2f-a95f-0fbf402c5ad9,DISK], DatanodeInfoWithStorage[127.0.0.1:44445,DS-350dbe39-4a8c-4163-8d64-f89d775392cf,DISK], DatanodeInfoWithStorage[127.0.0.1:44312,DS-adf39776-9f4e-495e-9c0f-d1401f599447,DISK], DatanodeInfoWithStorage[127.0.0.1:38680,DS-4ed0dfcb-1823-4feb-a78e-c34190aa43b1,DISK], DatanodeInfoWithStorage[127.0.0.1:36140,DS-65fbe59b-ff8b-4331-bff0-e44b48174987,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7, 8]}
2020-12-03 07:24:22,696 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(718)) - got reply from DatanodeInfoWithStorage[127.0.0.1:38730,DS-85392bd5-b857-478f-99fd-a6820ffff252,DISK]: blockChecksum=0xc1b6062b, blockChecksumType=COMPOSITE_CRC
2020-12-03 07:24:22,697 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:41069,DS-95103ad2-7662-4492-ba1b-5cecf313b9e8,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775696_1007; getBlockSize()=37748736; corrupt=false; offset=226492416; locs=[DatanodeInfoWithStorage[127.0.0.1:41069,DS-95103ad2-7662-4492-ba1b-5cecf313b9e8,DISK], DatanodeInfoWithStorage[127.0.0.1:38680,DS-9861837d-0d69-4f2f-b2aa-8b94418cf3c9,DISK], DatanodeInfoWithStorage[127.0.0.1:44445,DS-a51c32af-aab6-4fd9-b287-03a51134ce85,DISK], DatanodeInfoWithStorage[127.0.0.1:36140,DS-b5e98384-c299-4c9a-bc26-b30d23c7aa11,DISK], DatanodeInfoWithStorage[127.0.0.1:36287,DS-c8ee244d-58c6-400b-8261-c3c19df916dd,DISK], DatanodeInfoWithStorage[127.0.0.1:43902,DS-6d3e897d-ec54-4b20-9f96-ccf88af97eaa,DISK], DatanodeInfoWithStorage[127.0.0.1:37772,DS-c57dae7c-594d-4814-81e5-c9bf204f41ae,DISK], DatanodeInfoWithStorage[127.0.0.1:38730,DS-f5960db6-7b54-4094-bc6c-6bca60faedad,DISK], DatanodeInfoWithStorage[127.0.0.1:36526,DS-44a63aad-efff-4da7-8035-f12475e12191,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7, 8]}
2020-12-03 07:24:22,717 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(718)) - got reply from DatanodeInfoWithStorage[127.0.0.1:41069,DS-95103ad2-7662-4492-ba1b-5cecf313b9e8,DISK]: blockChecksum=0x065aaa2b, blockChecksumType=COMPOSITE_CRC
2020-12-03 07:24:22,718 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:36140,DS-65fbe59b-ff8b-4331-bff0-e44b48174987,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775680_1008; getBlockSize()=37748736; corrupt=false; offset=264241152; locs=[DatanodeInfoWithStorage[127.0.0.1:36140,DS-65fbe59b-ff8b-4331-bff0-e44b48174987,DISK], DatanodeInfoWithStorage[127.0.0.1:36287,DS-f0f6df85-a46f-4d89-b667-125138a11cef,DISK], DatanodeInfoWithStorage[127.0.0.1:41069,DS-8517971e-c5a6-4cda-8698-97e4d8202f94,DISK], DatanodeInfoWithStorage[127.0.0.1:37772,DS-fb82c216-d71b-4654-9ad9-21ab3a7512a6,DISK], DatanodeInfoWithStorage[127.0.0.1:43902,DS-b7b6573e-a1ea-4bd1-89c8-d3f7aef4bee9,DISK], DatanodeInfoWithStorage[127.0.0.1:38680,DS-4ed0dfcb-1823-4feb-a78e-c34190aa43b1,DISK], DatanodeInfoWithStorage[127.0.0.1:38730,DS-85392bd5-b857-478f-99fd-a6820ffff252,DISK], DatanodeInfoWithStorage[127.0.0.1:40243,DS-35c3127f-e8ca-452b-bcb1-392069cbc853,DISK], DatanodeInfoWithStorage[127.0.0.1:44312,DS-8cd30729-7874-4e19-9225-c6c7a923c355,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7, 8]}
2020-12-03 07:24:22,741 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(718)) - got reply from DatanodeInfoWithStorage[127.0.0.1:36140,DS-65fbe59b-ff8b-4331-bff0-e44b48174987,DISK]: blockChecksum=0x2113a54c, blockChecksumType=COMPOSITE_CRC
2020-12-03 07:24:22,742 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:41069,DS-95103ad2-7662-4492-ba1b-5cecf313b9e8,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775664_1009; getBlockSize()=37748736; corrupt=false; offset=301989888; locs=[DatanodeInfoWithStorage[127.0.0.1:41069,DS-95103ad2-7662-4492-ba1b-5cecf313b9e8,DISK], DatanodeInfoWithStorage[127.0.0.1:44312,DS-adf39776-9f4e-495e-9c0f-d1401f599447,DISK], DatanodeInfoWithStorage[127.0.0.1:36140,DS-b5e98384-c299-4c9a-bc26-b30d23c7aa11,DISK], DatanodeInfoWithStorage[127.0.0.1:38680,DS-9861837d-0d69-4f2f-b2aa-8b94418cf3c9,DISK], DatanodeInfoWithStorage[127.0.0.1:37772,DS-c57dae7c-594d-4814-81e5-c9bf204f41ae,DISK], DatanodeInfoWithStorage[127.0.0.1:44445,DS-350dbe39-4a8c-4163-8d64-f89d775392cf,DISK], DatanodeInfoWithStorage[127.0.0.1:40243,DS-02609b7a-10b1-4d2f-a95f-0fbf402c5ad9,DISK], DatanodeInfoWithStorage[127.0.0.1:36287,DS-c8ee244d-58c6-400b-8261-c3c19df916dd,DISK], DatanodeInfoWithStorage[127.0.0.1:36526,DS-bc4245b5-06be-49c3-84ee-8bb52e5db51d,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7, 8]}
2020-12-03 07:24:22,764 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(718)) - got reply from DatanodeInfoWithStorage[127.0.0.1:41069,DS-95103ad2-7662-4492-ba1b-5cecf313b9e8,DISK]: blockChecksum=0x77e6163d, blockChecksumType=COMPOSITE_CRC
2020-12-03 07:24:22,764 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:38730,DS-f5960db6-7b54-4094-bc6c-6bca60faedad,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775648_1010; getBlockSize()=37748736; corrupt=false; offset=339738624; locs=[DatanodeInfoWithStorage[127.0.0.1:38730,DS-f5960db6-7b54-4094-bc6c-6bca60faedad,DISK], DatanodeInfoWithStorage[127.0.0.1:36526,DS-44a63aad-efff-4da7-8035-f12475e12191,DISK], DatanodeInfoWithStorage[127.0.0.1:44445,DS-a51c32af-aab6-4fd9-b287-03a51134ce85,DISK], DatanodeInfoWithStorage[127.0.0.1:38680,DS-4ed0dfcb-1823-4feb-a78e-c34190aa43b1,DISK], DatanodeInfoWithStorage[127.0.0.1:40243,DS-35c3127f-e8ca-452b-bcb1-392069cbc853,DISK], DatanodeInfoWithStorage[127.0.0.1:36140,DS-65fbe59b-ff8b-4331-bff0-e44b48174987,DISK], DatanodeInfoWithStorage[127.0.0.1:37772,DS-fb82c216-d71b-4654-9ad9-21ab3a7512a6,DISK], DatanodeInfoWithStorage[127.0.0.1:44312,DS-8cd30729-7874-4e19-9225-c6c7a923c355,DISK], DatanodeInfoWithStorage[127.0.0.1:36287,DS-f0f6df85-a46f-4d89-b667-125138a11cef,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7, 8]}
2020-12-03 07:24:22,785 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(718)) - got reply from DatanodeInfoWithStorage[127.0.0.1:38730,DS-f5960db6-7b54-4094-bc6c-6bca60faedad,DISK]: blockChecksum=0xcd875cc4, blockChecksumType=COMPOSITE_CRC
2020-12-03 07:24:22,785 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:makeCompositeCrcResult(315)) - Added blockCrc 0x-47d30597 for block index 0 of size 37748736
2020-12-03 07:24:22,785 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:makeCompositeCrcResult(315)) - Added blockCrc 0x-696c0487 for block index 1 of size 37748736
2020-12-03 07:24:22,785 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:makeCompositeCrcResult(315)) - Added blockCrc 0x-40cbec3 for block index 2 of size 37748736
2020-12-03 07:24:22,786 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:makeCompositeCrcResult(315)) - Added blockCrc 0xc7ef6ff for block index 3 of size 37748736
2020-12-03 07:24:22,786 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:makeCompositeCrcResult(315)) - Added blockCrc 0x462cc45a for block index 4 of size 37748736
2020-12-03 07:24:22,786 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:makeCompositeCrcResult(315)) - Added blockCrc 0x-3e49f9d5 for block index 5 of size 37748736
2020-12-03 07:24:22,786 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:makeCompositeCrcResult(315)) - Added blockCrc 0x65aaa2b for block index 6 of size 37748736
2020-12-03 07:24:22,786 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:makeCompositeCrcResult(315)) - Added blockCrc 0x2113a54c for block index 7 of size 37748736
2020-12-03 07:24:22,786 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:makeCompositeCrcResult(315)) - Added blockCrc 0x77e6163d for block index 8 of size 37748736
2020-12-03 07:24:22,786 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:makeCompositeCrcResult(337)) - Added lastBlockCrc 0x-3278a33c for block index 9 of size 37748736
2020-12-03 07:24:22,790 [IPC Server handler 3 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/striped/stripedFileChecksum1	dst=null	perm=null	proto=rpc
2020-12-03 07:24:22,796 [Thread-417] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:24:22,796 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@7a48e6e2] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:24:22,798 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-65fbe59b-ff8b-4331-bff0-e44b48174987) exiting.
2020-12-03 07:24:22,798 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-b5e98384-c299-4c9a-bc26-b30d23c7aa11) exiting.
2020-12-03 07:24:22,829 [Thread-417] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@235f4c10{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:24:22,833 [Thread-417] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@743cb8e0{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:24:22,835 [Thread-417] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@76ba13c{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:24:22,836 [Thread-417] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@61526469{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:24:22,841 [Thread-417] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 43238
2020-12-03 07:24:22,851 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:24:22,852 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:24:22,854 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:24:22,854 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid 31491a6f-508d-45fd-8589-10a6a9373e21) service to localhost/127.0.0.1:38816
2020-12-03 07:24:22,854 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid 31491a6f-508d-45fd-8589-10a6a9373e21)
2020-12-03 07:24:22,855 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:22,856 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1088075852-172.17.0.6-1606980241923] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:24:22,857 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1088075852-172.17.0.6-1606980241923] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:24:22,861 [Thread-417] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:24:22,861 [Thread-417] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:24:22,862 [Thread-417] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:24:22,862 [Thread-417] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:24:22,869 [Thread-417] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:24:22,871 [Thread-417] INFO  hdfs.StateChange (DatanodeManager.java:removeDeadDatanode(754)) - BLOCK* removeDeadDatanode: lost heartbeat from 127.0.0.1:36140, removeBlocksFromBlockMap true
2020-12-03 07:24:22,874 [Thread-417] INFO  net.NetworkTopology (NetworkTopology.java:remove(219)) - Removing a node: /default-rack/127.0.0.1:36140
2020-12-03 07:24:22,881 [IPC Server handler 8 on default port 38816] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/striped/stripedFileChecksum1	dst=null	perm=null	proto=rpc
2020-12-03 07:24:22,890 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:36526,DS-44a63aad-efff-4da7-8035-f12475e12191,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36526,DS-44a63aad-efff-4da7-8035-f12475e12191,DISK], DatanodeInfoWithStorage[127.0.0.1:38730,DS-f5960db6-7b54-4094-bc6c-6bca60faedad,DISK], DatanodeInfoWithStorage[127.0.0.1:43902,DS-b7b6573e-a1ea-4bd1-89c8-d3f7aef4bee9,DISK], DatanodeInfoWithStorage[127.0.0.1:44445,DS-a51c32af-aab6-4fd9-b287-03a51134ce85,DISK], DatanodeInfoWithStorage[127.0.0.1:37772,DS-c57dae7c-594d-4814-81e5-c9bf204f41ae,DISK], DatanodeInfoWithStorage[127.0.0.1:44312,DS-adf39776-9f4e-495e-9c0f-d1401f599447,DISK], DatanodeInfoWithStorage[127.0.0.1:40243,DS-02609b7a-10b1-4d2f-a95f-0fbf402c5ad9,DISK], DatanodeInfoWithStorage[127.0.0.1:36287,DS-f0f6df85-a46f-4d89-b667-125138a11cef,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7]}
2020-12-03 07:24:22,912 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:extractChecksumProperties(430)) - set bytesPerCRC=512, crcPerBlock=12288
2020-12-03 07:24:22,913 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(718)) - got reply from DatanodeInfoWithStorage[127.0.0.1:36526,DS-44a63aad-efff-4da7-8035-f12475e12191,DISK]: blockChecksum=0xb82cfa69, blockChecksumType=COMPOSITE_CRC
2020-12-03 07:24:22,915 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:36287,DS-c8ee244d-58c6-400b-8261-c3c19df916dd,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=37748736; locs=[DatanodeInfoWithStorage[127.0.0.1:36287,DS-c8ee244d-58c6-400b-8261-c3c19df916dd,DISK], DatanodeInfoWithStorage[127.0.0.1:38680,DS-4ed0dfcb-1823-4feb-a78e-c34190aa43b1,DISK], DatanodeInfoWithStorage[127.0.0.1:38730,DS-85392bd5-b857-478f-99fd-a6820ffff252,DISK], DatanodeInfoWithStorage[127.0.0.1:44445,DS-350dbe39-4a8c-4163-8d64-f89d775392cf,DISK], DatanodeInfoWithStorage[127.0.0.1:37772,DS-fb82c216-d71b-4654-9ad9-21ab3a7512a6,DISK], DatanodeInfoWithStorage[127.0.0.1:36526,DS-bc4245b5-06be-49c3-84ee-8bb52e5db51d,DISK], DatanodeInfoWithStorage[127.0.0.1:40243,DS-35c3127f-e8ca-452b-bcb1-392069cbc853,DISK], DatanodeInfoWithStorage[127.0.0.1:41069,DS-8517971e-c5a6-4cda-8698-97e4d8202f94,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 8]}
2020-12-03 07:24:22,962 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(718)) - got reply from DatanodeInfoWithStorage[127.0.0.1:36287,DS-c8ee244d-58c6-400b-8261-c3c19df916dd,DISK]: blockChecksum=0x9693fb79, blockChecksumType=COMPOSITE_CRC
2020-12-03 07:24:22,963 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:36526,DS-44a63aad-efff-4da7-8035-f12475e12191,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775760_1003; getBlockSize()=37748736; corrupt=false; offset=75497472; locs=[DatanodeInfoWithStorage[127.0.0.1:36526,DS-44a63aad-efff-4da7-8035-f12475e12191,DISK], DatanodeInfoWithStorage[127.0.0.1:37772,DS-c57dae7c-594d-4814-81e5-c9bf204f41ae,DISK], DatanodeInfoWithStorage[127.0.0.1:41069,DS-95103ad2-7662-4492-ba1b-5cecf313b9e8,DISK], DatanodeInfoWithStorage[127.0.0.1:38680,DS-9861837d-0d69-4f2f-b2aa-8b94418cf3c9,DISK], DatanodeInfoWithStorage[127.0.0.1:44445,DS-a51c32af-aab6-4fd9-b287-03a51134ce85,DISK], DatanodeInfoWithStorage[127.0.0.1:36287,DS-f0f6df85-a46f-4d89-b667-125138a11cef,DISK], DatanodeInfoWithStorage[127.0.0.1:43902,DS-6d3e897d-ec54-4b20-9f96-ccf88af97eaa,DISK], DatanodeInfoWithStorage[127.0.0.1:38730,DS-f5960db6-7b54-4094-bc6c-6bca60faedad,DISK]]; indices=[0, 1, 2, 3, 4, 5, 7, 8]}
2020-12-03 07:24:22,990 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(718)) - got reply from DatanodeInfoWithStorage[127.0.0.1:36526,DS-44a63aad-efff-4da7-8035-f12475e12191,DISK]: blockChecksum=0xfbf3413d, blockChecksumType=COMPOSITE_CRC
2020-12-03 07:24:22,991 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:43902,DS-b7b6573e-a1ea-4bd1-89c8-d3f7aef4bee9,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775744_1004; getBlockSize()=37748736; corrupt=false; offset=113246208; locs=[DatanodeInfoWithStorage[127.0.0.1:43902,DS-b7b6573e-a1ea-4bd1-89c8-d3f7aef4bee9,DISK], DatanodeInfoWithStorage[127.0.0.1:44445,DS-350dbe39-4a8c-4163-8d64-f89d775392cf,DISK], DatanodeInfoWithStorage[127.0.0.1:41069,DS-8517971e-c5a6-4cda-8698-97e4d8202f94,DISK], DatanodeInfoWithStorage[127.0.0.1:37772,DS-fb82c216-d71b-4654-9ad9-21ab3a7512a6,DISK], DatanodeInfoWithStorage[127.0.0.1:40243,DS-02609b7a-10b1-4d2f-a95f-0fbf402c5ad9,DISK], DatanodeInfoWithStorage[127.0.0.1:36526,DS-bc4245b5-06be-49c3-84ee-8bb52e5db51d,DISK], DatanodeInfoWithStorage[127.0.0.1:36287,DS-c8ee244d-58c6-400b-8261-c3c19df916dd,DISK], DatanodeInfoWithStorage[127.0.0.1:38730,DS-85392bd5-b857-478f-99fd-a6820ffff252,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7]}
2020-12-03 07:24:23,018 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(718)) - got reply from DatanodeInfoWithStorage[127.0.0.1:43902,DS-b7b6573e-a1ea-4bd1-89c8-d3f7aef4bee9,DISK]: blockChecksum=0x0c7ef6ff, blockChecksumType=COMPOSITE_CRC
2020-12-03 07:24:23,019 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:36287,DS-f0f6df85-a46f-4d89-b667-125138a11cef,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005; getBlockSize()=37748736; corrupt=false; offset=150994944; locs=[DatanodeInfoWithStorage[127.0.0.1:36287,DS-f0f6df85-a46f-4d89-b667-125138a11cef,DISK], DatanodeInfoWithStorage[127.0.0.1:37772,DS-c57dae7c-594d-4814-81e5-c9bf204f41ae,DISK], DatanodeInfoWithStorage[127.0.0.1:41069,DS-95103ad2-7662-4492-ba1b-5cecf313b9e8,DISK], DatanodeInfoWithStorage[127.0.0.1:44312,DS-8cd30729-7874-4e19-9225-c6c7a923c355,DISK], DatanodeInfoWithStorage[127.0.0.1:44445,DS-a51c32af-aab6-4fd9-b287-03a51134ce85,DISK], DatanodeInfoWithStorage[127.0.0.1:40243,DS-35c3127f-e8ca-452b-bcb1-392069cbc853,DISK], DatanodeInfoWithStorage[127.0.0.1:38730,DS-f5960db6-7b54-4094-bc6c-6bca60faedad,DISK], DatanodeInfoWithStorage[127.0.0.1:43902,DS-6d3e897d-ec54-4b20-9f96-ccf88af97eaa,DISK]]; indices=[0, 1, 2, 3, 4, 6, 7, 8]}
2020-12-03 07:24:23,062 [DataXceiver for client /127.0.0.1:49612 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,090 [DataXceiver for client /127.0.0.1:49612 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,093 [DataXceiver for client /127.0.0.1:49612 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,094 [DataXceiver for client /127.0.0.1:49612 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,096 [DataXceiver for client /127.0.0.1:49612 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,098 [DataXceiver for client /127.0.0.1:49612 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,138 [Thread-417] WARN  hdfs.FileChecksumHelper (FileChecksumHelper.java:checksumBlockGroup(679)) - src=/striped/stripedFileChecksum1, datanodes[0]=DatanodeInfoWithStorage[127.0.0.1:36287,DS-f0f6df85-a46f-4d89-b667-125138a11cef,DISK]
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:550)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.tryDatanode(FileChecksumHelper.java:709)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlockGroup(FileChecksumHelper.java:664)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:638)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery15(TestFileChecksum.java:465)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-12-03 07:24:23,138 [DataXceiver for client /127.0.0.1:49612 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:36287:DataXceiver error processing BLOCK_GROUP_CHECKSUM operation  src: /127.0.0.1:49612 dst: /127.0.0.1:36287
java.lang.UnsupportedOperationException
	at java.nio.ByteBuffer.array(ByteBuffer.java:994)
	at org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockChecksumReconstructor.reconstruct(StripedBlockChecksumReconstructor.java:90)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.recalculateChecksum(BlockChecksumHelper.java:711)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.compute(BlockChecksumHelper.java:489)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.blockGroupChecksum(DataXceiver.java:1047)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opStripedBlockChecksum(Receiver.java:327)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:119)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:23,142 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:37772,DS-c57dae7c-594d-4814-81e5-c9bf204f41ae,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005; getBlockSize()=37748736; corrupt=false; offset=150994944; locs=[DatanodeInfoWithStorage[127.0.0.1:36287,DS-f0f6df85-a46f-4d89-b667-125138a11cef,DISK], DatanodeInfoWithStorage[127.0.0.1:37772,DS-c57dae7c-594d-4814-81e5-c9bf204f41ae,DISK], DatanodeInfoWithStorage[127.0.0.1:41069,DS-95103ad2-7662-4492-ba1b-5cecf313b9e8,DISK], DatanodeInfoWithStorage[127.0.0.1:44312,DS-8cd30729-7874-4e19-9225-c6c7a923c355,DISK], DatanodeInfoWithStorage[127.0.0.1:44445,DS-a51c32af-aab6-4fd9-b287-03a51134ce85,DISK], DatanodeInfoWithStorage[127.0.0.1:40243,DS-35c3127f-e8ca-452b-bcb1-392069cbc853,DISK], DatanodeInfoWithStorage[127.0.0.1:38730,DS-f5960db6-7b54-4094-bc6c-6bca60faedad,DISK], DatanodeInfoWithStorage[127.0.0.1:43902,DS-6d3e897d-ec54-4b20-9f96-ccf88af97eaa,DISK]]; indices=[0, 1, 2, 3, 4, 6, 7, 8]}
2020-12-03 07:24:23,166 [DataXceiver for client /127.0.0.1:44954 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,170 [DataXceiver for client /127.0.0.1:44954 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,172 [DataXceiver for client /127.0.0.1:44954 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,174 [DataXceiver for client /127.0.0.1:44954 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,175 [DataXceiver for client /127.0.0.1:44954 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,178 [DataXceiver for client /127.0.0.1:44954 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,192 [DataXceiver for client /127.0.0.1:44954 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:37772:DataXceiver error processing BLOCK_GROUP_CHECKSUM operation  src: /127.0.0.1:44954 dst: /127.0.0.1:37772
java.lang.UnsupportedOperationException
	at java.nio.ByteBuffer.array(ByteBuffer.java:994)
	at org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockChecksumReconstructor.reconstruct(StripedBlockChecksumReconstructor.java:90)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.recalculateChecksum(BlockChecksumHelper.java:711)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.compute(BlockChecksumHelper.java:489)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.blockGroupChecksum(DataXceiver.java:1047)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opStripedBlockChecksum(Receiver.java:327)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:119)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:23,192 [Thread-417] WARN  hdfs.FileChecksumHelper (FileChecksumHelper.java:checksumBlockGroup(679)) - src=/striped/stripedFileChecksum1, datanodes[1]=DatanodeInfoWithStorage[127.0.0.1:37772,DS-c57dae7c-594d-4814-81e5-c9bf204f41ae,DISK]
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:550)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.tryDatanode(FileChecksumHelper.java:709)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlockGroup(FileChecksumHelper.java:664)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:638)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery15(TestFileChecksum.java:465)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-12-03 07:24:23,198 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:41069,DS-95103ad2-7662-4492-ba1b-5cecf313b9e8,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005; getBlockSize()=37748736; corrupt=false; offset=150994944; locs=[DatanodeInfoWithStorage[127.0.0.1:36287,DS-f0f6df85-a46f-4d89-b667-125138a11cef,DISK], DatanodeInfoWithStorage[127.0.0.1:37772,DS-c57dae7c-594d-4814-81e5-c9bf204f41ae,DISK], DatanodeInfoWithStorage[127.0.0.1:41069,DS-95103ad2-7662-4492-ba1b-5cecf313b9e8,DISK], DatanodeInfoWithStorage[127.0.0.1:44312,DS-8cd30729-7874-4e19-9225-c6c7a923c355,DISK], DatanodeInfoWithStorage[127.0.0.1:44445,DS-a51c32af-aab6-4fd9-b287-03a51134ce85,DISK], DatanodeInfoWithStorage[127.0.0.1:40243,DS-35c3127f-e8ca-452b-bcb1-392069cbc853,DISK], DatanodeInfoWithStorage[127.0.0.1:38730,DS-f5960db6-7b54-4094-bc6c-6bca60faedad,DISK], DatanodeInfoWithStorage[127.0.0.1:43902,DS-6d3e897d-ec54-4b20-9f96-ccf88af97eaa,DISK]]; indices=[0, 1, 2, 3, 4, 6, 7, 8]}
2020-12-03 07:24:23,228 [DataXceiver for client /127.0.0.1:36462 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,234 [DataXceiver for client /127.0.0.1:36462 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,235 [DataXceiver for client /127.0.0.1:36462 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,243 [DataXceiver for client /127.0.0.1:36462 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,245 [DataXceiver for client /127.0.0.1:36462 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,247 [DataXceiver for client /127.0.0.1:36462 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,273 [DataXceiver for client /127.0.0.1:36462 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:41069:DataXceiver error processing BLOCK_GROUP_CHECKSUM operation  src: /127.0.0.1:36462 dst: /127.0.0.1:41069
java.lang.UnsupportedOperationException
	at java.nio.ByteBuffer.array(ByteBuffer.java:994)
	at org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockChecksumReconstructor.reconstruct(StripedBlockChecksumReconstructor.java:90)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.recalculateChecksum(BlockChecksumHelper.java:711)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.compute(BlockChecksumHelper.java:489)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.blockGroupChecksum(DataXceiver.java:1047)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opStripedBlockChecksum(Receiver.java:327)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:119)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:23,273 [Thread-417] WARN  hdfs.FileChecksumHelper (FileChecksumHelper.java:checksumBlockGroup(679)) - src=/striped/stripedFileChecksum1, datanodes[2]=DatanodeInfoWithStorage[127.0.0.1:41069,DS-95103ad2-7662-4492-ba1b-5cecf313b9e8,DISK]
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:550)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.tryDatanode(FileChecksumHelper.java:709)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlockGroup(FileChecksumHelper.java:664)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:638)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery15(TestFileChecksum.java:465)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-12-03 07:24:23,276 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:44312,DS-8cd30729-7874-4e19-9225-c6c7a923c355,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005; getBlockSize()=37748736; corrupt=false; offset=150994944; locs=[DatanodeInfoWithStorage[127.0.0.1:36287,DS-f0f6df85-a46f-4d89-b667-125138a11cef,DISK], DatanodeInfoWithStorage[127.0.0.1:37772,DS-c57dae7c-594d-4814-81e5-c9bf204f41ae,DISK], DatanodeInfoWithStorage[127.0.0.1:41069,DS-95103ad2-7662-4492-ba1b-5cecf313b9e8,DISK], DatanodeInfoWithStorage[127.0.0.1:44312,DS-8cd30729-7874-4e19-9225-c6c7a923c355,DISK], DatanodeInfoWithStorage[127.0.0.1:44445,DS-a51c32af-aab6-4fd9-b287-03a51134ce85,DISK], DatanodeInfoWithStorage[127.0.0.1:40243,DS-35c3127f-e8ca-452b-bcb1-392069cbc853,DISK], DatanodeInfoWithStorage[127.0.0.1:38730,DS-f5960db6-7b54-4094-bc6c-6bca60faedad,DISK], DatanodeInfoWithStorage[127.0.0.1:43902,DS-6d3e897d-ec54-4b20-9f96-ccf88af97eaa,DISK]]; indices=[0, 1, 2, 3, 4, 6, 7, 8]}
2020-12-03 07:24:23,314 [DataXceiver for client /127.0.0.1:35536 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,324 [DataXceiver for client /127.0.0.1:35536 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,326 [DataXceiver for client /127.0.0.1:35536 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,331 [DataXceiver for client /127.0.0.1:35536 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,341 [DataXceiver for client /127.0.0.1:35536 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,346 [DataXceiver for client /127.0.0.1:35536 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,352 [DataXceiver for client /127.0.0.1:35536 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:44312:DataXceiver error processing BLOCK_GROUP_CHECKSUM operation  src: /127.0.0.1:35536 dst: /127.0.0.1:44312
java.lang.UnsupportedOperationException
	at java.nio.ByteBuffer.array(ByteBuffer.java:994)
	at org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockChecksumReconstructor.reconstruct(StripedBlockChecksumReconstructor.java:90)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.recalculateChecksum(BlockChecksumHelper.java:711)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.compute(BlockChecksumHelper.java:489)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.blockGroupChecksum(DataXceiver.java:1047)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opStripedBlockChecksum(Receiver.java:327)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:119)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:23,352 [Thread-417] WARN  hdfs.FileChecksumHelper (FileChecksumHelper.java:checksumBlockGroup(679)) - src=/striped/stripedFileChecksum1, datanodes[3]=DatanodeInfoWithStorage[127.0.0.1:44312,DS-8cd30729-7874-4e19-9225-c6c7a923c355,DISK]
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:550)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.tryDatanode(FileChecksumHelper.java:709)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlockGroup(FileChecksumHelper.java:664)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:638)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery15(TestFileChecksum.java:465)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-12-03 07:24:23,354 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:44445,DS-a51c32af-aab6-4fd9-b287-03a51134ce85,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005; getBlockSize()=37748736; corrupt=false; offset=150994944; locs=[DatanodeInfoWithStorage[127.0.0.1:36287,DS-f0f6df85-a46f-4d89-b667-125138a11cef,DISK], DatanodeInfoWithStorage[127.0.0.1:37772,DS-c57dae7c-594d-4814-81e5-c9bf204f41ae,DISK], DatanodeInfoWithStorage[127.0.0.1:41069,DS-95103ad2-7662-4492-ba1b-5cecf313b9e8,DISK], DatanodeInfoWithStorage[127.0.0.1:44312,DS-8cd30729-7874-4e19-9225-c6c7a923c355,DISK], DatanodeInfoWithStorage[127.0.0.1:44445,DS-a51c32af-aab6-4fd9-b287-03a51134ce85,DISK], DatanodeInfoWithStorage[127.0.0.1:40243,DS-35c3127f-e8ca-452b-bcb1-392069cbc853,DISK], DatanodeInfoWithStorage[127.0.0.1:38730,DS-f5960db6-7b54-4094-bc6c-6bca60faedad,DISK], DatanodeInfoWithStorage[127.0.0.1:43902,DS-6d3e897d-ec54-4b20-9f96-ccf88af97eaa,DISK]]; indices=[0, 1, 2, 3, 4, 6, 7, 8]}
2020-12-03 07:24:23,370 [DataXceiver for client /127.0.0.1:37682 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,375 [DataXceiver for client /127.0.0.1:37682 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,379 [DataXceiver for client /127.0.0.1:37682 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,386 [DataXceiver for client /127.0.0.1:37682 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,391 [DataXceiver for client /127.0.0.1:37682 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,409 [DataXceiver for client /127.0.0.1:37682 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,428 [Thread-417] WARN  hdfs.FileChecksumHelper (FileChecksumHelper.java:checksumBlockGroup(679)) - src=/striped/stripedFileChecksum1, datanodes[4]=DatanodeInfoWithStorage[127.0.0.1:44445,DS-a51c32af-aab6-4fd9-b287-03a51134ce85,DISK]
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:550)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.tryDatanode(FileChecksumHelper.java:709)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlockGroup(FileChecksumHelper.java:664)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:638)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery15(TestFileChecksum.java:465)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-12-03 07:24:23,431 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:40243,DS-35c3127f-e8ca-452b-bcb1-392069cbc853,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005; getBlockSize()=37748736; corrupt=false; offset=150994944; locs=[DatanodeInfoWithStorage[127.0.0.1:36287,DS-f0f6df85-a46f-4d89-b667-125138a11cef,DISK], DatanodeInfoWithStorage[127.0.0.1:37772,DS-c57dae7c-594d-4814-81e5-c9bf204f41ae,DISK], DatanodeInfoWithStorage[127.0.0.1:41069,DS-95103ad2-7662-4492-ba1b-5cecf313b9e8,DISK], DatanodeInfoWithStorage[127.0.0.1:44312,DS-8cd30729-7874-4e19-9225-c6c7a923c355,DISK], DatanodeInfoWithStorage[127.0.0.1:44445,DS-a51c32af-aab6-4fd9-b287-03a51134ce85,DISK], DatanodeInfoWithStorage[127.0.0.1:40243,DS-35c3127f-e8ca-452b-bcb1-392069cbc853,DISK], DatanodeInfoWithStorage[127.0.0.1:38730,DS-f5960db6-7b54-4094-bc6c-6bca60faedad,DISK], DatanodeInfoWithStorage[127.0.0.1:43902,DS-6d3e897d-ec54-4b20-9f96-ccf88af97eaa,DISK]]; indices=[0, 1, 2, 3, 4, 6, 7, 8]}
2020-12-03 07:24:23,428 [DataXceiver for client /127.0.0.1:37682 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:44445:DataXceiver error processing BLOCK_GROUP_CHECKSUM operation  src: /127.0.0.1:37682 dst: /127.0.0.1:44445
java.lang.UnsupportedOperationException
	at java.nio.ByteBuffer.array(ByteBuffer.java:994)
	at org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockChecksumReconstructor.reconstruct(StripedBlockChecksumReconstructor.java:90)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.recalculateChecksum(BlockChecksumHelper.java:711)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.compute(BlockChecksumHelper.java:489)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.blockGroupChecksum(DataXceiver.java:1047)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opStripedBlockChecksum(Receiver.java:327)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:119)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:23,454 [DataXceiver for client /127.0.0.1:59270 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,463 [DataXceiver for client /127.0.0.1:59270 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,469 [DataXceiver for client /127.0.0.1:59270 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,476 [DataXceiver for client /127.0.0.1:59270 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,509 [DataXceiver for client /127.0.0.1:59270 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,544 [DataXceiver for client /127.0.0.1:59270 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,549 [DataXceiver for client /127.0.0.1:59270 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:40243:DataXceiver error processing BLOCK_GROUP_CHECKSUM operation  src: /127.0.0.1:59270 dst: /127.0.0.1:40243
java.lang.UnsupportedOperationException
	at java.nio.ByteBuffer.array(ByteBuffer.java:994)
	at org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockChecksumReconstructor.reconstruct(StripedBlockChecksumReconstructor.java:90)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.recalculateChecksum(BlockChecksumHelper.java:711)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.compute(BlockChecksumHelper.java:489)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.blockGroupChecksum(DataXceiver.java:1047)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opStripedBlockChecksum(Receiver.java:327)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:119)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:23,549 [Thread-417] WARN  hdfs.FileChecksumHelper (FileChecksumHelper.java:checksumBlockGroup(679)) - src=/striped/stripedFileChecksum1, datanodes[5]=DatanodeInfoWithStorage[127.0.0.1:40243,DS-35c3127f-e8ca-452b-bcb1-392069cbc853,DISK]
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:550)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.tryDatanode(FileChecksumHelper.java:709)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlockGroup(FileChecksumHelper.java:664)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:638)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery15(TestFileChecksum.java:465)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-12-03 07:24:23,550 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:38730,DS-f5960db6-7b54-4094-bc6c-6bca60faedad,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005; getBlockSize()=37748736; corrupt=false; offset=150994944; locs=[DatanodeInfoWithStorage[127.0.0.1:36287,DS-f0f6df85-a46f-4d89-b667-125138a11cef,DISK], DatanodeInfoWithStorage[127.0.0.1:37772,DS-c57dae7c-594d-4814-81e5-c9bf204f41ae,DISK], DatanodeInfoWithStorage[127.0.0.1:41069,DS-95103ad2-7662-4492-ba1b-5cecf313b9e8,DISK], DatanodeInfoWithStorage[127.0.0.1:44312,DS-8cd30729-7874-4e19-9225-c6c7a923c355,DISK], DatanodeInfoWithStorage[127.0.0.1:44445,DS-a51c32af-aab6-4fd9-b287-03a51134ce85,DISK], DatanodeInfoWithStorage[127.0.0.1:40243,DS-35c3127f-e8ca-452b-bcb1-392069cbc853,DISK], DatanodeInfoWithStorage[127.0.0.1:38730,DS-f5960db6-7b54-4094-bc6c-6bca60faedad,DISK], DatanodeInfoWithStorage[127.0.0.1:43902,DS-6d3e897d-ec54-4b20-9f96-ccf88af97eaa,DISK]]; indices=[0, 1, 2, 3, 4, 6, 7, 8]}
2020-12-03 07:24:23,567 [DataXceiver for client /127.0.0.1:50942 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,581 [DataXceiver for client /127.0.0.1:50942 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,583 [DataXceiver for client /127.0.0.1:50942 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,585 [DataXceiver for client /127.0.0.1:50942 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,586 [DataXceiver for client /127.0.0.1:50942 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,588 [DataXceiver for client /127.0.0.1:50942 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,590 [DataXceiver for client /127.0.0.1:50942 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:38730:DataXceiver error processing BLOCK_GROUP_CHECKSUM operation  src: /127.0.0.1:50942 dst: /127.0.0.1:38730
java.lang.UnsupportedOperationException
	at java.nio.ByteBuffer.array(ByteBuffer.java:994)
	at org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockChecksumReconstructor.reconstruct(StripedBlockChecksumReconstructor.java:90)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.recalculateChecksum(BlockChecksumHelper.java:711)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.compute(BlockChecksumHelper.java:489)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.blockGroupChecksum(DataXceiver.java:1047)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opStripedBlockChecksum(Receiver.java:327)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:119)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:23,590 [Thread-417] WARN  hdfs.FileChecksumHelper (FileChecksumHelper.java:checksumBlockGroup(679)) - src=/striped/stripedFileChecksum1, datanodes[6]=DatanodeInfoWithStorage[127.0.0.1:38730,DS-f5960db6-7b54-4094-bc6c-6bca60faedad,DISK]
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:550)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.tryDatanode(FileChecksumHelper.java:709)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlockGroup(FileChecksumHelper.java:664)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:638)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery15(TestFileChecksum.java:465)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-12-03 07:24:23,594 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:43902,DS-6d3e897d-ec54-4b20-9f96-ccf88af97eaa,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005; getBlockSize()=37748736; corrupt=false; offset=150994944; locs=[DatanodeInfoWithStorage[127.0.0.1:36287,DS-f0f6df85-a46f-4d89-b667-125138a11cef,DISK], DatanodeInfoWithStorage[127.0.0.1:37772,DS-c57dae7c-594d-4814-81e5-c9bf204f41ae,DISK], DatanodeInfoWithStorage[127.0.0.1:41069,DS-95103ad2-7662-4492-ba1b-5cecf313b9e8,DISK], DatanodeInfoWithStorage[127.0.0.1:44312,DS-8cd30729-7874-4e19-9225-c6c7a923c355,DISK], DatanodeInfoWithStorage[127.0.0.1:44445,DS-a51c32af-aab6-4fd9-b287-03a51134ce85,DISK], DatanodeInfoWithStorage[127.0.0.1:40243,DS-35c3127f-e8ca-452b-bcb1-392069cbc853,DISK], DatanodeInfoWithStorage[127.0.0.1:38730,DS-f5960db6-7b54-4094-bc6c-6bca60faedad,DISK], DatanodeInfoWithStorage[127.0.0.1:43902,DS-6d3e897d-ec54-4b20-9f96-ccf88af97eaa,DISK]]; indices=[0, 1, 2, 3, 4, 6, 7, 8]}
2020-12-03 07:24:23,611 [DataXceiver for client /127.0.0.1:44648 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,612 [DataXceiver for client /127.0.0.1:44648 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,614 [DataXceiver for client /127.0.0.1:44648 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,615 [DataXceiver for client /127.0.0.1:44648 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,616 [DataXceiver for client /127.0.0.1:44648 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,618 [DataXceiver for client /127.0.0.1:44648 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:24:23,620 [Thread-417] WARN  hdfs.FileChecksumHelper (FileChecksumHelper.java:checksumBlockGroup(679)) - src=/striped/stripedFileChecksum1, datanodes[7]=DatanodeInfoWithStorage[127.0.0.1:43902,DS-6d3e897d-ec54-4b20-9f96-ccf88af97eaa,DISK]
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:550)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.tryDatanode(FileChecksumHelper.java:709)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlockGroup(FileChecksumHelper.java:664)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:638)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery15(TestFileChecksum.java:465)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-12-03 07:24:23,620 [DataXceiver for client /127.0.0.1:44648 [Getting checksum for block groupBP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:43902:DataXceiver error processing BLOCK_GROUP_CHECKSUM operation  src: /127.0.0.1:44648 dst: /127.0.0.1:43902
java.lang.UnsupportedOperationException
	at java.nio.ByteBuffer.array(ByteBuffer.java:994)
	at org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockChecksumReconstructor.reconstruct(StripedBlockChecksumReconstructor.java:90)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.recalculateChecksum(BlockChecksumHelper.java:711)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.compute(BlockChecksumHelper.java:489)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.blockGroupChecksum(DataXceiver.java:1047)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opStripedBlockChecksum(Receiver.java:327)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:119)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:23,624 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(2049)) - Shutting down the Mini HDFS Cluster
2020-12-03 07:24:23,625 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 10
2020-12-03 07:24:23,625 [Listener at localhost/46163] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:24:23,625 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@50b1f030] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:24:23,627 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22, DS-6d3e897d-ec54-4b20-9f96-ccf88af97eaa) exiting.
2020-12-03 07:24:23,627 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21, DS-b7b6573e-a1ea-4bd1-89c8-d3f7aef4bee9) exiting.
2020-12-03 07:24:23,652 [Listener at localhost/46163] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@894858{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:24:23,653 [Listener at localhost/46163] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@7af707e0{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:24:23,654 [Listener at localhost/46163] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@2d83c5a5{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:24:23,654 [Listener at localhost/46163] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@680bddf5{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:24:23,655 [Listener at localhost/46163] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 46163
2020-12-03 07:24:23,664 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:24:23,675 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:24:23,676 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:24:23,677 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid 8797193e-f01a-4f9a-9a17-eb37f9f1de79) service to localhost/127.0.0.1:38816
2020-12-03 07:24:23,677 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid 8797193e-f01a-4f9a-9a17-eb37f9f1de79)
2020-12-03 07:24:23,677 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:23,679 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22/current/BP-1088075852-172.17.0.6-1606980241923] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:24:23,683 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21/current/BP-1088075852-172.17.0.6-1606980241923] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:24:23,686 [Listener at localhost/46163] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:24:23,687 [Listener at localhost/46163] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:24:23,688 [Listener at localhost/46163] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:24:23,690 [Listener at localhost/46163] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:24:23,692 [Listener at localhost/46163] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:24:23,692 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 9
2020-12-03 07:24:23,693 [Listener at localhost/46163] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:24:23,693 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@589b028e] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:24:23,695 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20, DS-c8ee244d-58c6-400b-8261-c3c19df916dd) exiting.
2020-12-03 07:24:23,695 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19, DS-f0f6df85-a46f-4d89-b667-125138a11cef) exiting.
2020-12-03 07:24:24,858 [Listener at localhost/46163] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@669d2b1b{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:24:24,859 [Listener at localhost/46163] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@721eb7df{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:24:24,860 [Listener at localhost/46163] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@55a8dc49{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:24:24,860 [Listener at localhost/46163] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@63034ed1{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:24:24,862 [Listener at localhost/46163] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 38720
2020-12-03 07:24:24,863 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:24:24,863 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:24:24,866 [DataXceiver for client  at /127.0.0.1:49628 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:24,866 [DataXceiver for client  at /127.0.0.1:49866 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:24,866 [DataXceiver for client  at /127.0.0.1:49742 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:24,866 [DataXceiver for client  at /127.0.0.1:49842 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:24,866 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:24:24,866 [DataXceiver for client  at /127.0.0.1:49704 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:24,866 [DataXceiver for client  at /127.0.0.1:49818 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:24,866 [DataXceiver for client  at /127.0.0.1:49794 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:24,866 [DataXceiver for client  at /127.0.0.1:49674 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:24,867 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid a8821b81-1e27-4139-97ff-1efdadfd6b4f) service to localhost/127.0.0.1:38816
2020-12-03 07:24:24,867 [DataXceiver for client  at /127.0.0.1:49628 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005 on DS-f0f6df85-a46f-4d89-b667-125138a11cef, because there is no volume scanner for that storageId.
2020-12-03 07:24:24,869 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid a8821b81-1e27-4139-97ff-1efdadfd6b4f)
2020-12-03 07:24:24,869 [DataXceiver for client  at /127.0.0.1:49674 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005 on DS-f0f6df85-a46f-4d89-b667-125138a11cef, because there is no volume scanner for that storageId.
2020-12-03 07:24:24,871 [DataXceiver for client  at /127.0.0.1:49628 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:36287, datanodeUuid=a8821b81-1e27-4139-97ff-1efdadfd6b4f, infoPort=35781, infoSecurePort=0, ipcPort=38720, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005 to /127.0.0.1:49628
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:24,871 [DataXceiver for client  at /127.0.0.1:49674 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:36287, datanodeUuid=a8821b81-1e27-4139-97ff-1efdadfd6b4f, infoPort=35781, infoSecurePort=0, ipcPort=38720, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005 to /127.0.0.1:49674
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:24,871 [DataXceiver for client  at /127.0.0.1:49794 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005 on DS-f0f6df85-a46f-4d89-b667-125138a11cef, because there is no volume scanner for that storageId.
2020-12-03 07:24:24,874 [DataXceiver for client  at /127.0.0.1:49818 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005 on DS-f0f6df85-a46f-4d89-b667-125138a11cef, because there is no volume scanner for that storageId.
2020-12-03 07:24:24,876 [DataXceiver for client  at /127.0.0.1:49794 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:36287, datanodeUuid=a8821b81-1e27-4139-97ff-1efdadfd6b4f, infoPort=35781, infoSecurePort=0, ipcPort=38720, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005 to /127.0.0.1:49794
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:24,876 [DataXceiver for client  at /127.0.0.1:49818 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:36287, datanodeUuid=a8821b81-1e27-4139-97ff-1efdadfd6b4f, infoPort=35781, infoSecurePort=0, ipcPort=38720, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005 to /127.0.0.1:49818
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:24,876 [DataXceiver for client  at /127.0.0.1:49704 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005 on DS-f0f6df85-a46f-4d89-b667-125138a11cef, because there is no volume scanner for that storageId.
2020-12-03 07:24:24,878 [DataXceiver for client  at /127.0.0.1:49704 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:36287, datanodeUuid=a8821b81-1e27-4139-97ff-1efdadfd6b4f, infoPort=35781, infoSecurePort=0, ipcPort=38720, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005 to /127.0.0.1:49704
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:24,878 [DataXceiver for client  at /127.0.0.1:49842 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005 on DS-f0f6df85-a46f-4d89-b667-125138a11cef, because there is no volume scanner for that storageId.
2020-12-03 07:24:24,880 [DataXceiver for client  at /127.0.0.1:49742 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005 on DS-f0f6df85-a46f-4d89-b667-125138a11cef, because there is no volume scanner for that storageId.
2020-12-03 07:24:24,880 [DataXceiver for client  at /127.0.0.1:49842 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:36287, datanodeUuid=a8821b81-1e27-4139-97ff-1efdadfd6b4f, infoPort=35781, infoSecurePort=0, ipcPort=38720, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005 to /127.0.0.1:49842
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:24,881 [DataXceiver for client  at /127.0.0.1:49742 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:36287, datanodeUuid=a8821b81-1e27-4139-97ff-1efdadfd6b4f, infoPort=35781, infoSecurePort=0, ipcPort=38720, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005 to /127.0.0.1:49742
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:24,881 [DataXceiver for client  at /127.0.0.1:49866 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005 on DS-f0f6df85-a46f-4d89-b667-125138a11cef, because there is no volume scanner for that storageId.
2020-12-03 07:24:24,883 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:24,885 [DataXceiver for client  at /127.0.0.1:49866 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:36287, datanodeUuid=a8821b81-1e27-4139-97ff-1efdadfd6b4f, infoPort=35781, infoSecurePort=0, ipcPort=38720, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005 to /127.0.0.1:49866
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:24,886 [DataXceiver for client  at /127.0.0.1:49628 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:36287:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:49628 dst: /127.0.0.1:36287
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:24,886 [DataXceiver for client  at /127.0.0.1:49674 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:36287:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:49674 dst: /127.0.0.1:36287
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:24,886 [DataXceiver for client  at /127.0.0.1:49794 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:36287:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:49794 dst: /127.0.0.1:36287
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:24,886 [DataXceiver for client  at /127.0.0.1:49818 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:36287:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:49818 dst: /127.0.0.1:36287
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:24,886 [DataXceiver for client  at /127.0.0.1:49704 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:36287:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:49704 dst: /127.0.0.1:36287
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:24,886 [DataXceiver for client  at /127.0.0.1:49842 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:36287:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:49842 dst: /127.0.0.1:36287
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:24,886 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19/current/BP-1088075852-172.17.0.6-1606980241923] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:24:24,886 [DataXceiver for client  at /127.0.0.1:49742 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:36287:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:49742 dst: /127.0.0.1:36287
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:24,886 [DataXceiver for client  at /127.0.0.1:49866 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775728_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:36287:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:49866 dst: /127.0.0.1:36287
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:24,889 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20/current/BP-1088075852-172.17.0.6-1606980241923] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:24:24,983 [Listener at localhost/46163] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:24:24,983 [Listener at localhost/46163] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:24:24,986 [Listener at localhost/46163] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:24:24,986 [Listener at localhost/46163] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:24:24,991 [Listener at localhost/46163] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:24:24,991 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 8
2020-12-03 07:24:24,992 [Listener at localhost/46163] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:24:24,994 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18, DS-85392bd5-b857-478f-99fd-a6820ffff252) exiting.
2020-12-03 07:24:25,002 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@315ba14a] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:24:25,008 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17, DS-f5960db6-7b54-4094-bc6c-6bca60faedad) exiting.
2020-12-03 07:24:25,121 [Listener at localhost/46163] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@2c1dc8e{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:24:25,123 [Listener at localhost/46163] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@b273a59{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:24:25,123 [Listener at localhost/46163] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@273c947f{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:24:25,124 [Listener at localhost/46163] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@50cf5a23{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:24:25,126 [Listener at localhost/46163] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 36951
2020-12-03 07:24:25,129 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:24:25,129 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:24:25,130 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid e6d8c096-523b-4539-b874-b517e981fab6) service to localhost/127.0.0.1:38816
2020-12-03 07:24:25,130 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:24:25,130 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid e6d8c096-523b-4539-b874-b517e981fab6)
2020-12-03 07:24:25,130 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:25,133 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17/current/BP-1088075852-172.17.0.6-1606980241923] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:24:25,141 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18/current/BP-1088075852-172.17.0.6-1606980241923] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:24:25,167 [Listener at localhost/46163] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:24:25,167 [Listener at localhost/46163] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:24:25,168 [Listener at localhost/46163] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:24:25,169 [Listener at localhost/46163] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:24:25,172 [Listener at localhost/46163] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:24:25,172 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 7
2020-12-03 07:24:25,172 [Listener at localhost/46163] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:24:25,172 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@2fb68ec6] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:24:25,174 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15, DS-a51c32af-aab6-4fd9-b287-03a51134ce85) exiting.
2020-12-03 07:24:25,174 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16, DS-350dbe39-4a8c-4163-8d64-f89d775392cf) exiting.
2020-12-03 07:24:25,193 [Listener at localhost/46163] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@530a8454{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:24:25,194 [Listener at localhost/46163] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@2b750e36{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:24:25,195 [Listener at localhost/46163] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@59221b97{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:24:25,195 [Listener at localhost/46163] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@24528a25{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:24:25,197 [Listener at localhost/46163] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 35715
2020-12-03 07:24:25,197 [DataXceiver for client  at /127.0.0.1:37760 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,197 [DataXceiver for client  at /127.0.0.1:37592 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,197 [DataXceiver for client  at /127.0.0.1:37736 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,197 [DataXceiver for client  at /127.0.0.1:37626 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,197 [DataXceiver for client  at /127.0.0.1:37668 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,197 [DataXceiver for client  at /127.0.0.1:37784 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,197 [DataXceiver for client  at /127.0.0.1:37712 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,205 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:24:25,205 [DataXceiver for client  at /127.0.0.1:37760 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005 on DS-a51c32af-aab6-4fd9-b287-03a51134ce85, because there is no volume scanner for that storageId.
2020-12-03 07:24:25,207 [DataXceiver for client  at /127.0.0.1:37712 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005 on DS-a51c32af-aab6-4fd9-b287-03a51134ce85, because there is no volume scanner for that storageId.
2020-12-03 07:24:25,207 [DataXceiver for client  at /127.0.0.1:37760 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:44445, datanodeUuid=ad0684d8-6863-4e19-b2b3-d95e7d2668d9, infoPort=40126, infoSecurePort=0, ipcPort=35715, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005 to /127.0.0.1:37760
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,203 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:24:25,203 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:24:25,212 [DataXceiver for client  at /127.0.0.1:37760 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:44445:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:37760 dst: /127.0.0.1:44445
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,208 [DataXceiver for client  at /127.0.0.1:37784 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005 on DS-a51c32af-aab6-4fd9-b287-03a51134ce85, because there is no volume scanner for that storageId.
2020-12-03 07:24:25,208 [DataXceiver for client  at /127.0.0.1:37712 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:44445, datanodeUuid=ad0684d8-6863-4e19-b2b3-d95e7d2668d9, infoPort=40126, infoSecurePort=0, ipcPort=35715, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005 to /127.0.0.1:37712
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,207 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid ad0684d8-6863-4e19-b2b3-d95e7d2668d9) service to localhost/127.0.0.1:38816
2020-12-03 07:24:25,221 [DataXceiver for client  at /127.0.0.1:37784 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:44445, datanodeUuid=ad0684d8-6863-4e19-b2b3-d95e7d2668d9, infoPort=40126, infoSecurePort=0, ipcPort=35715, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005 to /127.0.0.1:37784
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,221 [DataXceiver for client  at /127.0.0.1:37668 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005 on DS-a51c32af-aab6-4fd9-b287-03a51134ce85, because there is no volume scanner for that storageId.
2020-12-03 07:24:25,237 [DataXceiver for client  at /127.0.0.1:37784 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:44445:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:37784 dst: /127.0.0.1:44445
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,234 [DataXceiver for client  at /127.0.0.1:37712 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:44445:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:37712 dst: /127.0.0.1:44445
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,234 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid ad0684d8-6863-4e19-b2b3-d95e7d2668d9)
2020-12-03 07:24:25,242 [DataXceiver for client  at /127.0.0.1:37668 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:44445, datanodeUuid=ad0684d8-6863-4e19-b2b3-d95e7d2668d9, infoPort=40126, infoSecurePort=0, ipcPort=35715, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005 to /127.0.0.1:37668
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,242 [DataXceiver for client  at /127.0.0.1:37626 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005 on DS-a51c32af-aab6-4fd9-b287-03a51134ce85, because there is no volume scanner for that storageId.
2020-12-03 07:24:25,250 [DataXceiver for client  at /127.0.0.1:37668 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:44445:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:37668 dst: /127.0.0.1:44445
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,252 [DataXceiver for client  at /127.0.0.1:37626 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:44445, datanodeUuid=ad0684d8-6863-4e19-b2b3-d95e7d2668d9, infoPort=40126, infoSecurePort=0, ipcPort=35715, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005 to /127.0.0.1:37626
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,250 [DataXceiver for client  at /127.0.0.1:37736 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005 on DS-a51c32af-aab6-4fd9-b287-03a51134ce85, because there is no volume scanner for that storageId.
2020-12-03 07:24:25,260 [DataXceiver for client  at /127.0.0.1:37592 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005 on DS-a51c32af-aab6-4fd9-b287-03a51134ce85, because there is no volume scanner for that storageId.
2020-12-03 07:24:25,260 [DataXceiver for client  at /127.0.0.1:37626 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:44445:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:37626 dst: /127.0.0.1:44445
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,263 [DataXceiver for client  at /127.0.0.1:37592 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:44445, datanodeUuid=ad0684d8-6863-4e19-b2b3-d95e7d2668d9, infoPort=40126, infoSecurePort=0, ipcPort=35715, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005 to /127.0.0.1:37592
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,263 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:25,267 [DataXceiver for client  at /127.0.0.1:37592 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:44445:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:37592 dst: /127.0.0.1:44445
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,263 [DataXceiver for client  at /127.0.0.1:37736 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:44445, datanodeUuid=ad0684d8-6863-4e19-b2b3-d95e7d2668d9, infoPort=40126, infoSecurePort=0, ipcPort=35715, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005 to /127.0.0.1:37736
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,273 [DataXceiver for client  at /127.0.0.1:37736 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775724_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:44445:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:37736 dst: /127.0.0.1:44445
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,273 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15/current/BP-1088075852-172.17.0.6-1606980241923] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:24:25,278 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16/current/BP-1088075852-172.17.0.6-1606980241923] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:24:25,285 [Listener at localhost/46163] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:24:25,285 [Listener at localhost/46163] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:24:25,287 [Listener at localhost/46163] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:24:25,287 [Listener at localhost/46163] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:24:25,291 [Listener at localhost/46163] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:24:25,292 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 6
2020-12-03 07:24:25,292 [Listener at localhost/46163] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:24:25,292 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@3a71c100] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:24:25,294 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14, DS-95103ad2-7662-4492-ba1b-5cecf313b9e8) exiting.
2020-12-03 07:24:25,294 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13, DS-8517971e-c5a6-4cda-8698-97e4d8202f94) exiting.
2020-12-03 07:24:25,312 [Listener at localhost/46163] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@bae47a0{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:24:25,312 [Listener at localhost/46163] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@74a9c4b0{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:24:25,313 [Listener at localhost/46163] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@6bd51ed8{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:24:25,313 [Listener at localhost/46163] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@49ef32e0{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:24:25,315 [DataXceiver for client  at /127.0.0.1:36410 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,315 [DataXceiver for client  at /127.0.0.1:36642 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,315 [DataXceiver for client  at /127.0.0.1:36618 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,315 [DataXceiver for client  at /127.0.0.1:36518 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,315 [DataXceiver for client  at /127.0.0.1:36594 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,315 [DataXceiver for client  at /127.0.0.1:36570 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,315 [Listener at localhost/46163] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 37986
2020-12-03 07:24:25,315 [DataXceiver for client  at /127.0.0.1:36482 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,315 [DataXceiver for client  at /127.0.0.1:36450 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,315 [DataXceiver for client  at /127.0.0.1:36410 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005 on DS-95103ad2-7662-4492-ba1b-5cecf313b9e8, because there is no volume scanner for that storageId.
2020-12-03 07:24:25,337 [DataXceiver for client  at /127.0.0.1:36450 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005 on DS-95103ad2-7662-4492-ba1b-5cecf313b9e8, because there is no volume scanner for that storageId.
2020-12-03 07:24:25,337 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:24:25,337 [DataXceiver for client  at /127.0.0.1:36482 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005 on DS-95103ad2-7662-4492-ba1b-5cecf313b9e8, because there is no volume scanner for that storageId.
2020-12-03 07:24:25,337 [DataXceiver for client  at /127.0.0.1:36410 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:41069, datanodeUuid=31eff9f5-fd87-487e-8b1a-43cc24be4844, infoPort=41537, infoSecurePort=0, ipcPort=37986, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005 to /127.0.0.1:36410
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,337 [DataXceiver for client  at /127.0.0.1:36570 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005 on DS-95103ad2-7662-4492-ba1b-5cecf313b9e8, because there is no volume scanner for that storageId.
2020-12-03 07:24:25,342 [DataXceiver for client  at /127.0.0.1:36570 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:41069, datanodeUuid=31eff9f5-fd87-487e-8b1a-43cc24be4844, infoPort=41537, infoSecurePort=0, ipcPort=37986, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005 to /127.0.0.1:36570
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,342 [DataXceiver for client  at /127.0.0.1:36482 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:41069, datanodeUuid=31eff9f5-fd87-487e-8b1a-43cc24be4844, infoPort=41537, infoSecurePort=0, ipcPort=37986, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005 to /127.0.0.1:36482
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,337 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:24:25,337 [DataXceiver for client  at /127.0.0.1:36450 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:41069, datanodeUuid=31eff9f5-fd87-487e-8b1a-43cc24be4844, infoPort=41537, infoSecurePort=0, ipcPort=37986, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005 to /127.0.0.1:36450
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,352 [DataXceiver for client  at /127.0.0.1:36482 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:41069:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:36482 dst: /127.0.0.1:41069
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,346 [DataXceiver for client  at /127.0.0.1:36570 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:41069:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:36570 dst: /127.0.0.1:41069
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,346 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:24:25,342 [DataXceiver for client  at /127.0.0.1:36594 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005 on DS-95103ad2-7662-4492-ba1b-5cecf313b9e8, because there is no volume scanner for that storageId.
2020-12-03 07:24:25,342 [DataXceiver for client  at /127.0.0.1:36410 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:41069:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:36410 dst: /127.0.0.1:41069
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,366 [DataXceiver for client  at /127.0.0.1:36594 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:41069, datanodeUuid=31eff9f5-fd87-487e-8b1a-43cc24be4844, infoPort=41537, infoSecurePort=0, ipcPort=37986, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005 to /127.0.0.1:36594
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,366 [DataXceiver for client  at /127.0.0.1:36518 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005 on DS-95103ad2-7662-4492-ba1b-5cecf313b9e8, because there is no volume scanner for that storageId.
2020-12-03 07:24:25,365 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid 31eff9f5-fd87-487e-8b1a-43cc24be4844) service to localhost/127.0.0.1:38816
2020-12-03 07:24:25,352 [DataXceiver for client  at /127.0.0.1:36450 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:41069:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:36450 dst: /127.0.0.1:41069
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,373 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid 31eff9f5-fd87-487e-8b1a-43cc24be4844)
2020-12-03 07:24:25,373 [DataXceiver for client  at /127.0.0.1:36518 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:41069, datanodeUuid=31eff9f5-fd87-487e-8b1a-43cc24be4844, infoPort=41537, infoSecurePort=0, ipcPort=37986, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005 to /127.0.0.1:36518
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,373 [DataXceiver for client  at /127.0.0.1:36618 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005 on DS-95103ad2-7662-4492-ba1b-5cecf313b9e8, because there is no volume scanner for that storageId.
2020-12-03 07:24:25,370 [DataXceiver for client  at /127.0.0.1:36594 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:41069:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:36594 dst: /127.0.0.1:41069
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,403 [DataXceiver for client  at /127.0.0.1:36518 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:41069:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:36518 dst: /127.0.0.1:41069
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,404 [DataXceiver for client  at /127.0.0.1:36642 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005 on DS-95103ad2-7662-4492-ba1b-5cecf313b9e8, because there is no volume scanner for that storageId.
2020-12-03 07:24:25,404 [DataXceiver for client  at /127.0.0.1:36618 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:41069, datanodeUuid=31eff9f5-fd87-487e-8b1a-43cc24be4844, infoPort=41537, infoSecurePort=0, ipcPort=37986, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005 to /127.0.0.1:36618
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,404 [DataXceiver for client  at /127.0.0.1:36642 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:41069, datanodeUuid=31eff9f5-fd87-487e-8b1a-43cc24be4844, infoPort=41537, infoSecurePort=0, ipcPort=37986, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005 to /127.0.0.1:36642
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,404 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:25,404 [DataXceiver for client  at /127.0.0.1:36618 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:41069:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:36618 dst: /127.0.0.1:41069
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,409 [DataXceiver for client  at /127.0.0.1:36642 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775726_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:41069:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:36642 dst: /127.0.0.1:41069
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,410 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13/current/BP-1088075852-172.17.0.6-1606980241923] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:24:25,411 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14/current/BP-1088075852-172.17.0.6-1606980241923] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:24:25,418 [Listener at localhost/46163] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:24:25,418 [Listener at localhost/46163] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:24:25,420 [Listener at localhost/46163] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:24:25,420 [Listener at localhost/46163] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:24:25,425 [Listener at localhost/46163] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:24:25,426 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 5
2020-12-03 07:24:25,426 [Listener at localhost/46163] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:24:25,426 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@79f227a9] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:24:25,429 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11, DS-c57dae7c-594d-4814-81e5-c9bf204f41ae) exiting.
2020-12-03 07:24:25,429 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12, DS-fb82c216-d71b-4654-9ad9-21ab3a7512a6) exiting.
2020-12-03 07:24:25,448 [Listener at localhost/46163] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@22ee2d0{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:24:25,449 [Listener at localhost/46163] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@7bfc3126{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:24:25,449 [Listener at localhost/46163] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@a23a01d{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:24:25,449 [Listener at localhost/46163] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@619bd14c{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:24:25,451 [DataXceiver for client  at /127.0.0.1:44970 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,451 [DataXceiver for client  at /127.0.0.1:45002 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,451 [DataXceiver for client  at /127.0.0.1:44930 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,452 [DataXceiver for client  at /127.0.0.1:45038 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,451 [DataXceiver for client  at /127.0.0.1:44970 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005 on DS-c57dae7c-594d-4814-81e5-c9bf204f41ae, because there is no volume scanner for that storageId.
2020-12-03 07:24:25,452 [DataXceiver for client  at /127.0.0.1:45114 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,452 [DataXceiver for client  at /127.0.0.1:45162 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,452 [DataXceiver for client  at /127.0.0.1:45138 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,452 [Listener at localhost/46163] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 36035
2020-12-03 07:24:25,452 [DataXceiver for client  at /127.0.0.1:45090 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,452 [DataXceiver for client  at /127.0.0.1:44970 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:37772, datanodeUuid=e6a510c0-efbe-44e4-b3ca-915180c69d66, infoPort=41739, infoSecurePort=0, ipcPort=36035, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005 to /127.0.0.1:44970
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,452 [DataXceiver for client  at /127.0.0.1:45038 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005 on DS-c57dae7c-594d-4814-81e5-c9bf204f41ae, because there is no volume scanner for that storageId.
2020-12-03 07:24:25,454 [DataXceiver for client  at /127.0.0.1:44930 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005 on DS-c57dae7c-594d-4814-81e5-c9bf204f41ae, because there is no volume scanner for that storageId.
2020-12-03 07:24:25,454 [DataXceiver for client  at /127.0.0.1:45038 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:37772, datanodeUuid=e6a510c0-efbe-44e4-b3ca-915180c69d66, infoPort=41739, infoSecurePort=0, ipcPort=36035, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005 to /127.0.0.1:45038
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,460 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:24:25,464 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:24:25,460 [DataXceiver for client  at /127.0.0.1:45002 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005 on DS-c57dae7c-594d-4814-81e5-c9bf204f41ae, because there is no volume scanner for that storageId.
2020-12-03 07:24:25,459 [DataXceiver for client  at /127.0.0.1:44970 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:37772:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:44970 dst: /127.0.0.1:37772
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,459 [DataXceiver for client  at /127.0.0.1:44930 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:37772, datanodeUuid=e6a510c0-efbe-44e4-b3ca-915180c69d66, infoPort=41739, infoSecurePort=0, ipcPort=36035, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005 to /127.0.0.1:44930
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,472 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:24:25,472 [DataXceiver for client  at /127.0.0.1:45002 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:37772, datanodeUuid=e6a510c0-efbe-44e4-b3ca-915180c69d66, infoPort=41739, infoSecurePort=0, ipcPort=36035, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005 to /127.0.0.1:45002
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,472 [DataXceiver for client  at /127.0.0.1:45090 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005 on DS-c57dae7c-594d-4814-81e5-c9bf204f41ae, because there is no volume scanner for that storageId.
2020-12-03 07:24:25,485 [DataXceiver for client  at /127.0.0.1:45002 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:37772:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:45002 dst: /127.0.0.1:37772
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,468 [DataXceiver for client  at /127.0.0.1:45038 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:37772:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:45038 dst: /127.0.0.1:37772
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,488 [DataXceiver for client  at /127.0.0.1:45090 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:37772, datanodeUuid=e6a510c0-efbe-44e4-b3ca-915180c69d66, infoPort=41739, infoSecurePort=0, ipcPort=36035, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005 to /127.0.0.1:45090
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,488 [DataXceiver for client  at /127.0.0.1:45138 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005 on DS-c57dae7c-594d-4814-81e5-c9bf204f41ae, because there is no volume scanner for that storageId.
2020-12-03 07:24:25,481 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid e6a510c0-efbe-44e4-b3ca-915180c69d66) service to localhost/127.0.0.1:38816
2020-12-03 07:24:25,481 [DataXceiver for client  at /127.0.0.1:44930 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:37772:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:44930 dst: /127.0.0.1:37772
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,499 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid e6a510c0-efbe-44e4-b3ca-915180c69d66)
2020-12-03 07:24:25,499 [DataXceiver for client  at /127.0.0.1:45138 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:37772, datanodeUuid=e6a510c0-efbe-44e4-b3ca-915180c69d66, infoPort=41739, infoSecurePort=0, ipcPort=36035, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005 to /127.0.0.1:45138
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,499 [DataXceiver for client  at /127.0.0.1:45162 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005 on DS-c57dae7c-594d-4814-81e5-c9bf204f41ae, because there is no volume scanner for that storageId.
2020-12-03 07:24:25,499 [DataXceiver for client  at /127.0.0.1:45090 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:37772:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:45090 dst: /127.0.0.1:37772
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,516 [DataXceiver for client  at /127.0.0.1:45162 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:37772, datanodeUuid=e6a510c0-efbe-44e4-b3ca-915180c69d66, infoPort=41739, infoSecurePort=0, ipcPort=36035, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005 to /127.0.0.1:45162
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,516 [DataXceiver for client  at /127.0.0.1:45138 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:37772:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:45138 dst: /127.0.0.1:37772
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,504 [DataXceiver for client  at /127.0.0.1:45114 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005 on DS-c57dae7c-594d-4814-81e5-c9bf204f41ae, because there is no volume scanner for that storageId.
2020-12-03 07:24:25,522 [DataXceiver for client  at /127.0.0.1:45162 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:37772:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:45162 dst: /127.0.0.1:37772
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,526 [DataXceiver for client  at /127.0.0.1:45114 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:37772, datanodeUuid=e6a510c0-efbe-44e4-b3ca-915180c69d66, infoPort=41739, infoSecurePort=0, ipcPort=36035, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005 to /127.0.0.1:45114
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,526 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:25,531 [DataXceiver for client  at /127.0.0.1:45114 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775727_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:37772:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:45114 dst: /127.0.0.1:37772
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,539 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12/current/BP-1088075852-172.17.0.6-1606980241923] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:24:25,548 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11/current/BP-1088075852-172.17.0.6-1606980241923] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:24:25,547 [Listener at localhost/46163] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:24:25,549 [Listener at localhost/46163] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:24:25,553 [Listener at localhost/46163] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:24:25,553 [Listener at localhost/46163] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:24:25,559 [Listener at localhost/46163] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:24:25,559 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 4
2020-12-03 07:24:25,559 [Listener at localhost/46163] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:24:25,559 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@51c929ae] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:24:25,562 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10, DS-8cd30729-7874-4e19-9225-c6c7a923c355) exiting.
2020-12-03 07:24:25,562 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9, DS-adf39776-9f4e-495e-9c0f-d1401f599447) exiting.
2020-12-03 07:24:25,581 [Listener at localhost/46163] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@3abd581e{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:24:25,582 [Listener at localhost/46163] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@4d4d8fcf{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:24:25,582 [Listener at localhost/46163] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@1bc53649{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:24:25,583 [Listener at localhost/46163] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@62c5bbdc{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:24:25,584 [Listener at localhost/46163] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 41810
2020-12-03 07:24:25,584 [DataXceiver for client  at /127.0.0.1:35632 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,585 [DataXceiver for client  at /127.0.0.1:35632 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005 on DS-8cd30729-7874-4e19-9225-c6c7a923c355, because there is no volume scanner for that storageId.
2020-12-03 07:24:25,585 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:24:25,585 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid 941777f1-35bc-48df-a860-96f904e7fd34) service to localhost/127.0.0.1:38816
2020-12-03 07:24:25,585 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:24:25,585 [DataXceiver for client  at /127.0.0.1:35656 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,584 [DataXceiver for client  at /127.0.0.1:35608 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,584 [DataXceiver for client  at /127.0.0.1:35560 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,584 [DataXceiver for client  at /127.0.0.1:35522 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,584 [DataXceiver for client  at /127.0.0.1:35680 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,584 [DataXceiver for client  at /127.0.0.1:35488 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,584 [DataXceiver for client  at /127.0.0.1:35450 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,597 [DataXceiver for client  at /127.0.0.1:35656 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005 on DS-8cd30729-7874-4e19-9225-c6c7a923c355, because there is no volume scanner for that storageId.
2020-12-03 07:24:25,585 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid 941777f1-35bc-48df-a860-96f904e7fd34)
2020-12-03 07:24:25,599 [DataXceiver for client  at /127.0.0.1:35656 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:44312, datanodeUuid=941777f1-35bc-48df-a860-96f904e7fd34, infoPort=43524, infoSecurePort=0, ipcPort=41810, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005 to /127.0.0.1:35656
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,585 [DataXceiver for client  at /127.0.0.1:35632 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:44312, datanodeUuid=941777f1-35bc-48df-a860-96f904e7fd34, infoPort=43524, infoSecurePort=0, ipcPort=41810, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005 to /127.0.0.1:35632
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,585 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:24:25,605 [DataXceiver for client  at /127.0.0.1:35656 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:44312:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:35656 dst: /127.0.0.1:44312
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,599 [DataXceiver for client  at /127.0.0.1:35450 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005 on DS-8cd30729-7874-4e19-9225-c6c7a923c355, because there is no volume scanner for that storageId.
2020-12-03 07:24:25,606 [DataXceiver for client  at /127.0.0.1:35632 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:44312:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:35632 dst: /127.0.0.1:44312
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,611 [DataXceiver for client  at /127.0.0.1:35450 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:44312, datanodeUuid=941777f1-35bc-48df-a860-96f904e7fd34, infoPort=43524, infoSecurePort=0, ipcPort=41810, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005 to /127.0.0.1:35450
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,611 [DataXceiver for client  at /127.0.0.1:35488 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005 on DS-8cd30729-7874-4e19-9225-c6c7a923c355, because there is no volume scanner for that storageId.
2020-12-03 07:24:25,616 [DataXceiver for client  at /127.0.0.1:35450 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:44312:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:35450 dst: /127.0.0.1:44312
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,616 [DataXceiver for client  at /127.0.0.1:35488 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:44312, datanodeUuid=941777f1-35bc-48df-a860-96f904e7fd34, infoPort=43524, infoSecurePort=0, ipcPort=41810, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005 to /127.0.0.1:35488
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,616 [DataXceiver for client  at /127.0.0.1:35680 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005 on DS-8cd30729-7874-4e19-9225-c6c7a923c355, because there is no volume scanner for that storageId.
2020-12-03 07:24:25,621 [DataXceiver for client  at /127.0.0.1:35488 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:44312:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:35488 dst: /127.0.0.1:44312
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,621 [DataXceiver for client  at /127.0.0.1:35680 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:44312, datanodeUuid=941777f1-35bc-48df-a860-96f904e7fd34, infoPort=43524, infoSecurePort=0, ipcPort=41810, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005 to /127.0.0.1:35680
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,621 [DataXceiver for client  at /127.0.0.1:35522 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005 on DS-8cd30729-7874-4e19-9225-c6c7a923c355, because there is no volume scanner for that storageId.
2020-12-03 07:24:25,626 [DataXceiver for client  at /127.0.0.1:35680 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:44312:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:35680 dst: /127.0.0.1:44312
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,626 [DataXceiver for client  at /127.0.0.1:35522 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:44312, datanodeUuid=941777f1-35bc-48df-a860-96f904e7fd34, infoPort=43524, infoSecurePort=0, ipcPort=41810, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005 to /127.0.0.1:35522
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,626 [DataXceiver for client  at /127.0.0.1:35560 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005 on DS-8cd30729-7874-4e19-9225-c6c7a923c355, because there is no volume scanner for that storageId.
2020-12-03 07:24:25,632 [DataXceiver for client  at /127.0.0.1:35560 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:44312, datanodeUuid=941777f1-35bc-48df-a860-96f904e7fd34, infoPort=43524, infoSecurePort=0, ipcPort=41810, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005 to /127.0.0.1:35560
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,633 [DataXceiver for client  at /127.0.0.1:35560 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:44312:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:35560 dst: /127.0.0.1:44312
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,633 [DataXceiver for client  at /127.0.0.1:35608 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005 on DS-8cd30729-7874-4e19-9225-c6c7a923c355, because there is no volume scanner for that storageId.
2020-12-03 07:24:25,633 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:25,632 [DataXceiver for client  at /127.0.0.1:35522 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:44312:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:35522 dst: /127.0.0.1:44312
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,633 [DataXceiver for client  at /127.0.0.1:35608 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:44312, datanodeUuid=941777f1-35bc-48df-a860-96f904e7fd34, infoPort=43524, infoSecurePort=0, ipcPort=41810, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005 to /127.0.0.1:35608
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,652 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9/current/BP-1088075852-172.17.0.6-1606980241923] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:24:25,652 [DataXceiver for client  at /127.0.0.1:35608 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775725_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:44312:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:35608 dst: /127.0.0.1:44312
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,652 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10/current/BP-1088075852-172.17.0.6-1606980241923] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:24:25,667 [Listener at localhost/46163] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:24:25,668 [Listener at localhost/46163] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:24:25,670 [Listener at localhost/46163] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:24:25,670 [Listener at localhost/46163] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:24:25,675 [Listener at localhost/46163] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:24:25,675 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 3
2020-12-03 07:24:25,675 [Listener at localhost/46163] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:24:25,675 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@39c11e6c] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:24:25,678 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7, DS-44a63aad-efff-4da7-8035-f12475e12191) exiting.
2020-12-03 07:24:25,678 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8, DS-bc4245b5-06be-49c3-84ee-8bb52e5db51d) exiting.
2020-12-03 07:24:25,696 [Listener at localhost/46163] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@21680803{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:24:25,697 [Listener at localhost/46163] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@588ab592{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:24:25,697 [Listener at localhost/46163] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@2d35442b{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:24:25,698 [Listener at localhost/46163] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@329a1243{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:24:25,699 [Listener at localhost/46163] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 36553
2020-12-03 07:24:25,699 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:24:25,704 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:24:25,704 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:24:25,704 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid 53d1c8e9-e11d-4d1a-9a77-35621ba2c419) service to localhost/127.0.0.1:38816
2020-12-03 07:24:25,705 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid 53d1c8e9-e11d-4d1a-9a77-35621ba2c419)
2020-12-03 07:24:25,705 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:25,705 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7/current/BP-1088075852-172.17.0.6-1606980241923] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:24:25,706 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/current/BP-1088075852-172.17.0.6-1606980241923] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:24:25,731 [Listener at localhost/46163] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:24:25,731 [Listener at localhost/46163] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:24:25,734 [Listener at localhost/46163] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:24:25,734 [Listener at localhost/46163] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:24:25,752 [Listener at localhost/46163] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:24:25,752 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 2
2020-12-03 07:24:25,753 [Listener at localhost/46163] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:24:25,753 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@1734f68] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:24:25,757 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6, DS-35c3127f-e8ca-452b-bcb1-392069cbc853) exiting.
2020-12-03 07:24:25,757 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5, DS-02609b7a-10b1-4d2f-a95f-0fbf402c5ad9) exiting.
2020-12-03 07:24:25,779 [Listener at localhost/46163] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@52eacb4b{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:24:25,780 [Listener at localhost/46163] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@5528a42c{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:24:25,781 [Listener at localhost/46163] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@2102a4d5{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:24:25,781 [Listener at localhost/46163] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@7dac3fd8{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:24:25,785 [Listener at localhost/46163] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 41403
2020-12-03 07:24:25,785 [DataXceiver for client  at /127.0.0.1:59292 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,785 [DataXceiver for client  at /127.0.0.1:59230 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,785 [DataXceiver for client  at /127.0.0.1:59340 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,785 [DataXceiver for client  at /127.0.0.1:59112 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,785 [DataXceiver for client  at /127.0.0.1:59316 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,785 [DataXceiver for client  at /127.0.0.1:59268 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,785 [DataXceiver for client  at /127.0.0.1:59182 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,819 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:24:25,819 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:24:25,795 [DataXceiver for client  at /127.0.0.1:59292 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005 on DS-35c3127f-e8ca-452b-bcb1-392069cbc853, because there is no volume scanner for that storageId.
2020-12-03 07:24:25,819 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:24:25,824 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid 028e287b-d054-4bef-b640-ae581093b777) service to localhost/127.0.0.1:38816
2020-12-03 07:24:25,824 [DataXceiver for client  at /127.0.0.1:59292 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:40243, datanodeUuid=028e287b-d054-4bef-b640-ae581093b777, infoPort=46239, infoSecurePort=0, ipcPort=41403, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005 to /127.0.0.1:59292
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,824 [DataXceiver for client  at /127.0.0.1:59182 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005 on DS-35c3127f-e8ca-452b-bcb1-392069cbc853, because there is no volume scanner for that storageId.
2020-12-03 07:24:25,824 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid 028e287b-d054-4bef-b640-ae581093b777)
2020-12-03 07:24:25,828 [DataXceiver for client  at /127.0.0.1:59292 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:40243:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:59292 dst: /127.0.0.1:40243
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,828 [DataXceiver for client  at /127.0.0.1:59182 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:40243, datanodeUuid=028e287b-d054-4bef-b640-ae581093b777, infoPort=46239, infoSecurePort=0, ipcPort=41403, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005 to /127.0.0.1:59182
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,828 [DataXceiver for client  at /127.0.0.1:59268 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005 on DS-35c3127f-e8ca-452b-bcb1-392069cbc853, because there is no volume scanner for that storageId.
2020-12-03 07:24:25,836 [DataXceiver for client  at /127.0.0.1:59182 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:40243:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:59182 dst: /127.0.0.1:40243
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,836 [DataXceiver for client  at /127.0.0.1:59268 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:40243, datanodeUuid=028e287b-d054-4bef-b640-ae581093b777, infoPort=46239, infoSecurePort=0, ipcPort=41403, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005 to /127.0.0.1:59268
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,836 [DataXceiver for client  at /127.0.0.1:59316 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005 on DS-35c3127f-e8ca-452b-bcb1-392069cbc853, because there is no volume scanner for that storageId.
2020-12-03 07:24:25,845 [DataXceiver for client  at /127.0.0.1:59268 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:40243:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:59268 dst: /127.0.0.1:40243
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,845 [DataXceiver for client  at /127.0.0.1:59316 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:40243, datanodeUuid=028e287b-d054-4bef-b640-ae581093b777, infoPort=46239, infoSecurePort=0, ipcPort=41403, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005 to /127.0.0.1:59316
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,845 [DataXceiver for client  at /127.0.0.1:59112 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005 on DS-35c3127f-e8ca-452b-bcb1-392069cbc853, because there is no volume scanner for that storageId.
2020-12-03 07:24:25,884 [DataXceiver for client  at /127.0.0.1:59112 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:40243, datanodeUuid=028e287b-d054-4bef-b640-ae581093b777, infoPort=46239, infoSecurePort=0, ipcPort=41403, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005 to /127.0.0.1:59112
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,884 [DataXceiver for client  at /127.0.0.1:59340 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005 on DS-35c3127f-e8ca-452b-bcb1-392069cbc853, because there is no volume scanner for that storageId.
2020-12-03 07:24:25,885 [DataXceiver for client  at /127.0.0.1:59230 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005 on DS-35c3127f-e8ca-452b-bcb1-392069cbc853, because there is no volume scanner for that storageId.
2020-12-03 07:24:25,885 [DataXceiver for client  at /127.0.0.1:59340 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:40243, datanodeUuid=028e287b-d054-4bef-b640-ae581093b777, infoPort=46239, infoSecurePort=0, ipcPort=41403, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005 to /127.0.0.1:59340
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,886 [DataXceiver for client  at /127.0.0.1:59316 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:40243:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:59316 dst: /127.0.0.1:40243
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,886 [DataXceiver for client  at /127.0.0.1:59112 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:40243:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:59112 dst: /127.0.0.1:40243
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,885 [DataXceiver for client  at /127.0.0.1:59230 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:40243, datanodeUuid=028e287b-d054-4bef-b640-ae581093b777, infoPort=46239, infoSecurePort=0, ipcPort=41403, storageInfo=lv=-57;cid=testClusterID;nsid=110378089;c=1606980241923):Got exception while serving BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005 to /127.0.0.1:59230
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,885 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:25,887 [DataXceiver for client  at /127.0.0.1:59340 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:40243:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:59340 dst: /127.0.0.1:40243
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,889 [DataXceiver for client  at /127.0.0.1:59230 [Sending block BP-1088075852-172.17.0.6-1606980241923:blk_-9223372036854775722_1005]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:40243:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:59230 dst: /127.0.0.1:40243
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:24:25,890 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-1088075852-172.17.0.6-1606980241923] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:24:25,891 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/current/BP-1088075852-172.17.0.6-1606980241923] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:24:25,896 [Listener at localhost/46163] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:24:25,896 [Listener at localhost/46163] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:24:25,899 [Listener at localhost/46163] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:24:25,899 [Listener at localhost/46163] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:24:25,900 [Listener at localhost/46163] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:24:25,900 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 1
2020-12-03 07:24:25,900 [Listener at localhost/46163] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:24:25,900 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@42a9a63e] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:24:25,903 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, DS-9861837d-0d69-4f2f-b2aa-8b94418cf3c9) exiting.
2020-12-03 07:24:25,904 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3, DS-4ed0dfcb-1823-4feb-a78e-c34190aa43b1) exiting.
2020-12-03 07:24:25,919 [Listener at localhost/46163] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@285f09de{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:24:25,920 [Listener at localhost/46163] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@73393584{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:24:25,921 [Listener at localhost/46163] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@29ad44e3{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:24:25,921 [Listener at localhost/46163] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@6b9267b{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:24:25,922 [Listener at localhost/46163] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 33077
2020-12-03 07:24:25,923 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:24:25,928 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:24:25,928 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:24:25,929 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid 61b60f2f-795d-44aa-82c3-0ac238b8a077) service to localhost/127.0.0.1:38816
2020-12-03 07:24:26,036 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1088075852-172.17.0.6-1606980241923 (Datanode Uuid 61b60f2f-795d-44aa-82c3-0ac238b8a077)
2020-12-03 07:24:26,036 [BP-1088075852-172.17.0.6-1606980241923 heartbeating to localhost/127.0.0.1:38816] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1088075852-172.17.0.6-1606980241923
2020-12-03 07:24:26,037 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1088075852-172.17.0.6-1606980241923] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:24:26,038 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1088075852-172.17.0.6-1606980241923] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:24:26,043 [Listener at localhost/46163] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:24:26,043 [Listener at localhost/46163] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:24:26,047 [Listener at localhost/46163] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:24:26,047 [Listener at localhost/46163] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:24:26,048 [Listener at localhost/46163] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:24:26,048 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 0
2020-12-03 07:24:26,049 [Listener at localhost/46163] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(341)) - DirectoryScanner: shutdown has been called, but periodic scanner not started
2020-12-03 07:24:26,049 [Listener at localhost/46163] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 43238
2020-12-03 07:24:26,049 [Listener at localhost/46163] WARN  util.MBeans (MBeans.java:unregister(145)) - Error unregistering Hadoop:service=DataNode,name=FSDatasetState-31491a6f-508d-45fd-8589-10a6a9373e21
javax.management.InstanceNotFoundException: Hadoop:service=DataNode,name=FSDatasetState-31491a6f-508d-45fd-8589-10a6a9373e21
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1095)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.exclusiveUnregisterMBean(DefaultMBeanServerInterceptor.java:427)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.unregisterMBean(DefaultMBeanServerInterceptor.java:415)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.unregisterMBean(JmxMBeanServer.java:546)
	at org.apache.hadoop.metrics2.util.MBeans.unregister(MBeans.java:143)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.shutdown(FsDatasetImpl.java:2293)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdown(DataNode.java:2151)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdownDataNode(MiniDFSCluster.java:2099)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdownDataNodes(MiniDFSCluster.java:2089)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2068)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2042)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2035)
	at org.apache.hadoop.hdfs.TestFileChecksum.tearDown(TestFileChecksum.java:111)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:168)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-12-03 07:24:26,050 [Listener at localhost/46163] WARN  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(191)) - AsyncDiskService has already shut down.
2020-12-03 07:24:26,050 [Listener at localhost/46163] WARN  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(175)) - AsyncLazyPersistService has already shut down.
2020-12-03 07:24:26,051 [Listener at localhost/46163] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2020-12-03 07:24:26,052 [Listener at localhost/46163] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2020-12-03 07:24:26,053 [Listener at localhost/46163] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - DataNode metrics system shutdown complete.
2020-12-03 07:24:26,057 [Listener at localhost/46163] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:24:26,058 [Listener at localhost/46163] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:stopAndJoinNameNode(2130)) - Shutting down the namenode
2020-12-03 07:24:26,058 [Listener at localhost/46163] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1334)) - Stopping services started for active state
2020-12-03 07:24:26,059 [Listener at localhost/46163] INFO  namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1410)) - Ending log segment 1, 36
2020-12-03 07:24:26,059 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@3c435123] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4198)) - LazyPersistFileScrubber was interrupted, exiting
2020-12-03 07:24:26,059 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@150ab4ed] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4107)) - NameNodeEditLogRoller was interrupted, exiting
2020-12-03 07:24:26,059 [Listener at localhost/46163] INFO  namenode.FSEditLog (FSEditLog.java:printStatistics(778)) - Number of transactions: 37 Total time for transactions(ms): 22 Number of transactions batched in Syncs: 10 Number of syncs: 28 SyncTimes(ms): 59 3 
2020-12-03 07:24:26,061 [Listener at localhost/46163] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(145)) - Finalizing edits file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_inprogress_0000000000000000001 -> /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000001-0000000000000000037
2020-12-03 07:24:26,090 [Listener at localhost/46163] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(145)) - Finalizing edits file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_inprogress_0000000000000000001 -> /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000001-0000000000000000037
2020-12-03 07:24:26,145 [FSEditLogAsync] INFO  namenode.FSEditLog (FSEditLogAsync.java:run(253)) - FSEditLogAsync was interrupted, exiting
2020-12-03 07:24:26,146 [CacheReplicationMonitor(25655767)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2020-12-03 07:24:26,161 [Listener at localhost/46163] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 38816
2020-12-03 07:24:26,162 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:24:26,165 [RedundancyMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4687)) - Stopping RedundancyMonitor.
2020-12-03 07:24:26,168 [StorageInfoMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4722)) - Stopping thread.
2020-12-03 07:24:26,165 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:24:26,205 [Listener at localhost/46163] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1334)) - Stopping services started for active state
2020-12-03 07:24:26,206 [Listener at localhost/46163] INFO  namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1434)) - Stopping services started for standby state
2020-12-03 07:24:26,247 [Listener at localhost/46163] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@1c852c0f{/,null,UNAVAILABLE}{/hdfs}
2020-12-03 07:24:26,249 [Listener at localhost/46163] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@3b79fd76{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:24:26,249 [Listener at localhost/46163] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@7dc19a70{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:24:26,250 [Listener at localhost/46163] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@3e0e1046{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
msx-rc 1
