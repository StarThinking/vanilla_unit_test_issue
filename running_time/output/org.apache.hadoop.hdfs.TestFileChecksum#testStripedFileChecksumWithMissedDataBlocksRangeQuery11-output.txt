2020-12-03 07:19:28,216 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(493)) - starting cluster: numNameNodes=1, numDataNodes=11
Formatting using clusterid: testClusterID
2020-12-03 07:19:29,115 [main] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(229)) - Edit logging is async:true
2020-12-03 07:19:29,131 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(755)) - KeyProvider: null
2020-12-03 07:19:29,132 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(123)) - fsLock is fair: true
2020-12-03 07:19:29,133 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(141)) - Detailed lock hold time metrics enabled: false
2020-12-03 07:19:29,163 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(780)) - fsOwner             = root (auth:SIMPLE)
2020-12-03 07:19:29,163 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(781)) - supergroup          = supergroup
2020-12-03 07:19:29,164 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(782)) - isPermissionEnabled = true
2020-12-03 07:19:29,174 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(793)) - HA Enabled: false
2020-12-03 07:19:29,248 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:29,254 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - hadoop.configured.node.mapping is deprecated. Instead, use net.topology.configured.node.mapping
2020-12-03 07:19:29,254 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(303)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-12-03 07:19:29,255 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(311)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-12-03 07:19:29,261 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-12-03 07:19:29,261 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Dec 03 07:19:29
2020-12-03 07:19:29,264 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-12-03 07:19:29,266 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:19:29,268 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.9 GB = 39.3 MB
2020-12-03 07:19:29,268 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-12-03 07:19:29,290 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createSPSManager(5183)) - Storage policy satisfier is disabled
2020-12-03 07:19:29,290 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(595)) - dfs.block.access.token.enable = true
2020-12-03 07:19:29,291 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(617)) - dfs.block.access.key.update.interval=600 min(s), dfs.block.access.token.lifetime=600 min(s), dfs.encrypt.data.transfer.algorithm=null
2020-12-03 07:19:29,317 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-12-03 07:19:29,318 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-12-03 07:19:29,318 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-12-03 07:19:29,318 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-12-03 07:19:29,320 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(581)) - defaultReplication         = 3
2020-12-03 07:19:29,320 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(582)) - maxReplication             = 512
2020-12-03 07:19:29,320 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(583)) - minReplication             = 1
2020-12-03 07:19:29,321 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(584)) - maxReplicationStreams      = 0
2020-12-03 07:19:29,321 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(585)) - redundancyRecheckInterval  = 3000ms
2020-12-03 07:19:29,321 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(586)) - encryptDataTransfer        = false
2020-12-03 07:19:29,321 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(587)) - maxNumBlocksToLog          = 1000
2020-12-03 07:19:29,362 [main] INFO  namenode.FSDirectory (SerialNumberManager.java:<clinit>(51)) - GLOBAL serial map: bits=29 maxEntries=536870911
2020-12-03 07:19:29,362 [main] INFO  namenode.FSDirectory (SerialNumberManager.java:<clinit>(51)) - USER serial map: bits=24 maxEntries=16777215
2020-12-03 07:19:29,363 [main] INFO  namenode.FSDirectory (SerialNumberManager.java:<clinit>(51)) - GROUP serial map: bits=24 maxEntries=16777215
2020-12-03 07:19:29,363 [main] INFO  namenode.FSDirectory (SerialNumberManager.java:<clinit>(51)) - XATTR serial map: bits=24 maxEntries=16777215
2020-12-03 07:19:29,382 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-12-03 07:19:29,383 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:19:29,384 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.9 GB = 19.6 MB
2020-12-03 07:19:29,384 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-12-03 07:19:29,392 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(290)) - ACLs enabled? false
2020-12-03 07:19:29,392 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(294)) - POSIX ACL inheritance enabled? true
2020-12-03 07:19:29,392 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(298)) - XAttrs enabled? true
2020-12-03 07:19:29,393 [main] INFO  namenode.NameNode (FSDirectory.java:<init>(362)) - Caching file names occurring more than 10 times
2020-12-03 07:19:29,399 [main] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-12-03 07:19:29,402 [main] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-12-03 07:19:29,409 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-12-03 07:19:29,409 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:19:29,410 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.9 GB = 4.9 MB
2020-12-03 07:19:29,411 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-12-03 07:19:29,421 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-12-03 07:19:29,422 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-12-03 07:19:29,422 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-12-03 07:19:29,429 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(996)) - Retry cache on namenode is enabled
2020-12-03 07:19:29,429 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(1004)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-12-03 07:19:29,433 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-12-03 07:19:29,434 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:19:29,435 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.9 GB = 603.0 KB
2020-12-03 07:19:29,435 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-12-03 07:19:29,477 [main] INFO  namenode.FSImage (FSImage.java:format(185)) - Allocated new BlockPoolId: BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:29,578 [main] INFO  common.Storage (NNStorage.java:format(595)) - Storage directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1 has been successfully formatted.
2020-12-03 07:19:29,772 [main] INFO  common.Storage (NNStorage.java:format(595)) - Storage directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2 has been successfully formatted.
2020-12-03 07:19:29,809 [FSImageSaver for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(512)) - Saving image file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage.ckpt_0000000000000000000 using no compression
2020-12-03 07:19:29,809 [FSImageSaver for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(512)) - Saving image file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/fsimage.ckpt_0000000000000000000 using no compression
2020-12-03 07:19:29,979 [FSImageSaver for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(516)) - Image file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .
2020-12-03 07:19:29,979 [FSImageSaver for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(516)) - Image file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .
2020-12-03 07:19:30,028 [main] INFO  namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:getImageTxIdToRetain(203)) - Going to retain 1 images with txid >= 0
2020-12-03 07:19:30,032 [main] INFO  namenode.NameNode (NameNode.java:createNameNode(1632)) - createNameNode []
2020-12-03 07:19:30,591 [main] INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(118)) - Loaded properties from hadoop-metrics2.properties
2020-12-03 07:19:30,730 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 0 second(s).
2020-12-03 07:19:30,731 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2020-12-03 07:19:30,763 [main] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://127.0.0.1:0
2020-12-03 07:19:30,813 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@7bab3f1a] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:19:30,835 [main] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1641)) - Starting Web-server for hdfs at: http://localhost:0
2020-12-03 07:19:30,844 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:30,871 [main] INFO  util.log (Log.java:initialized(192)) - Logging initialized @3743ms
2020-12-03 07:19:31,049 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:19:31,053 [main] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-12-03 07:19:31,054 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:31,063 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:19:31,065 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-12-03 07:19:31,066 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:19:31,066 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:19:31,107 [main] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(102)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-12-03 07:19:31,108 [main] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(806)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-12-03 07:19:31,120 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 36009
2020-12-03 07:19:31,123 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:19:31,201 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@1b66c0fb{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:19:31,203 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@24c1b2d2{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:19:31,270 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@62833051{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-12-03 07:19:31,283 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@62ff1e74{HTTP/1.1,[http/1.1]}{localhost:36009}
2020-12-03 07:19:31,284 [main] INFO  server.Server (Server.java:doStart(419)) - Started @4156ms
2020-12-03 07:19:31,296 [main] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(229)) - Edit logging is async:true
2020-12-03 07:19:31,297 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(755)) - KeyProvider: null
2020-12-03 07:19:31,297 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(123)) - fsLock is fair: true
2020-12-03 07:19:31,298 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(141)) - Detailed lock hold time metrics enabled: false
2020-12-03 07:19:31,298 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(780)) - fsOwner             = root (auth:SIMPLE)
2020-12-03 07:19:31,299 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(781)) - supergroup          = supergroup
2020-12-03 07:19:31,299 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(782)) - isPermissionEnabled = true
2020-12-03 07:19:31,300 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(793)) - HA Enabled: false
2020-12-03 07:19:31,301 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:31,302 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(303)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-12-03 07:19:31,302 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(311)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-12-03 07:19:31,303 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-12-03 07:19:31,304 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Dec 03 07:19:31
2020-12-03 07:19:31,304 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-12-03 07:19:31,305 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:19:31,305 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.8 GB = 36.4 MB
2020-12-03 07:19:31,305 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-12-03 07:19:31,310 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createSPSManager(5183)) - Storage policy satisfier is disabled
2020-12-03 07:19:31,310 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(595)) - dfs.block.access.token.enable = true
2020-12-03 07:19:31,310 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(617)) - dfs.block.access.key.update.interval=600 min(s), dfs.block.access.token.lifetime=600 min(s), dfs.encrypt.data.transfer.algorithm=null
2020-12-03 07:19:31,312 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-12-03 07:19:31,312 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-12-03 07:19:31,313 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-12-03 07:19:31,313 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-12-03 07:19:31,313 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(581)) - defaultReplication         = 3
2020-12-03 07:19:31,314 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(582)) - maxReplication             = 512
2020-12-03 07:19:31,314 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(583)) - minReplication             = 1
2020-12-03 07:19:31,314 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(584)) - maxReplicationStreams      = 0
2020-12-03 07:19:31,315 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(585)) - redundancyRecheckInterval  = 3000ms
2020-12-03 07:19:31,315 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(586)) - encryptDataTransfer        = false
2020-12-03 07:19:31,315 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(587)) - maxNumBlocksToLog          = 1000
2020-12-03 07:19:31,316 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-12-03 07:19:31,316 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:19:31,317 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.8 GB = 18.2 MB
2020-12-03 07:19:31,317 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-12-03 07:19:31,320 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(290)) - ACLs enabled? false
2020-12-03 07:19:31,320 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(294)) - POSIX ACL inheritance enabled? true
2020-12-03 07:19:31,320 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(298)) - XAttrs enabled? true
2020-12-03 07:19:31,321 [main] INFO  namenode.NameNode (FSDirectory.java:<init>(362)) - Caching file names occurring more than 10 times
2020-12-03 07:19:31,321 [main] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-12-03 07:19:31,321 [main] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-12-03 07:19:31,321 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-12-03 07:19:31,322 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:19:31,322 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.8 GB = 4.6 MB
2020-12-03 07:19:31,323 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-12-03 07:19:31,324 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-12-03 07:19:31,324 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-12-03 07:19:31,325 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-12-03 07:19:31,325 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(996)) - Retry cache on namenode is enabled
2020-12-03 07:19:31,326 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(1004)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-12-03 07:19:31,326 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-12-03 07:19:31,326 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:19:31,326 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.8 GB = 559.3 KB
2020-12-03 07:19:31,327 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-12-03 07:19:31,365 [main] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/in_use.lock acquired by nodename 275@4c2c7c544001
2020-12-03 07:19:31,391 [main] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/in_use.lock acquired by nodename 275@4c2c7c544001
2020-12-03 07:19:31,396 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(428)) - Recovering unfinalized segments in /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current
2020-12-03 07:19:31,396 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(428)) - Recovering unfinalized segments in /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current
2020-12-03 07:19:31,397 [main] INFO  namenode.FSImage (FSImage.java:loadFSImage(733)) - No edit log streams selected.
2020-12-03 07:19:31,398 [main] INFO  namenode.FSImage (FSImage.java:loadFSImageFile(797)) - Planning to load image: FSImageFile(file=/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2020-12-03 07:19:31,459 [main] INFO  namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(234)) - Loading 1 INodes.
2020-12-03 07:19:31,469 [main] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(246)) - Loaded FSImage in 0 seconds.
2020-12-03 07:19:31,470 [main] INFO  namenode.FSImage (FSImage.java:loadFSImage(978)) - Loaded image for txid 0 from /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000000
2020-12-03 07:19:31,477 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1110)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-12-03 07:19:31,481 [main] INFO  namenode.FSEditLog (FSEditLog.java:startLogSegment(1365)) - Starting log segment at 1
2020-12-03 07:19:31,617 [main] INFO  namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2020-12-03 07:19:31,618 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(727)) - Finished loading FSImage in 289 msecs
2020-12-03 07:19:31,839 [main] INFO  namenode.NameNode (NameNodeRpcServer.java:<init>(448)) - RPC server is binding to localhost:0
2020-12-03 07:19:31,889 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:19:31,904 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:19:32,250 [Listener at localhost/35332] INFO  namenode.NameNode (NameNode.java:initialize(722)) - Clients are to use localhost:35332 to access this namenode/service.
2020-12-03 07:19:32,253 [Listener at localhost/35332] INFO  namenode.FSNamesystem (FSNamesystem.java:registerMBean(5090)) - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2020-12-03 07:19:32,370 [Listener at localhost/35332] INFO  namenode.LeaseManager (LeaseManager.java:getNumUnderConstructionBlocks(171)) - Number of blocks under construction: 0
2020-12-03 07:19:32,390 [Listener at localhost/35332] INFO  blockmanagement.BlockManager (BlockManager.java:initializeReplQueues(4922)) - initializing replication queues
2020-12-03 07:19:32,391 [org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager$Monitor@52e7a6b2] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:updateKeys(263)) - Updating block keys
2020-12-03 07:19:32,391 [Listener at localhost/35332] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(400)) - STATE* Leaving safe mode after 0 secs
2020-12-03 07:19:32,396 [Listener at localhost/35332] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(406)) - STATE* Network topology has 0 racks and 0 datanodes
2020-12-03 07:19:32,396 [Listener at localhost/35332] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(408)) - STATE* UnderReplicatedBlocks has 0 blocks
2020-12-03 07:19:32,404 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3585)) - Total number of blocks            = 0
2020-12-03 07:19:32,405 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3586)) - Number of invalid blocks          = 0
2020-12-03 07:19:32,405 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3587)) - Number of under-replicated blocks = 0
2020-12-03 07:19:32,405 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3588)) - Number of  over-replicated blocks = 0
2020-12-03 07:19:32,405 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3590)) - Number of blocks being written    = 0
2020-12-03 07:19:32,405 [Reconstruction Queue Initializer] INFO  hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(3593)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 7 msec
2020-12-03 07:19:32,493 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:19:32,495 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:19:32,503 [Listener at localhost/35332] INFO  namenode.NameNode (NameNode.java:startCommonServices(828)) - NameNode RPC up at: localhost/127.0.0.1:35332
2020-12-03 07:19:32,519 [Listener at localhost/35332] INFO  namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1222)) - Starting services required for active state
2020-12-03 07:19:32,519 [Listener at localhost/35332] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(777)) - Initializing quota with 4 thread(s)
2020-12-03 07:19:32,531 [Listener at localhost/35332] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(786)) - Quota initialization completed in 11 milliseconds
name space=1
storage space=0
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2020-12-03 07:19:32,550 [Listener at localhost/35332] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1,[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:19:32,556 [CacheReplicationMonitor(609079010)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-12-03 07:19:32,698 [Listener at localhost/35332] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:19:32,719 [Listener at localhost/35332] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:19:32,767 [Listener at localhost/35332] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:19:32,773 [Listener at localhost/35332] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:32,776 [Listener at localhost/35332] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:19:32,781 [Listener at localhost/35332] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:19:32,783 [Listener at localhost/35332] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:32,791 [Listener at localhost/35332] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:19:32,802 [Listener at localhost/35332] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:39913
2020-12-03 07:19:32,808 [Listener at localhost/35332] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:19:32,808 [Listener at localhost/35332] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:19:32,843 [Listener at localhost/35332] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:32,853 [Listener at localhost/35332] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:19:32,860 [Listener at localhost/35332] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:19:32,860 [Listener at localhost/35332] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:32,864 [Listener at localhost/35332] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:19:32,888 [Listener at localhost/35332] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:19:32,892 [Listener at localhost/35332] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:19:32,893 [Listener at localhost/35332] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:19:32,909 [Listener at localhost/35332] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 42961
2020-12-03 07:19:32,910 [Listener at localhost/35332] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:19:32,923 [Listener at localhost/35332] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@545de5a4{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:19:32,924 [Listener at localhost/35332] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@ab7a938{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:19:32,936 [Listener at localhost/35332] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@1536602f{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:19:32,937 [Listener at localhost/35332] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@4ebea12c{HTTP/1.1,[http/1.1]}{localhost:42961}
2020-12-03 07:19:32,938 [Listener at localhost/35332] INFO  server.Server (Server.java:doStart(419)) - Started @5810ms
2020-12-03 07:19:33,508 [Listener at localhost/35332] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:45738
2020-12-03 07:19:33,511 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@641856] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:19:33,511 [Listener at localhost/35332] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:19:33,512 [Listener at localhost/35332] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:19:33,942 [Listener at localhost/35332] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:19:33,964 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:19:33,986 [Listener at localhost/37908] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:37908
2020-12-03 07:19:34,009 [Listener at localhost/37908] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:19:34,012 [Listener at localhost/37908] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:19:34,050 [Thread-59] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:35332 starting to offer service
2020-12-03 07:19:34,063 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:19:34,063 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:19:34,083 [Listener at localhost/37908] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 1 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3,[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:19:34,088 [Listener at localhost/37908] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
2020-12-03 07:19:34,096 [Listener at localhost/37908] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:19:34,109 [Listener at localhost/37908] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:19:34,110 [Listener at localhost/37908] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:34,110 [Listener at localhost/37908] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:19:34,111 [Listener at localhost/37908] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:19:34,113 [Listener at localhost/37908] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:34,114 [Listener at localhost/37908] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:19:34,139 [Listener at localhost/37908] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:37110
2020-12-03 07:19:34,140 [Listener at localhost/37908] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:19:34,140 [Listener at localhost/37908] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:19:34,143 [Listener at localhost/37908] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:34,148 [Listener at localhost/37908] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:19:34,150 [Listener at localhost/37908] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:19:34,151 [Listener at localhost/37908] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:34,154 [Listener at localhost/37908] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:19:34,162 [Listener at localhost/37908] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:19:34,163 [Listener at localhost/37908] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:19:34,163 [Listener at localhost/37908] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:19:34,165 [Listener at localhost/37908] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 43912
2020-12-03 07:19:34,165 [Listener at localhost/37908] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:19:34,199 [Listener at localhost/37908] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@5d8445d7{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:19:34,201 [Listener at localhost/37908] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@384fc774{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:19:34,212 [Listener at localhost/37908] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@241a53ef{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:19:34,222 [Listener at localhost/37908] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@344344fa{HTTP/1.1,[http/1.1]}{localhost:43912}
2020-12-03 07:19:34,224 [Listener at localhost/37908] INFO  server.Server (Server.java:doStart(419)) - Started @7096ms
2020-12-03 07:19:34,323 [Listener at localhost/37908] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:40898
2020-12-03 07:19:34,325 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@70e659aa] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:19:34,326 [Listener at localhost/37908] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:19:34,326 [Listener at localhost/37908] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:19:34,327 [Listener at localhost/37908] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:19:34,328 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:19:34,338 [Listener at localhost/33076] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:33076
2020-12-03 07:19:34,356 [Listener at localhost/33076] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:19:34,357 [Listener at localhost/33076] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:19:34,358 [Thread-83] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:35332 starting to offer service
2020-12-03 07:19:34,359 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:19:34,362 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:19:34,379 [Listener at localhost/33076] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 2 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5,[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6
2020-12-03 07:19:34,383 [Listener at localhost/33076] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5
2020-12-03 07:19:34,384 [Listener at localhost/33076] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6
2020-12-03 07:19:34,387 [Listener at localhost/33076] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:19:34,399 [Listener at localhost/33076] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:34,399 [Listener at localhost/33076] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:19:34,400 [Listener at localhost/33076] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:19:34,401 [Listener at localhost/33076] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:34,401 [Listener at localhost/33076] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:19:34,402 [Listener at localhost/33076] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:44555
2020-12-03 07:19:34,403 [Listener at localhost/33076] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:19:34,403 [Listener at localhost/33076] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:19:34,406 [Listener at localhost/33076] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:34,414 [Listener at localhost/33076] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:19:34,425 [Listener at localhost/33076] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:19:34,426 [Listener at localhost/33076] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:34,434 [Listener at localhost/33076] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:19:34,444 [Listener at localhost/33076] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:19:34,445 [Listener at localhost/33076] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:19:34,445 [Listener at localhost/33076] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:19:34,480 [Listener at localhost/33076] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 43982
2020-12-03 07:19:34,480 [Listener at localhost/33076] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:19:34,500 [Listener at localhost/33076] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@5ed190be{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:19:34,505 [Listener at localhost/33076] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@5bbc9f97{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:19:34,516 [Listener at localhost/33076] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@20312893{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:19:34,517 [Listener at localhost/33076] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@70eecdc2{HTTP/1.1,[http/1.1]}{localhost:43982}
2020-12-03 07:19:34,518 [Listener at localhost/33076] INFO  server.Server (Server.java:doStart(419)) - Started @7390ms
2020-12-03 07:19:34,551 [Listener at localhost/33076] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:45501
2020-12-03 07:19:34,551 [Listener at localhost/33076] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:19:34,552 [Listener at localhost/33076] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:19:34,552 [Listener at localhost/33076] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:19:34,551 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@7db0565c] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:19:34,553 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:19:34,560 [Listener at localhost/43159] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:43159
2020-12-03 07:19:34,569 [Listener at localhost/43159] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:19:34,569 [Listener at localhost/43159] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:19:34,575 [Thread-106] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:35332 starting to offer service
2020-12-03 07:19:34,580 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:19:34,582 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:19:34,597 [Listener at localhost/43159] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 3 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7,[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8
2020-12-03 07:19:34,599 [Listener at localhost/43159] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7
2020-12-03 07:19:34,599 [Listener at localhost/43159] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8
2020-12-03 07:19:34,603 [Listener at localhost/43159] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:19:34,603 [Listener at localhost/43159] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:34,604 [Listener at localhost/43159] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:19:34,605 [Listener at localhost/43159] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:19:34,605 [Listener at localhost/43159] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:34,605 [Listener at localhost/43159] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:19:34,606 [Listener at localhost/43159] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:44715
2020-12-03 07:19:34,607 [Listener at localhost/43159] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:19:34,607 [Listener at localhost/43159] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:19:34,609 [Listener at localhost/43159] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:34,612 [Listener at localhost/43159] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:19:34,613 [Listener at localhost/43159] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:19:34,613 [Listener at localhost/43159] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:34,619 [Listener at localhost/43159] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:19:34,620 [Listener at localhost/43159] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:19:34,621 [Listener at localhost/43159] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:19:34,621 [Listener at localhost/43159] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:19:34,622 [Listener at localhost/43159] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 44625
2020-12-03 07:19:34,623 [Listener at localhost/43159] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:19:34,625 [Listener at localhost/43159] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@433ffad1{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:19:34,628 [Listener at localhost/43159] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@2575f671{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:19:34,641 [Listener at localhost/43159] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@238b521e{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:19:34,647 [Listener at localhost/43159] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@6b2c860{HTTP/1.1,[http/1.1]}{localhost:44625}
2020-12-03 07:19:34,648 [Listener at localhost/43159] INFO  server.Server (Server.java:doStart(419)) - Started @7519ms
2020-12-03 07:19:34,736 [Listener at localhost/43159] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:46372
2020-12-03 07:19:34,737 [Listener at localhost/43159] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:19:34,737 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@3e2fc448] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:19:34,737 [Listener at localhost/43159] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:19:34,755 [Listener at localhost/43159] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:19:34,756 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:19:34,768 [Listener at localhost/36006] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:36006
2020-12-03 07:19:34,792 [Thread-83] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:35332
2020-12-03 07:19:34,796 [Thread-59] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:35332
2020-12-03 07:19:34,796 [Thread-106] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:35332
2020-12-03 07:19:34,798 [Thread-83] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:19:34,798 [Thread-59] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:19:34,799 [Thread-106] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:19:34,800 [Listener at localhost/36006] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:19:34,800 [Listener at localhost/36006] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:19:34,801 [Thread-128] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:35332 starting to offer service
2020-12-03 07:19:34,807 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:19:34,808 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:19:34,811 [Thread-128] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:35332
2020-12-03 07:19:34,811 [Thread-128] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:19:34,820 [Listener at localhost/36006] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 4 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9,[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10
2020-12-03 07:19:34,822 [Listener at localhost/36006] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9
2020-12-03 07:19:34,823 [Listener at localhost/36006] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10
2020-12-03 07:19:34,824 [Listener at localhost/36006] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:19:34,825 [Listener at localhost/36006] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:34,825 [Listener at localhost/36006] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:19:34,825 [Listener at localhost/36006] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:19:34,826 [Listener at localhost/36006] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:34,826 [Listener at localhost/36006] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:19:34,827 [Listener at localhost/36006] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:46438
2020-12-03 07:19:34,828 [Listener at localhost/36006] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:19:34,828 [Listener at localhost/36006] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:19:34,829 [Listener at localhost/36006] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:34,831 [Listener at localhost/36006] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:19:34,832 [Listener at localhost/36006] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:19:34,833 [Listener at localhost/36006] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:34,835 [Listener at localhost/36006] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:19:34,836 [Listener at localhost/36006] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:19:34,836 [Listener at localhost/36006] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:19:34,837 [Listener at localhost/36006] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:19:34,838 [Listener at localhost/36006] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 45389
2020-12-03 07:19:34,838 [Listener at localhost/36006] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:19:34,842 [Listener at localhost/36006] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@51c929ae{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:19:34,843 [Listener at localhost/36006] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@29d2d081{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:19:34,853 [Listener at localhost/36006] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@ea27e34{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:19:34,854 [Listener at localhost/36006] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@33a2499c{HTTP/1.1,[http/1.1]}{localhost:45389}
2020-12-03 07:19:34,855 [Listener at localhost/36006] INFO  server.Server (Server.java:doStart(419)) - Started @7727ms
2020-12-03 07:19:34,861 [Thread-83] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/in_use.lock acquired by nodename 275@4c2c7c544001
2020-12-03 07:19:34,861 [Thread-106] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/in_use.lock acquired by nodename 275@4c2c7c544001
2020-12-03 07:19:34,862 [Thread-83] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3 is not formatted for namespace 352815665. Formatting...
2020-12-03 07:19:34,862 [Thread-106] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5 is not formatted for namespace 352815665. Formatting...
2020-12-03 07:19:34,865 [Thread-59] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/in_use.lock acquired by nodename 275@4c2c7c544001
2020-12-03 07:19:34,865 [Thread-83] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-f8fbbb47-9ffb-40e3-9817-22e567ac2d28 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3 
2020-12-03 07:19:34,866 [Thread-59] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1 is not formatted for namespace 352815665. Formatting...
2020-12-03 07:19:34,866 [Thread-59] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-91ea1090-f0db-4472-ad91-faa0d9302fbf for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1 
2020-12-03 07:19:34,866 [Thread-106] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-27376982-648a-435c-9530-844533c3fc3c for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5 
2020-12-03 07:19:34,876 [Listener at localhost/36006] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:41065
2020-12-03 07:19:34,877 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@33c2bd] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:19:34,877 [Listener at localhost/36006] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:19:34,877 [Listener at localhost/36006] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:19:34,878 [Listener at localhost/36006] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:19:34,879 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:19:34,885 [Listener at localhost/39004] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:39004
2020-12-03 07:19:34,901 [Listener at localhost/39004] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:19:34,902 [Listener at localhost/39004] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:19:34,903 [Thread-150] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:35332 starting to offer service
2020-12-03 07:19:34,918 [Thread-128] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7/in_use.lock acquired by nodename 275@4c2c7c544001
2020-12-03 07:19:34,919 [Thread-128] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7 is not formatted for namespace 352815665. Formatting...
2020-12-03 07:19:34,919 [Thread-128] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-2e1daa68-afa3-4e1f-a710-a40da63cedfc for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7 
2020-12-03 07:19:34,926 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:19:34,926 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:19:34,947 [Thread-150] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:35332
2020-12-03 07:19:34,948 [Thread-150] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:19:34,949 [Listener at localhost/39004] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 5 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11,[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12
2020-12-03 07:19:34,951 [Listener at localhost/39004] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11
2020-12-03 07:19:34,968 [Listener at localhost/39004] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12
2020-12-03 07:19:34,983 [Listener at localhost/39004] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:19:34,984 [Listener at localhost/39004] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:34,984 [Listener at localhost/39004] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:19:34,985 [Listener at localhost/39004] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:19:34,985 [Listener at localhost/39004] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:34,985 [Listener at localhost/39004] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:19:34,987 [Listener at localhost/39004] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:38308
2020-12-03 07:19:34,987 [Listener at localhost/39004] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:19:34,987 [Listener at localhost/39004] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:19:34,988 [Listener at localhost/39004] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:34,990 [Listener at localhost/39004] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:19:34,991 [Listener at localhost/39004] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:19:34,991 [Listener at localhost/39004] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:35,005 [Listener at localhost/39004] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:19:35,006 [Listener at localhost/39004] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:19:35,006 [Listener at localhost/39004] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:19:35,007 [Listener at localhost/39004] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:19:35,008 [Listener at localhost/39004] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 35666
2020-12-03 07:19:35,008 [Listener at localhost/39004] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:19:35,010 [Listener at localhost/39004] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@79f227a9{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:19:35,011 [Listener at localhost/39004] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@50d68830{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:19:35,021 [Listener at localhost/39004] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@32c0915e{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:19:35,023 [Listener at localhost/39004] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@106faf11{HTTP/1.1,[http/1.1]}{localhost:35666}
2020-12-03 07:19:35,023 [Listener at localhost/39004] INFO  server.Server (Server.java:doStart(419)) - Started @7895ms
2020-12-03 07:19:35,042 [Listener at localhost/39004] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:34220
2020-12-03 07:19:35,042 [Listener at localhost/39004] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:19:35,042 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@26d10f2e] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:19:35,043 [Listener at localhost/39004] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:19:35,044 [Listener at localhost/39004] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:19:35,044 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:19:35,050 [Listener at localhost/37261] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:37261
2020-12-03 07:19:35,058 [Listener at localhost/37261] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:19:35,058 [Listener at localhost/37261] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:19:35,059 [Thread-172] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:35332 starting to offer service
2020-12-03 07:19:35,064 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:19:35,064 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:19:35,077 [Thread-172] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:35332
2020-12-03 07:19:35,078 [Listener at localhost/37261] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 6 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13,[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14
2020-12-03 07:19:35,078 [Thread-172] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:19:35,080 [Listener at localhost/37261] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13
2020-12-03 07:19:35,081 [Listener at localhost/37261] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14
2020-12-03 07:19:35,082 [Listener at localhost/37261] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:19:35,083 [Listener at localhost/37261] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:35,083 [Listener at localhost/37261] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:19:35,084 [Listener at localhost/37261] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:19:35,084 [Listener at localhost/37261] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:35,084 [Listener at localhost/37261] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:19:35,086 [Listener at localhost/37261] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:38503
2020-12-03 07:19:35,086 [Listener at localhost/37261] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:19:35,086 [Listener at localhost/37261] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:19:35,088 [Listener at localhost/37261] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:35,090 [Listener at localhost/37261] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:19:35,091 [Listener at localhost/37261] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:19:35,091 [Listener at localhost/37261] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:35,094 [Listener at localhost/37261] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:19:35,095 [Listener at localhost/37261] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:19:35,096 [Listener at localhost/37261] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:19:35,096 [Listener at localhost/37261] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:19:35,097 [Listener at localhost/37261] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 34511
2020-12-03 07:19:35,098 [Listener at localhost/37261] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:19:35,100 [Listener at localhost/37261] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@61a5b4ae{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:19:35,101 [Listener at localhost/37261] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@5b69fd74{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:19:35,132 [Thread-150] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9/in_use.lock acquired by nodename 275@4c2c7c544001
2020-12-03 07:19:35,132 [Thread-150] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9 is not formatted for namespace 352815665. Formatting...
2020-12-03 07:19:35,142 [Thread-150] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-ac18a139-2578-41ac-8803-da9d22cc545d for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9 
2020-12-03 07:19:35,150 [Listener at localhost/37261] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@df5f5c0{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:19:35,160 [Listener at localhost/37261] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@308a6984{HTTP/1.1,[http/1.1]}{localhost:34511}
2020-12-03 07:19:35,160 [Listener at localhost/37261] INFO  server.Server (Server.java:doStart(419)) - Started @8032ms
2020-12-03 07:19:35,181 [Listener at localhost/37261] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:37465
2020-12-03 07:19:35,181 [Listener at localhost/37261] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:19:35,181 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@7a34b7b8] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:19:35,182 [Listener at localhost/37261] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:19:35,182 [Listener at localhost/37261] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:19:35,183 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:19:35,194 [Listener at localhost/35031] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:35031
2020-12-03 07:19:35,199 [Listener at localhost/35031] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:19:35,200 [Listener at localhost/35031] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:19:35,200 [Thread-194] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:35332 starting to offer service
2020-12-03 07:19:35,202 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:19:35,202 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:19:35,207 [Thread-194] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:35332
2020-12-03 07:19:35,209 [Thread-194] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:19:35,210 [Listener at localhost/35031] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 7 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15,[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16
2020-12-03 07:19:35,212 [Listener at localhost/35031] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15
2020-12-03 07:19:35,212 [Listener at localhost/35031] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16
2020-12-03 07:19:35,213 [Listener at localhost/35031] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:19:35,214 [Listener at localhost/35031] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:35,215 [Listener at localhost/35031] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:19:35,215 [Listener at localhost/35031] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:19:35,215 [Listener at localhost/35031] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:35,216 [Listener at localhost/35031] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:19:35,217 [Listener at localhost/35031] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:34075
2020-12-03 07:19:35,218 [Listener at localhost/35031] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:19:35,218 [Listener at localhost/35031] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:19:35,219 [Listener at localhost/35031] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:35,223 [Listener at localhost/35031] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:19:35,224 [Listener at localhost/35031] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:19:35,225 [Listener at localhost/35031] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:35,227 [Listener at localhost/35031] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:19:35,228 [Listener at localhost/35031] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:19:35,228 [Listener at localhost/35031] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:19:35,229 [Listener at localhost/35031] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:19:35,230 [Listener at localhost/35031] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 40125
2020-12-03 07:19:35,230 [Listener at localhost/35031] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:19:35,232 [Listener at localhost/35031] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@60b85ba1{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:19:35,233 [Listener at localhost/35031] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@117632cf{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:19:35,243 [Listener at localhost/35031] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@6d2260db{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:19:35,245 [Listener at localhost/35031] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@1f2d2181{HTTP/1.1,[http/1.1]}{localhost:40125}
2020-12-03 07:19:35,245 [Listener at localhost/35031] INFO  server.Server (Server.java:doStart(419)) - Started @8117ms
2020-12-03 07:19:35,249 [Thread-172] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11/in_use.lock acquired by nodename 275@4c2c7c544001
2020-12-03 07:19:35,250 [Thread-172] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11 is not formatted for namespace 352815665. Formatting...
2020-12-03 07:19:35,256 [Thread-172] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-6660ac3c-7f95-4544-99d0-9a26ef3ef67b for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11 
2020-12-03 07:19:35,341 [Listener at localhost/35031] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:37609
2020-12-03 07:19:35,342 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@7ee55e70] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:19:35,342 [Listener at localhost/35031] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:19:35,342 [Listener at localhost/35031] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:19:35,343 [Listener at localhost/35031] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:19:35,343 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:19:35,348 [Listener at localhost/40394] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:40394
2020-12-03 07:19:35,352 [Thread-194] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13/in_use.lock acquired by nodename 275@4c2c7c544001
2020-12-03 07:19:35,353 [Thread-194] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13 is not formatted for namespace 352815665. Formatting...
2020-12-03 07:19:35,355 [Listener at localhost/40394] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:19:35,356 [Listener at localhost/40394] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:19:35,357 [Thread-216] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:35332 starting to offer service
2020-12-03 07:19:35,359 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:19:35,359 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:19:35,365 [Thread-216] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:35332
2020-12-03 07:19:35,366 [Thread-216] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:19:35,368 [Thread-194] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-34cdaddf-9d99-43fe-ac94-a2d410b1c52a for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13 
2020-12-03 07:19:35,369 [Listener at localhost/40394] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 8 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17,[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18
2020-12-03 07:19:35,371 [Listener at localhost/40394] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17
2020-12-03 07:19:35,371 [Listener at localhost/40394] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18
2020-12-03 07:19:35,373 [Listener at localhost/40394] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:19:35,374 [Listener at localhost/40394] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:35,374 [Listener at localhost/40394] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:19:35,375 [Listener at localhost/40394] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:19:35,375 [Listener at localhost/40394] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:35,376 [Listener at localhost/40394] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:19:35,376 [Listener at localhost/40394] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:43005
2020-12-03 07:19:35,376 [Listener at localhost/40394] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:19:35,377 [Listener at localhost/40394] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:19:35,378 [Listener at localhost/40394] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:35,380 [Listener at localhost/40394] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:19:35,380 [Listener at localhost/40394] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:19:35,381 [Listener at localhost/40394] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:35,383 [Listener at localhost/40394] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:19:35,384 [Listener at localhost/40394] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:19:35,384 [Listener at localhost/40394] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:19:35,385 [Listener at localhost/40394] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:19:35,386 [Listener at localhost/40394] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 39769
2020-12-03 07:19:35,386 [Listener at localhost/40394] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:19:35,390 [Listener at localhost/40394] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@5a6d5a8f{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:19:35,391 [Listener at localhost/40394] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@315ba14a{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:19:35,417 [Listener at localhost/40394] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@2237bada{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:19:35,419 [Listener at localhost/40394] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@77e2a6e2{HTTP/1.1,[http/1.1]}{localhost:39769}
2020-12-03 07:19:35,419 [Listener at localhost/40394] INFO  server.Server (Server.java:doStart(419)) - Started @8291ms
2020-12-03 07:19:35,459 [Listener at localhost/40394] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:33760
2020-12-03 07:19:35,460 [Listener at localhost/40394] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:19:35,460 [Listener at localhost/40394] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:19:35,461 [Listener at localhost/40394] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:19:35,462 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@199e4c2b] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:19:35,462 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:19:35,468 [Listener at localhost/45450] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:45450
2020-12-03 07:19:35,477 [Listener at localhost/45450] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:19:35,478 [Listener at localhost/45450] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:19:35,479 [Thread-238] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:35332 starting to offer service
2020-12-03 07:19:35,483 [Thread-106] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/in_use.lock acquired by nodename 275@4c2c7c544001
2020-12-03 07:19:35,488 [Thread-106] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6 is not formatted for namespace 352815665. Formatting...
2020-12-03 07:19:35,489 [Thread-83] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/in_use.lock acquired by nodename 275@4c2c7c544001
2020-12-03 07:19:35,489 [Thread-106] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-06a2036e-43cd-41f6-bb27-4153a9f538e8 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6 
2020-12-03 07:19:35,489 [Thread-83] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4 is not formatted for namespace 352815665. Formatting...
2020-12-03 07:19:35,492 [Thread-83] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-54f4be28-ebb1-40b8-abf0-7b39c47dce17 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4 
2020-12-03 07:19:35,500 [Thread-59] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/in_use.lock acquired by nodename 275@4c2c7c544001
2020-12-03 07:19:35,500 [Thread-59] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2 is not formatted for namespace 352815665. Formatting...
2020-12-03 07:19:35,501 [Thread-59] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-245d3653-f81a-4228-a6a5-480c618ed25b for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2 
2020-12-03 07:19:35,518 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:19:35,518 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:19:35,548 [Listener at localhost/45450] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 9 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19,[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20
2020-12-03 07:19:35,550 [Thread-238] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:35332
2020-12-03 07:19:35,551 [Thread-238] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:19:35,552 [Listener at localhost/45450] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19
2020-12-03 07:19:35,553 [Listener at localhost/45450] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20
2020-12-03 07:19:35,555 [Listener at localhost/45450] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:19:35,556 [Listener at localhost/45450] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:35,556 [Listener at localhost/45450] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:19:35,557 [Listener at localhost/45450] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:19:35,557 [Listener at localhost/45450] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:35,558 [Listener at localhost/45450] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:19:35,559 [Listener at localhost/45450] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:46845
2020-12-03 07:19:35,559 [Listener at localhost/45450] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:19:35,560 [Listener at localhost/45450] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:19:35,561 [Listener at localhost/45450] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:35,563 [Listener at localhost/45450] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:19:35,564 [Listener at localhost/45450] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:19:35,564 [Listener at localhost/45450] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:35,567 [Listener at localhost/45450] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:19:35,567 [Listener at localhost/45450] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:19:35,567 [Listener at localhost/45450] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:19:35,567 [Listener at localhost/45450] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:19:35,568 [Listener at localhost/45450] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 42735
2020-12-03 07:19:35,569 [Listener at localhost/45450] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:19:35,574 [Listener at localhost/45450] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@3773862a{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:19:35,575 [Listener at localhost/45450] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@589b028e{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:19:35,580 [Listener at localhost/45450] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@4d666b41{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:19:35,587 [Listener at localhost/45450] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@6594402a{HTTP/1.1,[http/1.1]}{localhost:42735}
2020-12-03 07:19:35,588 [Listener at localhost/45450] INFO  server.Server (Server.java:doStart(419)) - Started @8460ms
2020-12-03 07:19:35,608 [Listener at localhost/45450] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:43849
2020-12-03 07:19:35,608 [Thread-216] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15/in_use.lock acquired by nodename 275@4c2c7c544001
2020-12-03 07:19:35,608 [Thread-216] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15 is not formatted for namespace 352815665. Formatting...
2020-12-03 07:19:35,608 [Listener at localhost/45450] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:19:35,609 [Listener at localhost/45450] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:19:35,608 [Thread-128] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/in_use.lock acquired by nodename 275@4c2c7c544001
2020-12-03 07:19:35,609 [Listener at localhost/45450] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:19:35,609 [Thread-128] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8 is not formatted for namespace 352815665. Formatting...
2020-12-03 07:19:35,610 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@405325cf] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:19:35,610 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:19:35,612 [Thread-216] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-b23a0f3d-1578-4216-aa54-57c659e1100a for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15 
2020-12-03 07:19:35,615 [Thread-128] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-13901838-f2e7-4531-9885-e6639368ca38 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8 
2020-12-03 07:19:35,618 [Listener at localhost/37058] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:37058
2020-12-03 07:19:35,625 [Listener at localhost/37058] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:19:35,626 [Listener at localhost/37058] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:19:35,633 [Thread-260] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:35332 starting to offer service
2020-12-03 07:19:35,634 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:19:35,634 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:19:35,646 [Listener at localhost/37058] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 10 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21,[DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22
2020-12-03 07:19:35,648 [Listener at localhost/37058] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21
2020-12-03 07:19:35,649 [Listener at localhost/37058] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22
2020-12-03 07:19:35,650 [Listener at localhost/37058] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:19:35,652 [Listener at localhost/37058] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:35,652 [Listener at localhost/37058] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:19:35,652 [Listener at localhost/37058] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:19:35,652 [Listener at localhost/37058] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:19:35,653 [Thread-260] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:35332
2020-12-03 07:19:35,653 [Listener at localhost/37058] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:19:35,654 [Thread-260] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:19:35,654 [Listener at localhost/37058] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:41862
2020-12-03 07:19:35,655 [Listener at localhost/37058] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:19:35,655 [Listener at localhost/37058] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:19:35,656 [Listener at localhost/37058] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:35,658 [Listener at localhost/37058] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:19:35,659 [Listener at localhost/37058] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:19:35,660 [Listener at localhost/37058] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:19:35,662 [Listener at localhost/37058] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:19:35,662 [Listener at localhost/37058] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:19:35,663 [Listener at localhost/37058] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:19:35,663 [Listener at localhost/37058] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:19:35,664 [Listener at localhost/37058] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 38049
2020-12-03 07:19:35,664 [Listener at localhost/37058] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:19:35,666 [Listener at localhost/37058] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@742d4e15{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:19:35,667 [Listener at localhost/37058] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@50b1f030{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:19:35,673 [Listener at localhost/37058] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@169da7f2{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:19:35,675 [Listener at localhost/37058] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@3c1e23ff{HTTP/1.1,[http/1.1]}{localhost:38049}
2020-12-03 07:19:35,675 [Listener at localhost/37058] INFO  server.Server (Server.java:doStart(419)) - Started @8547ms
2020-12-03 07:19:35,694 [Listener at localhost/37058] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:45315
2020-12-03 07:19:35,695 [Listener at localhost/37058] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:19:35,695 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@60297f36] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:19:35,695 [Listener at localhost/37058] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:19:35,696 [Listener at localhost/37058] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:19:35,697 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:19:35,701 [Listener at localhost/34517] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:34517
2020-12-03 07:19:35,708 [Listener at localhost/34517] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:19:35,708 [Listener at localhost/34517] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:19:35,709 [Thread-282] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:35332 starting to offer service
2020-12-03 07:19:35,712 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:19:35,712 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:19:35,720 [Thread-238] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17/in_use.lock acquired by nodename 275@4c2c7c544001
2020-12-03 07:19:35,721 [Thread-238] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17 is not formatted for namespace 352815665. Formatting...
2020-12-03 07:19:35,724 [Thread-238] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-15a77805-6e7f-4aa2-9de6-622edf3907c2 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17 
2020-12-03 07:19:35,733 [Thread-282] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:35332
2020-12-03 07:19:35,734 [Thread-282] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:19:35,861 [Thread-260] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19/in_use.lock acquired by nodename 275@4c2c7c544001
2020-12-03 07:19:35,861 [Thread-150] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10/in_use.lock acquired by nodename 275@4c2c7c544001
2020-12-03 07:19:35,862 [Thread-260] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19 is not formatted for namespace 352815665. Formatting...
2020-12-03 07:19:35,863 [Thread-150] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10 is not formatted for namespace 352815665. Formatting...
2020-12-03 07:19:35,863 [Thread-260] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-97859208-57d6-4d0f-a458-1e4adaf0a78a for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19 
2020-12-03 07:19:35,863 [Thread-150] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-c43137fa-5025-44bc-ad04-ca38d42d45a5 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10 
2020-12-03 07:19:35,990 [Thread-172] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12/in_use.lock acquired by nodename 275@4c2c7c544001
2020-12-03 07:19:35,990 [Thread-172] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12 is not formatted for namespace 352815665. Formatting...
2020-12-03 07:19:35,991 [Thread-282] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21/in_use.lock acquired by nodename 275@4c2c7c544001
2020-12-03 07:19:35,991 [Thread-282] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21 is not formatted for namespace 352815665. Formatting...
2020-12-03 07:19:35,995 [Thread-282] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-129d8406-e71c-470b-9430-2dc52ce7b622 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21 
2020-12-03 07:19:35,999 [Thread-172] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-5c6220ab-363a-4023-bc0c-7b6f261c270f for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12 
2020-12-03 07:19:36,007 [Thread-59] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:36,007 [Thread-83] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:36,008 [Thread-59] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:36,008 [Thread-106] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:36,008 [Thread-83] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:36,008 [Thread-106] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:36,010 [Thread-59] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1 and block pool id BP-1367394141-172.17.0.2-1606979969461 is not formatted. Formatting ...
2020-12-03 07:19:36,010 [Thread-83] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3 and block pool id BP-1367394141-172.17.0.2-1606979969461 is not formatted. Formatting ...
2020-12-03 07:19:36,010 [Thread-106] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5 and block pool id BP-1367394141-172.17.0.2-1606979969461 is not formatted. Formatting ...
2020-12-03 07:19:36,010 [Thread-83] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1367394141-172.17.0.2-1606979969461 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1367394141-172.17.0.2-1606979969461/current
2020-12-03 07:19:36,010 [Thread-59] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1367394141-172.17.0.2-1606979969461 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1367394141-172.17.0.2-1606979969461/current
2020-12-03 07:19:36,010 [Thread-106] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1367394141-172.17.0.2-1606979969461 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-1367394141-172.17.0.2-1606979969461/current
2020-12-03 07:19:36,118 [Thread-194] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14/in_use.lock acquired by nodename 275@4c2c7c544001
2020-12-03 07:19:36,119 [Thread-194] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14 is not formatted for namespace 352815665. Formatting...
2020-12-03 07:19:36,128 [Thread-194] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-bf9cf6c5-0493-4ac0-8724-c46bd212b0fc for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14 
2020-12-03 07:19:36,135 [Thread-128] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:36,135 [Thread-128] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7/current/BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:36,136 [Thread-128] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7 and block pool id BP-1367394141-172.17.0.2-1606979969461 is not formatted. Formatting ...
2020-12-03 07:19:36,136 [Thread-128] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1367394141-172.17.0.2-1606979969461 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7/current/BP-1367394141-172.17.0.2-1606979969461/current
2020-12-03 07:19:36,342 [IPC Server handler 6 on default port 35332] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:36,352 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:36,352 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:36,374 [Thread-216] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16/in_use.lock acquired by nodename 275@4c2c7c544001
2020-12-03 07:19:36,375 [Thread-216] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16 is not formatted for namespace 352815665. Formatting...
2020-12-03 07:19:36,378 [Thread-216] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-c23cda38-630d-4441-b4d8-5b0ca6373351 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16 
2020-12-03 07:19:36,387 [Thread-150] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:36,388 [Thread-150] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9/current/BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:36,388 [Thread-150] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9 and block pool id BP-1367394141-172.17.0.2-1606979969461 is not formatted. Formatting ...
2020-12-03 07:19:36,388 [Thread-150] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1367394141-172.17.0.2-1606979969461 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9/current/BP-1367394141-172.17.0.2-1606979969461/current
2020-12-03 07:19:36,456 [IPC Server handler 4 on default port 35332] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:36,457 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:36,457 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:36,500 [Thread-238] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18/in_use.lock acquired by nodename 275@4c2c7c544001
2020-12-03 07:19:36,500 [Thread-238] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18 is not formatted for namespace 352815665. Formatting...
2020-12-03 07:19:36,502 [Thread-238] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-65ca39ad-f8ba-4de7-a267-6c125a4b507b for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18 
2020-12-03 07:19:36,510 [Thread-83] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:36,510 [Thread-106] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:36,510 [Thread-83] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:36,511 [Thread-59] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:36,511 [Thread-106] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/current/BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:36,511 [Thread-83] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4 and block pool id BP-1367394141-172.17.0.2-1606979969461 is not formatted. Formatting ...
2020-12-03 07:19:36,511 [Thread-59] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:36,511 [Thread-106] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6 and block pool id BP-1367394141-172.17.0.2-1606979969461 is not formatted. Formatting ...
2020-12-03 07:19:36,511 [Thread-83] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1367394141-172.17.0.2-1606979969461 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1367394141-172.17.0.2-1606979969461/current
2020-12-03 07:19:36,512 [Thread-106] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1367394141-172.17.0.2-1606979969461 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/current/BP-1367394141-172.17.0.2-1606979969461/current
2020-12-03 07:19:36,512 [Thread-59] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2 and block pool id BP-1367394141-172.17.0.2-1606979969461 is not formatted. Formatting ...
2020-12-03 07:19:36,512 [Thread-59] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1367394141-172.17.0.2-1606979969461 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1367394141-172.17.0.2-1606979969461/current
2020-12-03 07:19:36,513 [Thread-172] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:36,513 [Thread-172] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11/current/BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:36,514 [Thread-172] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11 and block pool id BP-1367394141-172.17.0.2-1606979969461 is not formatted. Formatting ...
2020-12-03 07:19:36,514 [Thread-172] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1367394141-172.17.0.2-1606979969461 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11/current/BP-1367394141-172.17.0.2-1606979969461/current
2020-12-03 07:19:36,560 [IPC Server handler 2 on default port 35332] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:36,561 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:36,561 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:36,623 [Thread-260] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20/in_use.lock acquired by nodename 275@4c2c7c544001
2020-12-03 07:19:36,623 [Thread-260] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20 is not formatted for namespace 352815665. Formatting...
2020-12-03 07:19:36,630 [Thread-260] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-5beec8c8-67b2-429e-b7bb-c700d530aa6a for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20 
2020-12-03 07:19:36,634 [Thread-128] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:36,635 [Thread-128] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/current/BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:36,635 [Thread-128] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8 and block pool id BP-1367394141-172.17.0.2-1606979969461 is not formatted. Formatting ...
2020-12-03 07:19:36,635 [Thread-128] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1367394141-172.17.0.2-1606979969461 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/current/BP-1367394141-172.17.0.2-1606979969461/current
2020-12-03 07:19:36,640 [Thread-194] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:36,641 [Thread-194] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13/current/BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:36,642 [Thread-194] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13 and block pool id BP-1367394141-172.17.0.2-1606979969461 is not formatted. Formatting ...
2020-12-03 07:19:36,642 [Thread-194] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1367394141-172.17.0.2-1606979969461 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13/current/BP-1367394141-172.17.0.2-1606979969461/current
2020-12-03 07:19:36,664 [IPC Server handler 0 on default port 35332] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:36,665 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:36,665 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:36,765 [Thread-282] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22/in_use.lock acquired by nodename 275@4c2c7c544001
2020-12-03 07:19:36,765 [Thread-282] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22 is not formatted for namespace 352815665. Formatting...
2020-12-03 07:19:36,768 [IPC Server handler 3 on default port 35332] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:36,769 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:36,770 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:36,771 [Thread-282] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-695468af-ff85-41de-a309-3e9cf958ef94 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22 
2020-12-03 07:19:36,872 [IPC Server handler 5 on default port 35332] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:36,873 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:36,873 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:36,899 [Thread-216] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:36,899 [Thread-216] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15/current/BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:36,900 [Thread-216] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15 and block pool id BP-1367394141-172.17.0.2-1606979969461 is not formatted. Formatting ...
2020-12-03 07:19:36,900 [Thread-216] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1367394141-172.17.0.2-1606979969461 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15/current/BP-1367394141-172.17.0.2-1606979969461/current
2020-12-03 07:19:36,901 [Thread-150] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:36,901 [Thread-150] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10/current/BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:36,901 [Thread-150] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10 and block pool id BP-1367394141-172.17.0.2-1606979969461 is not formatted. Formatting ...
2020-12-03 07:19:36,901 [Thread-150] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1367394141-172.17.0.2-1606979969461 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10/current/BP-1367394141-172.17.0.2-1606979969461/current
2020-12-03 07:19:36,976 [IPC Server handler 7 on default port 35332] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:36,977 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:36,977 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:37,026 [Thread-83] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=352815665;bpid=BP-1367394141-172.17.0.2-1606979969461;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=352815665;c=1606979969461;bpid=BP-1367394141-172.17.0.2-1606979969461;dnuuid=null
2020-12-03 07:19:37,039 [Thread-238] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:37,040 [Thread-106] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=352815665;bpid=BP-1367394141-172.17.0.2-1606979969461;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=352815665;c=1606979969461;bpid=BP-1367394141-172.17.0.2-1606979969461;dnuuid=null
2020-12-03 07:19:37,039 [Thread-59] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=352815665;bpid=BP-1367394141-172.17.0.2-1606979969461;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=352815665;c=1606979969461;bpid=BP-1367394141-172.17.0.2-1606979969461;dnuuid=null
2020-12-03 07:19:37,040 [Thread-238] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17/current/BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:37,040 [Thread-238] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17 and block pool id BP-1367394141-172.17.0.2-1606979969461 is not formatted. Formatting ...
2020-12-03 07:19:37,041 [Thread-238] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1367394141-172.17.0.2-1606979969461 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17/current/BP-1367394141-172.17.0.2-1606979969461/current
2020-12-03 07:19:37,041 [Thread-172] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:37,042 [Thread-172] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12/current/BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:37,042 [Thread-172] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12 and block pool id BP-1367394141-172.17.0.2-1606979969461 is not formatted. Formatting ...
2020-12-03 07:19:37,042 [Thread-172] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1367394141-172.17.0.2-1606979969461 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12/current/BP-1367394141-172.17.0.2-1606979969461/current
2020-12-03 07:19:37,080 [IPC Server handler 9 on default port 35332] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:37,081 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:37,081 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:37,171 [Thread-128] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=352815665;bpid=BP-1367394141-172.17.0.2-1606979969461;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=352815665;c=1606979969461;bpid=BP-1367394141-172.17.0.2-1606979969461;dnuuid=null
2020-12-03 07:19:37,175 [Thread-194] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:37,175 [Thread-260] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:37,176 [Thread-260] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19/current/BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:37,176 [Thread-260] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19 and block pool id BP-1367394141-172.17.0.2-1606979969461 is not formatted. Formatting ...
2020-12-03 07:19:37,176 [Thread-260] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1367394141-172.17.0.2-1606979969461 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19/current/BP-1367394141-172.17.0.2-1606979969461/current
2020-12-03 07:19:37,175 [Thread-194] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14/current/BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:37,176 [Thread-194] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14 and block pool id BP-1367394141-172.17.0.2-1606979969461 is not formatted. Formatting ...
2020-12-03 07:19:37,176 [Thread-194] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1367394141-172.17.0.2-1606979969461 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14/current/BP-1367394141-172.17.0.2-1606979969461/current
2020-12-03 07:19:37,190 [IPC Server handler 6 on default port 35332] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:37,191 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:37,191 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:37,294 [IPC Server handler 4 on default port 35332] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:37,295 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:37,295 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:37,359 [Thread-282] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:37,359 [Thread-282] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21/current/BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:37,359 [Thread-282] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21 and block pool id BP-1367394141-172.17.0.2-1606979969461 is not formatted. Formatting ...
2020-12-03 07:19:37,359 [Thread-282] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1367394141-172.17.0.2-1606979969461 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21/current/BP-1367394141-172.17.0.2-1606979969461/current
2020-12-03 07:19:37,397 [IPC Server handler 8 on default port 35332] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:37,398 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:37,399 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:37,469 [Thread-150] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=352815665;bpid=BP-1367394141-172.17.0.2-1606979969461;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=352815665;c=1606979969461;bpid=BP-1367394141-172.17.0.2-1606979969461;dnuuid=null
2020-12-03 07:19:37,477 [Thread-216] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:37,478 [Thread-216] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16/current/BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:37,478 [Thread-216] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16 and block pool id BP-1367394141-172.17.0.2-1606979969461 is not formatted. Formatting ...
2020-12-03 07:19:37,478 [Thread-216] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1367394141-172.17.0.2-1606979969461 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16/current/BP-1367394141-172.17.0.2-1606979969461/current
2020-12-03 07:19:37,501 [IPC Server handler 1 on default port 35332] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:37,502 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:37,503 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:37,605 [IPC Server handler 0 on default port 35332] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:37,606 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:37,606 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:37,613 [Thread-106] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID 811e904d-4b58-4d09-9a37-ecd8d80551dd
2020-12-03 07:19:37,613 [Thread-172] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=352815665;bpid=BP-1367394141-172.17.0.2-1606979969461;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=352815665;c=1606979969461;bpid=BP-1367394141-172.17.0.2-1606979969461;dnuuid=null
2020-12-03 07:19:37,613 [Thread-59] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID 69a597d2-2edd-4e23-9c38-3328eccbc91b
2020-12-03 07:19:37,617 [Thread-83] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID 6433c7f1-b9cb-4ed4-9404-85e3ec0e7233
2020-12-03 07:19:37,625 [Thread-238] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:37,626 [Thread-238] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18/current/BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:37,626 [Thread-238] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18 and block pool id BP-1367394141-172.17.0.2-1606979969461 is not formatted. Formatting ...
2020-12-03 07:19:37,626 [Thread-238] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1367394141-172.17.0.2-1606979969461 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18/current/BP-1367394141-172.17.0.2-1606979969461/current
2020-12-03 07:19:37,708 [IPC Server handler 3 on default port 35332] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:37,709 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:37,710 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:37,752 [Thread-106] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-27376982-648a-435c-9530-844533c3fc3c
2020-12-03 07:19:37,752 [Thread-83] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-f8fbbb47-9ffb-40e3-9817-22e567ac2d28
2020-12-03 07:19:37,753 [Thread-59] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-91ea1090-f0db-4472-ad91-faa0d9302fbf
2020-12-03 07:19:37,754 [Thread-83] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3, StorageType: DISK
2020-12-03 07:19:37,754 [Thread-106] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5, StorageType: DISK
2020-12-03 07:19:37,754 [Thread-59] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, StorageType: DISK
2020-12-03 07:19:37,756 [Thread-83] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-54f4be28-ebb1-40b8-abf0-7b39c47dce17
2020-12-03 07:19:37,757 [Thread-83] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, StorageType: DISK
2020-12-03 07:19:37,758 [Thread-59] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-245d3653-f81a-4228-a6a5-480c618ed25b
2020-12-03 07:19:37,758 [Thread-59] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, StorageType: DISK
2020-12-03 07:19:37,760 [Thread-106] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-06a2036e-43cd-41f6-bb27-4153a9f538e8
2020-12-03 07:19:37,760 [Thread-106] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6, StorageType: DISK
2020-12-03 07:19:37,764 [Thread-83] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:19:37,766 [Thread-59] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:19:37,766 [Thread-128] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID 72c80882-49c9-444a-982c-993baca6bb62
2020-12-03 07:19:37,767 [Thread-194] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=352815665;bpid=BP-1367394141-172.17.0.2-1606979969461;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=352815665;c=1606979969461;bpid=BP-1367394141-172.17.0.2-1606979969461;dnuuid=null
2020-12-03 07:19:37,767 [Thread-106] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:19:37,773 [Thread-128] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-2e1daa68-afa3-4e1f-a710-a40da63cedfc
2020-12-03 07:19:37,775 [Thread-128] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7, StorageType: DISK
2020-12-03 07:19:37,775 [Thread-59] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:19:37,771 [Thread-83] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
2020-12-03 07:19:37,771 [Thread-106] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5
2020-12-03 07:19:37,784 [Thread-128] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-13901838-f2e7-4531-9885-e6639368ca38
2020-12-03 07:19:37,785 [Thread-128] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8, StorageType: DISK
2020-12-03 07:19:37,786 [Thread-128] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:19:37,786 [Thread-260] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:37,786 [Thread-260] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20/current/BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:37,787 [Thread-260] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20 and block pool id BP-1367394141-172.17.0.2-1606979969461 is not formatted. Formatting ...
2020-12-03 07:19:37,787 [Thread-260] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1367394141-172.17.0.2-1606979969461 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20/current/BP-1367394141-172.17.0.2-1606979969461/current
2020-12-03 07:19:37,787 [Thread-59] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:19:37,787 [Thread-106] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5
2020-12-03 07:19:37,787 [Thread-83] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
2020-12-03 07:19:37,790 [Thread-83] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:19:37,790 [Thread-106] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6
2020-12-03 07:19:37,790 [Thread-59] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:19:37,791 [Thread-106] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6
2020-12-03 07:19:37,791 [Thread-59] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:19:37,790 [Thread-83] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:19:37,794 [Thread-83] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:37,795 [Thread-59] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:37,795 [Thread-305] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4...
2020-12-03 07:19:37,793 [Thread-106] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:37,793 [Thread-128] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7
2020-12-03 07:19:37,795 [Thread-304] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3...
2020-12-03 07:19:37,796 [Thread-307] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5...
2020-12-03 07:19:37,796 [Thread-308] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6...
2020-12-03 07:19:37,796 [Thread-306] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2020-12-03 07:19:37,796 [Thread-128] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7
2020-12-03 07:19:37,797 [Thread-309] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2020-12-03 07:19:37,797 [Thread-128] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8
2020-12-03 07:19:37,797 [Thread-128] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8
2020-12-03 07:19:37,801 [Thread-128] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:37,801 [Thread-310] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7...
2020-12-03 07:19:37,801 [Thread-311] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8...
2020-12-03 07:19:37,812 [IPC Server handler 5 on default port 35332] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:37,813 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:37,814 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:37,872 [Thread-310] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1367394141-172.17.0.2-1606979969461 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7: 71ms
2020-12-03 07:19:37,881 [Thread-304] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1367394141-172.17.0.2-1606979969461 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3: 85ms
2020-12-03 07:19:37,881 [Thread-306] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1367394141-172.17.0.2-1606979969461 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 84ms
2020-12-03 07:19:37,882 [Thread-307] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1367394141-172.17.0.2-1606979969461 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5: 86ms
2020-12-03 07:19:37,883 [Thread-309] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1367394141-172.17.0.2-1606979969461 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 86ms
2020-12-03 07:19:37,883 [Thread-59] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1367394141-172.17.0.2-1606979969461: 88ms
2020-12-03 07:19:37,883 [Thread-305] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1367394141-172.17.0.2-1606979969461 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4: 89ms
2020-12-03 07:19:37,884 [Thread-83] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1367394141-172.17.0.2-1606979969461: 90ms
2020-12-03 07:19:37,885 [Thread-321] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2020-12-03 07:19:37,886 [Thread-320] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3...
2020-12-03 07:19:37,886 [Thread-322] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2020-12-03 07:19:37,886 [Thread-321] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1367394141-172.17.0.2-1606979969461/current/replicas doesn't exist 
2020-12-03 07:19:37,887 [Thread-322] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1367394141-172.17.0.2-1606979969461/current/replicas doesn't exist 
2020-12-03 07:19:37,886 [Thread-323] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4...
2020-12-03 07:19:37,886 [Thread-320] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1367394141-172.17.0.2-1606979969461/current/replicas doesn't exist 
2020-12-03 07:19:37,887 [Thread-323] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1367394141-172.17.0.2-1606979969461/current/replicas doesn't exist 
2020-12-03 07:19:37,888 [Thread-321] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 2ms
2020-12-03 07:19:37,889 [Thread-320] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3: 3ms
2020-12-03 07:19:37,889 [Thread-323] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4: 2ms
2020-12-03 07:19:37,889 [Thread-322] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 4ms
2020-12-03 07:19:37,889 [Thread-83] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461: 5ms
2020-12-03 07:19:37,889 [Thread-59] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461: 5ms
2020-12-03 07:19:37,891 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:19:37,892 [Thread-308] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1367394141-172.17.0.2-1606979969461 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6: 96ms
2020-12-03 07:19:37,892 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:19:37,891 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:19:37,892 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
2020-12-03 07:19:37,892 [Thread-106] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1367394141-172.17.0.2-1606979969461: 98ms
2020-12-03 07:19:37,892 [Thread-311] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1367394141-172.17.0.2-1606979969461 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8: 91ms
2020-12-03 07:19:37,893 [Thread-128] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1367394141-172.17.0.2-1606979969461: 92ms
2020-12-03 07:19:37,893 [Thread-324] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5...
2020-12-03 07:19:37,893 [Thread-325] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6...
2020-12-03 07:19:37,893 [Thread-324] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-1367394141-172.17.0.2-1606979969461/current/replicas doesn't exist 
2020-12-03 07:19:37,893 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-91ea1090-f0db-4472-ad91-faa0d9302fbf): finished scanning block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:37,893 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, DS-54f4be28-ebb1-40b8-abf0-7b39c47dce17): finished scanning block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:37,894 [Thread-327] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8...
2020-12-03 07:19:37,894 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-245d3653-f81a-4228-a6a5-480c618ed25b): finished scanning block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:37,894 [Thread-327] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/current/BP-1367394141-172.17.0.2-1606979969461/current/replicas doesn't exist 
2020-12-03 07:19:37,894 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3, DS-f8fbbb47-9ffb-40e3-9817-22e567ac2d28): finished scanning block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:37,893 [Thread-325] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/current/BP-1367394141-172.17.0.2-1606979969461/current/replicas doesn't exist 
2020-12-03 07:19:37,893 [Thread-326] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7...
2020-12-03 07:19:37,894 [Thread-327] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8: 1ms
2020-12-03 07:19:37,894 [Thread-324] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5: 0ms
2020-12-03 07:19:37,894 [Thread-326] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7/current/BP-1367394141-172.17.0.2-1606979969461/current/replicas doesn't exist 
2020-12-03 07:19:37,894 [Thread-325] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6: 1ms
2020-12-03 07:19:37,897 [Thread-106] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461: 4ms
2020-12-03 07:19:37,897 [Thread-326] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7: 2ms
2020-12-03 07:19:37,897 [Thread-128] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461: 4ms
2020-12-03 07:19:37,897 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5
2020-12-03 07:19:37,897 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6
2020-12-03 07:19:37,897 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7
2020-12-03 07:19:37,897 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6, DS-06a2036e-43cd-41f6-bb27-4153a9f538e8): finished scanning block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:37,897 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5, DS-27376982-648a-435c-9530-844533c3fc3c): finished scanning block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:37,898 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8
2020-12-03 07:19:37,898 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7, DS-2e1daa68-afa3-4e1f-a710-a40da63cedfc): finished scanning block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:37,898 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8, DS-13901838-f2e7-4531-9885-e6639368ca38): finished scanning block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:37,917 [IPC Server handler 7 on default port 35332] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:37,918 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5, DS-27376982-648a-435c-9530-844533c3fc3c): no suitable block pools found to scan.  Waiting 1814399979 ms.
2020-12-03 07:19:37,920 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7, DS-2e1daa68-afa3-4e1f-a710-a40da63cedfc): no suitable block pools found to scan.  Waiting 1814399977 ms.
2020-12-03 07:19:37,920 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, DS-54f4be28-ebb1-40b8-abf0-7b39c47dce17): no suitable block pools found to scan.  Waiting 1814399971 ms.
2020-12-03 07:19:37,921 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6, DS-06a2036e-43cd-41f6-bb27-4153a9f538e8): no suitable block pools found to scan.  Waiting 1814399976 ms.
2020-12-03 07:19:37,920 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:37,922 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:37,920 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3, DS-f8fbbb47-9ffb-40e3-9817-22e567ac2d28): no suitable block pools found to scan.  Waiting 1814399971 ms.
2020-12-03 07:19:37,920 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8, DS-13901838-f2e7-4531-9885-e6639368ca38): no suitable block pools found to scan.  Waiting 1814399977 ms.
2020-12-03 07:19:37,920 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-245d3653-f81a-4228-a6a5-480c618ed25b): no suitable block pools found to scan.  Waiting 1814399971 ms.
2020-12-03 07:19:37,921 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-91ea1090-f0db-4472-ad91-faa0d9302fbf): no suitable block pools found to scan.  Waiting 1814399970 ms.
2020-12-03 07:19:37,923 [Thread-128] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 8:14 AM with interval of 21600000ms
2020-12-03 07:19:37,923 [Thread-83] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 9:31 AM with interval of 21600000ms
2020-12-03 07:19:37,923 [Thread-59] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 9:25 AM with interval of 21600000ms
2020-12-03 07:19:37,923 [Thread-106] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 9:28 AM with interval of 21600000ms
2020-12-03 07:19:38,034 [IPC Server handler 9 on default port 35332] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:38,050 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:38,050 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:38,054 [Thread-282] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:38,054 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid 6433c7f1-b9cb-4ed4-9404-85e3ec0e7233) service to localhost/127.0.0.1:35332 beginning handshake with NN
2020-12-03 07:19:38,054 [Thread-282] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22/current/BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:38,054 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid 72c80882-49c9-444a-982c-993baca6bb62) service to localhost/127.0.0.1:35332 beginning handshake with NN
2020-12-03 07:19:38,054 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid 811e904d-4b58-4d09-9a37-ecd8d80551dd) service to localhost/127.0.0.1:35332 beginning handshake with NN
2020-12-03 07:19:38,054 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid 69a597d2-2edd-4e23-9c38-3328eccbc91b) service to localhost/127.0.0.1:35332 beginning handshake with NN
2020-12-03 07:19:38,056 [Thread-282] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22 and block pool id BP-1367394141-172.17.0.2-1606979969461 is not formatted. Formatting ...
2020-12-03 07:19:38,060 [Thread-282] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1367394141-172.17.0.2-1606979969461 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22/current/BP-1367394141-172.17.0.2-1606979969461/current
2020-12-03 07:19:38,072 [Thread-150] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID 0e810974-a498-4742-a093-0c4d95031785
2020-12-03 07:19:38,076 [Thread-150] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-ac18a139-2578-41ac-8803-da9d22cc545d
2020-12-03 07:19:38,076 [Thread-150] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9, StorageType: DISK
2020-12-03 07:19:38,077 [IPC Server handler 4 on default port 35332] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:44555, datanodeUuid=811e904d-4b58-4d09-9a37-ecd8d80551dd, infoPort=45501, infoSecurePort=0, ipcPort=43159, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461) storage 811e904d-4b58-4d09-9a37-ecd8d80551dd
2020-12-03 07:19:38,078 [Thread-216] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=352815665;bpid=BP-1367394141-172.17.0.2-1606979969461;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=352815665;c=1606979969461;bpid=BP-1367394141-172.17.0.2-1606979969461;dnuuid=null
2020-12-03 07:19:38,078 [Thread-150] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-c43137fa-5025-44bc-ad04-ca38d42d45a5
2020-12-03 07:19:38,078 [Thread-150] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10, StorageType: DISK
2020-12-03 07:19:38,084 [Thread-150] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:19:38,085 [IPC Server handler 4 on default port 35332] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:44555
2020-12-03 07:19:38,086 [Thread-150] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9
2020-12-03 07:19:38,086 [IPC Server handler 4 on default port 35332] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 811e904d-4b58-4d09-9a37-ecd8d80551dd (127.0.0.1:44555).
2020-12-03 07:19:38,099 [Thread-150] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9
2020-12-03 07:19:38,099 [Thread-150] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10
2020-12-03 07:19:38,101 [Thread-150] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10
2020-12-03 07:19:38,105 [IPC Server handler 8 on default port 35332] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:39913, datanodeUuid=69a597d2-2edd-4e23-9c38-3328eccbc91b, infoPort=45738, infoSecurePort=0, ipcPort=37908, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461) storage 69a597d2-2edd-4e23-9c38-3328eccbc91b
2020-12-03 07:19:38,105 [Thread-150] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:38,105 [IPC Server handler 8 on default port 35332] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:39913
2020-12-03 07:19:38,106 [IPC Server handler 8 on default port 35332] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 69a597d2-2edd-4e23-9c38-3328eccbc91b (127.0.0.1:39913).
2020-12-03 07:19:38,106 [Thread-342] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9...
2020-12-03 07:19:38,107 [Thread-343] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10...
2020-12-03 07:19:38,108 [IPC Server handler 1 on default port 35332] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:44715, datanodeUuid=72c80882-49c9-444a-982c-993baca6bb62, infoPort=46372, infoSecurePort=0, ipcPort=36006, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461) storage 72c80882-49c9-444a-982c-993baca6bb62
2020-12-03 07:19:38,108 [IPC Server handler 1 on default port 35332] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:44715
2020-12-03 07:19:38,108 [IPC Server handler 1 on default port 35332] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 72c80882-49c9-444a-982c-993baca6bb62 (127.0.0.1:44715).
2020-12-03 07:19:38,110 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid 72c80882-49c9-444a-982c-993baca6bb62) service to localhost/127.0.0.1:35332 successfully registered with NN
2020-12-03 07:19:38,111 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (DataNode.java:registerBlockPoolWithSecretManager(1615)) - Block token params received from NN: for block pool BP-1367394141-172.17.0.2-1606979969461 keyUpdateInterval=600 min(s), tokenLifetime=600 min(s)
2020-12-03 07:19:38,112 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:19:38,112 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:35332 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:19:38,112 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid 69a597d2-2edd-4e23-9c38-3328eccbc91b) service to localhost/127.0.0.1:35332 successfully registered with NN
2020-12-03 07:19:38,112 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (DataNode.java:registerBlockPoolWithSecretManager(1615)) - Block token params received from NN: for block pool BP-1367394141-172.17.0.2-1606979969461 keyUpdateInterval=600 min(s), tokenLifetime=600 min(s)
2020-12-03 07:19:38,113 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:19:38,113 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:35332 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:19:38,114 [IPC Server handler 6 on default port 35332] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:37110, datanodeUuid=6433c7f1-b9cb-4ed4-9404-85e3ec0e7233, infoPort=40898, infoSecurePort=0, ipcPort=33076, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461) storage 6433c7f1-b9cb-4ed4-9404-85e3ec0e7233
2020-12-03 07:19:38,117 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid 811e904d-4b58-4d09-9a37-ecd8d80551dd) service to localhost/127.0.0.1:35332 successfully registered with NN
2020-12-03 07:19:38,118 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (DataNode.java:registerBlockPoolWithSecretManager(1615)) - Block token params received from NN: for block pool BP-1367394141-172.17.0.2-1606979969461 keyUpdateInterval=600 min(s), tokenLifetime=600 min(s)
2020-12-03 07:19:38,118 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:19:38,118 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:35332 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:19:38,120 [IPC Server handler 6 on default port 35332] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:37110
2020-12-03 07:19:38,120 [IPC Server handler 6 on default port 35332] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 6433c7f1-b9cb-4ed4-9404-85e3ec0e7233 (127.0.0.1:37110).
2020-12-03 07:19:38,122 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid 6433c7f1-b9cb-4ed4-9404-85e3ec0e7233) service to localhost/127.0.0.1:35332 successfully registered with NN
2020-12-03 07:19:38,122 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (DataNode.java:registerBlockPoolWithSecretManager(1615)) - Block token params received from NN: for block pool BP-1367394141-172.17.0.2-1606979969461 keyUpdateInterval=600 min(s), tokenLifetime=600 min(s)
2020-12-03 07:19:38,122 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:19:38,123 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:35332 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:19:38,145 [IPC Server handler 2 on default port 35332] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-f8fbbb47-9ffb-40e3-9817-22e567ac2d28 for DN 127.0.0.1:37110
2020-12-03 07:19:38,147 [IPC Server handler 2 on default port 35332] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-54f4be28-ebb1-40b8-abf0-7b39c47dce17 for DN 127.0.0.1:37110
2020-12-03 07:19:38,149 [IPC Server handler 5 on default port 35332] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-2e1daa68-afa3-4e1f-a710-a40da63cedfc for DN 127.0.0.1:44715
2020-12-03 07:19:38,151 [IPC Server handler 5 on default port 35332] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-13901838-f2e7-4531-9885-e6639368ca38 for DN 127.0.0.1:44715
2020-12-03 07:19:38,152 [IPC Server handler 0 on default port 35332] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-91ea1090-f0db-4472-ad91-faa0d9302fbf for DN 127.0.0.1:39913
2020-12-03 07:19:38,152 [IPC Server handler 0 on default port 35332] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-245d3653-f81a-4228-a6a5-480c618ed25b for DN 127.0.0.1:39913
2020-12-03 07:19:38,154 [IPC Server handler 3 on default port 35332] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-27376982-648a-435c-9530-844533c3fc3c for DN 127.0.0.1:44555
2020-12-03 07:19:38,154 [IPC Server handler 3 on default port 35332] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-06a2036e-43cd-41f6-bb27-4153a9f538e8 for DN 127.0.0.1:44555
2020-12-03 07:19:38,157 [Thread-342] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1367394141-172.17.0.2-1606979969461 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9: 51ms
2020-12-03 07:19:38,159 [IPC Server handler 7 on default port 35332] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:38,163 [Thread-343] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1367394141-172.17.0.2-1606979969461 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10: 56ms
2020-12-03 07:19:38,163 [Thread-150] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1367394141-172.17.0.2-1606979969461: 58ms
2020-12-03 07:19:38,163 [Thread-346] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9...
2020-12-03 07:19:38,164 [Thread-347] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10...
2020-12-03 07:19:38,164 [Thread-346] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9/current/BP-1367394141-172.17.0.2-1606979969461/current/replicas doesn't exist 
2020-12-03 07:19:38,164 [Thread-347] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10/current/BP-1367394141-172.17.0.2-1606979969461/current/replicas doesn't exist 
2020-12-03 07:19:38,164 [Thread-347] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10: 0ms
2020-12-03 07:19:38,167 [Thread-346] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9: 3ms
2020-12-03 07:19:38,169 [Thread-150] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461: 6ms
2020-12-03 07:19:38,170 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10
2020-12-03 07:19:38,170 [Thread-150] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 12:49 PM with interval of 21600000ms
2020-12-03 07:19:38,170 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9
2020-12-03 07:19:38,170 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10, DS-c43137fa-5025-44bc-ad04-ca38d42d45a5): finished scanning block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:38,171 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:38,171 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:38,171 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9, DS-ac18a139-2578-41ac-8803-da9d22cc545d): finished scanning block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:38,173 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9, DS-ac18a139-2578-41ac-8803-da9d22cc545d): no suitable block pools found to scan.  Waiting 1814399996 ms.
2020-12-03 07:19:38,173 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10, DS-c43137fa-5025-44bc-ad04-ca38d42d45a5): no suitable block pools found to scan.  Waiting 1814399997 ms.
2020-12-03 07:19:38,175 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid 0e810974-a498-4742-a093-0c4d95031785) service to localhost/127.0.0.1:35332 beginning handshake with NN
2020-12-03 07:19:38,194 [IPC Server handler 9 on default port 35332] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:46438, datanodeUuid=0e810974-a498-4742-a093-0c4d95031785, infoPort=41065, infoSecurePort=0, ipcPort=39004, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461) storage 0e810974-a498-4742-a093-0c4d95031785
2020-12-03 07:19:38,195 [IPC Server handler 9 on default port 35332] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:46438
2020-12-03 07:19:38,195 [IPC Server handler 9 on default port 35332] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 0e810974-a498-4742-a093-0c4d95031785 (127.0.0.1:46438).
2020-12-03 07:19:38,204 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid 0e810974-a498-4742-a093-0c4d95031785) service to localhost/127.0.0.1:35332 successfully registered with NN
2020-12-03 07:19:38,205 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (DataNode.java:registerBlockPoolWithSecretManager(1615)) - Block token params received from NN: for block pool BP-1367394141-172.17.0.2-1606979969461 keyUpdateInterval=600 min(s), tokenLifetime=600 min(s)
2020-12-03 07:19:38,205 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:19:38,205 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:35332 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:19:38,213 [IPC Server handler 4 on default port 35332] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-ac18a139-2578-41ac-8803-da9d22cc545d for DN 127.0.0.1:46438
2020-12-03 07:19:38,213 [IPC Server handler 4 on default port 35332] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-c43137fa-5025-44bc-ad04-ca38d42d45a5 for DN 127.0.0.1:46438
2020-12-03 07:19:38,226 [Thread-238] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=352815665;bpid=BP-1367394141-172.17.0.2-1606979969461;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=352815665;c=1606979969461;bpid=BP-1367394141-172.17.0.2-1606979969461;dnuuid=null
2020-12-03 07:19:38,227 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x25430c3ff8e91fd5: Processing first storage report for DS-2e1daa68-afa3-4e1f-a710-a40da63cedfc from datanode 72c80882-49c9-444a-982c-993baca6bb62
2020-12-03 07:19:38,230 [Thread-172] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID e5737a0c-e925-4b69-8166-e4391ac5f8bb
2020-12-03 07:19:38,231 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x25430c3ff8e91fd5: from storage DS-2e1daa68-afa3-4e1f-a710-a40da63cedfc node DatanodeRegistration(127.0.0.1:44715, datanodeUuid=72c80882-49c9-444a-982c-993baca6bb62, infoPort=46372, infoSecurePort=0, ipcPort=36006, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461), blocks: 0, hasStaleStorage: true, processing time: 4 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,231 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x9e4957376aed9cd4: Processing first storage report for DS-c43137fa-5025-44bc-ad04-ca38d42d45a5 from datanode 0e810974-a498-4742-a093-0c4d95031785
2020-12-03 07:19:38,231 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x9e4957376aed9cd4: from storage DS-c43137fa-5025-44bc-ad04-ca38d42d45a5 node DatanodeRegistration(127.0.0.1:46438, datanodeUuid=0e810974-a498-4742-a093-0c4d95031785, infoPort=41065, infoSecurePort=0, ipcPort=39004, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461), blocks: 0, hasStaleStorage: true, processing time: 1 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,232 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0xee52c1bbcb3a4467: Processing first storage report for DS-f8fbbb47-9ffb-40e3-9817-22e567ac2d28 from datanode 6433c7f1-b9cb-4ed4-9404-85e3ec0e7233
2020-12-03 07:19:38,232 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0xee52c1bbcb3a4467: from storage DS-f8fbbb47-9ffb-40e3-9817-22e567ac2d28 node DatanodeRegistration(127.0.0.1:37110, datanodeUuid=6433c7f1-b9cb-4ed4-9404-85e3ec0e7233, infoPort=40898, infoSecurePort=0, ipcPort=33076, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,232 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x191d81637f39924c: Processing first storage report for DS-06a2036e-43cd-41f6-bb27-4153a9f538e8 from datanode 811e904d-4b58-4d09-9a37-ecd8d80551dd
2020-12-03 07:19:38,232 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x191d81637f39924c: from storage DS-06a2036e-43cd-41f6-bb27-4153a9f538e8 node DatanodeRegistration(127.0.0.1:44555, datanodeUuid=811e904d-4b58-4d09-9a37-ecd8d80551dd, infoPort=45501, infoSecurePort=0, ipcPort=43159, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461), blocks: 0, hasStaleStorage: true, processing time: 1 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,232 [Thread-172] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-6660ac3c-7f95-4544-99d0-9a26ef3ef67b
2020-12-03 07:19:38,234 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x9e4957376aed9cd4: Processing first storage report for DS-ac18a139-2578-41ac-8803-da9d22cc545d from datanode 0e810974-a498-4742-a093-0c4d95031785
2020-12-03 07:19:38,234 [Thread-172] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11, StorageType: DISK
2020-12-03 07:19:38,234 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x9e4957376aed9cd4: from storage DS-ac18a139-2578-41ac-8803-da9d22cc545d node DatanodeRegistration(127.0.0.1:46438, datanodeUuid=0e810974-a498-4742-a093-0c4d95031785, infoPort=41065, infoSecurePort=0, ipcPort=39004, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,234 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x25430c3ff8e91fd5: Processing first storage report for DS-13901838-f2e7-4531-9885-e6639368ca38 from datanode 72c80882-49c9-444a-982c-993baca6bb62
2020-12-03 07:19:38,234 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x25430c3ff8e91fd5: from storage DS-13901838-f2e7-4531-9885-e6639368ca38 node DatanodeRegistration(127.0.0.1:44715, datanodeUuid=72c80882-49c9-444a-982c-993baca6bb62, infoPort=46372, infoSecurePort=0, ipcPort=36006, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,235 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0xee52c1bbcb3a4467: Processing first storage report for DS-54f4be28-ebb1-40b8-abf0-7b39c47dce17 from datanode 6433c7f1-b9cb-4ed4-9404-85e3ec0e7233
2020-12-03 07:19:38,235 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0xee52c1bbcb3a4467: from storage DS-54f4be28-ebb1-40b8-abf0-7b39c47dce17 node DatanodeRegistration(127.0.0.1:37110, datanodeUuid=6433c7f1-b9cb-4ed4-9404-85e3ec0e7233, infoPort=40898, infoSecurePort=0, ipcPort=33076, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,235 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x191d81637f39924c: Processing first storage report for DS-27376982-648a-435c-9530-844533c3fc3c from datanode 811e904d-4b58-4d09-9a37-ecd8d80551dd
2020-12-03 07:19:38,235 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x191d81637f39924c: from storage DS-27376982-648a-435c-9530-844533c3fc3c node DatanodeRegistration(127.0.0.1:44555, datanodeUuid=811e904d-4b58-4d09-9a37-ecd8d80551dd, infoPort=45501, infoSecurePort=0, ipcPort=43159, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,235 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x4d7e56629c8db380: Processing first storage report for DS-245d3653-f81a-4228-a6a5-480c618ed25b from datanode 69a597d2-2edd-4e23-9c38-3328eccbc91b
2020-12-03 07:19:38,235 [Thread-172] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-5c6220ab-363a-4023-bc0c-7b6f261c270f
2020-12-03 07:19:38,235 [Thread-172] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12, StorageType: DISK
2020-12-03 07:19:38,235 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x4d7e56629c8db380: from storage DS-245d3653-f81a-4228-a6a5-480c618ed25b node DatanodeRegistration(127.0.0.1:39913, datanodeUuid=69a597d2-2edd-4e23-9c38-3328eccbc91b, infoPort=45738, infoSecurePort=0, ipcPort=37908, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461), blocks: 0, hasStaleStorage: true, processing time: 1 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,236 [Thread-172] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:19:38,236 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x4d7e56629c8db380: Processing first storage report for DS-91ea1090-f0db-4472-ad91-faa0d9302fbf from datanode 69a597d2-2edd-4e23-9c38-3328eccbc91b
2020-12-03 07:19:38,237 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x4d7e56629c8db380: from storage DS-91ea1090-f0db-4472-ad91-faa0d9302fbf node DatanodeRegistration(127.0.0.1:39913, datanodeUuid=69a597d2-2edd-4e23-9c38-3328eccbc91b, infoPort=45738, infoSecurePort=0, ipcPort=37908, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,237 [Thread-172] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11
2020-12-03 07:19:38,239 [Thread-172] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11
2020-12-03 07:19:38,239 [Thread-172] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12
2020-12-03 07:19:38,239 [Thread-172] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12
2020-12-03 07:19:38,240 [Thread-172] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:38,240 [Thread-353] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11...
2020-12-03 07:19:38,240 [Thread-354] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12...
2020-12-03 07:19:38,270 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x4d7e56629c8db380,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 9 msec to generate and 90 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:19:38,277 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x9e4957376aed9cd4,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 1 msec to generate and 51 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:19:38,270 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x191d81637f39924c,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 9 msec to generate and 90 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:19:38,277 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:38,270 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0xee52c1bbcb3a4467,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 9 msec to generate and 91 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:19:38,277 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:38,278 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:38,270 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x25430c3ff8e91fd5,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 9 msec to generate and 90 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:19:38,278 [IPC Server handler 3 on default port 35332] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:38,277 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:38,280 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:38,288 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:38,288 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:38,294 [Thread-353] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1367394141-172.17.0.2-1606979969461 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11: 55ms
2020-12-03 07:19:38,294 [Thread-354] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1367394141-172.17.0.2-1606979969461 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12: 55ms
2020-12-03 07:19:38,297 [Thread-172] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1367394141-172.17.0.2-1606979969461: 57ms
2020-12-03 07:19:38,297 [Thread-357] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11...
2020-12-03 07:19:38,297 [Thread-358] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12...
2020-12-03 07:19:38,297 [Thread-357] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11/current/BP-1367394141-172.17.0.2-1606979969461/current/replicas doesn't exist 
2020-12-03 07:19:38,297 [Thread-358] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12/current/BP-1367394141-172.17.0.2-1606979969461/current/replicas doesn't exist 
2020-12-03 07:19:38,298 [Thread-357] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11: 1ms
2020-12-03 07:19:38,298 [Thread-358] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12: 0ms
2020-12-03 07:19:38,298 [Thread-172] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461: 1ms
2020-12-03 07:19:38,298 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12
2020-12-03 07:19:38,299 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11
2020-12-03 07:19:38,299 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12, DS-5c6220ab-363a-4023-bc0c-7b6f261c270f): finished scanning block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:38,299 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11, DS-6660ac3c-7f95-4544-99d0-9a26ef3ef67b): finished scanning block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:38,299 [Thread-172] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 10:11 AM with interval of 21600000ms
2020-12-03 07:19:38,300 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11, DS-6660ac3c-7f95-4544-99d0-9a26ef3ef67b): no suitable block pools found to scan.  Waiting 1814399998 ms.
2020-12-03 07:19:38,300 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12, DS-5c6220ab-363a-4023-bc0c-7b6f261c270f): no suitable block pools found to scan.  Waiting 1814399998 ms.
2020-12-03 07:19:38,301 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid e5737a0c-e925-4b69-8166-e4391ac5f8bb) service to localhost/127.0.0.1:35332 beginning handshake with NN
2020-12-03 07:19:38,303 [IPC Server handler 5 on default port 35332] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:38308, datanodeUuid=e5737a0c-e925-4b69-8166-e4391ac5f8bb, infoPort=34220, infoSecurePort=0, ipcPort=37261, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461) storage e5737a0c-e925-4b69-8166-e4391ac5f8bb
2020-12-03 07:19:38,303 [IPC Server handler 5 on default port 35332] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:38308
2020-12-03 07:19:38,303 [IPC Server handler 5 on default port 35332] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN e5737a0c-e925-4b69-8166-e4391ac5f8bb (127.0.0.1:38308).
2020-12-03 07:19:38,305 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid e5737a0c-e925-4b69-8166-e4391ac5f8bb) service to localhost/127.0.0.1:35332 successfully registered with NN
2020-12-03 07:19:38,305 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (DataNode.java:registerBlockPoolWithSecretManager(1615)) - Block token params received from NN: for block pool BP-1367394141-172.17.0.2-1606979969461 keyUpdateInterval=600 min(s), tokenLifetime=600 min(s)
2020-12-03 07:19:38,305 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:19:38,305 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:35332 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:19:38,311 [IPC Server handler 7 on default port 35332] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-6660ac3c-7f95-4544-99d0-9a26ef3ef67b for DN 127.0.0.1:38308
2020-12-03 07:19:38,311 [IPC Server handler 7 on default port 35332] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-5c6220ab-363a-4023-bc0c-7b6f261c270f for DN 127.0.0.1:38308
2020-12-03 07:19:38,320 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0xdcfb432c092a4199: Processing first storage report for DS-6660ac3c-7f95-4544-99d0-9a26ef3ef67b from datanode e5737a0c-e925-4b69-8166-e4391ac5f8bb
2020-12-03 07:19:38,321 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0xdcfb432c092a4199: from storage DS-6660ac3c-7f95-4544-99d0-9a26ef3ef67b node DatanodeRegistration(127.0.0.1:38308, datanodeUuid=e5737a0c-e925-4b69-8166-e4391ac5f8bb, infoPort=34220, infoSecurePort=0, ipcPort=37261, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,321 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0xdcfb432c092a4199: Processing first storage report for DS-5c6220ab-363a-4023-bc0c-7b6f261c270f from datanode e5737a0c-e925-4b69-8166-e4391ac5f8bb
2020-12-03 07:19:38,321 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0xdcfb432c092a4199: from storage DS-5c6220ab-363a-4023-bc0c-7b6f261c270f node DatanodeRegistration(127.0.0.1:38308, datanodeUuid=e5737a0c-e925-4b69-8166-e4391ac5f8bb, infoPort=34220, infoSecurePort=0, ipcPort=37261, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,324 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0xdcfb432c092a4199,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 1 msec to generate and 5 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:19:38,324 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:38,330 [Thread-260] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=352815665;bpid=BP-1367394141-172.17.0.2-1606979969461;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=352815665;c=1606979969461;bpid=BP-1367394141-172.17.0.2-1606979969461;dnuuid=null
2020-12-03 07:19:38,331 [Thread-194] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID 1948f61e-988c-4bd0-ad24-d3ae7d4c96b1
2020-12-03 07:19:38,336 [Thread-194] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-34cdaddf-9d99-43fe-ac94-a2d410b1c52a
2020-12-03 07:19:38,338 [Thread-194] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13, StorageType: DISK
2020-12-03 07:19:38,340 [Thread-194] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-bf9cf6c5-0493-4ac0-8724-c46bd212b0fc
2020-12-03 07:19:38,344 [Thread-194] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14, StorageType: DISK
2020-12-03 07:19:38,345 [Thread-194] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:19:38,347 [Thread-194] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13
2020-12-03 07:19:38,349 [Thread-194] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13
2020-12-03 07:19:38,349 [Thread-194] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14
2020-12-03 07:19:38,349 [Thread-194] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14
2020-12-03 07:19:38,356 [Thread-194] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:38,361 [Thread-365] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14...
2020-12-03 07:19:38,363 [Thread-364] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13...
2020-12-03 07:19:38,390 [IPC Server handler 4 on default port 35332] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:38,391 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:38,391 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:38,415 [Thread-364] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1367394141-172.17.0.2-1606979969461 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13: 52ms
2020-12-03 07:19:38,423 [Thread-282] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=352815665;bpid=BP-1367394141-172.17.0.2-1606979969461;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=352815665;c=1606979969461;bpid=BP-1367394141-172.17.0.2-1606979969461;dnuuid=null
2020-12-03 07:19:38,432 [Thread-365] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1367394141-172.17.0.2-1606979969461 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14: 71ms
2020-12-03 07:19:38,432 [Thread-194] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1367394141-172.17.0.2-1606979969461: 76ms
2020-12-03 07:19:38,433 [Thread-368] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13...
2020-12-03 07:19:38,433 [Thread-369] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14...
2020-12-03 07:19:38,433 [Thread-368] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13/current/BP-1367394141-172.17.0.2-1606979969461/current/replicas doesn't exist 
2020-12-03 07:19:38,433 [Thread-369] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14/current/BP-1367394141-172.17.0.2-1606979969461/current/replicas doesn't exist 
2020-12-03 07:19:38,433 [Thread-369] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14: 1ms
2020-12-03 07:19:38,433 [Thread-368] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13: 1ms
2020-12-03 07:19:38,435 [Thread-194] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461: 3ms
2020-12-03 07:19:38,435 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14
2020-12-03 07:19:38,435 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13
2020-12-03 07:19:38,436 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14, DS-bf9cf6c5-0493-4ac0-8724-c46bd212b0fc): finished scanning block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:38,436 [Thread-194] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 8:21 AM with interval of 21600000ms
2020-12-03 07:19:38,436 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13, DS-34cdaddf-9d99-43fe-ac94-a2d410b1c52a): finished scanning block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:38,437 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid 1948f61e-988c-4bd0-ad24-d3ae7d4c96b1) service to localhost/127.0.0.1:35332 beginning handshake with NN
2020-12-03 07:19:38,436 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14, DS-bf9cf6c5-0493-4ac0-8724-c46bd212b0fc): no suitable block pools found to scan.  Waiting 1814399999 ms.
2020-12-03 07:19:38,439 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13, DS-34cdaddf-9d99-43fe-ac94-a2d410b1c52a): no suitable block pools found to scan.  Waiting 1814399996 ms.
2020-12-03 07:19:38,439 [IPC Server handler 1 on default port 35332] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:38503, datanodeUuid=1948f61e-988c-4bd0-ad24-d3ae7d4c96b1, infoPort=37465, infoSecurePort=0, ipcPort=35031, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461) storage 1948f61e-988c-4bd0-ad24-d3ae7d4c96b1
2020-12-03 07:19:38,440 [IPC Server handler 1 on default port 35332] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:38503
2020-12-03 07:19:38,440 [IPC Server handler 1 on default port 35332] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 1948f61e-988c-4bd0-ad24-d3ae7d4c96b1 (127.0.0.1:38503).
2020-12-03 07:19:38,441 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid 1948f61e-988c-4bd0-ad24-d3ae7d4c96b1) service to localhost/127.0.0.1:35332 successfully registered with NN
2020-12-03 07:19:38,441 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (DataNode.java:registerBlockPoolWithSecretManager(1615)) - Block token params received from NN: for block pool BP-1367394141-172.17.0.2-1606979969461 keyUpdateInterval=600 min(s), tokenLifetime=600 min(s)
2020-12-03 07:19:38,442 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:19:38,442 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:35332 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:19:38,449 [IPC Server handler 6 on default port 35332] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-34cdaddf-9d99-43fe-ac94-a2d410b1c52a for DN 127.0.0.1:38503
2020-12-03 07:19:38,449 [IPC Server handler 6 on default port 35332] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-bf9cf6c5-0493-4ac0-8724-c46bd212b0fc for DN 127.0.0.1:38503
2020-12-03 07:19:38,455 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x9a3f878c789f450e: Processing first storage report for DS-34cdaddf-9d99-43fe-ac94-a2d410b1c52a from datanode 1948f61e-988c-4bd0-ad24-d3ae7d4c96b1
2020-12-03 07:19:38,456 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x9a3f878c789f450e: from storage DS-34cdaddf-9d99-43fe-ac94-a2d410b1c52a node DatanodeRegistration(127.0.0.1:38503, datanodeUuid=1948f61e-988c-4bd0-ad24-d3ae7d4c96b1, infoPort=37465, infoSecurePort=0, ipcPort=35031, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,456 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x9a3f878c789f450e: Processing first storage report for DS-bf9cf6c5-0493-4ac0-8724-c46bd212b0fc from datanode 1948f61e-988c-4bd0-ad24-d3ae7d4c96b1
2020-12-03 07:19:38,456 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x9a3f878c789f450e: from storage DS-bf9cf6c5-0493-4ac0-8724-c46bd212b0fc node DatanodeRegistration(127.0.0.1:38503, datanodeUuid=1948f61e-988c-4bd0-ad24-d3ae7d4c96b1, infoPort=37465, infoSecurePort=0, ipcPort=35031, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,457 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x9a3f878c789f450e,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 4 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:19:38,457 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:38,497 [IPC Server handler 0 on default port 35332] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:38,498 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:38,499 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:38,508 [Thread-216] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID f1c32787-7e56-457d-90b0-80f26f785f52
2020-12-03 07:19:38,522 [Thread-216] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-b23a0f3d-1578-4216-aa54-57c659e1100a
2020-12-03 07:19:38,523 [Thread-216] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15, StorageType: DISK
2020-12-03 07:19:38,527 [Thread-216] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-c23cda38-630d-4441-b4d8-5b0ca6373351
2020-12-03 07:19:38,527 [Thread-216] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16, StorageType: DISK
2020-12-03 07:19:38,528 [Thread-216] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:19:38,530 [Thread-216] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15
2020-12-03 07:19:38,530 [Thread-216] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15
2020-12-03 07:19:38,531 [Thread-216] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16
2020-12-03 07:19:38,531 [Thread-216] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16
2020-12-03 07:19:38,531 [Thread-216] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:38,532 [Thread-375] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15...
2020-12-03 07:19:38,535 [Thread-376] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16...
2020-12-03 07:19:38,576 [Thread-238] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID b771649f-3cb7-43a2-9de1-a830c893f998
2020-12-03 07:19:38,583 [Thread-238] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-15a77805-6e7f-4aa2-9de6-622edf3907c2
2020-12-03 07:19:38,583 [Thread-238] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17, StorageType: DISK
2020-12-03 07:19:38,586 [Thread-238] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-65ca39ad-f8ba-4de7-a267-6c125a4b507b
2020-12-03 07:19:38,592 [Thread-238] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18, StorageType: DISK
2020-12-03 07:19:38,593 [Thread-238] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:19:38,601 [Thread-238] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17
2020-12-03 07:19:38,603 [Thread-238] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17
2020-12-03 07:19:38,603 [IPC Server handler 2 on default port 35332] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:38,603 [Thread-238] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18
2020-12-03 07:19:38,605 [Thread-238] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18
2020-12-03 07:19:38,605 [Thread-238] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:38,606 [Thread-381] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17...
2020-12-03 07:19:38,606 [Thread-382] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18...
2020-12-03 07:19:38,607 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:38,607 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:38,611 [Thread-376] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1367394141-172.17.0.2-1606979969461 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16: 75ms
2020-12-03 07:19:38,643 [Thread-375] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1367394141-172.17.0.2-1606979969461 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15: 111ms
2020-12-03 07:19:38,643 [Thread-260] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID be96e054-5ebf-4915-a869-89779c6b94e9
2020-12-03 07:19:38,652 [Thread-216] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1367394141-172.17.0.2-1606979969461: 120ms
2020-12-03 07:19:38,652 [Thread-260] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-97859208-57d6-4d0f-a458-1e4adaf0a78a
2020-12-03 07:19:38,652 [Thread-260] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19, StorageType: DISK
2020-12-03 07:19:38,654 [Thread-260] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-5beec8c8-67b2-429e-b7bb-c700d530aa6a
2020-12-03 07:19:38,654 [Thread-260] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20, StorageType: DISK
2020-12-03 07:19:38,654 [Thread-386] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15...
2020-12-03 07:19:38,654 [Thread-386] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15/current/BP-1367394141-172.17.0.2-1606979969461/current/replicas doesn't exist 
2020-12-03 07:19:38,655 [Thread-387] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16...
2020-12-03 07:19:38,655 [Thread-260] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:19:38,656 [Thread-260] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19
2020-12-03 07:19:38,656 [Thread-387] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16/current/BP-1367394141-172.17.0.2-1606979969461/current/replicas doesn't exist 
2020-12-03 07:19:38,657 [Thread-382] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1367394141-172.17.0.2-1606979969461 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18: 50ms
2020-12-03 07:19:38,657 [Thread-387] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16: 0ms
2020-12-03 07:19:38,662 [Thread-260] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19
2020-12-03 07:19:38,662 [Thread-260] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20
2020-12-03 07:19:38,670 [Thread-260] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20
2020-12-03 07:19:38,671 [Thread-260] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:38,671 [Thread-386] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15: 16ms
2020-12-03 07:19:38,672 [Thread-389] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19...
2020-12-03 07:19:38,674 [Thread-390] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20...
2020-12-03 07:19:38,675 [Thread-216] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461: 21ms
2020-12-03 07:19:38,676 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16
2020-12-03 07:19:38,692 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15
2020-12-03 07:19:38,693 [Thread-282] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID 0bedb3f9-8b9a-41b5-a47d-b766930b42be
2020-12-03 07:19:38,693 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15, DS-b23a0f3d-1578-4216-aa54-57c659e1100a): finished scanning block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:38,693 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16, DS-c23cda38-630d-4441-b4d8-5b0ca6373351): finished scanning block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:38,694 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15, DS-b23a0f3d-1578-4216-aa54-57c659e1100a): no suitable block pools found to scan.  Waiting 1814399981 ms.
2020-12-03 07:19:38,694 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16, DS-c23cda38-630d-4441-b4d8-5b0ca6373351): no suitable block pools found to scan.  Waiting 1814399981 ms.
2020-12-03 07:19:38,694 [Thread-216] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 10:40 AM with interval of 21600000ms
2020-12-03 07:19:38,695 [Thread-282] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-129d8406-e71c-470b-9430-2dc52ce7b622
2020-12-03 07:19:38,704 [Thread-282] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21, StorageType: DISK
2020-12-03 07:19:38,706 [Thread-282] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-695468af-ff85-41de-a309-3e9cf958ef94
2020-12-03 07:19:38,706 [Thread-282] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22, StorageType: DISK
2020-12-03 07:19:38,706 [Thread-282] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:19:38,708 [Thread-282] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21
2020-12-03 07:19:38,709 [IPC Server handler 3 on default port 35332] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:38,710 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid f1c32787-7e56-457d-90b0-80f26f785f52) service to localhost/127.0.0.1:35332 beginning handshake with NN
2020-12-03 07:19:38,710 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:38,711 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:38,711 [Thread-282] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21
2020-12-03 07:19:38,711 [Thread-282] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22
2020-12-03 07:19:38,711 [Thread-282] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22
2020-12-03 07:19:38,711 [Thread-282] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:38,712 [IPC Server handler 5 on default port 35332] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:34075, datanodeUuid=f1c32787-7e56-457d-90b0-80f26f785f52, infoPort=37609, infoSecurePort=0, ipcPort=40394, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461) storage f1c32787-7e56-457d-90b0-80f26f785f52
2020-12-03 07:19:38,733 [IPC Server handler 5 on default port 35332] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:34075
2020-12-03 07:19:38,734 [IPC Server handler 5 on default port 35332] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN f1c32787-7e56-457d-90b0-80f26f785f52 (127.0.0.1:34075).
2020-12-03 07:19:38,735 [Thread-396] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21...
2020-12-03 07:19:38,741 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid f1c32787-7e56-457d-90b0-80f26f785f52) service to localhost/127.0.0.1:35332 successfully registered with NN
2020-12-03 07:19:38,741 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (DataNode.java:registerBlockPoolWithSecretManager(1615)) - Block token params received from NN: for block pool BP-1367394141-172.17.0.2-1606979969461 keyUpdateInterval=600 min(s), tokenLifetime=600 min(s)
2020-12-03 07:19:38,742 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:19:38,742 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:35332 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:19:38,761 [Thread-381] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1367394141-172.17.0.2-1606979969461 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17: 155ms
2020-12-03 07:19:38,761 [Thread-238] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1367394141-172.17.0.2-1606979969461: 155ms
2020-12-03 07:19:38,761 [Thread-397] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22...
2020-12-03 07:19:38,761 [Thread-401] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17...
2020-12-03 07:19:38,765 [Thread-402] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18...
2020-12-03 07:19:38,765 [Thread-401] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17/current/BP-1367394141-172.17.0.2-1606979969461/current/replicas doesn't exist 
2020-12-03 07:19:38,765 [Thread-402] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18/current/BP-1367394141-172.17.0.2-1606979969461/current/replicas doesn't exist 
2020-12-03 07:19:38,771 [Thread-402] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18: 6ms
2020-12-03 07:19:38,772 [Thread-401] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17: 8ms
2020-12-03 07:19:38,773 [Thread-238] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461: 12ms
2020-12-03 07:19:38,784 [Thread-389] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1367394141-172.17.0.2-1606979969461 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19: 112ms
2020-12-03 07:19:38,792 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18
2020-12-03 07:19:38,792 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17
2020-12-03 07:19:38,796 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18, DS-65ca39ad-f8ba-4de7-a267-6c125a4b507b): finished scanning block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:38,796 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17, DS-15a77805-6e7f-4aa2-9de6-622edf3907c2): finished scanning block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:38,797 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17, DS-15a77805-6e7f-4aa2-9de6-622edf3907c2): no suitable block pools found to scan.  Waiting 1814399987 ms.
2020-12-03 07:19:38,799 [Thread-396] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1367394141-172.17.0.2-1606979969461 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21: 64ms
2020-12-03 07:19:38,792 [Thread-238] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 7:55 AM with interval of 21600000ms
2020-12-03 07:19:38,786 [IPC Server handler 7 on default port 35332] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-b23a0f3d-1578-4216-aa54-57c659e1100a for DN 127.0.0.1:34075
2020-12-03 07:19:38,820 [IPC Server handler 7 on default port 35332] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-c23cda38-630d-4441-b4d8-5b0ca6373351 for DN 127.0.0.1:34075
2020-12-03 07:19:38,803 [Thread-390] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1367394141-172.17.0.2-1606979969461 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20: 129ms
2020-12-03 07:19:38,797 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18, DS-65ca39ad-f8ba-4de7-a267-6c125a4b507b): no suitable block pools found to scan.  Waiting 1814399987 ms.
2020-12-03 07:19:38,824 [Thread-260] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1367394141-172.17.0.2-1606979969461: 153ms
2020-12-03 07:19:38,825 [Thread-407] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19...
2020-12-03 07:19:38,825 [Thread-408] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20...
2020-12-03 07:19:38,825 [Thread-407] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19/current/BP-1367394141-172.17.0.2-1606979969461/current/replicas doesn't exist 
2020-12-03 07:19:38,825 [Thread-408] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20/current/BP-1367394141-172.17.0.2-1606979969461/current/replicas doesn't exist 
2020-12-03 07:19:38,825 [Thread-407] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19: 1ms
2020-12-03 07:19:38,826 [Thread-408] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20: 1ms
2020-12-03 07:19:38,831 [Thread-260] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461: 7ms
2020-12-03 07:19:38,832 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20
2020-12-03 07:19:38,832 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20, DS-5beec8c8-67b2-429e-b7bb-c700d530aa6a): finished scanning block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:38,832 [Thread-260] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 7:43 AM with interval of 21600000ms
2020-12-03 07:19:38,834 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid be96e054-5ebf-4915-a869-89779c6b94e9) service to localhost/127.0.0.1:35332 beginning handshake with NN
2020-12-03 07:19:38,834 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20, DS-5beec8c8-67b2-429e-b7bb-c700d530aa6a): no suitable block pools found to scan.  Waiting 1814399997 ms.
2020-12-03 07:19:38,834 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19
2020-12-03 07:19:38,835 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19, DS-97859208-57d6-4d0f-a458-1e4adaf0a78a): finished scanning block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:38,836 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19, DS-97859208-57d6-4d0f-a458-1e4adaf0a78a): no suitable block pools found to scan.  Waiting 1814399995 ms.
2020-12-03 07:19:38,846 [IPC Server handler 9 on default port 35332] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:38,849 [IPC Server handler 4 on default port 35332] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:46845, datanodeUuid=be96e054-5ebf-4915-a869-89779c6b94e9, infoPort=43849, infoSecurePort=0, ipcPort=37058, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461) storage be96e054-5ebf-4915-a869-89779c6b94e9
2020-12-03 07:19:38,850 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:19:38,850 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:19:38,850 [IPC Server handler 4 on default port 35332] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:46845
2020-12-03 07:19:38,851 [IPC Server handler 4 on default port 35332] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN be96e054-5ebf-4915-a869-89779c6b94e9 (127.0.0.1:46845).
2020-12-03 07:19:38,857 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid be96e054-5ebf-4915-a869-89779c6b94e9) service to localhost/127.0.0.1:35332 successfully registered with NN
2020-12-03 07:19:38,857 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (DataNode.java:registerBlockPoolWithSecretManager(1615)) - Block token params received from NN: for block pool BP-1367394141-172.17.0.2-1606979969461 keyUpdateInterval=600 min(s), tokenLifetime=600 min(s)
2020-12-03 07:19:38,857 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:19:38,857 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x6767d08cdb01c1db: Processing first storage report for DS-b23a0f3d-1578-4216-aa54-57c659e1100a from datanode f1c32787-7e56-457d-90b0-80f26f785f52
2020-12-03 07:19:38,857 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x6767d08cdb01c1db: from storage DS-b23a0f3d-1578-4216-aa54-57c659e1100a node DatanodeRegistration(127.0.0.1:34075, datanodeUuid=f1c32787-7e56-457d-90b0-80f26f785f52, infoPort=37609, infoSecurePort=0, ipcPort=40394, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461), blocks: 0, hasStaleStorage: true, processing time: 1 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,858 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x6767d08cdb01c1db: Processing first storage report for DS-c23cda38-630d-4441-b4d8-5b0ca6373351 from datanode f1c32787-7e56-457d-90b0-80f26f785f52
2020-12-03 07:19:38,858 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x6767d08cdb01c1db: from storage DS-c23cda38-630d-4441-b4d8-5b0ca6373351 node DatanodeRegistration(127.0.0.1:34075, datanodeUuid=f1c32787-7e56-457d-90b0-80f26f785f52, infoPort=37609, infoSecurePort=0, ipcPort=40394, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,857 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:35332 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:19:38,859 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x6767d08cdb01c1db,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 9 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:19:38,859 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:38,861 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid b771649f-3cb7-43a2-9de1-a830c893f998) service to localhost/127.0.0.1:35332 beginning handshake with NN
2020-12-03 07:19:38,863 [IPC Server handler 6 on default port 35332] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:43005, datanodeUuid=b771649f-3cb7-43a2-9de1-a830c893f998, infoPort=33760, infoSecurePort=0, ipcPort=45450, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461) storage b771649f-3cb7-43a2-9de1-a830c893f998
2020-12-03 07:19:38,863 [IPC Server handler 6 on default port 35332] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:43005
2020-12-03 07:19:38,864 [IPC Server handler 6 on default port 35332] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN b771649f-3cb7-43a2-9de1-a830c893f998 (127.0.0.1:43005).
2020-12-03 07:19:38,865 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid b771649f-3cb7-43a2-9de1-a830c893f998) service to localhost/127.0.0.1:35332 successfully registered with NN
2020-12-03 07:19:38,865 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (DataNode.java:registerBlockPoolWithSecretManager(1615)) - Block token params received from NN: for block pool BP-1367394141-172.17.0.2-1606979969461 keyUpdateInterval=600 min(s), tokenLifetime=600 min(s)
2020-12-03 07:19:38,866 [IPC Server handler 8 on default port 35332] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-97859208-57d6-4d0f-a458-1e4adaf0a78a for DN 127.0.0.1:46845
2020-12-03 07:19:38,866 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:19:38,867 [IPC Server handler 8 on default port 35332] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-5beec8c8-67b2-429e-b7bb-c700d530aa6a for DN 127.0.0.1:46845
2020-12-03 07:19:38,867 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:35332 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:19:38,874 [Thread-397] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1367394141-172.17.0.2-1606979969461 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22: 113ms
2020-12-03 07:19:38,874 [Thread-282] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1367394141-172.17.0.2-1606979969461: 162ms
2020-12-03 07:19:38,874 [IPC Server handler 0 on default port 35332] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-15a77805-6e7f-4aa2-9de6-622edf3907c2 for DN 127.0.0.1:43005
2020-12-03 07:19:38,875 [Thread-412] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21...
2020-12-03 07:19:38,875 [Thread-413] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22...
2020-12-03 07:19:38,875 [IPC Server handler 0 on default port 35332] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-65ca39ad-f8ba-4de7-a267-6c125a4b507b for DN 127.0.0.1:43005
2020-12-03 07:19:38,875 [Thread-413] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22/current/BP-1367394141-172.17.0.2-1606979969461/current/replicas doesn't exist 
2020-12-03 07:19:38,875 [Thread-412] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21/current/BP-1367394141-172.17.0.2-1606979969461/current/replicas doesn't exist 
2020-12-03 07:19:38,875 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x9c20a2f6675c4ecd: Processing first storage report for DS-5beec8c8-67b2-429e-b7bb-c700d530aa6a from datanode be96e054-5ebf-4915-a869-89779c6b94e9
2020-12-03 07:19:38,875 [Thread-413] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22: 0ms
2020-12-03 07:19:38,875 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x9c20a2f6675c4ecd: from storage DS-5beec8c8-67b2-429e-b7bb-c700d530aa6a node DatanodeRegistration(127.0.0.1:46845, datanodeUuid=be96e054-5ebf-4915-a869-89779c6b94e9, infoPort=43849, infoSecurePort=0, ipcPort=37058, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,875 [Thread-412] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21: 1ms
2020-12-03 07:19:38,879 [Thread-282] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1367394141-172.17.0.2-1606979969461: 5ms
2020-12-03 07:19:38,880 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22
2020-12-03 07:19:38,879 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x9c20a2f6675c4ecd: Processing first storage report for DS-97859208-57d6-4d0f-a458-1e4adaf0a78a from datanode be96e054-5ebf-4915-a869-89779c6b94e9
2020-12-03 07:19:38,880 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x9c20a2f6675c4ecd: from storage DS-97859208-57d6-4d0f-a458-1e4adaf0a78a node DatanodeRegistration(127.0.0.1:46845, datanodeUuid=be96e054-5ebf-4915-a869-89779c6b94e9, infoPort=43849, infoSecurePort=0, ipcPort=37058, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461), blocks: 0, hasStaleStorage: false, processing time: 2 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,881 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22, DS-695468af-ff85-41de-a309-3e9cf958ef94): finished scanning block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:38,881 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22, DS-695468af-ff85-41de-a309-3e9cf958ef94): no suitable block pools found to scan.  Waiting 1814399999 ms.
2020-12-03 07:19:38,881 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1367394141-172.17.0.2-1606979969461 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21
2020-12-03 07:19:38,881 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x6ee4c61a5b64683e: Processing first storage report for DS-65ca39ad-f8ba-4de7-a267-6c125a4b507b from datanode b771649f-3cb7-43a2-9de1-a830c893f998
2020-12-03 07:19:38,881 [Thread-282] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 12:13 PM with interval of 21600000ms
2020-12-03 07:19:38,882 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x6ee4c61a5b64683e: from storage DS-65ca39ad-f8ba-4de7-a267-6c125a4b507b node DatanodeRegistration(127.0.0.1:43005, datanodeUuid=b771649f-3cb7-43a2-9de1-a830c893f998, infoPort=33760, infoSecurePort=0, ipcPort=45450, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461), blocks: 0, hasStaleStorage: true, processing time: 1 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,882 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21, DS-129d8406-e71c-470b-9430-2dc52ce7b622): finished scanning block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:38,882 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x6ee4c61a5b64683e: Processing first storage report for DS-15a77805-6e7f-4aa2-9de6-622edf3907c2 from datanode b771649f-3cb7-43a2-9de1-a830c893f998
2020-12-03 07:19:38,884 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x6ee4c61a5b64683e: from storage DS-15a77805-6e7f-4aa2-9de6-622edf3907c2 node DatanodeRegistration(127.0.0.1:43005, datanodeUuid=b771649f-3cb7-43a2-9de1-a830c893f998, infoPort=33760, infoSecurePort=0, ipcPort=45450, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461), blocks: 0, hasStaleStorage: false, processing time: 2 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,891 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid 0bedb3f9-8b9a-41b5-a47d-b766930b42be) service to localhost/127.0.0.1:35332 beginning handshake with NN
2020-12-03 07:19:38,891 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21, DS-129d8406-e71c-470b-9430-2dc52ce7b622): no suitable block pools found to scan.  Waiting 1814399989 ms.
2020-12-03 07:19:38,892 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x9c20a2f6675c4ecd,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 23 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:19:38,892 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:38,893 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x6ee4c61a5b64683e,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 14 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:19:38,893 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:38,893 [IPC Server handler 5 on default port 35332] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:41862, datanodeUuid=0bedb3f9-8b9a-41b5-a47d-b766930b42be, infoPort=45315, infoSecurePort=0, ipcPort=34517, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461) storage 0bedb3f9-8b9a-41b5-a47d-b766930b42be
2020-12-03 07:19:38,894 [IPC Server handler 5 on default port 35332] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:41862
2020-12-03 07:19:38,894 [IPC Server handler 5 on default port 35332] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 0bedb3f9-8b9a-41b5-a47d-b766930b42be (127.0.0.1:41862).
2020-12-03 07:19:38,895 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid 0bedb3f9-8b9a-41b5-a47d-b766930b42be) service to localhost/127.0.0.1:35332 successfully registered with NN
2020-12-03 07:19:38,895 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (DataNode.java:registerBlockPoolWithSecretManager(1615)) - Block token params received from NN: for block pool BP-1367394141-172.17.0.2-1606979969461 keyUpdateInterval=600 min(s), tokenLifetime=600 min(s)
2020-12-03 07:19:38,895 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  block.BlockTokenSecretManager (BlockTokenSecretManager.java:addKeys(233)) - Setting block keys
2020-12-03 07:19:38,895 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:35332 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-12-03 07:19:38,900 [IPC Server handler 9 on default port 35332] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-129d8406-e71c-470b-9430-2dc52ce7b622 for DN 127.0.0.1:41862
2020-12-03 07:19:38,900 [IPC Server handler 9 on default port 35332] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-695468af-ff85-41de-a309-3e9cf958ef94 for DN 127.0.0.1:41862
2020-12-03 07:19:38,904 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0xd794fdd8f1d81d8e: Processing first storage report for DS-695468af-ff85-41de-a309-3e9cf958ef94 from datanode 0bedb3f9-8b9a-41b5-a47d-b766930b42be
2020-12-03 07:19:38,904 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0xd794fdd8f1d81d8e: from storage DS-695468af-ff85-41de-a309-3e9cf958ef94 node DatanodeRegistration(127.0.0.1:41862, datanodeUuid=0bedb3f9-8b9a-41b5-a47d-b766930b42be, infoPort=45315, infoSecurePort=0, ipcPort=34517, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,904 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0xd794fdd8f1d81d8e: Processing first storage report for DS-129d8406-e71c-470b-9430-2dc52ce7b622 from datanode 0bedb3f9-8b9a-41b5-a47d-b766930b42be
2020-12-03 07:19:38,904 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0xd794fdd8f1d81d8e: from storage DS-129d8406-e71c-470b-9430-2dc52ce7b622 node DatanodeRegistration(127.0.0.1:41862, datanodeUuid=0bedb3f9-8b9a-41b5-a47d-b766930b42be, infoPort=45315, infoSecurePort=0, ipcPort=34517, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:19:38,908 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0xd794fdd8f1d81d8e,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 6 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:19:38,908 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:38,952 [IPC Server handler 4 on default port 35332] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:19:38,956 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2758)) - Cluster is active
2020-12-03 07:19:38,986 [IPC Server handler 1 on default port 35332] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/striped	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-12-03 07:19:39,030 [IPC Server handler 6 on default port 35332] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setErasureCodingPolicy	src=/striped	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-12-03 07:19:39,038 [IPC Server handler 8 on default port 35332] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=enableErasureCodingPolicy	src=RS-6-3-1024k	dst=null	perm=null	proto=rpc
2020-12-03 07:19:39,048 [Thread-417] INFO  hdfs.TestFileChecksum (TestFileChecksum.java:testStripedFileChecksumWithMissedDataBlocksRangeQuery(290)) - Checksum file:/striped/stripedFileChecksum1, requested length:37748735
2020-12-03 07:19:40,116 [IPC Server handler 0 on default port 35332] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/striped/stripedFileChecksum1	dst=null	perm=null	proto=rpc
2020-12-03 07:19:40,146 [IPC Server handler 3 on default port 35332] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/striped/stripedFileChecksum1	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2020-12-03 07:19:40,592 [IPC Server handler 2 on default port 35332] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_-9223372036854775792_1001, replicas=127.0.0.1:44555, 127.0.0.1:46845, 127.0.0.1:34075, 127.0.0.1:38308, 127.0.0.1:41862, 127.0.0.1:44715, 127.0.0.1:38503, 127.0.0.1:37110, 127.0.0.1:43005 for /striped/stripedFileChecksum1
2020-12-03 07:19:40,627 [Thread-418] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:40,648 [Thread-419] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:40,734 [Thread-420] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:40,734 [Thread-421] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:40,744 [Thread-422] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:40,756 [Thread-423] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:40,770 [Thread-424] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:40,825 [Thread-425] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:40,856 [Thread-426] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:40,928 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:46914 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001 src: /127.0.0.1:46914 dest: /127.0.0.1:46845
2020-12-03 07:19:40,932 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:48362 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001 src: /127.0.0.1:48362 dest: /127.0.0.1:38503
2020-12-03 07:19:40,936 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:41584 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775785_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775785_1001 src: /127.0.0.1:41584 dest: /127.0.0.1:37110
2020-12-03 07:19:40,937 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:53536 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001 src: /127.0.0.1:53536 dest: /127.0.0.1:41862
2020-12-03 07:19:40,937 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:43258 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775784_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775784_1001 src: /127.0.0.1:43258 dest: /127.0.0.1:43005
2020-12-03 07:19:40,938 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:39468 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775789_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775789_1001 src: /127.0.0.1:39468 dest: /127.0.0.1:38308
2020-12-03 07:19:40,938 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:51208 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001 src: /127.0.0.1:51208 dest: /127.0.0.1:44555
2020-12-03 07:19:40,938 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:39378 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001 src: /127.0.0.1:39378 dest: /127.0.0.1:34075
2020-12-03 07:19:40,939 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:54572 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001 src: /127.0.0.1:54572 dest: /127.0.0.1:44715
2020-12-03 07:19:42,996 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:54572 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001]] WARN  impl.FsDatasetImpl (InstrumentedLock.java:logWarning(143)) - Lock held time above threshold: lock identifier: org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl lockHeldTimeMs=2033 ms. Suppressed 0 lock warnings. The stack trace is: java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.util.StringUtils.getStackTrace(StringUtils.java:1032)
org.apache.hadoop.util.InstrumentedLock.logWarning(InstrumentedLock.java:148)
org.apache.hadoop.util.InstrumentedLock.check(InstrumentedLock.java:186)
org.apache.hadoop.util.InstrumentedLock.unlock(InstrumentedLock.java:133)
org.apache.hadoop.util.AutoCloseableLock.release(AutoCloseableLock.java:84)
org.apache.hadoop.util.AutoCloseableLock.close(AutoCloseableLock.java:96)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap.add(ReplicaMap.java:135)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.createRbw(FsDatasetImpl.java:1434)
org.apache.hadoop.hdfs.server.datanode.BlockReceiver.<init>(BlockReceiver.java:216)
org.apache.hadoop.hdfs.server.datanode.DataXceiver.getBlockReceiver(DataXceiver.java:1312)
org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:763)
org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
java.lang.Thread.run(Thread.java:748)

2020-12-03 07:19:43,060 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@60297f36] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(205)) - Detected pause in JVM or host machine (eg GC): pause of approximately 1826ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=899ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1126ms
2020-12-03 07:19:43,061 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@7bab3f1a] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(205)) - Detected pause in JVM or host machine (eg GC): pause of approximately 2071ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=899ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1126ms
2020-12-03 07:19:43,062 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@7a34b7b8] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(205)) - Detected pause in JVM or host machine (eg GC): pause of approximately 1838ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=899ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1126ms
2020-12-03 07:19:43,062 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@405325cf] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(205)) - Detected pause in JVM or host machine (eg GC): pause of approximately 1844ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=899ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1126ms
2020-12-03 07:19:43,062 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@7db0565c] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(205)) - Detected pause in JVM or host machine (eg GC): pause of approximately 1982ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=899ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1126ms
2020-12-03 07:19:43,062 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@26d10f2e] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(205)) - Detected pause in JVM or host machine (eg GC): pause of approximately 1989ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=899ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1126ms
2020-12-03 07:19:43,062 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@641856] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(205)) - Detected pause in JVM or host machine (eg GC): pause of approximately 2004ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=899ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1126ms
2020-12-03 07:19:43,061 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@199e4c2b] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(205)) - Detected pause in JVM or host machine (eg GC): pause of approximately 2004ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=899ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1126ms
2020-12-03 07:19:43,061 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@3e2fc448] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(205)) - Detected pause in JVM or host machine (eg GC): pause of approximately 2070ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=899ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1126ms
2020-12-03 07:19:43,061 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@70e659aa] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(205)) - Detected pause in JVM or host machine (eg GC): pause of approximately 2070ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=899ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1126ms
2020-12-03 07:19:43,061 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@7ee55e70] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(205)) - Detected pause in JVM or host machine (eg GC): pause of approximately 2070ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=899ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1126ms
2020-12-03 07:19:43,061 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@33c2bd] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(205)) - Detected pause in JVM or host machine (eg GC): pause of approximately 2071ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=899ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1126ms
2020-12-03 07:19:43,971 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:54572, dest: /127.0.0.1:44715, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 72c80882-49c9-444a-982c-993baca6bb62, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001, duration(ns): 597326254
2020-12-03 07:19:43,972 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:43,974 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:46914, dest: /127.0.0.1:46845, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: be96e054-5ebf-4915-a869-89779c6b94e9, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001, duration(ns): 620052587
2020-12-03 07:19:43,976 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:43,974 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775789_1001, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:39468, dest: /127.0.0.1:38308, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: e5737a0c-e925-4b69-8166-e4391ac5f8bb, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775789_1001, duration(ns): 655362097
2020-12-03 07:19:43,977 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775789_1001, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775789_1001, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:43,984 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:48362, dest: /127.0.0.1:38503, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 1948f61e-988c-4bd0-ad24-d3ae7d4c96b1, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001, duration(ns): 592739939
2020-12-03 07:19:43,984 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:43,998 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775784_1001, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:43258, dest: /127.0.0.1:43005, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: b771649f-3cb7-43a2-9de1-a830c893f998, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775784_1001, duration(ns): 592847038
2020-12-03 07:19:43,999 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775784_1001, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775784_1001, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:44,007 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:39378, dest: /127.0.0.1:34075, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: f1c32787-7e56-457d-90b0-80f26f785f52, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001, duration(ns): 611021455
2020-12-03 07:19:44,007 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:44,008 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:51208, dest: /127.0.0.1:44555, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 811e904d-4b58-4d09-9a37-ecd8d80551dd, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001, duration(ns): 576622740
2020-12-03 07:19:44,008 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:44,047 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:53536, dest: /127.0.0.1:41862, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 0bedb3f9-8b9a-41b5-a47d-b766930b42be, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001, duration(ns): 691718621
2020-12-03 07:19:44,047 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:44,053 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775785_1001, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:41584, dest: /127.0.0.1:37110, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 6433c7f1-b9cb-4ed4-9404-85e3ec0e7233, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775785_1001, duration(ns): 722945098
2020-12-03 07:19:44,053 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775785_1001, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775785_1001, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:44,079 [IPC Server handler 3 on default port 35332] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_-9223372036854775776_1002, replicas=127.0.0.1:44555, 127.0.0.1:44715, 127.0.0.1:41862, 127.0.0.1:46438, 127.0.0.1:34075, 127.0.0.1:43005, 127.0.0.1:46845, 127.0.0.1:39913, 127.0.0.1:38308 for /striped/stripedFileChecksum1
2020-12-03 07:19:44,088 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:44,098 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:44,102 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:44,103 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:51292 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775776_1002]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775776_1002 src: /127.0.0.1:51292 dest: /127.0.0.1:44555
2020-12-03 07:19:44,130 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:53612 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775774_1002]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775774_1002 src: /127.0.0.1:53612 dest: /127.0.0.1:41862
2020-12-03 07:19:44,144 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:54648 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775775_1002]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775775_1002 src: /127.0.0.1:54648 dest: /127.0.0.1:44715
2020-12-03 07:19:44,182 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:44,205 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:47754 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775773_1002]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775773_1002 src: /127.0.0.1:47754 dest: /127.0.0.1:46438
2020-12-03 07:19:44,217 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:44,220 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:44,231 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:39482 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775772_1002]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775772_1002 src: /127.0.0.1:39482 dest: /127.0.0.1:34075
2020-12-03 07:19:44,236 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:43346 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775771_1002]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775771_1002 src: /127.0.0.1:43346 dest: /127.0.0.1:43005
2020-12-03 07:19:44,240 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:44,270 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:44,277 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:47026 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775770_1002]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775770_1002 src: /127.0.0.1:47026 dest: /127.0.0.1:46845
2020-12-03 07:19:44,304 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:47114 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775769_1002]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775769_1002 src: /127.0.0.1:47114 dest: /127.0.0.1:39913
2020-12-03 07:19:44,316 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:44,333 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:39584 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775768_1002]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775768_1002 src: /127.0.0.1:39584 dest: /127.0.0.1:38308
2020-12-03 07:19:44,618 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775774_1002, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:53612, dest: /127.0.0.1:41862, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 0bedb3f9-8b9a-41b5-a47d-b766930b42be, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775774_1002, duration(ns): 467292885
2020-12-03 07:19:44,618 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775776_1002, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:51292, dest: /127.0.0.1:44555, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 811e904d-4b58-4d09-9a37-ecd8d80551dd, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775776_1002, duration(ns): 488339509
2020-12-03 07:19:44,618 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775774_1002, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775774_1002, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:44,620 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775768_1002, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:39584, dest: /127.0.0.1:38308, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: e5737a0c-e925-4b69-8166-e4391ac5f8bb, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775768_1002, duration(ns): 277937296
2020-12-03 07:19:44,620 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775772_1002, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:39482, dest: /127.0.0.1:34075, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: f1c32787-7e56-457d-90b0-80f26f785f52, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775772_1002, duration(ns): 375410612
2020-12-03 07:19:44,620 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775768_1002, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775768_1002, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:44,620 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775772_1002, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775772_1002, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:44,627 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775771_1002, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:43346, dest: /127.0.0.1:43005, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: b771649f-3cb7-43a2-9de1-a830c893f998, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775771_1002, duration(ns): 364311889
2020-12-03 07:19:44,627 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775775_1002, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:54648, dest: /127.0.0.1:44715, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 72c80882-49c9-444a-982c-993baca6bb62, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775775_1002, duration(ns): 454939372
2020-12-03 07:19:44,620 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775776_1002, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775776_1002, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:44,632 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775775_1002, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775775_1002, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:44,620 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775770_1002, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:47026, dest: /127.0.0.1:46845, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: be96e054-5ebf-4915-a869-89779c6b94e9, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775770_1002, duration(ns): 311032076
2020-12-03 07:19:44,632 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775771_1002, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775771_1002, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:44,639 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775770_1002, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775770_1002, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:44,641 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775773_1002, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:47754, dest: /127.0.0.1:46438, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 0e810974-a498-4742-a093-0c4d95031785, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775773_1002, duration(ns): 400540450
2020-12-03 07:19:44,642 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775773_1002, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775773_1002, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:44,650 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775769_1002, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:47114, dest: /127.0.0.1:39913, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 69a597d2-2edd-4e23-9c38-3328eccbc91b, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775769_1002, duration(ns): 300032064
2020-12-03 07:19:44,650 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775769_1002, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775769_1002, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:44,659 [IPC Server handler 7 on default port 35332] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_-9223372036854775760_1003, replicas=127.0.0.1:39913, 127.0.0.1:44555, 127.0.0.1:38308, 127.0.0.1:46845, 127.0.0.1:34075, 127.0.0.1:46438, 127.0.0.1:37110, 127.0.0.1:44715, 127.0.0.1:41862 for /striped/stripedFileChecksum1
2020-12-03 07:19:44,665 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:44,666 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:44,667 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:47160 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775760_1003]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775760_1003 src: /127.0.0.1:47160 dest: /127.0.0.1:39913
2020-12-03 07:19:44,667 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:51376 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775759_1003]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775759_1003 src: /127.0.0.1:51376 dest: /127.0.0.1:44555
2020-12-03 07:19:44,674 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:44,692 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:39632 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775758_1003]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775758_1003 src: /127.0.0.1:39632 dest: /127.0.0.1:38308
2020-12-03 07:19:44,695 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:44,712 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:47098 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775757_1003]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775757_1003 src: /127.0.0.1:47098 dest: /127.0.0.1:46845
2020-12-03 07:19:44,715 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:44,732 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:44,736 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:39564 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775756_1003]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775756_1003 src: /127.0.0.1:39564 dest: /127.0.0.1:34075
2020-12-03 07:19:44,736 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:47840 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775755_1003]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775755_1003 src: /127.0.0.1:47840 dest: /127.0.0.1:46438
2020-12-03 07:19:44,781 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:44,786 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:44,860 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:44,861 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:53720 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775752_1003]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775752_1003 src: /127.0.0.1:53720 dest: /127.0.0.1:41862
2020-12-03 07:19:44,862 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:54756 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775753_1003]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775753_1003 src: /127.0.0.1:54756 dest: /127.0.0.1:44715
2020-12-03 07:19:44,862 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:41756 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775754_1003]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775754_1003 src: /127.0.0.1:41756 dest: /127.0.0.1:37110
2020-12-03 07:19:45,023 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775752_1003, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:53720, dest: /127.0.0.1:41862, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 0bedb3f9-8b9a-41b5-a47d-b766930b42be, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775752_1003, duration(ns): 150849017
2020-12-03 07:19:45,023 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775752_1003, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775752_1003, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,024 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775759_1003, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:51376, dest: /127.0.0.1:44555, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 811e904d-4b58-4d09-9a37-ecd8d80551dd, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775759_1003, duration(ns): 333970199
2020-12-03 07:19:45,024 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775759_1003, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775759_1003, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,032 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775756_1003, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:39564, dest: /127.0.0.1:34075, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: f1c32787-7e56-457d-90b0-80f26f785f52, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775756_1003, duration(ns): 270295269
2020-12-03 07:19:45,033 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775756_1003, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775756_1003, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,033 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775758_1003, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:39632, dest: /127.0.0.1:38308, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: e5737a0c-e925-4b69-8166-e4391ac5f8bb, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775758_1003, duration(ns): 332481967
2020-12-03 07:19:45,033 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775760_1003, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:47160, dest: /127.0.0.1:39913, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 69a597d2-2edd-4e23-9c38-3328eccbc91b, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775760_1003, duration(ns): 345886052
2020-12-03 07:19:45,033 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775758_1003, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775758_1003, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,033 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775754_1003, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:41756, dest: /127.0.0.1:37110, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 6433c7f1-b9cb-4ed4-9404-85e3ec0e7233, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775754_1003, duration(ns): 152707610
2020-12-03 07:19:45,033 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775757_1003, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:47098, dest: /127.0.0.1:46845, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: be96e054-5ebf-4915-a869-89779c6b94e9, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775757_1003, duration(ns): 318785386
2020-12-03 07:19:45,034 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775755_1003, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:47840, dest: /127.0.0.1:46438, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 0e810974-a498-4742-a093-0c4d95031785, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775755_1003, duration(ns): 160423794
2020-12-03 07:19:45,034 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775754_1003, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775754_1003, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,034 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775755_1003, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775755_1003, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,034 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775757_1003, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775757_1003, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,034 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775760_1003, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775760_1003, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,056 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775753_1003, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:54756, dest: /127.0.0.1:44715, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 72c80882-49c9-444a-982c-993baca6bb62, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775753_1003, duration(ns): 163011302
2020-12-03 07:19:45,057 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775753_1003, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775753_1003, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,062 [IPC Server handler 7 on default port 35332] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_-9223372036854775744_1004, replicas=127.0.0.1:38503, 127.0.0.1:46845, 127.0.0.1:44555, 127.0.0.1:39913, 127.0.0.1:37110, 127.0.0.1:41862, 127.0.0.1:38308, 127.0.0.1:46438, 127.0.0.1:34075 for /striped/stripedFileChecksum1
2020-12-03 07:19:45,068 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,069 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:48606 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775744_1004]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775744_1004 src: /127.0.0.1:48606 dest: /127.0.0.1:38503
2020-12-03 07:19:45,070 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,081 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:47172 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775743_1004]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775743_1004 src: /127.0.0.1:47172 dest: /127.0.0.1:46845
2020-12-03 07:19:45,083 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,088 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:51470 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775742_1004]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775742_1004 src: /127.0.0.1:51470 dest: /127.0.0.1:44555
2020-12-03 07:19:45,096 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,106 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:47260 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775741_1004]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775741_1004 src: /127.0.0.1:47260 dest: /127.0.0.1:39913
2020-12-03 07:19:45,112 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,117 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:41830 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775740_1004]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775740_1004 src: /127.0.0.1:41830 dest: /127.0.0.1:37110
2020-12-03 07:19:45,121 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,130 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:53792 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775739_1004]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775739_1004 src: /127.0.0.1:53792 dest: /127.0.0.1:41862
2020-12-03 07:19:45,159 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,169 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:39732 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775738_1004]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775738_1004 src: /127.0.0.1:39732 dest: /127.0.0.1:38308
2020-12-03 07:19:45,178 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,180 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,206 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:47924 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775737_1004]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775737_1004 src: /127.0.0.1:47924 dest: /127.0.0.1:46438
2020-12-03 07:19:45,209 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:39648 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775736_1004]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775736_1004 src: /127.0.0.1:39648 dest: /127.0.0.1:34075
2020-12-03 07:19:45,437 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775739_1004, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:53792, dest: /127.0.0.1:41862, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 0bedb3f9-8b9a-41b5-a47d-b766930b42be, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775739_1004, duration(ns): 293113779
2020-12-03 07:19:45,438 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775738_1004, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:39732, dest: /127.0.0.1:38308, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: e5737a0c-e925-4b69-8166-e4391ac5f8bb, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775738_1004, duration(ns): 249791303
2020-12-03 07:19:45,444 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775738_1004, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775738_1004, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,444 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775739_1004, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775739_1004, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,457 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775736_1004, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:39648, dest: /127.0.0.1:34075, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: f1c32787-7e56-457d-90b0-80f26f785f52, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775736_1004, duration(ns): 187096780
2020-12-03 07:19:45,458 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775736_1004, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775736_1004, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,457 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775737_1004, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:47924, dest: /127.0.0.1:46438, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 0e810974-a498-4742-a093-0c4d95031785, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775737_1004, duration(ns): 218706317
2020-12-03 07:19:45,458 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775737_1004, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775737_1004, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,459 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775743_1004, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:47172, dest: /127.0.0.1:46845, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: be96e054-5ebf-4915-a869-89779c6b94e9, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775743_1004, duration(ns): 354410109
2020-12-03 07:19:45,459 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775743_1004, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775743_1004, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,467 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775742_1004, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:51470, dest: /127.0.0.1:44555, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 811e904d-4b58-4d09-9a37-ecd8d80551dd, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775742_1004, duration(ns): 339930018
2020-12-03 07:19:45,468 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775742_1004, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775742_1004, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,473 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775741_1004, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:47260, dest: /127.0.0.1:39913, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 69a597d2-2edd-4e23-9c38-3328eccbc91b, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775741_1004, duration(ns): 321525055
2020-12-03 07:19:45,473 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775741_1004, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775741_1004, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,482 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775740_1004, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:41830, dest: /127.0.0.1:37110, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 6433c7f1-b9cb-4ed4-9404-85e3ec0e7233, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775740_1004, duration(ns): 310418034
2020-12-03 07:19:45,482 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775740_1004, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775740_1004, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,484 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775744_1004, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:48606, dest: /127.0.0.1:38503, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 1948f61e-988c-4bd0-ad24-d3ae7d4c96b1, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775744_1004, duration(ns): 358185505
2020-12-03 07:19:45,484 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775744_1004, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775744_1004, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,499 [IPC Server handler 7 on default port 35332] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_-9223372036854775728_1005, replicas=127.0.0.1:37110, 127.0.0.1:41862, 127.0.0.1:44715, 127.0.0.1:34075, 127.0.0.1:44555, 127.0.0.1:38503, 127.0.0.1:39913, 127.0.0.1:38308, 127.0.0.1:43005 for /striped/stripedFileChecksum1
2020-12-03 07:19:45,514 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,517 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:41922 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775728_1005]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775728_1005 src: /127.0.0.1:41922 dest: /127.0.0.1:37110
2020-12-03 07:19:45,519 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,522 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:53884 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775727_1005]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775727_1005 src: /127.0.0.1:53884 dest: /127.0.0.1:41862
2020-12-03 07:19:45,522 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,528 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:54924 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775726_1005]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775726_1005 src: /127.0.0.1:54924 dest: /127.0.0.1:44715
2020-12-03 07:19:45,529 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,551 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:39738 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775725_1005]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775725_1005 src: /127.0.0.1:39738 dest: /127.0.0.1:34075
2020-12-03 07:19:45,552 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,555 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,560 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:51578 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775724_1005]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775724_1005 src: /127.0.0.1:51578 dest: /127.0.0.1:44555
2020-12-03 07:19:45,563 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:48722 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775723_1005]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775723_1005 src: /127.0.0.1:48722 dest: /127.0.0.1:38503
2020-12-03 07:19:45,576 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,579 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:47374 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775722_1005]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775722_1005 src: /127.0.0.1:47374 dest: /127.0.0.1:39913
2020-12-03 07:19:45,583 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,584 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:39844 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775721_1005]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775721_1005 src: /127.0.0.1:39844 dest: /127.0.0.1:38308
2020-12-03 07:19:45,587 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,589 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:43618 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775720_1005]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775720_1005 src: /127.0.0.1:43618 dest: /127.0.0.1:43005
2020-12-03 07:19:45,794 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775721_1005, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:39844, dest: /127.0.0.1:38308, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: e5737a0c-e925-4b69-8166-e4391ac5f8bb, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775721_1005, duration(ns): 200215568
2020-12-03 07:19:45,795 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775721_1005, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775721_1005, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,804 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775727_1005, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:53884, dest: /127.0.0.1:41862, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 0bedb3f9-8b9a-41b5-a47d-b766930b42be, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775727_1005, duration(ns): 262761303
2020-12-03 07:19:45,822 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775727_1005, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775727_1005, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,814 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775722_1005, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:47374, dest: /127.0.0.1:39913, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 69a597d2-2edd-4e23-9c38-3328eccbc91b, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775722_1005, duration(ns): 223225975
2020-12-03 07:19:45,832 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775722_1005, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775722_1005, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,833 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775724_1005, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:51578, dest: /127.0.0.1:44555, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 811e904d-4b58-4d09-9a37-ecd8d80551dd, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775724_1005, duration(ns): 227080746
2020-12-03 07:19:45,834 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775724_1005, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775724_1005, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,835 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775726_1005, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:54924, dest: /127.0.0.1:44715, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 72c80882-49c9-444a-982c-993baca6bb62, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775726_1005, duration(ns): 288572859
2020-12-03 07:19:45,835 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775726_1005, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775726_1005, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,821 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775728_1005, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:41922, dest: /127.0.0.1:37110, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 6433c7f1-b9cb-4ed4-9404-85e3ec0e7233, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775728_1005, duration(ns): 265374284
2020-12-03 07:19:45,835 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775728_1005, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775728_1005, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,821 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775725_1005, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:39738, dest: /127.0.0.1:34075, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: f1c32787-7e56-457d-90b0-80f26f785f52, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775725_1005, duration(ns): 235072328
2020-12-03 07:19:45,852 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775725_1005, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775725_1005, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,821 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775723_1005, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:48722, dest: /127.0.0.1:38503, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 1948f61e-988c-4bd0-ad24-d3ae7d4c96b1, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775723_1005, duration(ns): 220035309
2020-12-03 07:19:45,860 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775723_1005, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775723_1005, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,821 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775720_1005, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:43618, dest: /127.0.0.1:43005, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: b771649f-3cb7-43a2-9de1-a830c893f998, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775720_1005, duration(ns): 191813050
2020-12-03 07:19:45,862 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775720_1005, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775720_1005, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:45,870 [IPC Server handler 7 on default port 35332] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_-9223372036854775712_1006, replicas=127.0.0.1:38308, 127.0.0.1:34075, 127.0.0.1:37110, 127.0.0.1:44555, 127.0.0.1:46438, 127.0.0.1:43005, 127.0.0.1:46845, 127.0.0.1:44715, 127.0.0.1:39913 for /striped/stripedFileChecksum1
2020-12-03 07:19:45,876 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,880 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:39910 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775712_1006]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775712_1006 src: /127.0.0.1:39910 dest: /127.0.0.1:38308
2020-12-03 07:19:45,882 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,886 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,888 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:42016 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775710_1006]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775710_1006 src: /127.0.0.1:42016 dest: /127.0.0.1:37110
2020-12-03 07:19:45,888 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,905 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:51662 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775709_1006]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775709_1006 src: /127.0.0.1:51662 dest: /127.0.0.1:44555
2020-12-03 07:19:45,905 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:39824 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775711_1006]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775711_1006 src: /127.0.0.1:39824 dest: /127.0.0.1:34075
2020-12-03 07:19:45,917 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,926 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:48120 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775708_1006]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775708_1006 src: /127.0.0.1:48120 dest: /127.0.0.1:46438
2020-12-03 07:19:45,932 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,936 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:43708 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775707_1006]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775707_1006 src: /127.0.0.1:43708 dest: /127.0.0.1:43005
2020-12-03 07:19:45,948 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,955 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:47390 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775706_1006]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775706_1006 src: /127.0.0.1:47390 dest: /127.0.0.1:46845
2020-12-03 07:19:45,972 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:45,989 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:55050 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775705_1006]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775705_1006 src: /127.0.0.1:55050 dest: /127.0.0.1:44715
2020-12-03 07:19:45,996 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:46,005 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:47486 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775704_1006]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775704_1006 src: /127.0.0.1:47486 dest: /127.0.0.1:39913
2020-12-03 07:19:46,384 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775708_1006, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:48120, dest: /127.0.0.1:46438, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 0e810974-a498-4742-a093-0c4d95031785, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775708_1006, duration(ns): 426598705
2020-12-03 07:19:46,384 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775708_1006, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775708_1006, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,386 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775706_1006, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:47390, dest: /127.0.0.1:46845, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: be96e054-5ebf-4915-a869-89779c6b94e9, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775706_1006, duration(ns): 409631531
2020-12-03 07:19:46,386 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775712_1006, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:39910, dest: /127.0.0.1:38308, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: e5737a0c-e925-4b69-8166-e4391ac5f8bb, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775712_1006, duration(ns): 498195490
2020-12-03 07:19:46,386 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775712_1006, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775712_1006, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,387 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775710_1006, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:42016, dest: /127.0.0.1:37110, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 6433c7f1-b9cb-4ed4-9404-85e3ec0e7233, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775710_1006, duration(ns): 468396107
2020-12-03 07:19:46,386 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775706_1006, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775706_1006, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,387 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775710_1006, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775710_1006, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,386 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775707_1006, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:43708, dest: /127.0.0.1:43005, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: b771649f-3cb7-43a2-9de1-a830c893f998, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775707_1006, duration(ns): 437184152
2020-12-03 07:19:46,387 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775707_1006, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775707_1006, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,387 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775711_1006, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:39824, dest: /127.0.0.1:34075, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: f1c32787-7e56-457d-90b0-80f26f785f52, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775711_1006, duration(ns): 459110685
2020-12-03 07:19:46,388 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775711_1006, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775711_1006, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,387 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775709_1006, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:51662, dest: /127.0.0.1:44555, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 811e904d-4b58-4d09-9a37-ecd8d80551dd, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775709_1006, duration(ns): 459107269
2020-12-03 07:19:46,388 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775709_1006, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775709_1006, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,387 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775705_1006, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:55050, dest: /127.0.0.1:44715, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 72c80882-49c9-444a-982c-993baca6bb62, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775705_1006, duration(ns): 383805568
2020-12-03 07:19:46,387 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775704_1006, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:47486, dest: /127.0.0.1:39913, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 69a597d2-2edd-4e23-9c38-3328eccbc91b, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775704_1006, duration(ns): 344492989
2020-12-03 07:19:46,389 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775705_1006, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775705_1006, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,389 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775704_1006, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775704_1006, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,396 [IPC Server handler 6 on default port 35332] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_-9223372036854775696_1007, replicas=127.0.0.1:44555, 127.0.0.1:43005, 127.0.0.1:46438, 127.0.0.1:44715, 127.0.0.1:38503, 127.0.0.1:37110, 127.0.0.1:41862, 127.0.0.1:46845, 127.0.0.1:34075 for /striped/stripedFileChecksum1
2020-12-03 07:19:46,402 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:46,409 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:46,412 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:51776 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775696_1007]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775696_1007 src: /127.0.0.1:51776 dest: /127.0.0.1:44555
2020-12-03 07:19:46,420 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:46,437 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:43804 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775695_1007]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775695_1007 src: /127.0.0.1:43804 dest: /127.0.0.1:43005
2020-12-03 07:19:46,438 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:48224 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775694_1007]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775694_1007 src: /127.0.0.1:48224 dest: /127.0.0.1:46438
2020-12-03 07:19:46,469 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:46,471 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:55138 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775693_1007]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775693_1007 src: /127.0.0.1:55138 dest: /127.0.0.1:44715
2020-12-03 07:19:46,475 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:46,477 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:48928 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775692_1007]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775692_1007 src: /127.0.0.1:48928 dest: /127.0.0.1:38503
2020-12-03 07:19:46,480 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:46,482 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:42146 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775691_1007]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775691_1007 src: /127.0.0.1:42146 dest: /127.0.0.1:37110
2020-12-03 07:19:46,500 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:46,507 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:46,510 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:47498 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775689_1007]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775689_1007 src: /127.0.0.1:47498 dest: /127.0.0.1:46845
2020-12-03 07:19:46,510 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:54108 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775690_1007]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775690_1007 src: /127.0.0.1:54108 dest: /127.0.0.1:41862
2020-12-03 07:19:46,514 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:46,519 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:39962 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775688_1007]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775688_1007 src: /127.0.0.1:39962 dest: /127.0.0.1:34075
2020-12-03 07:19:46,840 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775695_1007, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:43804, dest: /127.0.0.1:43005, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: b771649f-3cb7-43a2-9de1-a830c893f998, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775695_1007, duration(ns): 391115798
2020-12-03 07:19:46,840 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775692_1007, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:48928, dest: /127.0.0.1:38503, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 1948f61e-988c-4bd0-ad24-d3ae7d4c96b1, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775692_1007, duration(ns): 361222054
2020-12-03 07:19:46,840 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775696_1007, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:51776, dest: /127.0.0.1:44555, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 811e904d-4b58-4d09-9a37-ecd8d80551dd, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775696_1007, duration(ns): 402411398
2020-12-03 07:19:46,840 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775690_1007, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:54108, dest: /127.0.0.1:41862, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 0bedb3f9-8b9a-41b5-a47d-b766930b42be, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775690_1007, duration(ns): 323260236
2020-12-03 07:19:46,841 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775689_1007, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:47498, dest: /127.0.0.1:46845, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: be96e054-5ebf-4915-a869-89779c6b94e9, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775689_1007, duration(ns): 320618238
2020-12-03 07:19:46,841 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775689_1007, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775689_1007, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,841 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775693_1007, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:55138, dest: /127.0.0.1:44715, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 72c80882-49c9-444a-982c-993baca6bb62, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775693_1007, duration(ns): 366574529
2020-12-03 07:19:46,841 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775696_1007, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775696_1007, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,841 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775692_1007, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775692_1007, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,842 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775694_1007, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:48224, dest: /127.0.0.1:46438, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 0e810974-a498-4742-a093-0c4d95031785, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775694_1007, duration(ns): 385147264
2020-12-03 07:19:46,841 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775695_1007, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775695_1007, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,842 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775694_1007, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775694_1007, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,842 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775691_1007, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:42146, dest: /127.0.0.1:37110, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 6433c7f1-b9cb-4ed4-9404-85e3ec0e7233, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775691_1007, duration(ns): 357258842
2020-12-03 07:19:46,842 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775693_1007, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775693_1007, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,841 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775690_1007, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775690_1007, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,843 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775688_1007, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:39962, dest: /127.0.0.1:34075, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: f1c32787-7e56-457d-90b0-80f26f785f52, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775688_1007, duration(ns): 320618657
2020-12-03 07:19:46,842 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775691_1007, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775691_1007, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,846 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775688_1007, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775688_1007, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:46,862 [IPC Server handler 6 on default port 35332] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_-9223372036854775680_1008, replicas=127.0.0.1:37110, 127.0.0.1:38503, 127.0.0.1:46845, 127.0.0.1:44715, 127.0.0.1:46438, 127.0.0.1:43005, 127.0.0.1:39913, 127.0.0.1:44555, 127.0.0.1:34075 for /striped/stripedFileChecksum1
2020-12-03 07:19:46,867 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:46,869 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:42242 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775680_1008]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775680_1008 src: /127.0.0.1:42242 dest: /127.0.0.1:37110
2020-12-03 07:19:46,870 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:46,885 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:49028 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775679_1008]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775679_1008 src: /127.0.0.1:49028 dest: /127.0.0.1:38503
2020-12-03 07:19:46,890 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:46,891 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:46,894 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:46,900 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:46,909 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:46,914 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:46,919 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:47,203 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:40086 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775672_1008]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775672_1008 src: /127.0.0.1:40086 dest: /127.0.0.1:34075
2020-12-03 07:19:47,204 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:55258 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775677_1008]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775677_1008 src: /127.0.0.1:55258 dest: /127.0.0.1:44715
2020-12-03 07:19:47,203 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:51918 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775673_1008]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775673_1008 src: /127.0.0.1:51918 dest: /127.0.0.1:44555
2020-12-03 07:19:47,207 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:48350 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775676_1008]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775676_1008 src: /127.0.0.1:48350 dest: /127.0.0.1:46438
2020-12-03 07:19:47,207 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:47704 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775674_1008]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775674_1008 src: /127.0.0.1:47704 dest: /127.0.0.1:39913
2020-12-03 07:19:47,203 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:43936 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775675_1008]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775675_1008 src: /127.0.0.1:43936 dest: /127.0.0.1:43005
2020-12-03 07:19:47,275 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:47604 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775678_1008]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775678_1008 src: /127.0.0.1:47604 dest: /127.0.0.1:46845
2020-12-03 07:19:47,430 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775673_1008, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:51918, dest: /127.0.0.1:44555, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 811e904d-4b58-4d09-9a37-ecd8d80551dd, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775673_1008, duration(ns): 163062985
2020-12-03 07:19:47,430 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775673_1008, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775673_1008, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,431 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775677_1008, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:55258, dest: /127.0.0.1:44715, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 72c80882-49c9-444a-982c-993baca6bb62, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775677_1008, duration(ns): 162349685
2020-12-03 07:19:47,431 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775677_1008, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775677_1008, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,432 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775678_1008, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:47604, dest: /127.0.0.1:46845, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: be96e054-5ebf-4915-a869-89779c6b94e9, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775678_1008, duration(ns): 154514470
2020-12-03 07:19:47,432 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775676_1008, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:48350, dest: /127.0.0.1:46438, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 0e810974-a498-4742-a093-0c4d95031785, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775676_1008, duration(ns): 191652005
2020-12-03 07:19:47,432 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775676_1008, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775676_1008, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,432 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775678_1008, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775678_1008, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,433 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775680_1008, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:42242, dest: /127.0.0.1:37110, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 6433c7f1-b9cb-4ed4-9404-85e3ec0e7233, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775680_1008, duration(ns): 548689360
2020-12-03 07:19:47,434 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775680_1008, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775680_1008, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,462 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775674_1008, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:47704, dest: /127.0.0.1:39913, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 69a597d2-2edd-4e23-9c38-3328eccbc91b, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775674_1008, duration(ns): 220245421
2020-12-03 07:19:47,462 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775672_1008, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:40086, dest: /127.0.0.1:34075, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: f1c32787-7e56-457d-90b0-80f26f785f52, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775672_1008, duration(ns): 154204666
2020-12-03 07:19:47,463 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775674_1008, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775674_1008, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,464 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775679_1008, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:49028, dest: /127.0.0.1:38503, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 1948f61e-988c-4bd0-ad24-d3ae7d4c96b1, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775679_1008, duration(ns): 191609117
2020-12-03 07:19:47,463 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775672_1008, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775672_1008, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,464 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775679_1008, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775679_1008, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,465 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775675_1008, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:43936, dest: /127.0.0.1:43005, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: b771649f-3cb7-43a2-9de1-a830c893f998, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775675_1008, duration(ns): 209836228
2020-12-03 07:19:47,465 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775675_1008, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775675_1008, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,482 [IPC Server handler 5 on default port 35332] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_-9223372036854775664_1009, replicas=127.0.0.1:46845, 127.0.0.1:38503, 127.0.0.1:34075, 127.0.0.1:39913, 127.0.0.1:37110, 127.0.0.1:41862, 127.0.0.1:38308, 127.0.0.1:44555, 127.0.0.1:44715 for /striped/stripedFileChecksum1
2020-12-03 07:19:47,487 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:47,491 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:47762 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775664_1009]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775664_1009 src: /127.0.0.1:47762 dest: /127.0.0.1:46845
2020-12-03 07:19:47,492 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:47,493 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:49200 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775663_1009]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775663_1009 src: /127.0.0.1:49200 dest: /127.0.0.1:38503
2020-12-03 07:19:47,496 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:47,497 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:40228 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775662_1009]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775662_1009 src: /127.0.0.1:40228 dest: /127.0.0.1:34075
2020-12-03 07:19:47,499 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:47,501 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:47854 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775661_1009]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775661_1009 src: /127.0.0.1:47854 dest: /127.0.0.1:39913
2020-12-03 07:19:47,505 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:47,506 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:47,509 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:54390 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775659_1009]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775659_1009 src: /127.0.0.1:54390 dest: /127.0.0.1:41862
2020-12-03 07:19:47,509 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:42426 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775660_1009]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775660_1009 src: /127.0.0.1:42426 dest: /127.0.0.1:37110
2020-12-03 07:19:47,516 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:47,518 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:40330 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775658_1009]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775658_1009 src: /127.0.0.1:40330 dest: /127.0.0.1:38308
2020-12-03 07:19:47,521 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:47,523 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:52078 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775657_1009]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775657_1009 src: /127.0.0.1:52078 dest: /127.0.0.1:44555
2020-12-03 07:19:47,527 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:47,529 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:55434 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775656_1009]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775656_1009 src: /127.0.0.1:55434 dest: /127.0.0.1:44715
2020-12-03 07:19:47,746 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775656_1009, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:55434, dest: /127.0.0.1:44715, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 72c80882-49c9-444a-982c-993baca6bb62, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775656_1009, duration(ns): 214743304
2020-12-03 07:19:47,746 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775664_1009, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:47762, dest: /127.0.0.1:46845, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: be96e054-5ebf-4915-a869-89779c6b94e9, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775664_1009, duration(ns): 252656235
2020-12-03 07:19:47,746 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775656_1009, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775656_1009, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,746 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775660_1009, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:42426, dest: /127.0.0.1:37110, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 6433c7f1-b9cb-4ed4-9404-85e3ec0e7233, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775660_1009, duration(ns): 225783857
2020-12-03 07:19:47,747 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775662_1009, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:40228, dest: /127.0.0.1:34075, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: f1c32787-7e56-457d-90b0-80f26f785f52, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775662_1009, duration(ns): 243233185
2020-12-03 07:19:47,747 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775661_1009, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:47854, dest: /127.0.0.1:39913, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 69a597d2-2edd-4e23-9c38-3328eccbc91b, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775661_1009, duration(ns): 242530835
2020-12-03 07:19:47,746 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775664_1009, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775664_1009, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,746 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775658_1009, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:40330, dest: /127.0.0.1:38308, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: e5737a0c-e925-4b69-8166-e4391ac5f8bb, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775658_1009, duration(ns): 224606844
2020-12-03 07:19:47,746 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775663_1009, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:49200, dest: /127.0.0.1:38503, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 1948f61e-988c-4bd0-ad24-d3ae7d4c96b1, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775663_1009, duration(ns): 251129480
2020-12-03 07:19:47,748 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775663_1009, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775663_1009, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,747 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775658_1009, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775658_1009, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,747 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775661_1009, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775661_1009, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,748 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775659_1009, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:54390, dest: /127.0.0.1:41862, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 0bedb3f9-8b9a-41b5-a47d-b766930b42be, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775659_1009, duration(ns): 237432089
2020-12-03 07:19:47,747 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775662_1009, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775662_1009, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,747 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775660_1009, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775660_1009, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,749 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775657_1009, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:52078, dest: /127.0.0.1:44555, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 811e904d-4b58-4d09-9a37-ecd8d80551dd, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775657_1009, duration(ns): 220021911
2020-12-03 07:19:47,749 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775657_1009, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775657_1009, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,749 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775659_1009, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775659_1009, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,762 [IPC Server handler 5 on default port 35332] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_-9223372036854775648_1010, replicas=127.0.0.1:38503, 127.0.0.1:44715, 127.0.0.1:44555, 127.0.0.1:37110, 127.0.0.1:41862, 127.0.0.1:38308, 127.0.0.1:46845, 127.0.0.1:46438, 127.0.0.1:39913 for /striped/stripedFileChecksum1
2020-12-03 07:19:47,769 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:47,770 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:47,772 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:55528 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775647_1010]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775647_1010 src: /127.0.0.1:55528 dest: /127.0.0.1:44715
2020-12-03 07:19:47,773 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:49312 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775648_1010]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775648_1010 src: /127.0.0.1:49312 dest: /127.0.0.1:38503
2020-12-03 07:19:47,775 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:47,775 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:47,776 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:42532 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775645_1010]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775645_1010 src: /127.0.0.1:42532 dest: /127.0.0.1:37110
2020-12-03 07:19:47,780 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:47,780 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:47,786 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:52176 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775646_1010]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775646_1010 src: /127.0.0.1:52176 dest: /127.0.0.1:44555
2020-12-03 07:19:47,786 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:54496 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775644_1010]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775644_1010 src: /127.0.0.1:54496 dest: /127.0.0.1:41862
2020-12-03 07:19:47,790 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:40436 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775643_1010]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775643_1010 src: /127.0.0.1:40436 dest: /127.0.0.1:38308
2020-12-03 07:19:47,797 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:47,801 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:47,803 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:47888 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775642_1010]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775642_1010 src: /127.0.0.1:47888 dest: /127.0.0.1:46845
2020-12-03 07:19:47,804 [DataStreamer for file /striped/stripedFileChecksum1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:47,809 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:48630 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775641_1010]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775641_1010 src: /127.0.0.1:48630 dest: /127.0.0.1:46438
2020-12-03 07:19:47,809 [DataXceiver for client DFSClient_NONMAPREDUCE_-1424947841_1 at /127.0.0.1:47976 [Receiving block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775640_1010]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775640_1010 src: /127.0.0.1:47976 dest: /127.0.0.1:39913
2020-12-03 07:19:47,967 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775648_1010, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:49312, dest: /127.0.0.1:38503, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 1948f61e-988c-4bd0-ad24-d3ae7d4c96b1, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775648_1010, duration(ns): 185720668
2020-12-03 07:19:47,968 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775648_1010, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775648_1010, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,969 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775643_1010, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:40436, dest: /127.0.0.1:38308, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: e5737a0c-e925-4b69-8166-e4391ac5f8bb, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775643_1010, duration(ns): 173674326
2020-12-03 07:19:47,969 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775643_1010, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775643_1010, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,969 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775645_1010, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:42532, dest: /127.0.0.1:37110, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 6433c7f1-b9cb-4ed4-9404-85e3ec0e7233, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775645_1010, duration(ns): 170761351
2020-12-03 07:19:47,969 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775645_1010, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775645_1010, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,970 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775640_1010, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:47976, dest: /127.0.0.1:39913, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 69a597d2-2edd-4e23-9c38-3328eccbc91b, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775640_1010, duration(ns): 158652509
2020-12-03 07:19:47,969 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775641_1010, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:48630, dest: /127.0.0.1:46438, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 0e810974-a498-4742-a093-0c4d95031785, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775641_1010, duration(ns): 158710447
2020-12-03 07:19:47,970 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775641_1010, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775641_1010, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,970 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775640_1010, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775640_1010, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,970 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775647_1010, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:55528, dest: /127.0.0.1:44715, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 72c80882-49c9-444a-982c-993baca6bb62, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775647_1010, duration(ns): 196383849
2020-12-03 07:19:47,971 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775647_1010, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775647_1010, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,969 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775644_1010, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:54496, dest: /127.0.0.1:41862, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 0bedb3f9-8b9a-41b5-a47d-b766930b42be, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775644_1010, duration(ns): 159634261
2020-12-03 07:19:47,972 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775644_1010, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775644_1010, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,972 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775646_1010, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:52176, dest: /127.0.0.1:44555, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: 811e904d-4b58-4d09-9a37-ecd8d80551dd, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775646_1010, duration(ns): 162804288
2020-12-03 07:19:47,973 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775646_1010, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775646_1010, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:47,978 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775642_1010, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:47888, dest: /127.0.0.1:46845, bytes: 6291456, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1424947841_1, offset: 0, srvID: be96e054-5ebf-4915-a869-89779c6b94e9, blockid: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775642_1010, duration(ns): 164970555
2020-12-03 07:19:47,978 [PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775642_1010, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775642_1010, type=LAST_IN_PIPELINE terminating
2020-12-03 07:19:48,002 [IPC Server handler 5 on default port 35332] INFO  hdfs.StateChange (FSNamesystem.java:completeFile(2948)) - DIR* completeFile: /striped/stripedFileChecksum1 is closed by DFSClient_NONMAPREDUCE_-1424947841_1
2020-12-03 07:19:48,017 [IPC Server handler 7 on default port 35332] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/striped/stripedFileChecksum1	dst=null	perm=null	proto=rpc
2020-12-03 07:19:48,039 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:44555,DS-27376982-648a-435c-9530-844533c3fc3c,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44555,DS-27376982-648a-435c-9530-844533c3fc3c,DISK], DatanodeInfoWithStorage[127.0.0.1:46845,DS-97859208-57d6-4d0f-a458-1e4adaf0a78a,DISK], DatanodeInfoWithStorage[127.0.0.1:34075,DS-b23a0f3d-1578-4216-aa54-57c659e1100a,DISK], DatanodeInfoWithStorage[127.0.0.1:38308,DS-6660ac3c-7f95-4544-99d0-9a26ef3ef67b,DISK], DatanodeInfoWithStorage[127.0.0.1:41862,DS-129d8406-e71c-470b-9430-2dc52ce7b622,DISK], DatanodeInfoWithStorage[127.0.0.1:44715,DS-2e1daa68-afa3-4e1f-a710-a40da63cedfc,DISK], DatanodeInfoWithStorage[127.0.0.1:38503,DS-34cdaddf-9d99-43fe-ac94-a2d410b1c52a,DISK], DatanodeInfoWithStorage[127.0.0.1:37110,DS-f8fbbb47-9ffb-40e3-9817-22e567ac2d28,DISK], DatanodeInfoWithStorage[127.0.0.1:43005,DS-15a77805-6e7f-4aa2-9de6-622edf3907c2,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7, 8]}
2020-12-03 07:19:48,097 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:extractChecksumProperties(430)) - set bytesPerCRC=512, crcPerBlock=0
2020-12-03 07:19:48,098 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(718)) - got reply from DatanodeInfoWithStorage[127.0.0.1:44555,DS-27376982-648a-435c-9530-844533c3fc3c,DISK]: blockChecksum=7aadeb5ec88ce104512c0bf725b39f72, blockChecksumType=MD5CRC
2020-12-03 07:19:48,103 [IPC Server handler 4 on default port 35332] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/striped/stripedFileChecksum1	dst=null	perm=null	proto=rpc
2020-12-03 07:19:48,111 [Thread-417] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:19:48,111 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@42deb43a] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:19:48,116 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11, DS-6660ac3c-7f95-4544-99d0-9a26ef3ef67b) exiting.
2020-12-03 07:19:48,116 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12, DS-5c6220ab-363a-4023-bc0c-7b6f261c270f) exiting.
2020-12-03 07:19:48,213 [Thread-417] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@32c0915e{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:19:48,225 [Thread-417] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@106faf11{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:19:48,227 [Thread-417] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@50d68830{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:19:48,230 [Thread-417] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@79f227a9{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:19:48,239 [Thread-417] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 37261
2020-12-03 07:19:48,247 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:19:48,261 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:19:48,267 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:19:48,267 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid e5737a0c-e925-4b69-8166-e4391ac5f8bb) service to localhost/127.0.0.1:35332
2020-12-03 07:19:48,267 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid e5737a0c-e925-4b69-8166-e4391ac5f8bb)
2020-12-03 07:19:48,267 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:48,270 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11/current/BP-1367394141-172.17.0.2-1606979969461] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:48,278 [Thread-417] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:19:48,278 [Thread-417] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:19:48,303 [Thread-417] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:19:48,304 [Thread-417] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:19:48,280 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12/current/BP-1367394141-172.17.0.2-1606979969461] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:48,320 [Thread-417] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:19:48,322 [Thread-417] INFO  hdfs.StateChange (DatanodeManager.java:removeDeadDatanode(754)) - BLOCK* removeDeadDatanode: lost heartbeat from 127.0.0.1:38308, removeBlocksFromBlockMap true
2020-12-03 07:19:48,326 [Thread-417] INFO  net.NetworkTopology (NetworkTopology.java:remove(219)) - Removing a node: /default-rack/127.0.0.1:38308
2020-12-03 07:19:48,330 [IPC Server handler 9 on default port 35332] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/striped/stripedFileChecksum1	dst=null	perm=null	proto=rpc
2020-12-03 07:19:48,349 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:44555,DS-27376982-648a-435c-9530-844533c3fc3c,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44555,DS-27376982-648a-435c-9530-844533c3fc3c,DISK], DatanodeInfoWithStorage[127.0.0.1:46845,DS-97859208-57d6-4d0f-a458-1e4adaf0a78a,DISK], DatanodeInfoWithStorage[127.0.0.1:34075,DS-b23a0f3d-1578-4216-aa54-57c659e1100a,DISK], DatanodeInfoWithStorage[127.0.0.1:41862,DS-129d8406-e71c-470b-9430-2dc52ce7b622,DISK], DatanodeInfoWithStorage[127.0.0.1:44715,DS-2e1daa68-afa3-4e1f-a710-a40da63cedfc,DISK], DatanodeInfoWithStorage[127.0.0.1:38503,DS-34cdaddf-9d99-43fe-ac94-a2d410b1c52a,DISK], DatanodeInfoWithStorage[127.0.0.1:37110,DS-f8fbbb47-9ffb-40e3-9817-22e567ac2d28,DISK], DatanodeInfoWithStorage[127.0.0.1:43005,DS-15a77805-6e7f-4aa2-9de6-622edf3907c2,DISK]]; indices=[0, 1, 2, 4, 5, 6, 7, 8]}
2020-12-03 07:19:48,386 [DataXceiver for client /127.0.0.1:52434 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,422 [DataXceiver for client /127.0.0.1:52434 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,428 [DataXceiver for client /127.0.0.1:52434 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,430 [DataXceiver for client /127.0.0.1:52434 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,432 [DataXceiver for client /127.0.0.1:52434 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,434 [DataXceiver for client /127.0.0.1:52434 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,453 [DataXceiver for client /127.0.0.1:52434 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:44555:DataXceiver error processing BLOCK_GROUP_CHECKSUM operation  src: /127.0.0.1:52434 dst: /127.0.0.1:44555
java.lang.UnsupportedOperationException
	at java.nio.ByteBuffer.array(ByteBuffer.java:994)
	at org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockChecksumReconstructor.reconstruct(StripedBlockChecksumReconstructor.java:90)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.recalculateChecksum(BlockChecksumHelper.java:711)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.compute(BlockChecksumHelper.java:489)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.blockGroupChecksum(DataXceiver.java:1047)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opStripedBlockChecksum(Receiver.java:327)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:119)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:48,456 [Thread-417] WARN  hdfs.FileChecksumHelper (FileChecksumHelper.java:checksumBlockGroup(679)) - src=/striped/stripedFileChecksum1, datanodes[0]=DatanodeInfoWithStorage[127.0.0.1:44555,DS-27376982-648a-435c-9530-844533c3fc3c,DISK]
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:550)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.tryDatanode(FileChecksumHelper.java:709)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlockGroup(FileChecksumHelper.java:664)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:638)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery11(TestFileChecksum.java:421)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-12-03 07:19:48,460 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:46845,DS-97859208-57d6-4d0f-a458-1e4adaf0a78a,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44555,DS-27376982-648a-435c-9530-844533c3fc3c,DISK], DatanodeInfoWithStorage[127.0.0.1:46845,DS-97859208-57d6-4d0f-a458-1e4adaf0a78a,DISK], DatanodeInfoWithStorage[127.0.0.1:34075,DS-b23a0f3d-1578-4216-aa54-57c659e1100a,DISK], DatanodeInfoWithStorage[127.0.0.1:41862,DS-129d8406-e71c-470b-9430-2dc52ce7b622,DISK], DatanodeInfoWithStorage[127.0.0.1:44715,DS-2e1daa68-afa3-4e1f-a710-a40da63cedfc,DISK], DatanodeInfoWithStorage[127.0.0.1:38503,DS-34cdaddf-9d99-43fe-ac94-a2d410b1c52a,DISK], DatanodeInfoWithStorage[127.0.0.1:37110,DS-f8fbbb47-9ffb-40e3-9817-22e567ac2d28,DISK], DatanodeInfoWithStorage[127.0.0.1:43005,DS-15a77805-6e7f-4aa2-9de6-622edf3907c2,DISK]]; indices=[0, 1, 2, 4, 5, 6, 7, 8]}
2020-12-03 07:19:48,477 [DataXceiver for client /127.0.0.1:48220 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,479 [DataXceiver for client /127.0.0.1:48220 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,482 [DataXceiver for client /127.0.0.1:48220 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,484 [DataXceiver for client /127.0.0.1:48220 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,490 [DataXceiver for client /127.0.0.1:48220 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,493 [DataXceiver for client /127.0.0.1:48220 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,509 [DataXceiver for client /127.0.0.1:48220 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:46845:DataXceiver error processing BLOCK_GROUP_CHECKSUM operation  src: /127.0.0.1:48220 dst: /127.0.0.1:46845
java.lang.UnsupportedOperationException
	at java.nio.ByteBuffer.array(ByteBuffer.java:994)
	at org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockChecksumReconstructor.reconstruct(StripedBlockChecksumReconstructor.java:90)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.recalculateChecksum(BlockChecksumHelper.java:711)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.compute(BlockChecksumHelper.java:489)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.blockGroupChecksum(DataXceiver.java:1047)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opStripedBlockChecksum(Receiver.java:327)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:119)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:48,509 [Thread-417] WARN  hdfs.FileChecksumHelper (FileChecksumHelper.java:checksumBlockGroup(679)) - src=/striped/stripedFileChecksum1, datanodes[1]=DatanodeInfoWithStorage[127.0.0.1:46845,DS-97859208-57d6-4d0f-a458-1e4adaf0a78a,DISK]
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:550)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.tryDatanode(FileChecksumHelper.java:709)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlockGroup(FileChecksumHelper.java:664)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:638)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery11(TestFileChecksum.java:421)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-12-03 07:19:48,515 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:34075,DS-b23a0f3d-1578-4216-aa54-57c659e1100a,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44555,DS-27376982-648a-435c-9530-844533c3fc3c,DISK], DatanodeInfoWithStorage[127.0.0.1:46845,DS-97859208-57d6-4d0f-a458-1e4adaf0a78a,DISK], DatanodeInfoWithStorage[127.0.0.1:34075,DS-b23a0f3d-1578-4216-aa54-57c659e1100a,DISK], DatanodeInfoWithStorage[127.0.0.1:41862,DS-129d8406-e71c-470b-9430-2dc52ce7b622,DISK], DatanodeInfoWithStorage[127.0.0.1:44715,DS-2e1daa68-afa3-4e1f-a710-a40da63cedfc,DISK], DatanodeInfoWithStorage[127.0.0.1:38503,DS-34cdaddf-9d99-43fe-ac94-a2d410b1c52a,DISK], DatanodeInfoWithStorage[127.0.0.1:37110,DS-f8fbbb47-9ffb-40e3-9817-22e567ac2d28,DISK], DatanodeInfoWithStorage[127.0.0.1:43005,DS-15a77805-6e7f-4aa2-9de6-622edf3907c2,DISK]]; indices=[0, 1, 2, 4, 5, 6, 7, 8]}
2020-12-03 07:19:48,525 [DataXceiver for client /127.0.0.1:40712 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,528 [DataXceiver for client /127.0.0.1:40712 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,531 [DataXceiver for client /127.0.0.1:40712 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,536 [DataXceiver for client /127.0.0.1:40712 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,539 [DataXceiver for client /127.0.0.1:40712 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,543 [DataXceiver for client /127.0.0.1:40712 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,569 [DataXceiver for client /127.0.0.1:40712 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:34075:DataXceiver error processing BLOCK_GROUP_CHECKSUM operation  src: /127.0.0.1:40712 dst: /127.0.0.1:34075
java.lang.UnsupportedOperationException
	at java.nio.ByteBuffer.array(ByteBuffer.java:994)
	at org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockChecksumReconstructor.reconstruct(StripedBlockChecksumReconstructor.java:90)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.recalculateChecksum(BlockChecksumHelper.java:711)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.compute(BlockChecksumHelper.java:489)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.blockGroupChecksum(DataXceiver.java:1047)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opStripedBlockChecksum(Receiver.java:327)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:119)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:48,569 [Thread-417] WARN  hdfs.FileChecksumHelper (FileChecksumHelper.java:checksumBlockGroup(679)) - src=/striped/stripedFileChecksum1, datanodes[2]=DatanodeInfoWithStorage[127.0.0.1:34075,DS-b23a0f3d-1578-4216-aa54-57c659e1100a,DISK]
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:550)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.tryDatanode(FileChecksumHelper.java:709)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlockGroup(FileChecksumHelper.java:664)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:638)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery11(TestFileChecksum.java:421)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-12-03 07:19:48,573 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:41862,DS-129d8406-e71c-470b-9430-2dc52ce7b622,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44555,DS-27376982-648a-435c-9530-844533c3fc3c,DISK], DatanodeInfoWithStorage[127.0.0.1:46845,DS-97859208-57d6-4d0f-a458-1e4adaf0a78a,DISK], DatanodeInfoWithStorage[127.0.0.1:34075,DS-b23a0f3d-1578-4216-aa54-57c659e1100a,DISK], DatanodeInfoWithStorage[127.0.0.1:41862,DS-129d8406-e71c-470b-9430-2dc52ce7b622,DISK], DatanodeInfoWithStorage[127.0.0.1:44715,DS-2e1daa68-afa3-4e1f-a710-a40da63cedfc,DISK], DatanodeInfoWithStorage[127.0.0.1:38503,DS-34cdaddf-9d99-43fe-ac94-a2d410b1c52a,DISK], DatanodeInfoWithStorage[127.0.0.1:37110,DS-f8fbbb47-9ffb-40e3-9817-22e567ac2d28,DISK], DatanodeInfoWithStorage[127.0.0.1:43005,DS-15a77805-6e7f-4aa2-9de6-622edf3907c2,DISK]]; indices=[0, 1, 2, 4, 5, 6, 7, 8]}
2020-12-03 07:19:48,583 [DataXceiver for client /127.0.0.1:54888 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,588 [DataXceiver for client /127.0.0.1:54888 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,618 [DataXceiver for client /127.0.0.1:54888 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,623 [DataXceiver for client /127.0.0.1:54888 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,652 [DataXceiver for client /127.0.0.1:54888 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,661 [DataXceiver for client /127.0.0.1:54888 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,666 [DataXceiver for client /127.0.0.1:54888 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:41862:DataXceiver error processing BLOCK_GROUP_CHECKSUM operation  src: /127.0.0.1:54888 dst: /127.0.0.1:41862
java.lang.UnsupportedOperationException
	at java.nio.ByteBuffer.array(ByteBuffer.java:994)
	at org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockChecksumReconstructor.reconstruct(StripedBlockChecksumReconstructor.java:90)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.recalculateChecksum(BlockChecksumHelper.java:711)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.compute(BlockChecksumHelper.java:489)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.blockGroupChecksum(DataXceiver.java:1047)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opStripedBlockChecksum(Receiver.java:327)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:119)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:48,666 [Thread-417] WARN  hdfs.FileChecksumHelper (FileChecksumHelper.java:checksumBlockGroup(679)) - src=/striped/stripedFileChecksum1, datanodes[3]=DatanodeInfoWithStorage[127.0.0.1:41862,DS-129d8406-e71c-470b-9430-2dc52ce7b622,DISK]
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:550)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.tryDatanode(FileChecksumHelper.java:709)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlockGroup(FileChecksumHelper.java:664)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:638)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery11(TestFileChecksum.java:421)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-12-03 07:19:48,672 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:44715,DS-2e1daa68-afa3-4e1f-a710-a40da63cedfc,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44555,DS-27376982-648a-435c-9530-844533c3fc3c,DISK], DatanodeInfoWithStorage[127.0.0.1:46845,DS-97859208-57d6-4d0f-a458-1e4adaf0a78a,DISK], DatanodeInfoWithStorage[127.0.0.1:34075,DS-b23a0f3d-1578-4216-aa54-57c659e1100a,DISK], DatanodeInfoWithStorage[127.0.0.1:41862,DS-129d8406-e71c-470b-9430-2dc52ce7b622,DISK], DatanodeInfoWithStorage[127.0.0.1:44715,DS-2e1daa68-afa3-4e1f-a710-a40da63cedfc,DISK], DatanodeInfoWithStorage[127.0.0.1:38503,DS-34cdaddf-9d99-43fe-ac94-a2d410b1c52a,DISK], DatanodeInfoWithStorage[127.0.0.1:37110,DS-f8fbbb47-9ffb-40e3-9817-22e567ac2d28,DISK], DatanodeInfoWithStorage[127.0.0.1:43005,DS-15a77805-6e7f-4aa2-9de6-622edf3907c2,DISK]]; indices=[0, 1, 2, 4, 5, 6, 7, 8]}
2020-12-03 07:19:48,685 [DataXceiver for client /127.0.0.1:55984 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,704 [DataXceiver for client /127.0.0.1:55984 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,706 [DataXceiver for client /127.0.0.1:55984 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,708 [DataXceiver for client /127.0.0.1:55984 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,725 [DataXceiver for client /127.0.0.1:55984 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,727 [DataXceiver for client /127.0.0.1:55984 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,737 [DataXceiver for client /127.0.0.1:55984 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:44715:DataXceiver error processing BLOCK_GROUP_CHECKSUM operation  src: /127.0.0.1:55984 dst: /127.0.0.1:44715
java.lang.UnsupportedOperationException
	at java.nio.ByteBuffer.array(ByteBuffer.java:994)
	at org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockChecksumReconstructor.reconstruct(StripedBlockChecksumReconstructor.java:90)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.recalculateChecksum(BlockChecksumHelper.java:711)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.compute(BlockChecksumHelper.java:489)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.blockGroupChecksum(DataXceiver.java:1047)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opStripedBlockChecksum(Receiver.java:327)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:119)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:48,737 [Thread-417] WARN  hdfs.FileChecksumHelper (FileChecksumHelper.java:checksumBlockGroup(679)) - src=/striped/stripedFileChecksum1, datanodes[4]=DatanodeInfoWithStorage[127.0.0.1:44715,DS-2e1daa68-afa3-4e1f-a710-a40da63cedfc,DISK]
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:550)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.tryDatanode(FileChecksumHelper.java:709)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlockGroup(FileChecksumHelper.java:664)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:638)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery11(TestFileChecksum.java:421)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-12-03 07:19:48,740 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:38503,DS-34cdaddf-9d99-43fe-ac94-a2d410b1c52a,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44555,DS-27376982-648a-435c-9530-844533c3fc3c,DISK], DatanodeInfoWithStorage[127.0.0.1:46845,DS-97859208-57d6-4d0f-a458-1e4adaf0a78a,DISK], DatanodeInfoWithStorage[127.0.0.1:34075,DS-b23a0f3d-1578-4216-aa54-57c659e1100a,DISK], DatanodeInfoWithStorage[127.0.0.1:41862,DS-129d8406-e71c-470b-9430-2dc52ce7b622,DISK], DatanodeInfoWithStorage[127.0.0.1:44715,DS-2e1daa68-afa3-4e1f-a710-a40da63cedfc,DISK], DatanodeInfoWithStorage[127.0.0.1:38503,DS-34cdaddf-9d99-43fe-ac94-a2d410b1c52a,DISK], DatanodeInfoWithStorage[127.0.0.1:37110,DS-f8fbbb47-9ffb-40e3-9817-22e567ac2d28,DISK], DatanodeInfoWithStorage[127.0.0.1:43005,DS-15a77805-6e7f-4aa2-9de6-622edf3907c2,DISK]]; indices=[0, 1, 2, 4, 5, 6, 7, 8]}
2020-12-03 07:19:48,755 [DataXceiver for client /127.0.0.1:49860 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,764 [DataXceiver for client /127.0.0.1:49860 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,777 [DataXceiver for client /127.0.0.1:49860 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,798 [DataXceiver for client /127.0.0.1:49860 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,805 [DataXceiver for client /127.0.0.1:49860 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,815 [DataXceiver for client /127.0.0.1:49860 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,889 [DataXceiver for client /127.0.0.1:49860 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:38503:DataXceiver error processing BLOCK_GROUP_CHECKSUM operation  src: /127.0.0.1:49860 dst: /127.0.0.1:38503
java.lang.UnsupportedOperationException
	at java.nio.ByteBuffer.array(ByteBuffer.java:994)
	at org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockChecksumReconstructor.reconstruct(StripedBlockChecksumReconstructor.java:90)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.recalculateChecksum(BlockChecksumHelper.java:711)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.compute(BlockChecksumHelper.java:489)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.blockGroupChecksum(DataXceiver.java:1047)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opStripedBlockChecksum(Receiver.java:327)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:119)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:48,889 [Thread-417] WARN  hdfs.FileChecksumHelper (FileChecksumHelper.java:checksumBlockGroup(679)) - src=/striped/stripedFileChecksum1, datanodes[5]=DatanodeInfoWithStorage[127.0.0.1:38503,DS-34cdaddf-9d99-43fe-ac94-a2d410b1c52a,DISK]
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:550)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.tryDatanode(FileChecksumHelper.java:709)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlockGroup(FileChecksumHelper.java:664)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:638)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery11(TestFileChecksum.java:421)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-12-03 07:19:48,892 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:37110,DS-f8fbbb47-9ffb-40e3-9817-22e567ac2d28,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44555,DS-27376982-648a-435c-9530-844533c3fc3c,DISK], DatanodeInfoWithStorage[127.0.0.1:46845,DS-97859208-57d6-4d0f-a458-1e4adaf0a78a,DISK], DatanodeInfoWithStorage[127.0.0.1:34075,DS-b23a0f3d-1578-4216-aa54-57c659e1100a,DISK], DatanodeInfoWithStorage[127.0.0.1:41862,DS-129d8406-e71c-470b-9430-2dc52ce7b622,DISK], DatanodeInfoWithStorage[127.0.0.1:44715,DS-2e1daa68-afa3-4e1f-a710-a40da63cedfc,DISK], DatanodeInfoWithStorage[127.0.0.1:38503,DS-34cdaddf-9d99-43fe-ac94-a2d410b1c52a,DISK], DatanodeInfoWithStorage[127.0.0.1:37110,DS-f8fbbb47-9ffb-40e3-9817-22e567ac2d28,DISK], DatanodeInfoWithStorage[127.0.0.1:43005,DS-15a77805-6e7f-4aa2-9de6-622edf3907c2,DISK]]; indices=[0, 1, 2, 4, 5, 6, 7, 8]}
2020-12-03 07:19:48,904 [DataXceiver for client /127.0.0.1:43134 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,936 [DataXceiver for client /127.0.0.1:43134 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,939 [DataXceiver for client /127.0.0.1:43134 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,944 [DataXceiver for client /127.0.0.1:43134 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,946 [DataXceiver for client /127.0.0.1:43134 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,949 [DataXceiver for client /127.0.0.1:43134 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,954 [DataXceiver for client /127.0.0.1:43134 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:37110:DataXceiver error processing BLOCK_GROUP_CHECKSUM operation  src: /127.0.0.1:43134 dst: /127.0.0.1:37110
java.lang.UnsupportedOperationException
	at java.nio.ByteBuffer.array(ByteBuffer.java:994)
	at org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockChecksumReconstructor.reconstruct(StripedBlockChecksumReconstructor.java:90)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.recalculateChecksum(BlockChecksumHelper.java:711)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.compute(BlockChecksumHelper.java:489)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.blockGroupChecksum(DataXceiver.java:1047)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opStripedBlockChecksum(Receiver.java:327)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:119)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:48,960 [Thread-417] WARN  hdfs.FileChecksumHelper (FileChecksumHelper.java:checksumBlockGroup(679)) - src=/striped/stripedFileChecksum1, datanodes[6]=DatanodeInfoWithStorage[127.0.0.1:37110,DS-f8fbbb47-9ffb-40e3-9817-22e567ac2d28,DISK]
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:550)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.tryDatanode(FileChecksumHelper.java:709)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlockGroup(FileChecksumHelper.java:664)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:638)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery11(TestFileChecksum.java:421)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-12-03 07:19:48,965 [Thread-417] DEBUG hdfs.FileChecksumHelper (FileChecksumHelper.java:tryDatanode(698)) - write to DatanodeInfoWithStorage[127.0.0.1:43005,DS-15a77805-6e7f-4aa2-9de6-622edf3907c2,DISK]: BLOCK_GROUP_CHECKSUM, blockGroup=LocatedStripedBlock{BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44555,DS-27376982-648a-435c-9530-844533c3fc3c,DISK], DatanodeInfoWithStorage[127.0.0.1:46845,DS-97859208-57d6-4d0f-a458-1e4adaf0a78a,DISK], DatanodeInfoWithStorage[127.0.0.1:34075,DS-b23a0f3d-1578-4216-aa54-57c659e1100a,DISK], DatanodeInfoWithStorage[127.0.0.1:41862,DS-129d8406-e71c-470b-9430-2dc52ce7b622,DISK], DatanodeInfoWithStorage[127.0.0.1:44715,DS-2e1daa68-afa3-4e1f-a710-a40da63cedfc,DISK], DatanodeInfoWithStorage[127.0.0.1:38503,DS-34cdaddf-9d99-43fe-ac94-a2d410b1c52a,DISK], DatanodeInfoWithStorage[127.0.0.1:37110,DS-f8fbbb47-9ffb-40e3-9817-22e567ac2d28,DISK], DatanodeInfoWithStorage[127.0.0.1:43005,DS-15a77805-6e7f-4aa2-9de6-622edf3907c2,DISK]]; indices=[0, 1, 2, 4, 5, 6, 7, 8]}
2020-12-03 07:19:48,974 [DataXceiver for client /127.0.0.1:44880 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,977 [DataXceiver for client /127.0.0.1:44880 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,979 [DataXceiver for client /127.0.0.1:44880 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,984 [DataXceiver for client /127.0.0.1:44880 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,986 [DataXceiver for client /127.0.0.1:44880 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,991 [DataXceiver for client /127.0.0.1:44880 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:19:48,995 [DataXceiver for client /127.0.0.1:44880 [Getting checksum for block groupBP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:43005:DataXceiver error processing BLOCK_GROUP_CHECKSUM operation  src: /127.0.0.1:44880 dst: /127.0.0.1:43005
java.lang.UnsupportedOperationException
	at java.nio.ByteBuffer.array(ByteBuffer.java:994)
	at org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockChecksumReconstructor.reconstruct(StripedBlockChecksumReconstructor.java:90)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.recalculateChecksum(BlockChecksumHelper.java:711)
	at org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer.compute(BlockChecksumHelper.java:489)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.blockGroupChecksum(DataXceiver.java:1047)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opStripedBlockChecksum(Receiver.java:327)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:119)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:48,996 [Thread-417] WARN  hdfs.FileChecksumHelper (FileChecksumHelper.java:checksumBlockGroup(679)) - src=/striped/stripedFileChecksum1, datanodes[7]=DatanodeInfoWithStorage[127.0.0.1:43005,DS-15a77805-6e7f-4aa2-9de6-622edf3907c2,DISK]
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:550)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.tryDatanode(FileChecksumHelper.java:709)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlockGroup(FileChecksumHelper.java:664)
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:638)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery11(TestFileChecksum.java:421)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-12-03 07:19:48,999 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(2049)) - Shutting down the Mini HDFS Cluster
2020-12-03 07:19:49,003 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 10
2020-12-03 07:19:49,003 [Listener at localhost/34517] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:19:49,004 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@64a1923a] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:19:49,012 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22, DS-695468af-ff85-41de-a309-3e9cf958ef94) exiting.
2020-12-03 07:19:49,012 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21, DS-129d8406-e71c-470b-9430-2dc52ce7b622) exiting.
2020-12-03 07:19:49,014 [DataXceiver for client  at /127.0.0.1:55208 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:41862:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:55208 dst: /127.0.0.1:41862
java.lang.NullPointerException
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:671)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,052 [Listener at localhost/34517] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@169da7f2{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:19:49,053 [Listener at localhost/34517] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@3c1e23ff{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:19:49,054 [Listener at localhost/34517] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@50b1f030{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:19:49,054 [Listener at localhost/34517] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@742d4e15{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:19:49,082 [DataXceiver for client  at /127.0.0.1:54910 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,082 [Listener at localhost/34517] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 34517
2020-12-03 07:19:49,082 [DataXceiver for client  at /127.0.0.1:54818 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,082 [DataXceiver for client  at /127.0.0.1:55016 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,082 [DataXceiver for client  at /127.0.0.1:54850 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,109 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:19:49,082 [DataXceiver for client  at /127.0.0.1:55142 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,082 [DataXceiver for client  at /127.0.0.1:54876 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,110 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid 0bedb3f9-8b9a-41b5-a47d-b766930b42be) service to localhost/127.0.0.1:35332
2020-12-03 07:19:49,104 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:19:49,111 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid 0bedb3f9-8b9a-41b5-a47d-b766930b42be)
2020-12-03 07:19:49,091 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:19:49,083 [DataXceiver for client  at /127.0.0.1:54910 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001 on DS-129d8406-e71c-470b-9430-2dc52ce7b622, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,118 [DataXceiver for client  at /127.0.0.1:54910 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:41862, datanodeUuid=0bedb3f9-8b9a-41b5-a47d-b766930b42be, infoPort=45315, infoSecurePort=0, ipcPort=34517, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001 to /127.0.0.1:54910
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,122 [DataXceiver for client  at /127.0.0.1:54910 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:41862:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:54910 dst: /127.0.0.1:41862
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,128 [DataXceiver for client  at /127.0.0.1:55056 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,124 [DataXceiver for client  at /127.0.0.1:54876 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001 on DS-129d8406-e71c-470b-9430-2dc52ce7b622, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,137 [DataXceiver for client  at /127.0.0.1:55142 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001 on DS-129d8406-e71c-470b-9430-2dc52ce7b622, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,124 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:49,138 [DataXceiver for client  at /127.0.0.1:55142 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:41862, datanodeUuid=0bedb3f9-8b9a-41b5-a47d-b766930b42be, infoPort=45315, infoSecurePort=0, ipcPort=34517, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001 to /127.0.0.1:55142
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,138 [DataXceiver for client  at /127.0.0.1:54850 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001 on DS-129d8406-e71c-470b-9430-2dc52ce7b622, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,137 [DataXceiver for client  at /127.0.0.1:54876 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:41862, datanodeUuid=0bedb3f9-8b9a-41b5-a47d-b766930b42be, infoPort=45315, infoSecurePort=0, ipcPort=34517, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001 to /127.0.0.1:54876
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,148 [DataXceiver for client  at /127.0.0.1:55142 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:41862:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:55142 dst: /127.0.0.1:41862
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,142 [DataXceiver for client  at /127.0.0.1:54850 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:41862, datanodeUuid=0bedb3f9-8b9a-41b5-a47d-b766930b42be, infoPort=45315, infoSecurePort=0, ipcPort=34517, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001 to /127.0.0.1:54850
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,140 [DataXceiver for client  at /127.0.0.1:55016 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001 on DS-129d8406-e71c-470b-9430-2dc52ce7b622, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,143 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21/current/BP-1367394141-172.17.0.2-1606979969461] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:49,154 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data22/current/BP-1367394141-172.17.0.2-1606979969461] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:49,149 [DataXceiver for client  at /127.0.0.1:54876 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:41862:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:54876 dst: /127.0.0.1:41862
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,172 [Listener at localhost/34517] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:19:49,170 [DataXceiver for client  at /127.0.0.1:55016 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:41862, datanodeUuid=0bedb3f9-8b9a-41b5-a47d-b766930b42be, infoPort=45315, infoSecurePort=0, ipcPort=34517, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001 to /127.0.0.1:55016
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,170 [DataXceiver for client  at /127.0.0.1:54850 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:41862:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:54850 dst: /127.0.0.1:41862
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,170 [DataXceiver for client  at /127.0.0.1:54818 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001 on DS-129d8406-e71c-470b-9430-2dc52ce7b622, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,181 [DataXceiver for client  at /127.0.0.1:55016 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:41862:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:55016 dst: /127.0.0.1:41862
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,180 [Listener at localhost/34517] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:19:49,189 [DataXceiver for client  at /127.0.0.1:54818 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:41862, datanodeUuid=0bedb3f9-8b9a-41b5-a47d-b766930b42be, infoPort=45315, infoSecurePort=0, ipcPort=34517, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001 to /127.0.0.1:54818
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,189 [DataXceiver for client  at /127.0.0.1:55056 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001 on DS-129d8406-e71c-470b-9430-2dc52ce7b622, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,196 [DataXceiver for client  at /127.0.0.1:54818 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:41862:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:54818 dst: /127.0.0.1:41862
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,196 [Listener at localhost/34517] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:19:49,204 [Listener at localhost/34517] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:19:49,200 [DataXceiver for client  at /127.0.0.1:55056 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:41862, datanodeUuid=0bedb3f9-8b9a-41b5-a47d-b766930b42be, infoPort=45315, infoSecurePort=0, ipcPort=34517, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001 to /127.0.0.1:55056
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,208 [DataXceiver for client  at /127.0.0.1:55056 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775788_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:41862:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:55056 dst: /127.0.0.1:41862
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,208 [Listener at localhost/34517] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:19:49,212 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 9
2020-12-03 07:19:49,212 [Listener at localhost/34517] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:19:49,212 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@14c053c6] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:19:49,216 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19, DS-97859208-57d6-4d0f-a458-1e4adaf0a78a) exiting.
2020-12-03 07:19:49,216 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20, DS-5beec8c8-67b2-429e-b7bb-c700d530aa6a) exiting.
2020-12-03 07:19:49,256 [Listener at localhost/34517] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@4d666b41{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:19:49,257 [Listener at localhost/34517] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@6594402a{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:19:49,258 [Listener at localhost/34517] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@589b028e{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:19:49,258 [Listener at localhost/34517] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@3773862a{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:19:49,261 [Listener at localhost/34517] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 37058
2020-12-03 07:19:49,261 [DataXceiver for client  at /127.0.0.1:48260 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,268 [DataXceiver for client  at /127.0.0.1:48260 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001 on DS-97859208-57d6-4d0f-a458-1e4adaf0a78a, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,268 [DataXceiver for client  at /127.0.0.1:48260 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:46845, datanodeUuid=be96e054-5ebf-4915-a869-89779c6b94e9, infoPort=43849, infoSecurePort=0, ipcPort=37058, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001 to /127.0.0.1:48260
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,261 [DataXceiver for client  at /127.0.0.1:48388 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,261 [DataXceiver for client  at /127.0.0.1:48288 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,261 [DataXceiver for client  at /127.0.0.1:48518 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,261 [DataXceiver for client  at /127.0.0.1:48438 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,261 [DataXceiver for client  at /127.0.0.1:48584 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,261 [DataXceiver for client  at /127.0.0.1:48234 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,274 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:19:49,274 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:19:49,273 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:19:49,273 [DataXceiver for client  at /127.0.0.1:48260 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:46845:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:48260 dst: /127.0.0.1:46845
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,268 [DataXceiver for client  at /127.0.0.1:48388 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001 on DS-97859208-57d6-4d0f-a458-1e4adaf0a78a, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,288 [DataXceiver for client  at /127.0.0.1:48388 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:46845, datanodeUuid=be96e054-5ebf-4915-a869-89779c6b94e9, infoPort=43849, infoSecurePort=0, ipcPort=37058, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001 to /127.0.0.1:48388
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,280 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid be96e054-5ebf-4915-a869-89779c6b94e9) service to localhost/127.0.0.1:35332
2020-12-03 07:19:49,290 [DataXceiver for client  at /127.0.0.1:48234 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001 on DS-97859208-57d6-4d0f-a458-1e4adaf0a78a, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,292 [DataXceiver for client  at /127.0.0.1:48388 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:46845:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:48388 dst: /127.0.0.1:46845
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,292 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid be96e054-5ebf-4915-a869-89779c6b94e9)
2020-12-03 07:19:49,293 [DataXceiver for client  at /127.0.0.1:48234 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:46845, datanodeUuid=be96e054-5ebf-4915-a869-89779c6b94e9, infoPort=43849, infoSecurePort=0, ipcPort=37058, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001 to /127.0.0.1:48234
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,293 [DataXceiver for client  at /127.0.0.1:48584 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001 on DS-97859208-57d6-4d0f-a458-1e4adaf0a78a, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,297 [DataXceiver for client  at /127.0.0.1:48234 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:46845:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:48234 dst: /127.0.0.1:46845
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,316 [DataXceiver for client  at /127.0.0.1:48438 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001 on DS-97859208-57d6-4d0f-a458-1e4adaf0a78a, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,326 [DataXceiver for client  at /127.0.0.1:48518 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001 on DS-97859208-57d6-4d0f-a458-1e4adaf0a78a, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,316 [DataXceiver for client  at /127.0.0.1:48584 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:46845, datanodeUuid=be96e054-5ebf-4915-a869-89779c6b94e9, infoPort=43849, infoSecurePort=0, ipcPort=37058, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001 to /127.0.0.1:48584
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,326 [DataXceiver for client  at /127.0.0.1:48584 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:46845:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:48584 dst: /127.0.0.1:46845
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,326 [DataXceiver for client  at /127.0.0.1:48288 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001 on DS-97859208-57d6-4d0f-a458-1e4adaf0a78a, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,326 [DataXceiver for client  at /127.0.0.1:48518 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:46845, datanodeUuid=be96e054-5ebf-4915-a869-89779c6b94e9, infoPort=43849, infoSecurePort=0, ipcPort=37058, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001 to /127.0.0.1:48518
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,326 [DataXceiver for client  at /127.0.0.1:48438 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:46845, datanodeUuid=be96e054-5ebf-4915-a869-89779c6b94e9, infoPort=43849, infoSecurePort=0, ipcPort=37058, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001 to /127.0.0.1:48438
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,334 [DataXceiver for client  at /127.0.0.1:48518 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:46845:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:48518 dst: /127.0.0.1:46845
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,339 [DataXceiver for client  at /127.0.0.1:48438 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:46845:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:48438 dst: /127.0.0.1:46845
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,331 [DataXceiver for client  at /127.0.0.1:48288 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:46845, datanodeUuid=be96e054-5ebf-4915-a869-89779c6b94e9, infoPort=43849, infoSecurePort=0, ipcPort=37058, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001 to /127.0.0.1:48288
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,331 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:49,342 [DataXceiver for client  at /127.0.0.1:48288 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775791_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:46845:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:48288 dst: /127.0.0.1:46845
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,353 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19/current/BP-1367394141-172.17.0.2-1606979969461] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:49,356 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20/current/BP-1367394141-172.17.0.2-1606979969461] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:49,374 [Listener at localhost/34517] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:19:49,374 [Listener at localhost/34517] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:19:49,377 [Listener at localhost/34517] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:19:49,377 [Listener at localhost/34517] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:19:49,382 [Listener at localhost/34517] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:19:49,383 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 8
2020-12-03 07:19:49,383 [Listener at localhost/34517] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:19:49,383 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@5bbbdd4b] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:19:49,387 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17, DS-15a77805-6e7f-4aa2-9de6-622edf3907c2) exiting.
2020-12-03 07:19:49,387 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18, DS-65ca39ad-f8ba-4de7-a267-6c125a4b507b) exiting.
2020-12-03 07:19:49,463 [Listener at localhost/34517] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@2237bada{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:19:49,466 [Listener at localhost/34517] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@77e2a6e2{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:19:49,467 [Listener at localhost/34517] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@315ba14a{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:19:49,467 [Listener at localhost/34517] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@5a6d5a8f{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:19:49,472 [Listener at localhost/34517] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 45450
2020-12-03 07:19:49,473 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:19:49,473 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:19:49,477 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:19:49,478 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid b771649f-3cb7-43a2-9de1-a830c893f998) service to localhost/127.0.0.1:35332
2020-12-03 07:19:49,478 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid b771649f-3cb7-43a2-9de1-a830c893f998)
2020-12-03 07:19:49,478 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:49,493 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17/current/BP-1367394141-172.17.0.2-1606979969461] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:49,493 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18/current/BP-1367394141-172.17.0.2-1606979969461] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:49,525 [Listener at localhost/34517] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:19:49,525 [Listener at localhost/34517] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:19:49,529 [Listener at localhost/34517] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:19:49,529 [Listener at localhost/34517] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:19:49,536 [Listener at localhost/34517] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:19:49,536 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 7
2020-12-03 07:19:49,537 [Listener at localhost/34517] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:19:49,537 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@772485dd] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:19:49,541 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16, DS-c23cda38-630d-4441-b4d8-5b0ca6373351) exiting.
2020-12-03 07:19:49,541 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15, DS-b23a0f3d-1578-4216-aa54-57c659e1100a) exiting.
2020-12-03 07:19:49,636 [Listener at localhost/34517] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@6d2260db{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:19:49,637 [Listener at localhost/34517] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@1f2d2181{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:19:49,637 [Listener at localhost/34517] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@117632cf{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:19:49,637 [Listener at localhost/34517] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@60b85ba1{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:19:49,641 [Listener at localhost/34517] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 40394
2020-12-03 07:19:49,641 [DataXceiver for client  at /127.0.0.1:40902 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,641 [DataXceiver for client  at /127.0.0.1:40858 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,641 [DataXceiver for client  at /127.0.0.1:41052 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,641 [DataXceiver for client  at /127.0.0.1:40724 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,641 [DataXceiver for client  at /127.0.0.1:40698 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,641 [DataXceiver for client  at /127.0.0.1:40666 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,641 [DataXceiver for client  at /127.0.0.1:40752 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,643 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:19:49,642 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:19:49,642 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:19:49,642 [DataXceiver for client  at /127.0.0.1:40902 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001 on DS-b23a0f3d-1578-4216-aa54-57c659e1100a, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,642 [DataXceiver for client  at /127.0.0.1:40986 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,672 [DataXceiver for client  at /127.0.0.1:40752 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001 on DS-b23a0f3d-1578-4216-aa54-57c659e1100a, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,662 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid f1c32787-7e56-457d-90b0-80f26f785f52) service to localhost/127.0.0.1:35332
2020-12-03 07:19:49,672 [DataXceiver for client  at /127.0.0.1:40752 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:34075, datanodeUuid=f1c32787-7e56-457d-90b0-80f26f785f52, infoPort=37609, infoSecurePort=0, ipcPort=40394, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001 to /127.0.0.1:40752
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,672 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid f1c32787-7e56-457d-90b0-80f26f785f52)
2020-12-03 07:19:49,672 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:49,672 [DataXceiver for client  at /127.0.0.1:40752 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:34075:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:40752 dst: /127.0.0.1:34075
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,677 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15/current/BP-1367394141-172.17.0.2-1606979969461] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:49,676 [DataXceiver for client  at /127.0.0.1:40666 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001 on DS-b23a0f3d-1578-4216-aa54-57c659e1100a, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,676 [DataXceiver for client  at /127.0.0.1:40902 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:34075, datanodeUuid=f1c32787-7e56-457d-90b0-80f26f785f52, infoPort=37609, infoSecurePort=0, ipcPort=40394, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001 to /127.0.0.1:40902
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,696 [DataXceiver for client  at /127.0.0.1:40666 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:34075, datanodeUuid=f1c32787-7e56-457d-90b0-80f26f785f52, infoPort=37609, infoSecurePort=0, ipcPort=40394, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001 to /127.0.0.1:40666
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,696 [DataXceiver for client  at /127.0.0.1:40698 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001 on DS-b23a0f3d-1578-4216-aa54-57c659e1100a, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,688 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16/current/BP-1367394141-172.17.0.2-1606979969461] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:49,707 [DataXceiver for client  at /127.0.0.1:40698 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:34075, datanodeUuid=f1c32787-7e56-457d-90b0-80f26f785f52, infoPort=37609, infoSecurePort=0, ipcPort=40394, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001 to /127.0.0.1:40698
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,688 [Listener at localhost/34517] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:19:49,707 [DataXceiver for client  at /127.0.0.1:40698 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:34075:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:40698 dst: /127.0.0.1:34075
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,707 [DataXceiver for client  at /127.0.0.1:40724 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001 on DS-b23a0f3d-1578-4216-aa54-57c659e1100a, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,700 [DataXceiver for client  at /127.0.0.1:40666 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:34075:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:40666 dst: /127.0.0.1:34075
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,700 [DataXceiver for client  at /127.0.0.1:40902 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:34075:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:40902 dst: /127.0.0.1:34075
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,720 [DataXceiver for client  at /127.0.0.1:41052 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001 on DS-b23a0f3d-1578-4216-aa54-57c659e1100a, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,727 [DataXceiver for client  at /127.0.0.1:40858 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001 on DS-b23a0f3d-1578-4216-aa54-57c659e1100a, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,720 [DataXceiver for client  at /127.0.0.1:40724 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:34075, datanodeUuid=f1c32787-7e56-457d-90b0-80f26f785f52, infoPort=37609, infoSecurePort=0, ipcPort=40394, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001 to /127.0.0.1:40724
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,707 [Listener at localhost/34517] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:19:49,728 [DataXceiver for client  at /127.0.0.1:40724 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:34075:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:40724 dst: /127.0.0.1:34075
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,728 [DataXceiver for client  at /127.0.0.1:40986 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001 on DS-b23a0f3d-1578-4216-aa54-57c659e1100a, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,728 [DataXceiver for client  at /127.0.0.1:40858 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:34075, datanodeUuid=f1c32787-7e56-457d-90b0-80f26f785f52, infoPort=37609, infoSecurePort=0, ipcPort=40394, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001 to /127.0.0.1:40858
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,727 [DataXceiver for client  at /127.0.0.1:41052 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:34075, datanodeUuid=f1c32787-7e56-457d-90b0-80f26f785f52, infoPort=37609, infoSecurePort=0, ipcPort=40394, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001 to /127.0.0.1:41052
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,757 [DataXceiver for client  at /127.0.0.1:40858 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:34075:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:40858 dst: /127.0.0.1:34075
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,757 [DataXceiver for client  at /127.0.0.1:40986 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:34075, datanodeUuid=f1c32787-7e56-457d-90b0-80f26f785f52, infoPort=37609, infoSecurePort=0, ipcPort=40394, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001 to /127.0.0.1:40986
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,737 [Listener at localhost/34517] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:19:49,781 [DataXceiver for client  at /127.0.0.1:40986 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:34075:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:40986 dst: /127.0.0.1:34075
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,757 [DataXceiver for client  at /127.0.0.1:41052 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775790_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:34075:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:41052 dst: /127.0.0.1:34075
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,781 [Listener at localhost/34517] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:19:49,795 [Listener at localhost/34517] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:19:49,796 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 6
2020-12-03 07:19:49,796 [Listener at localhost/34517] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:19:49,796 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@3d526ad9] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:19:49,800 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13, DS-34cdaddf-9d99-43fe-ac94-a2d410b1c52a) exiting.
2020-12-03 07:19:49,801 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14, DS-bf9cf6c5-0493-4ac0-8724-c46bd212b0fc) exiting.
2020-12-03 07:19:49,825 [Listener at localhost/34517] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@df5f5c0{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:19:49,825 [Listener at localhost/34517] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@308a6984{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:19:49,826 [Listener at localhost/34517] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@5b69fd74{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:19:49,826 [Listener at localhost/34517] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@61a5b4ae{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:19:49,828 [Listener at localhost/34517] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 35031
2020-12-03 07:19:49,828 [DataXceiver for client  at /127.0.0.1:49974 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,828 [DataXceiver for client  at /127.0.0.1:49856 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,828 [DataXceiver for client  at /127.0.0.1:49704 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,828 [DataXceiver for client  at /127.0.0.1:49646 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,828 [DataXceiver for client  at /127.0.0.1:49762 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,828 [DataXceiver for client  at /127.0.0.1:49678 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,828 [DataXceiver for client  at /127.0.0.1:50048 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,828 [DataXceiver for client  at /127.0.0.1:49888 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,829 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:19:49,829 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:19:49,829 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:19:49,835 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid 1948f61e-988c-4bd0-ad24-d3ae7d4c96b1) service to localhost/127.0.0.1:35332
2020-12-03 07:19:49,828 [DataXceiver for client  at /127.0.0.1:49974 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001 on DS-34cdaddf-9d99-43fe-ac94-a2d410b1c52a, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,835 [DataXceiver for client  at /127.0.0.1:49888 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001 on DS-34cdaddf-9d99-43fe-ac94-a2d410b1c52a, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,836 [DataXceiver for client  at /127.0.0.1:50048 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001 on DS-34cdaddf-9d99-43fe-ac94-a2d410b1c52a, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,835 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid 1948f61e-988c-4bd0-ad24-d3ae7d4c96b1)
2020-12-03 07:19:49,836 [DataXceiver for client  at /127.0.0.1:50048 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:38503, datanodeUuid=1948f61e-988c-4bd0-ad24-d3ae7d4c96b1, infoPort=37465, infoSecurePort=0, ipcPort=35031, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001 to /127.0.0.1:50048
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,836 [DataXceiver for client  at /127.0.0.1:49678 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001 on DS-34cdaddf-9d99-43fe-ac94-a2d410b1c52a, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,836 [DataXceiver for client  at /127.0.0.1:49678 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:38503, datanodeUuid=1948f61e-988c-4bd0-ad24-d3ae7d4c96b1, infoPort=37465, infoSecurePort=0, ipcPort=35031, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001 to /127.0.0.1:49678
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,836 [DataXceiver for client  at /127.0.0.1:49888 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:38503, datanodeUuid=1948f61e-988c-4bd0-ad24-d3ae7d4c96b1, infoPort=37465, infoSecurePort=0, ipcPort=35031, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001 to /127.0.0.1:49888
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,835 [DataXceiver for client  at /127.0.0.1:49974 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:38503, datanodeUuid=1948f61e-988c-4bd0-ad24-d3ae7d4c96b1, infoPort=37465, infoSecurePort=0, ipcPort=35031, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001 to /127.0.0.1:49974
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,846 [DataXceiver for client  at /127.0.0.1:49888 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:38503:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:49888 dst: /127.0.0.1:38503
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,845 [DataXceiver for client  at /127.0.0.1:49678 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:38503:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:49678 dst: /127.0.0.1:38503
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,845 [DataXceiver for client  at /127.0.0.1:49762 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001 on DS-34cdaddf-9d99-43fe-ac94-a2d410b1c52a, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,871 [DataXceiver for client  at /127.0.0.1:49646 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001 on DS-34cdaddf-9d99-43fe-ac94-a2d410b1c52a, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,871 [DataXceiver for client  at /127.0.0.1:49704 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001 on DS-34cdaddf-9d99-43fe-ac94-a2d410b1c52a, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,871 [DataXceiver for client  at /127.0.0.1:49704 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:38503, datanodeUuid=1948f61e-988c-4bd0-ad24-d3ae7d4c96b1, infoPort=37465, infoSecurePort=0, ipcPort=35031, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001 to /127.0.0.1:49704
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,871 [DataXceiver for client  at /127.0.0.1:49704 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:38503:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:49704 dst: /127.0.0.1:38503
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,872 [DataXceiver for client  at /127.0.0.1:49856 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001 on DS-34cdaddf-9d99-43fe-ac94-a2d410b1c52a, because there is no volume scanner for that storageId.
2020-12-03 07:19:49,836 [DataXceiver for client  at /127.0.0.1:50048 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:38503:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:50048 dst: /127.0.0.1:38503
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,882 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:49,882 [DataXceiver for client  at /127.0.0.1:49856 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:38503, datanodeUuid=1948f61e-988c-4bd0-ad24-d3ae7d4c96b1, infoPort=37465, infoSecurePort=0, ipcPort=35031, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001 to /127.0.0.1:49856
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,883 [DataXceiver for client  at /127.0.0.1:49856 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:38503:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:49856 dst: /127.0.0.1:38503
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,883 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13/current/BP-1367394141-172.17.0.2-1606979969461] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:49,871 [DataXceiver for client  at /127.0.0.1:49646 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:38503, datanodeUuid=1948f61e-988c-4bd0-ad24-d3ae7d4c96b1, infoPort=37465, infoSecurePort=0, ipcPort=35031, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001 to /127.0.0.1:49646
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,901 [DataXceiver for client  at /127.0.0.1:49646 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:38503:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:49646 dst: /127.0.0.1:38503
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,871 [DataXceiver for client  at /127.0.0.1:49762 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:38503, datanodeUuid=1948f61e-988c-4bd0-ad24-d3ae7d4c96b1, infoPort=37465, infoSecurePort=0, ipcPort=35031, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001 to /127.0.0.1:49762
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,870 [DataXceiver for client  at /127.0.0.1:49974 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:38503:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:49974 dst: /127.0.0.1:38503
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,936 [DataXceiver for client  at /127.0.0.1:49762 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775786_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:38503:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:49762 dst: /127.0.0.1:38503
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:49,918 [Listener at localhost/34517] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:19:49,884 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14/current/BP-1367394141-172.17.0.2-1606979969461] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:49,953 [Listener at localhost/34517] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:19:49,958 [Listener at localhost/34517] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:19:49,958 [Listener at localhost/34517] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:19:49,966 [Listener at localhost/34517] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:19:49,967 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 5
2020-12-03 07:19:49,967 [Listener at localhost/34517] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(341)) - DirectoryScanner: shutdown has been called, but periodic scanner not started
2020-12-03 07:19:49,967 [Listener at localhost/34517] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 37261
2020-12-03 07:19:49,968 [Listener at localhost/34517] WARN  util.MBeans (MBeans.java:unregister(145)) - Error unregistering Hadoop:service=DataNode,name=FSDatasetState-e5737a0c-e925-4b69-8166-e4391ac5f8bb
javax.management.InstanceNotFoundException: Hadoop:service=DataNode,name=FSDatasetState-e5737a0c-e925-4b69-8166-e4391ac5f8bb
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1095)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.exclusiveUnregisterMBean(DefaultMBeanServerInterceptor.java:427)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.unregisterMBean(DefaultMBeanServerInterceptor.java:415)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.unregisterMBean(JmxMBeanServer.java:546)
	at org.apache.hadoop.metrics2.util.MBeans.unregister(MBeans.java:143)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.shutdown(FsDatasetImpl.java:2293)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdown(DataNode.java:2151)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdownDataNode(MiniDFSCluster.java:2099)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdownDataNodes(MiniDFSCluster.java:2089)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2068)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2042)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2035)
	at org.apache.hadoop.hdfs.TestFileChecksum.tearDown(TestFileChecksum.java:111)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:168)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-12-03 07:19:49,969 [Listener at localhost/34517] WARN  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(191)) - AsyncDiskService has already shut down.
2020-12-03 07:19:49,969 [Listener at localhost/34517] WARN  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(175)) - AsyncLazyPersistService has already shut down.
2020-12-03 07:19:49,973 [Listener at localhost/34517] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:19:49,973 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 4
2020-12-03 07:19:49,973 [Listener at localhost/34517] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:19:49,974 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@4d157787] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:19:49,978 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10, DS-c43137fa-5025-44bc-ad04-ca38d42d45a5) exiting.
2020-12-03 07:19:49,978 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9, DS-ac18a139-2578-41ac-8803-da9d22cc545d) exiting.
2020-12-03 07:19:50,001 [Listener at localhost/34517] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@ea27e34{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:19:50,002 [Listener at localhost/34517] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@33a2499c{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:19:50,002 [Listener at localhost/34517] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@29d2d081{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:19:50,003 [Listener at localhost/34517] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@51c929ae{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:19:50,005 [Listener at localhost/34517] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 39004
2020-12-03 07:19:50,005 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:19:50,006 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:19:50,006 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:19:50,006 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid 0e810974-a498-4742-a093-0c4d95031785) service to localhost/127.0.0.1:35332
2020-12-03 07:19:50,007 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid 0e810974-a498-4742-a093-0c4d95031785)
2020-12-03 07:19:50,012 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:50,014 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10/current/BP-1367394141-172.17.0.2-1606979969461] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:50,021 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9/current/BP-1367394141-172.17.0.2-1606979969461] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:50,026 [Listener at localhost/34517] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:19:50,026 [Listener at localhost/34517] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:19:50,030 [Listener at localhost/34517] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:19:50,030 [Listener at localhost/34517] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:19:50,039 [Listener at localhost/34517] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:19:50,040 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 3
2020-12-03 07:19:50,040 [Listener at localhost/34517] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:19:50,040 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@781e7326] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:19:50,045 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7, DS-2e1daa68-afa3-4e1f-a710-a40da63cedfc) exiting.
2020-12-03 07:19:50,045 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8, DS-13901838-f2e7-4531-9885-e6639368ca38) exiting.
2020-12-03 07:19:50,070 [Listener at localhost/34517] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@238b521e{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:19:50,071 [Listener at localhost/34517] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@6b2c860{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:19:50,071 [Listener at localhost/34517] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@2575f671{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:19:50,072 [Listener at localhost/34517] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@433ffad1{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:19:50,073 [Listener at localhost/34517] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 36006
2020-12-03 07:19:50,073 [DataXceiver for client  at /127.0.0.1:56252 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,073 [DataXceiver for client  at /127.0.0.1:56098 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,073 [DataXceiver for client  at /127.0.0.1:56182 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,073 [DataXceiver for client  at /127.0.0.1:55916 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,073 [DataXceiver for client  at /127.0.0.1:56068 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,073 [DataXceiver for client  at /127.0.0.1:55970 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,073 [DataXceiver for client  at /127.0.0.1:55890 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,073 [DataXceiver for client  at /127.0.0.1:55858 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,074 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:19:50,074 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:19:50,074 [DataXceiver for client  at /127.0.0.1:56252 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001 on DS-2e1daa68-afa3-4e1f-a710-a40da63cedfc, because there is no volume scanner for that storageId.
2020-12-03 07:19:50,074 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:19:50,077 [DataXceiver for client  at /127.0.0.1:56252 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:44715, datanodeUuid=72c80882-49c9-444a-982c-993baca6bb62, infoPort=46372, infoSecurePort=0, ipcPort=36006, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001 to /127.0.0.1:56252
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,077 [DataXceiver for client  at /127.0.0.1:55858 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001 on DS-2e1daa68-afa3-4e1f-a710-a40da63cedfc, because there is no volume scanner for that storageId.
2020-12-03 07:19:50,076 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid 72c80882-49c9-444a-982c-993baca6bb62) service to localhost/127.0.0.1:35332
2020-12-03 07:19:50,082 [DataXceiver for client  at /127.0.0.1:55890 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001 on DS-2e1daa68-afa3-4e1f-a710-a40da63cedfc, because there is no volume scanner for that storageId.
2020-12-03 07:19:50,082 [DataXceiver for client  at /127.0.0.1:56252 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:44715:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:56252 dst: /127.0.0.1:44715
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,083 [DataXceiver for client  at /127.0.0.1:55890 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:44715, datanodeUuid=72c80882-49c9-444a-982c-993baca6bb62, infoPort=46372, infoSecurePort=0, ipcPort=36006, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001 to /127.0.0.1:55890
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,083 [DataXceiver for client  at /127.0.0.1:55970 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001 on DS-2e1daa68-afa3-4e1f-a710-a40da63cedfc, because there is no volume scanner for that storageId.
2020-12-03 07:19:50,082 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid 72c80882-49c9-444a-982c-993baca6bb62)
2020-12-03 07:19:50,099 [DataXceiver for client  at /127.0.0.1:55970 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:44715, datanodeUuid=72c80882-49c9-444a-982c-993baca6bb62, infoPort=46372, infoSecurePort=0, ipcPort=36006, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001 to /127.0.0.1:55970
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,082 [DataXceiver for client  at /127.0.0.1:55858 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:44715, datanodeUuid=72c80882-49c9-444a-982c-993baca6bb62, infoPort=46372, infoSecurePort=0, ipcPort=36006, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001 to /127.0.0.1:55858
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,100 [DataXceiver for client  at /127.0.0.1:55970 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:44715:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:55970 dst: /127.0.0.1:44715
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,099 [DataXceiver for client  at /127.0.0.1:56068 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001 on DS-2e1daa68-afa3-4e1f-a710-a40da63cedfc, because there is no volume scanner for that storageId.
2020-12-03 07:19:50,099 [DataXceiver for client  at /127.0.0.1:55890 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:44715:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:55890 dst: /127.0.0.1:44715
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,109 [DataXceiver for client  at /127.0.0.1:56068 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:44715, datanodeUuid=72c80882-49c9-444a-982c-993baca6bb62, infoPort=46372, infoSecurePort=0, ipcPort=36006, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001 to /127.0.0.1:56068
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,100 [DataXceiver for client  at /127.0.0.1:55858 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:44715:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:55858 dst: /127.0.0.1:44715
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,119 [DataXceiver for client  at /127.0.0.1:56068 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:44715:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:56068 dst: /127.0.0.1:44715
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,119 [DataXceiver for client  at /127.0.0.1:55916 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001 on DS-2e1daa68-afa3-4e1f-a710-a40da63cedfc, because there is no volume scanner for that storageId.
2020-12-03 07:19:50,140 [DataXceiver for client  at /127.0.0.1:56182 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001 on DS-2e1daa68-afa3-4e1f-a710-a40da63cedfc, because there is no volume scanner for that storageId.
2020-12-03 07:19:50,140 [DataXceiver for client  at /127.0.0.1:55916 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:44715, datanodeUuid=72c80882-49c9-444a-982c-993baca6bb62, infoPort=46372, infoSecurePort=0, ipcPort=36006, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001 to /127.0.0.1:55916
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,140 [DataXceiver for client  at /127.0.0.1:56182 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:44715, datanodeUuid=72c80882-49c9-444a-982c-993baca6bb62, infoPort=46372, infoSecurePort=0, ipcPort=36006, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001 to /127.0.0.1:56182
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,140 [DataXceiver for client  at /127.0.0.1:56098 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001 on DS-2e1daa68-afa3-4e1f-a710-a40da63cedfc, because there is no volume scanner for that storageId.
2020-12-03 07:19:50,140 [DataXceiver for client  at /127.0.0.1:56182 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:44715:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:56182 dst: /127.0.0.1:44715
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,140 [DataXceiver for client  at /127.0.0.1:55916 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:44715:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:55916 dst: /127.0.0.1:44715
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,141 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:50,141 [DataXceiver for client  at /127.0.0.1:56098 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:44715, datanodeUuid=72c80882-49c9-444a-982c-993baca6bb62, infoPort=46372, infoSecurePort=0, ipcPort=36006, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001 to /127.0.0.1:56098
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,165 [DataXceiver for client  at /127.0.0.1:56098 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775787_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:44715:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:56098 dst: /127.0.0.1:44715
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,165 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7/current/BP-1367394141-172.17.0.2-1606979969461] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:50,166 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/current/BP-1367394141-172.17.0.2-1606979969461] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:50,184 [Listener at localhost/34517] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:19:50,185 [Listener at localhost/34517] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:19:50,189 [Listener at localhost/34517] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:19:50,189 [Listener at localhost/34517] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:19:50,198 [Listener at localhost/34517] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:19:50,198 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 2
2020-12-03 07:19:50,198 [Listener at localhost/34517] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:19:50,198 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@600b0b7] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:19:50,203 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6, DS-06a2036e-43cd-41f6-bb27-4153a9f538e8) exiting.
2020-12-03 07:19:50,203 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5, DS-27376982-648a-435c-9530-844533c3fc3c) exiting.
2020-12-03 07:19:50,243 [Listener at localhost/34517] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@20312893{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:19:50,243 [Listener at localhost/34517] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@70eecdc2{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:19:50,244 [Listener at localhost/34517] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@5bbc9f97{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:19:50,244 [Listener at localhost/34517] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@5ed190be{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:19:50,246 [Listener at localhost/34517] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 43159
2020-12-03 07:19:50,246 [DataXceiver for client  at /127.0.0.1:52554 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,259 [DataXceiver for client  at /127.0.0.1:52554 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001 on DS-27376982-648a-435c-9530-844533c3fc3c, because there is no volume scanner for that storageId.
2020-12-03 07:19:50,259 [DataXceiver for client  at /127.0.0.1:52554 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:44555, datanodeUuid=811e904d-4b58-4d09-9a37-ecd8d80551dd, infoPort=45501, infoSecurePort=0, ipcPort=43159, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001 to /127.0.0.1:52554
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,246 [DataXceiver for client  at /127.0.0.1:52870 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,246 [DataXceiver for client  at /127.0.0.1:52788 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,246 [DataXceiver for client  at /127.0.0.1:52732 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,246 [DataXceiver for client  at /127.0.0.1:52528 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,246 [DataXceiver for client  at /127.0.0.1:52582 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,246 [DataXceiver for client  at /127.0.0.1:52470 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (BlockSender.java:sendPacket(675)) - BlockSender.sendChunks() exception: 
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,272 [DataXceiver for client  at /127.0.0.1:52870 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001 on DS-27376982-648a-435c-9530-844533c3fc3c, because there is no volume scanner for that storageId.
2020-12-03 07:19:50,265 [DataXceiver for client  at /127.0.0.1:52554 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:44555:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:52554 dst: /127.0.0.1:44555
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,259 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:19:50,259 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:19:50,258 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:19:50,309 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid 811e904d-4b58-4d09-9a37-ecd8d80551dd) service to localhost/127.0.0.1:35332
2020-12-03 07:19:50,302 [DataXceiver for client  at /127.0.0.1:52870 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:44555, datanodeUuid=811e904d-4b58-4d09-9a37-ecd8d80551dd, infoPort=45501, infoSecurePort=0, ipcPort=43159, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001 to /127.0.0.1:52870
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,302 [DataXceiver for client  at /127.0.0.1:52470 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001 on DS-27376982-648a-435c-9530-844533c3fc3c, because there is no volume scanner for that storageId.
2020-12-03 07:19:50,312 [DataXceiver for client  at /127.0.0.1:52870 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:44555:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:52870 dst: /127.0.0.1:44555
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,309 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid 811e904d-4b58-4d09-9a37-ecd8d80551dd)
2020-12-03 07:19:50,312 [DataXceiver for client  at /127.0.0.1:52470 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:44555, datanodeUuid=811e904d-4b58-4d09-9a37-ecd8d80551dd, infoPort=45501, infoSecurePort=0, ipcPort=43159, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001 to /127.0.0.1:52470
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,312 [DataXceiver for client  at /127.0.0.1:52582 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001 on DS-27376982-648a-435c-9530-844533c3fc3c, because there is no volume scanner for that storageId.
2020-12-03 07:19:50,317 [DataXceiver for client  at /127.0.0.1:52470 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:44555:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:52470 dst: /127.0.0.1:44555
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,318 [DataXceiver for client  at /127.0.0.1:52582 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:44555, datanodeUuid=811e904d-4b58-4d09-9a37-ecd8d80551dd, infoPort=45501, infoSecurePort=0, ipcPort=43159, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001 to /127.0.0.1:52582
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,318 [DataXceiver for client  at /127.0.0.1:52528 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001 on DS-27376982-648a-435c-9530-844533c3fc3c, because there is no volume scanner for that storageId.
2020-12-03 07:19:50,429 [DataXceiver for client  at /127.0.0.1:52582 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:44555:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:52582 dst: /127.0.0.1:44555
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,429 [DataXceiver for client  at /127.0.0.1:52528 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:44555, datanodeUuid=811e904d-4b58-4d09-9a37-ecd8d80551dd, infoPort=45501, infoSecurePort=0, ipcPort=43159, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001 to /127.0.0.1:52528
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,429 [DataXceiver for client  at /127.0.0.1:52732 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001 on DS-27376982-648a-435c-9530-844533c3fc3c, because there is no volume scanner for that storageId.
2020-12-03 07:19:50,429 [DataXceiver for client  at /127.0.0.1:52528 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:44555:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:52528 dst: /127.0.0.1:44555
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,429 [DataXceiver for client  at /127.0.0.1:52788 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] INFO  datanode.BlockScanner (BlockScanner.java:markSuspectBlock(342)) - Not scanning suspicious block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001 on DS-27376982-648a-435c-9530-844533c3fc3c, because there is no volume scanner for that storageId.
2020-12-03 07:19:50,430 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:50,431 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-1367394141-172.17.0.2-1606979969461] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:50,431 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/current/BP-1367394141-172.17.0.2-1606979969461] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:50,429 [DataXceiver for client  at /127.0.0.1:52732 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:44555, datanodeUuid=811e904d-4b58-4d09-9a37-ecd8d80551dd, infoPort=45501, infoSecurePort=0, ipcPort=43159, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001 to /127.0.0.1:52732
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,430 [DataXceiver for client  at /127.0.0.1:52788 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] WARN  datanode.DataNode (DataXceiver.java:readBlock(646)) - DatanodeRegistration(127.0.0.1:44555, datanodeUuid=811e904d-4b58-4d09-9a37-ecd8d80551dd, infoPort=45501, infoSecurePort=0, ipcPort=43159, storageInfo=lv=-57;cid=testClusterID;nsid=352815665;c=1606979969461):Got exception while serving BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001 to /127.0.0.1:52788
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,432 [DataXceiver for client  at /127.0.0.1:52732 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:44555:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:52732 dst: /127.0.0.1:44555
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,432 [DataXceiver for client  at /127.0.0.1:52788 [Sending block BP-1367394141-172.17.0.2-1606979969461:blk_-9223372036854775792_1001]] ERROR datanode.DataNode (DataXceiver.java:run(324)) - 127.0.0.1:44555:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:52788 dst: /127.0.0.1:44555
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 480000 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)
	at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:245)
	at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
	at org.apache.hadoop.hdfs.server.datanode.FileIoProvider.transferToSocketFully(FileIoProvider.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:637)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:829)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:776)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:610)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:152)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:104)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:19:50,509 [Listener at localhost/34517] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:19:50,510 [Listener at localhost/34517] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:19:50,513 [Listener at localhost/34517] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:19:50,513 [Listener at localhost/34517] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:19:50,540 [Listener at localhost/34517] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:19:50,540 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 1
2020-12-03 07:19:50,540 [Listener at localhost/34517] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:19:50,540 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@70e29e14] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:19:50,546 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3, DS-f8fbbb47-9ffb-40e3-9817-22e567ac2d28) exiting.
2020-12-03 07:19:50,546 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, DS-54f4be28-ebb1-40b8-abf0-7b39c47dce17) exiting.
2020-12-03 07:19:50,563 [Listener at localhost/34517] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@241a53ef{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:19:50,564 [Listener at localhost/34517] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@344344fa{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:19:50,564 [Listener at localhost/34517] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@384fc774{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:19:50,565 [Listener at localhost/34517] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@5d8445d7{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:19:50,566 [Listener at localhost/34517] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 33076
2020-12-03 07:19:50,567 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:19:50,567 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:19:50,568 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid 6433c7f1-b9cb-4ed4-9404-85e3ec0e7233) service to localhost/127.0.0.1:35332
2020-12-03 07:19:50,568 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid 6433c7f1-b9cb-4ed4-9404-85e3ec0e7233)
2020-12-03 07:19:50,568 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:50,568 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:19:50,575 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1367394141-172.17.0.2-1606979969461] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:50,575 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1367394141-172.17.0.2-1606979969461] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:50,580 [Listener at localhost/34517] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:19:50,580 [Listener at localhost/34517] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:19:50,584 [Listener at localhost/34517] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:19:50,584 [Listener at localhost/34517] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:19:50,585 [Listener at localhost/34517] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:19:50,585 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 0
2020-12-03 07:19:50,585 [Listener at localhost/34517] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:19:50,585 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@24fb6a80] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:19:50,590 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-245d3653-f81a-4228-a6a5-480c618ed25b) exiting.
2020-12-03 07:19:50,590 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-91ea1090-f0db-4472-ad91-faa0d9302fbf) exiting.
2020-12-03 07:19:50,609 [Listener at localhost/34517] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@1536602f{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:19:50,609 [Listener at localhost/34517] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@4ebea12c{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:19:50,610 [Listener at localhost/34517] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@ab7a938{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:19:50,610 [Listener at localhost/34517] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@545de5a4{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:19:50,611 [Listener at localhost/34517] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 37908
2020-12-03 07:19:50,612 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:19:50,612 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:19:50,619 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:19:50,620 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid 69a597d2-2edd-4e23-9c38-3328eccbc91b) service to localhost/127.0.0.1:35332
2020-12-03 07:19:50,720 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1367394141-172.17.0.2-1606979969461 (Datanode Uuid 69a597d2-2edd-4e23-9c38-3328eccbc91b)
2020-12-03 07:19:50,721 [BP-1367394141-172.17.0.2-1606979969461 heartbeating to localhost/127.0.0.1:35332] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1367394141-172.17.0.2-1606979969461
2020-12-03 07:19:50,721 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1367394141-172.17.0.2-1606979969461] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:50,722 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1367394141-172.17.0.2-1606979969461] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:19:50,727 [Listener at localhost/34517] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:19:50,727 [Listener at localhost/34517] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:19:50,731 [Listener at localhost/34517] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:19:50,731 [Listener at localhost/34517] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:19:50,732 [Listener at localhost/34517] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2020-12-03 07:19:50,733 [Listener at localhost/34517] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2020-12-03 07:19:50,734 [Listener at localhost/34517] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - DataNode metrics system shutdown complete.
2020-12-03 07:19:50,734 [Listener at localhost/34517] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:19:50,735 [Listener at localhost/34517] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:stopAndJoinNameNode(2130)) - Shutting down the namenode
2020-12-03 07:19:50,735 [Listener at localhost/34517] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1334)) - Stopping services started for active state
2020-12-03 07:19:50,735 [Listener at localhost/34517] INFO  namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1410)) - Ending log segment 1, 36
2020-12-03 07:19:50,736 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@a8c1f44] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4198)) - LazyPersistFileScrubber was interrupted, exiting
2020-12-03 07:19:50,736 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@71b3bc45] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4107)) - NameNodeEditLogRoller was interrupted, exiting
2020-12-03 07:19:50,736 [Listener at localhost/34517] INFO  namenode.FSEditLog (FSEditLog.java:printStatistics(778)) - Number of transactions: 37 Total time for transactions(ms): 32 Number of transactions batched in Syncs: 13 Number of syncs: 25 SyncTimes(ms): 5 3 
2020-12-03 07:19:51,508 [Listener at localhost/34517] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(145)) - Finalizing edits file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_inprogress_0000000000000000001 -> /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000001-0000000000000000037
2020-12-03 07:19:51,515 [Listener at localhost/34517] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(145)) - Finalizing edits file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_inprogress_0000000000000000001 -> /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000001-0000000000000000037
2020-12-03 07:19:51,516 [FSEditLogAsync] INFO  namenode.FSEditLog (FSEditLogAsync.java:run(253)) - FSEditLogAsync was interrupted, exiting
2020-12-03 07:19:51,517 [CacheReplicationMonitor(609079010)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2020-12-03 07:19:51,517 [Listener at localhost/34517] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 35332
2020-12-03 07:19:51,518 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:19:51,518 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:19:51,524 [RedundancyMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4687)) - Stopping RedundancyMonitor.
2020-12-03 07:19:51,528 [StorageInfoMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4722)) - Stopping thread.
2020-12-03 07:19:51,567 [Listener at localhost/34517] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1334)) - Stopping services started for active state
2020-12-03 07:19:51,568 [Listener at localhost/34517] INFO  namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1434)) - Stopping services started for standby state
2020-12-03 07:19:51,569 [Listener at localhost/34517] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@62833051{/,null,UNAVAILABLE}{/hdfs}
2020-12-03 07:19:51,571 [Listener at localhost/34517] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@62ff1e74{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:19:51,571 [Listener at localhost/34517] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@24c1b2d2{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:19:51,572 [Listener at localhost/34517] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@1b66c0fb{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
msx-rc 1
