2020-12-03 07:21:47,785 [main] INFO  mover.TestStorageMover (TestStorageMover.java:testNoSpaceArchive(714)) - testNoSpaceArchive
2020-12-03 07:21:47,848 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(493)) - starting cluster: numNameNodes=1, numDataNodes=6
Formatting using clusterid: testClusterID
2020-12-03 07:21:48,574 [main] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(229)) - Edit logging is async:true
2020-12-03 07:21:48,589 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(755)) - KeyProvider: null
2020-12-03 07:21:48,590 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(123)) - fsLock is fair: true
2020-12-03 07:21:48,591 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(141)) - Detailed lock hold time metrics enabled: false
2020-12-03 07:21:48,599 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(780)) - fsOwner             = root (auth:SIMPLE)
2020-12-03 07:21:48,599 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(781)) - supergroup          = supergroup
2020-12-03 07:21:48,599 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(782)) - isPermissionEnabled = true
2020-12-03 07:21:48,600 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(793)) - HA Enabled: false
2020-12-03 07:21:48,639 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:21:48,644 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - hadoop.configured.node.mapping is deprecated. Instead, use net.topology.configured.node.mapping
2020-12-03 07:21:48,645 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:21:48,645 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(303)) - dfs.block.invalidate.limit: configured=1000, counted=20, effected=1000
2020-12-03 07:21:48,645 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(311)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-12-03 07:21:48,646 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:21:48,653 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-12-03 07:21:48,653 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Dec 03 07:21:48
2020-12-03 07:21:48,656 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-12-03 07:21:48,656 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:21:48,658 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.9 GB = 39.3 MB
2020-12-03 07:21:48,658 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-12-03 07:21:48,675 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:21:48,676 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:21:48,677 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createSPSManager(5183)) - Storage policy satisfier is disabled
2020-12-03 07:21:48,677 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(595)) - dfs.block.access.token.enable = false
2020-12-03 07:21:48,680 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.namenode.redundancy.interval.seconds(2) assuming SECONDS
2020-12-03 07:21:48,680 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:21:48,685 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-12-03 07:21:48,685 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-12-03 07:21:48,686 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-12-03 07:21:48,686 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-12-03 07:21:48,687 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(581)) - defaultReplication         = 3
2020-12-03 07:21:48,687 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(582)) - maxReplication             = 512
2020-12-03 07:21:48,688 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(583)) - minReplication             = 1
2020-12-03 07:21:48,688 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(584)) - maxReplicationStreams      = 2
2020-12-03 07:21:48,688 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(585)) - redundancyRecheckInterval  = 2000ms
2020-12-03 07:21:48,688 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(586)) - encryptDataTransfer        = false
2020-12-03 07:21:48,689 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(587)) - maxNumBlocksToLog          = 1000
2020-12-03 07:21:48,735 [main] INFO  namenode.FSDirectory (SerialNumberManager.java:<clinit>(51)) - GLOBAL serial map: bits=29 maxEntries=536870911
2020-12-03 07:21:48,735 [main] INFO  namenode.FSDirectory (SerialNumberManager.java:<clinit>(51)) - USER serial map: bits=24 maxEntries=16777215
2020-12-03 07:21:48,736 [main] INFO  namenode.FSDirectory (SerialNumberManager.java:<clinit>(51)) - GROUP serial map: bits=24 maxEntries=16777215
2020-12-03 07:21:48,736 [main] INFO  namenode.FSDirectory (SerialNumberManager.java:<clinit>(51)) - XATTR serial map: bits=24 maxEntries=16777215
2020-12-03 07:21:48,752 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-12-03 07:21:48,752 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:21:48,753 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.9 GB = 19.6 MB
2020-12-03 07:21:48,753 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-12-03 07:21:48,760 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(290)) - ACLs enabled? false
2020-12-03 07:21:48,760 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(294)) - POSIX ACL inheritance enabled? true
2020-12-03 07:21:48,760 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(298)) - XAttrs enabled? true
2020-12-03 07:21:48,761 [main] INFO  namenode.NameNode (FSDirectory.java:<init>(362)) - Caching file names occurring more than 10 times
2020-12-03 07:21:48,769 [main] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-12-03 07:21:48,772 [main] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-12-03 07:21:48,778 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-12-03 07:21:48,779 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:21:48,780 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.9 GB = 4.9 MB
2020-12-03 07:21:48,780 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-12-03 07:21:48,794 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-12-03 07:21:48,795 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-12-03 07:21:48,795 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-12-03 07:21:48,800 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(996)) - Retry cache on namenode is enabled
2020-12-03 07:21:48,801 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(1004)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-12-03 07:21:48,804 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-12-03 07:21:48,805 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:21:48,805 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.9 GB = 603.0 KB
2020-12-03 07:21:48,806 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-12-03 07:21:48,854 [main] INFO  namenode.FSImage (FSImage.java:format(185)) - Allocated new BlockPoolId: BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:49,124 [main] INFO  common.Storage (NNStorage.java:format(595)) - Storage directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1 has been successfully formatted.
2020-12-03 07:21:49,261 [main] INFO  common.Storage (NNStorage.java:format(595)) - Storage directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2 has been successfully formatted.
2020-12-03 07:21:49,299 [FSImageSaver for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(512)) - Saving image file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/fsimage.ckpt_0000000000000000000 using no compression
2020-12-03 07:21:49,299 [FSImageSaver for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(512)) - Saving image file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage.ckpt_0000000000000000000 using no compression
2020-12-03 07:21:49,464 [FSImageSaver for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(516)) - Image file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .
2020-12-03 07:21:49,464 [FSImageSaver for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(516)) - Image file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .
2020-12-03 07:21:49,603 [main] INFO  namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:getImageTxIdToRetain(203)) - Going to retain 1 images with txid >= 0
2020-12-03 07:21:49,607 [main] INFO  namenode.NameNode (NameNode.java:createNameNode(1632)) - createNameNode []
2020-12-03 07:21:49,733 [main] INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(118)) - Loaded properties from hadoop-metrics2.properties
2020-12-03 07:21:50,107 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 0 second(s).
2020-12-03 07:21:50,108 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2020-12-03 07:21:50,144 [main] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://127.0.0.1:0
2020-12-03 07:21:50,190 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@1de76cc7] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:21:50,208 [main] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1641)) - Starting Web-server for hdfs at: http://localhost:0
2020-12-03 07:21:50,214 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:21:50,228 [main] INFO  util.log (Log.java:initialized(192)) - Logging initialized @3364ms
2020-12-03 07:21:50,364 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:21:50,369 [main] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-12-03 07:21:50,370 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:21:50,382 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:21:50,385 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-12-03 07:21:50,385 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:21:50,386 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:21:50,428 [main] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(102)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-12-03 07:21:50,429 [main] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(806)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-12-03 07:21:50,442 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 42107
2020-12-03 07:21:50,445 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:21:50,494 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@5c7bfdc1{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:21:50,496 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@71687585{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:21:50,548 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@7a3793c7{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-12-03 07:21:50,559 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@4c012563{HTTP/1.1,[http/1.1]}{localhost:42107}
2020-12-03 07:21:50,560 [main] INFO  server.Server (Server.java:doStart(419)) - Started @3696ms
2020-12-03 07:21:50,570 [main] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(229)) - Edit logging is async:true
2020-12-03 07:21:50,571 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(755)) - KeyProvider: null
2020-12-03 07:21:50,571 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(123)) - fsLock is fair: true
2020-12-03 07:21:50,572 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(141)) - Detailed lock hold time metrics enabled: false
2020-12-03 07:21:50,572 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(780)) - fsOwner             = root (auth:SIMPLE)
2020-12-03 07:21:50,572 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(781)) - supergroup          = supergroup
2020-12-03 07:21:50,573 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(782)) - isPermissionEnabled = true
2020-12-03 07:21:50,573 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(793)) - HA Enabled: false
2020-12-03 07:21:50,574 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:21:50,575 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:21:50,575 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(303)) - dfs.block.invalidate.limit: configured=1000, counted=20, effected=1000
2020-12-03 07:21:50,575 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(311)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-12-03 07:21:50,575 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:21:50,576 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-12-03 07:21:50,576 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Dec 03 07:21:50
2020-12-03 07:21:50,577 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-12-03 07:21:50,577 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:21:50,577 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.8 GB = 36.4 MB
2020-12-03 07:21:50,578 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-12-03 07:21:50,582 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:21:50,583 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:21:50,583 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createSPSManager(5183)) - Storage policy satisfier is disabled
2020-12-03 07:21:50,583 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(595)) - dfs.block.access.token.enable = false
2020-12-03 07:21:50,584 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.namenode.redundancy.interval.seconds(2) assuming SECONDS
2020-12-03 07:21:50,584 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:21:50,585 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-12-03 07:21:50,585 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-12-03 07:21:50,585 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-12-03 07:21:50,586 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-12-03 07:21:50,586 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(581)) - defaultReplication         = 3
2020-12-03 07:21:50,586 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(582)) - maxReplication             = 512
2020-12-03 07:21:50,586 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(583)) - minReplication             = 1
2020-12-03 07:21:50,587 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(584)) - maxReplicationStreams      = 2
2020-12-03 07:21:50,587 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(585)) - redundancyRecheckInterval  = 2000ms
2020-12-03 07:21:50,587 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(586)) - encryptDataTransfer        = false
2020-12-03 07:21:50,587 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(587)) - maxNumBlocksToLog          = 1000
2020-12-03 07:21:50,588 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-12-03 07:21:50,588 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:21:50,589 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.8 GB = 18.2 MB
2020-12-03 07:21:50,589 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-12-03 07:21:50,591 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(290)) - ACLs enabled? false
2020-12-03 07:21:50,591 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(294)) - POSIX ACL inheritance enabled? true
2020-12-03 07:21:50,592 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(298)) - XAttrs enabled? true
2020-12-03 07:21:50,592 [main] INFO  namenode.NameNode (FSDirectory.java:<init>(362)) - Caching file names occurring more than 10 times
2020-12-03 07:21:50,592 [main] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-12-03 07:21:50,593 [main] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-12-03 07:21:50,593 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-12-03 07:21:50,593 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:21:50,594 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.8 GB = 4.6 MB
2020-12-03 07:21:50,594 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-12-03 07:21:50,595 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-12-03 07:21:50,596 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-12-03 07:21:50,596 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-12-03 07:21:50,596 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(996)) - Retry cache on namenode is enabled
2020-12-03 07:21:50,596 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(1004)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-12-03 07:21:50,597 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-12-03 07:21:50,597 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-12-03 07:21:50,598 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.8 GB = 559.3 KB
2020-12-03 07:21:50,598 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-12-03 07:21:50,758 [main] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/in_use.lock acquired by nodename 3128@789b9a0e3b85
2020-12-03 07:21:50,917 [main] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/in_use.lock acquired by nodename 3128@789b9a0e3b85
2020-12-03 07:21:50,921 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(428)) - Recovering unfinalized segments in /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current
2020-12-03 07:21:50,921 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(428)) - Recovering unfinalized segments in /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current
2020-12-03 07:21:50,922 [main] INFO  namenode.FSImage (FSImage.java:loadFSImage(733)) - No edit log streams selected.
2020-12-03 07:21:50,922 [main] INFO  namenode.FSImage (FSImage.java:loadFSImageFile(797)) - Planning to load image: FSImageFile(file=/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2020-12-03 07:21:50,960 [main] INFO  namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(234)) - Loading 1 INodes.
2020-12-03 07:21:50,967 [main] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(246)) - Loaded FSImage in 0 seconds.
2020-12-03 07:21:50,967 [main] INFO  namenode.FSImage (FSImage.java:loadFSImage(978)) - Loaded image for txid 0 from /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000000
2020-12-03 07:21:50,972 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1110)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-12-03 07:21:50,973 [main] INFO  namenode.FSEditLog (FSEditLog.java:startLogSegment(1365)) - Starting log segment at 1
2020-12-03 07:21:51,317 [main] INFO  namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2020-12-03 07:21:51,318 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(727)) - Finished loading FSImage in 718 msecs
2020-12-03 07:21:51,538 [main] INFO  namenode.NameNode (NameNodeRpcServer.java:<init>(448)) - RPC server is binding to localhost:0
2020-12-03 07:21:51,591 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:21:51,609 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:21:51,930 [Listener at localhost/44708] INFO  namenode.NameNode (NameNode.java:initialize(722)) - Clients are to use localhost:44708 to access this namenode/service.
2020-12-03 07:21:51,934 [Listener at localhost/44708] INFO  namenode.FSNamesystem (FSNamesystem.java:registerMBean(5090)) - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2020-12-03 07:21:51,949 [Listener at localhost/44708] INFO  namenode.LeaseManager (LeaseManager.java:getNumUnderConstructionBlocks(171)) - Number of blocks under construction: 0
2020-12-03 07:21:51,960 [Listener at localhost/44708] INFO  blockmanagement.BlockManager (BlockManager.java:initializeReplQueues(4922)) - initializing replication queues
2020-12-03 07:21:51,960 [Listener at localhost/44708] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(400)) - STATE* Leaving safe mode after 0 secs
2020-12-03 07:21:51,960 [Listener at localhost/44708] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(406)) - STATE* Network topology has 0 racks and 0 datanodes
2020-12-03 07:21:51,961 [Listener at localhost/44708] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(408)) - STATE* UnderReplicatedBlocks has 0 blocks
2020-12-03 07:21:51,964 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3585)) - Total number of blocks            = 0
2020-12-03 07:21:51,964 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3586)) - Number of invalid blocks          = 0
2020-12-03 07:21:51,964 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3587)) - Number of under-replicated blocks = 0
2020-12-03 07:21:51,965 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3588)) - Number of  over-replicated blocks = 0
2020-12-03 07:21:51,965 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3590)) - Number of blocks being written    = 0
2020-12-03 07:21:51,965 [Reconstruction Queue Initializer] INFO  hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(3593)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 5 msec
2020-12-03 07:21:51,991 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:21:51,991 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:21:51,994 [Listener at localhost/44708] INFO  namenode.NameNode (NameNode.java:startCommonServices(828)) - NameNode RPC up at: localhost/127.0.0.1:44708
2020-12-03 07:21:51,998 [Listener at localhost/44708] INFO  namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1222)) - Starting services required for active state
2020-12-03 07:21:51,998 [Listener at localhost/44708] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(777)) - Initializing quota with 4 thread(s)
2020-12-03 07:21:52,005 [Listener at localhost/44708] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(786)) - Quota initialization completed in 7 milliseconds
name space=1
storage space=0
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2020-12-03 07:21:52,009 [CacheReplicationMonitor(1134073148)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-12-03 07:21:52,142 [Listener at localhost/44708] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1,[ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:21:52,228 [Listener at localhost/44708] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:21:52,244 [Listener at localhost/44708] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:21:52,264 [Listener at localhost/44708] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:21:52,270 [Listener at localhost/44708] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:21:52,273 [Listener at localhost/44708] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:21:52,278 [Listener at localhost/44708] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:21:52,279 [Listener at localhost/44708] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:21:52,279 [Listener at localhost/44708] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:21:52,279 [Listener at localhost/44708] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:21:52,284 [Listener at localhost/44708] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:21:52,292 [Listener at localhost/44708] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:33340
2020-12-03 07:21:52,295 [Listener at localhost/44708] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:21:52,295 [Listener at localhost/44708] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:21:52,314 [Listener at localhost/44708] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:21:52,316 [Listener at localhost/44708] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:21:52,319 [Listener at localhost/44708] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:21:52,319 [Listener at localhost/44708] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:21:52,322 [Listener at localhost/44708] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:21:52,323 [Listener at localhost/44708] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:21:52,323 [Listener at localhost/44708] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:21:52,324 [Listener at localhost/44708] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:21:52,327 [Listener at localhost/44708] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 43450
2020-12-03 07:21:52,327 [Listener at localhost/44708] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:21:52,329 [Listener at localhost/44708] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@7a7471ce{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:21:52,330 [Listener at localhost/44708] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@62e70ea3{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:21:52,338 [Listener at localhost/44708] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@61526469{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:21:52,341 [Listener at localhost/44708] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@274872f8{HTTP/1.1,[http/1.1]}{localhost:43450}
2020-12-03 07:21:52,341 [Listener at localhost/44708] INFO  server.Server (Server.java:doStart(419)) - Started @5477ms
2020-12-03 07:21:52,691 [Listener at localhost/44708] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:45658
2020-12-03 07:21:52,692 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@200a26bc] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:21:52,697 [Listener at localhost/44708] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:21:52,697 [Listener at localhost/44708] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:21:52,904 [Listener at localhost/44708] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:21:52,905 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:21:52,914 [Listener at localhost/38790] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:38790
2020-12-03 07:21:52,935 [Listener at localhost/38790] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:21:52,937 [Listener at localhost/38790] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:21:52,952 [Thread-59] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:44708 starting to offer service
2020-12-03 07:21:52,961 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:21:52,961 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:21:52,970 [Listener at localhost/38790] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 1 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3,[ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:21:52,973 [Listener at localhost/38790] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
2020-12-03 07:21:52,973 [Listener at localhost/38790] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:21:52,974 [Listener at localhost/38790] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:21:52,975 [Listener at localhost/38790] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:21:52,975 [Listener at localhost/38790] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:21:52,976 [Listener at localhost/38790] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:21:52,976 [Listener at localhost/38790] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:21:52,976 [Listener at localhost/38790] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:21:52,977 [Listener at localhost/38790] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:21:52,977 [Listener at localhost/38790] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:21:52,978 [Listener at localhost/38790] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:44597
2020-12-03 07:21:52,978 [Listener at localhost/38790] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:21:52,978 [Listener at localhost/38790] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:21:52,980 [Listener at localhost/38790] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:21:52,981 [Listener at localhost/38790] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:21:52,982 [Listener at localhost/38790] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:21:52,982 [Listener at localhost/38790] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:21:52,984 [Listener at localhost/38790] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:21:52,985 [Listener at localhost/38790] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:21:52,986 [Listener at localhost/38790] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:21:52,986 [Listener at localhost/38790] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:21:52,987 [Listener at localhost/38790] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 37740
2020-12-03 07:21:52,987 [Listener at localhost/38790] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:21:53,002 [Listener at localhost/38790] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@24be2d9c{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:21:53,003 [Listener at localhost/38790] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@aec50a1{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:21:53,016 [Listener at localhost/38790] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@29ad44e3{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:21:53,018 [Listener at localhost/38790] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@15bcf458{HTTP/1.1,[http/1.1]}{localhost:37740}
2020-12-03 07:21:53,018 [Listener at localhost/38790] INFO  server.Server (Server.java:doStart(419)) - Started @6154ms
2020-12-03 07:21:53,103 [Listener at localhost/38790] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:39415
2020-12-03 07:21:53,104 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@43c67247] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:21:53,104 [Listener at localhost/38790] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:21:53,105 [Listener at localhost/38790] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:21:53,105 [Listener at localhost/38790] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:21:53,106 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:21:53,113 [Listener at localhost/35175] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:35175
2020-12-03 07:21:53,127 [Listener at localhost/35175] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:21:53,128 [Listener at localhost/35175] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:21:53,129 [Thread-83] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:44708 starting to offer service
2020-12-03 07:21:53,131 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:21:53,131 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:21:53,139 [Listener at localhost/35175] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 2 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5,[ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6
2020-12-03 07:21:53,142 [Listener at localhost/35175] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5
2020-12-03 07:21:53,142 [Listener at localhost/35175] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6
2020-12-03 07:21:53,149 [Listener at localhost/35175] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:21:53,151 [Listener at localhost/35175] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:21:53,151 [Listener at localhost/35175] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:21:53,152 [Listener at localhost/35175] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:21:53,152 [Listener at localhost/35175] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:21:53,152 [Listener at localhost/35175] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:21:53,153 [Listener at localhost/35175] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:21:53,153 [Listener at localhost/35175] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:21:53,154 [Listener at localhost/35175] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:33403
2020-12-03 07:21:53,155 [Listener at localhost/35175] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:21:53,155 [Listener at localhost/35175] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:21:53,156 [Listener at localhost/35175] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:21:53,158 [Listener at localhost/35175] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:21:53,159 [Listener at localhost/35175] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:21:53,160 [Listener at localhost/35175] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:21:53,162 [Listener at localhost/35175] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:21:53,163 [Listener at localhost/35175] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:21:53,163 [Listener at localhost/35175] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:21:53,164 [Listener at localhost/35175] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:21:53,164 [Listener at localhost/35175] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 43478
2020-12-03 07:21:53,165 [Listener at localhost/35175] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:21:53,167 [Listener at localhost/35175] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@63fd4873{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:21:53,168 [Listener at localhost/35175] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@7544a1e4{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:21:53,177 [Listener at localhost/35175] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@210386e0{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:21:53,179 [Listener at localhost/35175] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@3d4d3fe7{HTTP/1.1,[http/1.1]}{localhost:43478}
2020-12-03 07:21:53,179 [Listener at localhost/35175] INFO  server.Server (Server.java:doStart(419)) - Started @6316ms
2020-12-03 07:21:53,207 [Listener at localhost/35175] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:41687
2020-12-03 07:21:53,208 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@51684e4a] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:21:53,208 [Listener at localhost/35175] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:21:53,208 [Listener at localhost/35175] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:21:53,209 [Listener at localhost/35175] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:21:53,210 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:21:53,215 [Listener at localhost/42709] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:42709
2020-12-03 07:21:53,226 [Listener at localhost/42709] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:21:53,226 [Listener at localhost/42709] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:21:53,227 [Thread-105] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:44708 starting to offer service
2020-12-03 07:21:53,229 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:21:53,231 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:21:53,233 [Listener at localhost/42709] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 3 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7,[ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8
2020-12-03 07:21:53,239 [Listener at localhost/42709] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7
2020-12-03 07:21:53,240 [Listener at localhost/42709] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8
2020-12-03 07:21:53,242 [Listener at localhost/42709] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:21:53,243 [Listener at localhost/42709] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:21:53,243 [Listener at localhost/42709] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:21:53,244 [Listener at localhost/42709] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:21:53,244 [Listener at localhost/42709] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:21:53,245 [Listener at localhost/42709] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:21:53,245 [Listener at localhost/42709] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:21:53,245 [Listener at localhost/42709] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:21:53,247 [Listener at localhost/42709] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:42991
2020-12-03 07:21:53,247 [Listener at localhost/42709] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:21:53,248 [Listener at localhost/42709] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:21:53,250 [Listener at localhost/42709] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:21:53,252 [Listener at localhost/42709] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:21:53,253 [Listener at localhost/42709] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:21:53,253 [Listener at localhost/42709] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:21:53,257 [Listener at localhost/42709] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:21:53,259 [Listener at localhost/42709] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:21:53,259 [Listener at localhost/42709] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:21:53,259 [Listener at localhost/42709] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:21:53,261 [Listener at localhost/42709] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 33601
2020-12-03 07:21:53,261 [Listener at localhost/42709] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:21:53,264 [Listener at localhost/42709] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@344561e0{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:21:53,265 [Listener at localhost/42709] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@36ac8a63{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:21:53,275 [Listener at localhost/42709] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@30c0ccff{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:21:53,276 [Listener at localhost/42709] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@581d969c{HTTP/1.1,[http/1.1]}{localhost:33601}
2020-12-03 07:21:53,277 [Listener at localhost/42709] INFO  server.Server (Server.java:doStart(419)) - Started @6413ms
2020-12-03 07:21:53,379 [Thread-83] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:44708
2020-12-03 07:21:53,381 [Listener at localhost/42709] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:37554
2020-12-03 07:21:53,381 [Thread-59] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:44708
2020-12-03 07:21:53,381 [Thread-105] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:44708
2020-12-03 07:21:53,382 [Listener at localhost/42709] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:21:53,383 [Thread-105] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:21:53,383 [Thread-83] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:21:53,383 [Thread-59] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:21:53,383 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@2b46a8c1] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:21:53,383 [Listener at localhost/42709] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:21:53,384 [Listener at localhost/42709] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:21:53,385 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:21:53,388 [Listener at localhost/43194] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:43194
2020-12-03 07:21:53,393 [Listener at localhost/43194] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:21:53,393 [Listener at localhost/43194] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:21:53,394 [Thread-127] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:44708 starting to offer service
2020-12-03 07:21:53,395 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:21:53,395 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:21:53,398 [Thread-127] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:44708
2020-12-03 07:21:53,399 [Thread-127] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:21:53,401 [Listener at localhost/43194] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 4 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9,[ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10
2020-12-03 07:21:53,403 [Listener at localhost/43194] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9
2020-12-03 07:21:53,408 [Listener at localhost/43194] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10
2020-12-03 07:21:53,409 [Listener at localhost/43194] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:21:53,410 [Listener at localhost/43194] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:21:53,410 [Listener at localhost/43194] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:21:53,411 [Listener at localhost/43194] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:21:53,411 [Listener at localhost/43194] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:21:53,411 [Listener at localhost/43194] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:21:53,411 [Listener at localhost/43194] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:21:53,412 [Listener at localhost/43194] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:21:53,413 [Listener at localhost/43194] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:44891
2020-12-03 07:21:53,416 [Listener at localhost/43194] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:21:53,417 [Listener at localhost/43194] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:21:53,420 [Listener at localhost/43194] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:21:53,424 [Listener at localhost/43194] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:21:53,426 [Listener at localhost/43194] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:21:53,426 [Listener at localhost/43194] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:21:53,429 [Listener at localhost/43194] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:21:53,430 [Listener at localhost/43194] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:21:53,431 [Listener at localhost/43194] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:21:53,431 [Listener at localhost/43194] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:21:53,433 [Listener at localhost/43194] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 36840
2020-12-03 07:21:53,433 [Listener at localhost/43194] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:21:53,438 [Listener at localhost/43194] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@320e400{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:21:53,439 [Listener at localhost/43194] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@1cfd1875{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:21:53,448 [Listener at localhost/43194] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@88d6f9b{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:21:53,449 [Listener at localhost/43194] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@47d93e0d{HTTP/1.1,[http/1.1]}{localhost:36840}
2020-12-03 07:21:53,450 [Listener at localhost/43194] INFO  server.Server (Server.java:doStart(419)) - Started @6586ms
2020-12-03 07:21:53,465 [Listener at localhost/43194] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:39970
2020-12-03 07:21:53,466 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@751e664e] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:21:53,466 [Listener at localhost/43194] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:21:53,466 [Listener at localhost/43194] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:21:53,467 [Listener at localhost/43194] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:21:53,468 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:21:53,474 [Listener at localhost/35404] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:35404
2020-12-03 07:21:53,478 [Listener at localhost/35404] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:21:53,479 [Listener at localhost/35404] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:21:53,480 [Thread-149] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:44708 starting to offer service
2020-12-03 07:21:53,482 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:21:53,482 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:21:53,486 [Listener at localhost/35404] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1659)) - Starting DataNode 5 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11,[ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12
2020-12-03 07:21:53,488 [Thread-149] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:44708
2020-12-03 07:21:53,489 [Thread-149] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:21:53,489 [Listener at localhost/35404] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11
2020-12-03 07:21:53,489 [Listener at localhost/35404] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12
2020-12-03 07:21:53,491 [Listener at localhost/35404] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-12-03 07:21:53,492 [Listener at localhost/35404] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:21:53,492 [Listener at localhost/35404] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-12-03 07:21:53,492 [Listener at localhost/35404] INFO  datanode.DataNode (DataNode.java:<init>(500)) - Configured hostname is 127.0.0.1
2020-12-03 07:21:53,493 [Listener at localhost/35404] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-12-03 07:21:53,493 [Listener at localhost/35404] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:21:53,493 [Listener at localhost/35404] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:21:53,493 [Listener at localhost/35404] INFO  datanode.DataNode (DataNode.java:startDataNode(1400)) - Starting DataNode with maxLockedMemory = 0
2020-12-03 07:21:53,494 [Listener at localhost/35404] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1148)) - Opened streaming server at /127.0.0.1:46034
2020-12-03 07:21:53,494 [Listener at localhost/35404] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-12-03 07:21:53,494 [Listener at localhost/35404] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-12-03 07:21:53,495 [Listener at localhost/35404] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:21:53,497 [Listener at localhost/35404] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-12-03 07:21:53,498 [Listener at localhost/35404] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-12-03 07:21:53,498 [Listener at localhost/35404] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1072)) - Web server is in development mode. Resources will be read from the source tree.
2020-12-03 07:21:53,500 [Listener at localhost/35404] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(990)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-12-03 07:21:53,500 [Listener at localhost/35404] INFO  http.HttpServer2 (HttpServer2.java:addFilter(963)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-12-03 07:21:53,501 [Listener at localhost/35404] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-12-03 07:21:53,501 [Listener at localhost/35404] INFO  http.HttpServer2 (HttpServer2.java:addFilter(973)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-12-03 07:21:53,501 [Listener at localhost/35404] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1206)) - Jetty bound to port 36721
2020-12-03 07:21:53,502 [Listener at localhost/35404] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-12-03 07:21:53,503 [Listener at localhost/35404] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@27dc79f7{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-12-03 07:21:53,504 [Listener at localhost/35404] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@3aaf4f07{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-12-03 07:21:53,510 [Listener at localhost/35404] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@a23a01d{/,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-12-03 07:21:53,511 [Listener at localhost/35404] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@4acf72b6{HTTP/1.1,[http/1.1]}{localhost:36721}
2020-12-03 07:21:53,511 [Listener at localhost/35404] INFO  server.Server (Server.java:doStart(419)) - Started @6648ms
2020-12-03 07:21:53,514 [Thread-83] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/in_use.lock acquired by nodename 3128@789b9a0e3b85
2020-12-03 07:21:53,515 [Thread-59] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/in_use.lock acquired by nodename 3128@789b9a0e3b85
2020-12-03 07:21:53,515 [Thread-105] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/in_use.lock acquired by nodename 3128@789b9a0e3b85
2020-12-03 07:21:53,516 [Thread-105] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5 is not formatted for namespace 333324072. Formatting...
2020-12-03 07:21:53,516 [Thread-59] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1 is not formatted for namespace 333324072. Formatting...
2020-12-03 07:21:53,516 [Thread-83] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3 is not formatted for namespace 333324072. Formatting...
2020-12-03 07:21:53,518 [Thread-59] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-5a3cdcb1-c70d-48fc-9151-fc286cea3570 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1 
2020-12-03 07:21:53,518 [Thread-83] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-058ff3c4-f186-458c-8f1b-9ea2f1373a45 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3 
2020-12-03 07:21:53,518 [Thread-105] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-5cbe80bb-e620-48de-90dc-09c79ddc8402 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5 
2020-12-03 07:21:53,529 [Listener at localhost/35404] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:39776
2020-12-03 07:21:53,530 [Listener at localhost/35404] INFO  datanode.DataNode (DataNode.java:startDataNode(1428)) - dnUserName = root
2020-12-03 07:21:53,530 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@3301500b] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-12-03 07:21:53,530 [Listener at localhost/35404] INFO  datanode.DataNode (DataNode.java:startDataNode(1429)) - supergroup = supergroup
2020-12-03 07:21:53,530 [Listener at localhost/35404] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2020-12-03 07:21:53,531 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1219)) - Starting Socket Reader #1 for port 0
2020-12-03 07:21:53,535 [Listener at localhost/41652] INFO  datanode.DataNode (DataNode.java:initIpcServer(1034)) - Opened IPC server at /127.0.0.1:41652
2020-12-03 07:21:53,539 [Listener at localhost/41652] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-12-03 07:21:53,539 [Listener at localhost/41652] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-12-03 07:21:53,540 [Thread-171] INFO  datanode.DataNode (BPServiceActor.java:run(817)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:44708 starting to offer service
2020-12-03 07:21:53,541 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1460)) - IPC Server Responder: starting
2020-12-03 07:21:53,541 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1298)) - IPC Server listener on 0: starting
2020-12-03 07:21:53,545 [Thread-171] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:44708
2020-12-03 07:21:53,545 [Thread-171] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-12-03 07:21:53,634 [Thread-149] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9/in_use.lock acquired by nodename 3128@789b9a0e3b85
2020-12-03 07:21:53,634 [Thread-127] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7/in_use.lock acquired by nodename 3128@789b9a0e3b85
2020-12-03 07:21:53,634 [Thread-149] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9 is not formatted for namespace 333324072. Formatting...
2020-12-03 07:21:53,634 [Thread-127] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7 is not formatted for namespace 333324072. Formatting...
2020-12-03 07:21:53,635 [Thread-127] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-7f1774f2-7b1d-42eb-be32-aeaf5b4b35f9 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7 
2020-12-03 07:21:53,635 [Thread-149] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-60aaafac-0dea-4e4b-8271-d136469da168 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9 
2020-12-03 07:21:53,727 [Thread-171] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11/in_use.lock acquired by nodename 3128@789b9a0e3b85
2020-12-03 07:21:53,728 [Thread-171] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11 is not formatted for namespace 333324072. Formatting...
2020-12-03 07:21:53,731 [Thread-171] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-6cbd1a79-1320-44b1-8284-940a3e816e05 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11 
2020-12-03 07:21:54,044 [IPC Server handler 8 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:54,053 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:21:54,053 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:21:54,156 [IPC Server handler 2 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:54,157 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:21:54,157 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:21:54,181 [Thread-59] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/in_use.lock acquired by nodename 3128@789b9a0e3b85
2020-12-03 07:21:54,181 [Thread-83] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/in_use.lock acquired by nodename 3128@789b9a0e3b85
2020-12-03 07:21:54,181 [Thread-105] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/in_use.lock acquired by nodename 3128@789b9a0e3b85
2020-12-03 07:21:54,181 [Thread-83] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4 is not formatted for namespace 333324072. Formatting...
2020-12-03 07:21:54,181 [Thread-59] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2 is not formatted for namespace 333324072. Formatting...
2020-12-03 07:21:54,181 [Thread-105] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6 is not formatted for namespace 333324072. Formatting...
2020-12-03 07:21:54,184 [Thread-83] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-c7c90efb-327f-4b73-88a6-34752b5003a8 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4 
2020-12-03 07:21:54,184 [Thread-105] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-af93b569-157d-478e-8452-49963f34fadf for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6 
2020-12-03 07:21:54,184 [Thread-59] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-31c88246-3f1d-4052-a932-2eefddf8a505 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2 
2020-12-03 07:21:54,259 [IPC Server handler 3 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:54,260 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:21:54,260 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:21:54,308 [Thread-149] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10/in_use.lock acquired by nodename 3128@789b9a0e3b85
2020-12-03 07:21:54,308 [Thread-127] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/in_use.lock acquired by nodename 3128@789b9a0e3b85
2020-12-03 07:21:54,308 [Thread-149] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10 is not formatted for namespace 333324072. Formatting...
2020-12-03 07:21:54,308 [Thread-127] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8 is not formatted for namespace 333324072. Formatting...
2020-12-03 07:21:54,311 [Thread-149] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-6ee6c00c-1ea1-4eca-be51-a4e9a83f8e11 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10 
2020-12-03 07:21:54,312 [Thread-127] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-fa19f5df-de0d-41f8-ae1e-004956343da7 for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8 
2020-12-03 07:21:54,363 [IPC Server handler 0 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:54,363 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:21:54,364 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:21:54,414 [Thread-171] INFO  common.Storage (Storage.java:tryLock(923)) - Lock on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12/in_use.lock acquired by nodename 3128@789b9a0e3b85
2020-12-03 07:21:54,415 [Thread-171] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12 is not formatted for namespace 333324072. Formatting...
2020-12-03 07:21:54,416 [Thread-171] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-c1ddbde3-64ec-4172-87a7-93013a994b4a for directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12 
2020-12-03 07:21:54,465 [IPC Server handler 5 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:54,466 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:21:54,466 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:21:54,537 [Thread-59] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:54,537 [Thread-83] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:54,537 [Thread-59] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:54,537 [Thread-83] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:54,538 [Thread-59] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1 and block pool id BP-1344107566-172.17.0.11-1606980108837 is not formatted. Formatting ...
2020-12-03 07:21:54,538 [Thread-83] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3 and block pool id BP-1344107566-172.17.0.11-1606980108837 is not formatted. Formatting ...
2020-12-03 07:21:54,538 [Thread-59] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1344107566-172.17.0.11-1606980108837 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1344107566-172.17.0.11-1606980108837/current
2020-12-03 07:21:54,538 [Thread-83] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1344107566-172.17.0.11-1606980108837 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1344107566-172.17.0.11-1606980108837/current
2020-12-03 07:21:54,539 [Thread-105] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:54,539 [Thread-105] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:54,539 [Thread-105] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5 and block pool id BP-1344107566-172.17.0.11-1606980108837 is not formatted. Formatting ...
2020-12-03 07:21:54,539 [Thread-105] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1344107566-172.17.0.11-1606980108837 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-1344107566-172.17.0.11-1606980108837/current
2020-12-03 07:21:54,568 [IPC Server handler 7 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:54,569 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:21:54,570 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:21:54,661 [Thread-127] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:54,661 [Thread-149] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:54,662 [Thread-127] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7/current/BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:54,662 [Thread-149] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9/current/BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:54,662 [Thread-127] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7 and block pool id BP-1344107566-172.17.0.11-1606980108837 is not formatted. Formatting ...
2020-12-03 07:21:54,662 [Thread-149] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9 and block pool id BP-1344107566-172.17.0.11-1606980108837 is not formatted. Formatting ...
2020-12-03 07:21:54,662 [Thread-127] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1344107566-172.17.0.11-1606980108837 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7/current/BP-1344107566-172.17.0.11-1606980108837/current
2020-12-03 07:21:54,662 [Thread-149] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1344107566-172.17.0.11-1606980108837 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9/current/BP-1344107566-172.17.0.11-1606980108837/current
2020-12-03 07:21:54,672 [IPC Server handler 1 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:54,672 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:21:54,672 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:21:54,749 [Thread-171] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:54,749 [Thread-171] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11/current/BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:54,749 [Thread-171] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11 and block pool id BP-1344107566-172.17.0.11-1606980108837 is not formatted. Formatting ...
2020-12-03 07:21:54,749 [Thread-171] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1344107566-172.17.0.11-1606980108837 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11/current/BP-1344107566-172.17.0.11-1606980108837/current
2020-12-03 07:21:54,775 [IPC Server handler 9 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:54,776 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:21:54,776 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:21:54,878 [IPC Server handler 8 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:54,879 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:21:54,879 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:21:54,901 [Thread-83] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:54,901 [Thread-83] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:54,901 [Thread-83] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4 and block pool id BP-1344107566-172.17.0.11-1606980108837 is not formatted. Formatting ...
2020-12-03 07:21:54,902 [Thread-83] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1344107566-172.17.0.11-1606980108837 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1344107566-172.17.0.11-1606980108837/current
2020-12-03 07:21:54,904 [Thread-105] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:54,904 [Thread-59] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:54,904 [Thread-105] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/current/BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:54,904 [Thread-105] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6 and block pool id BP-1344107566-172.17.0.11-1606980108837 is not formatted. Formatting ...
2020-12-03 07:21:54,904 [Thread-59] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:54,904 [Thread-105] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1344107566-172.17.0.11-1606980108837 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/current/BP-1344107566-172.17.0.11-1606980108837/current
2020-12-03 07:21:54,904 [Thread-59] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2 and block pool id BP-1344107566-172.17.0.11-1606980108837 is not formatted. Formatting ...
2020-12-03 07:21:54,905 [Thread-59] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1344107566-172.17.0.11-1606980108837 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1344107566-172.17.0.11-1606980108837/current
2020-12-03 07:21:54,982 [IPC Server handler 2 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:54,983 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:21:54,984 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:21:55,009 [Thread-127] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:55,009 [Thread-127] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/current/BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:55,010 [Thread-127] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8 and block pool id BP-1344107566-172.17.0.11-1606980108837 is not formatted. Formatting ...
2020-12-03 07:21:55,010 [Thread-127] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1344107566-172.17.0.11-1606980108837 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/current/BP-1344107566-172.17.0.11-1606980108837/current
2020-12-03 07:21:55,012 [Thread-149] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:55,012 [Thread-149] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10/current/BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:55,013 [Thread-149] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10 and block pool id BP-1344107566-172.17.0.11-1606980108837 is not formatted. Formatting ...
2020-12-03 07:21:55,013 [Thread-149] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1344107566-172.17.0.11-1606980108837 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10/current/BP-1344107566-172.17.0.11-1606980108837/current
2020-12-03 07:21:55,086 [IPC Server handler 3 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:55,086 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:21:55,087 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:21:55,088 [Thread-171] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:55,088 [Thread-171] INFO  common.Storage (Storage.java:lock(882)) - Locking is disabled for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12/current/BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:55,088 [Thread-171] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12 and block pool id BP-1344107566-172.17.0.11-1606980108837 is not formatted. Formatting ...
2020-12-03 07:21:55,088 [Thread-171] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1344107566-172.17.0.11-1606980108837 directory /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12/current/BP-1344107566-172.17.0.11-1606980108837/current
2020-12-03 07:21:55,193 [IPC Server handler 0 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:55,195 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:21:55,196 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:21:55,224 [Thread-59] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=333324072;bpid=BP-1344107566-172.17.0.11-1606980108837;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=333324072;c=1606980108837;bpid=BP-1344107566-172.17.0.11-1606980108837;dnuuid=null
2020-12-03 07:21:55,224 [Thread-105] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=333324072;bpid=BP-1344107566-172.17.0.11-1606980108837;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=333324072;c=1606980108837;bpid=BP-1344107566-172.17.0.11-1606980108837;dnuuid=null
2020-12-03 07:21:55,225 [Thread-83] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=333324072;bpid=BP-1344107566-172.17.0.11-1606980108837;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=333324072;c=1606980108837;bpid=BP-1344107566-172.17.0.11-1606980108837;dnuuid=null
2020-12-03 07:21:55,298 [IPC Server handler 4 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:55,299 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:21:55,299 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:21:55,312 [Thread-149] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=333324072;bpid=BP-1344107566-172.17.0.11-1606980108837;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=333324072;c=1606980108837;bpid=BP-1344107566-172.17.0.11-1606980108837;dnuuid=null
2020-12-03 07:21:55,313 [Thread-127] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=333324072;bpid=BP-1344107566-172.17.0.11-1606980108837;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=333324072;c=1606980108837;bpid=BP-1344107566-172.17.0.11-1606980108837;dnuuid=null
2020-12-03 07:21:55,405 [IPC Server handler 5 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:55,406 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:21:55,406 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:21:55,425 [Thread-171] INFO  datanode.DataNode (DataNode.java:initStorage(1746)) - Setting up storage: nsid=333324072;bpid=BP-1344107566-172.17.0.11-1606980108837;lv=-57;nsInfo=lv=-65;cid=testClusterID;nsid=333324072;c=1606980108837;bpid=BP-1344107566-172.17.0.11-1606980108837;dnuuid=null
2020-12-03 07:21:55,508 [IPC Server handler 6 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:55,509 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:21:55,510 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:21:55,557 [Thread-83] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID 622bf17c-c047-4086-9cd5-302fd27f9de3
2020-12-03 07:21:55,557 [Thread-105] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID 0a567f53-c6f1-41b4-9ac9-d95bf8d50a27
2020-12-03 07:21:55,557 [Thread-59] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID 846ff15f-ed7d-4cf3-8112-e86de1180f07
2020-12-03 07:21:55,612 [IPC Server handler 1 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:55,613 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:21:55,613 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:21:55,656 [Thread-127] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID 98225bc9-d9e5-48c4-b0b8-a893c3451b76
2020-12-03 07:21:55,656 [Thread-149] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID e627ed49-82da-4339-b7cf-f2a6c12fcee4
2020-12-03 07:21:55,691 [Thread-83] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-058ff3c4-f186-458c-8f1b-9ea2f1373a45
2020-12-03 07:21:55,692 [Thread-83] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3, StorageType: DISK
2020-12-03 07:21:55,691 [Thread-127] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-7f1774f2-7b1d-42eb-be32-aeaf5b4b35f9
2020-12-03 07:21:55,691 [Thread-149] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-60aaafac-0dea-4e4b-8271-d136469da168
2020-12-03 07:21:55,692 [Thread-105] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-5cbe80bb-e620-48de-90dc-09c79ddc8402
2020-12-03 07:21:55,691 [Thread-59] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-5a3cdcb1-c70d-48fc-9151-fc286cea3570
2020-12-03 07:21:55,694 [Thread-105] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5, StorageType: DISK
2020-12-03 07:21:55,694 [Thread-149] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9, StorageType: DISK
2020-12-03 07:21:55,694 [Thread-127] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7, StorageType: DISK
2020-12-03 07:21:55,694 [Thread-59] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, StorageType: DISK
2020-12-03 07:21:55,696 [Thread-83] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-c7c90efb-327f-4b73-88a6-34752b5003a8
2020-12-03 07:21:55,697 [Thread-83] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, StorageType: ARCHIVE
2020-12-03 07:21:55,697 [Thread-59] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-31c88246-3f1d-4052-a932-2eefddf8a505
2020-12-03 07:21:55,697 [Thread-59] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, StorageType: ARCHIVE
2020-12-03 07:21:55,699 [Thread-127] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-fa19f5df-de0d-41f8-ae1e-004956343da7
2020-12-03 07:21:55,700 [Thread-127] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8, StorageType: ARCHIVE
2020-12-03 07:21:55,701 [Thread-171] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1546)) - Generated and persisted new Datanode UUID 5e2422dd-430a-4435-a145-829f854bf7e8
2020-12-03 07:21:55,701 [Thread-149] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-6ee6c00c-1ea1-4eca-be51-a4e9a83f8e11
2020-12-03 07:21:55,702 [Thread-149] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10, StorageType: ARCHIVE
2020-12-03 07:21:55,703 [Thread-105] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-af93b569-157d-478e-8452-49963f34fadf
2020-12-03 07:21:55,703 [Thread-105] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6, StorageType: ARCHIVE
2020-12-03 07:21:55,705 [Thread-59] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:21:55,705 [Thread-105] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:21:55,705 [Thread-171] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-6cbd1a79-1320-44b1-8284-940a3e816e05
2020-12-03 07:21:55,705 [Thread-83] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:21:55,705 [Thread-149] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:21:55,705 [Thread-127] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:21:55,705 [Thread-171] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11, StorageType: DISK
2020-12-03 07:21:55,712 [Thread-59] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:21:55,713 [Thread-171] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(304)) - Added new volume: DS-c1ddbde3-64ec-4172-87a7-93013a994b4a
2020-12-03 07:21:55,713 [Thread-171] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [ARCHIVE]file:/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12, StorageType: ARCHIVE
2020-12-03 07:21:55,715 [Thread-171] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-12-03 07:21:55,716 [Thread-127] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7
2020-12-03 07:21:55,716 [IPC Server handler 9 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:55,716 [Thread-149] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9
2020-12-03 07:21:55,717 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:21:55,717 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:21:55,717 [Thread-83] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
2020-12-03 07:21:55,718 [Thread-105] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5
2020-12-03 07:21:55,719 [Thread-171] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11
2020-12-03 07:21:55,723 [Thread-127] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7
2020-12-03 07:21:55,723 [Thread-59] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:21:55,723 [Thread-171] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11
2020-12-03 07:21:55,723 [Thread-83] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
2020-12-03 07:21:55,724 [Thread-149] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9
2020-12-03 07:21:55,724 [Thread-105] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5
2020-12-03 07:21:55,725 [Thread-127] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8
2020-12-03 07:21:55,725 [Thread-149] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10
2020-12-03 07:21:55,725 [Thread-83] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:21:55,725 [Thread-105] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6
2020-12-03 07:21:55,725 [Thread-59] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:21:55,725 [Thread-171] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12
2020-12-03 07:21:55,728 [Thread-59] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:21:55,727 [Thread-105] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6
2020-12-03 07:21:55,727 [Thread-83] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:21:55,726 [Thread-149] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10
2020-12-03 07:21:55,725 [Thread-127] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8
2020-12-03 07:21:55,729 [Thread-149] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:55,729 [Thread-105] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:55,729 [Thread-59] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:55,729 [Thread-83] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:55,728 [Thread-171] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(222)) - Scheduled health check for volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12
2020-12-03 07:21:55,729 [Thread-127] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:55,730 [Thread-171] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:55,730 [Thread-198] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3...
2020-12-03 07:21:55,730 [Thread-201] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7...
2020-12-03 07:21:55,731 [Thread-199] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9...
2020-12-03 07:21:55,731 [Thread-197] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2020-12-03 07:21:55,731 [Thread-205] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10...
2020-12-03 07:21:55,731 [Thread-204] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8...
2020-12-03 07:21:55,731 [Thread-200] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5...
2020-12-03 07:21:55,731 [Thread-203] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11...
2020-12-03 07:21:55,732 [Thread-206] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2020-12-03 07:21:55,732 [Thread-202] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4...
2020-12-03 07:21:55,732 [Thread-208] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12...
2020-12-03 07:21:55,732 [Thread-207] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(406)) - Scanning block pool BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6...
2020-12-03 07:21:55,798 [Thread-203] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1344107566-172.17.0.11-1606980108837 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11: 66ms
2020-12-03 07:21:55,798 [Thread-208] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1344107566-172.17.0.11-1606980108837 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12: 66ms
2020-12-03 07:21:55,798 [Thread-202] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1344107566-172.17.0.11-1606980108837 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4: 66ms
2020-12-03 07:21:55,798 [Thread-200] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1344107566-172.17.0.11-1606980108837 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5: 66ms
2020-12-03 07:21:55,798 [Thread-171] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1344107566-172.17.0.11-1606980108837: 68ms
2020-12-03 07:21:55,799 [Thread-199] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1344107566-172.17.0.11-1606980108837 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9: 68ms
2020-12-03 07:21:55,799 [Thread-206] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1344107566-172.17.0.11-1606980108837 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 67ms
2020-12-03 07:21:55,799 [Thread-204] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1344107566-172.17.0.11-1606980108837 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8: 68ms
2020-12-03 07:21:55,802 [Thread-221] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11...
2020-12-03 07:21:55,802 [Thread-222] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12...
2020-12-03 07:21:55,802 [Thread-221] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11/current/BP-1344107566-172.17.0.11-1606980108837/current/replicas doesn't exist 
2020-12-03 07:21:55,802 [Thread-222] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12/current/BP-1344107566-172.17.0.11-1606980108837/current/replicas doesn't exist 
2020-12-03 07:21:55,803 [Thread-201] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1344107566-172.17.0.11-1606980108837 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7: 73ms
2020-12-03 07:21:55,803 [Thread-127] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1344107566-172.17.0.11-1606980108837: 73ms
2020-12-03 07:21:55,804 [Thread-198] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1344107566-172.17.0.11-1606980108837 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3: 74ms
2020-12-03 07:21:55,804 [Thread-223] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7...
2020-12-03 07:21:55,804 [Thread-221] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11: 2ms
2020-12-03 07:21:55,804 [Thread-207] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1344107566-172.17.0.11-1606980108837 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6: 72ms
2020-12-03 07:21:55,804 [Thread-222] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12: 2ms
2020-12-03 07:21:55,804 [Thread-223] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7/current/BP-1344107566-172.17.0.11-1606980108837/current/replicas doesn't exist 
2020-12-03 07:21:55,804 [Thread-83] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1344107566-172.17.0.11-1606980108837: 74ms
2020-12-03 07:21:55,805 [Thread-224] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8...
2020-12-03 07:21:55,805 [Thread-223] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7: 1ms
2020-12-03 07:21:55,804 [Thread-205] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1344107566-172.17.0.11-1606980108837 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10: 73ms
2020-12-03 07:21:55,805 [Thread-197] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(411)) - Time taken to scan block pool BP-1344107566-172.17.0.11-1606980108837 on /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 73ms
2020-12-03 07:21:55,804 [Thread-171] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1344107566-172.17.0.11-1606980108837: 3ms
2020-12-03 07:21:55,804 [Thread-105] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1344107566-172.17.0.11-1606980108837: 74ms
2020-12-03 07:21:55,805 [Thread-59] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1344107566-172.17.0.11-1606980108837: 75ms
2020-12-03 07:21:55,805 [Thread-226] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4...
2020-12-03 07:21:55,805 [Thread-149] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(431)) - Total time to scan all replicas for block pool BP-1344107566-172.17.0.11-1606980108837: 76ms
2020-12-03 07:21:55,805 [Thread-225] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3...
2020-12-03 07:21:55,805 [Thread-224] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/current/BP-1344107566-172.17.0.11-1606980108837/current/replicas doesn't exist 
2020-12-03 07:21:55,806 [Thread-225] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1344107566-172.17.0.11-1606980108837/current/replicas doesn't exist 
2020-12-03 07:21:55,806 [Thread-230] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2020-12-03 07:21:55,806 [Thread-228] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2020-12-03 07:21:55,806 [Thread-229] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6...
2020-12-03 07:21:55,806 [Thread-227] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5...
2020-12-03 07:21:55,807 [Thread-229] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/current/BP-1344107566-172.17.0.11-1606980108837/current/replicas doesn't exist 
2020-12-03 07:21:55,806 [Thread-226] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1344107566-172.17.0.11-1606980108837/current/replicas doesn't exist 
2020-12-03 07:21:55,807 [Thread-227] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-1344107566-172.17.0.11-1606980108837/current/replicas doesn't exist 
2020-12-03 07:21:55,807 [Thread-228] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1344107566-172.17.0.11-1606980108837/current/replicas doesn't exist 
2020-12-03 07:21:55,807 [Thread-225] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3: 0ms
2020-12-03 07:21:55,807 [Thread-230] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1344107566-172.17.0.11-1606980108837/current/replicas doesn't exist 
2020-12-03 07:21:55,806 [Thread-224] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8: 1ms
2020-12-03 07:21:55,808 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11
2020-12-03 07:21:55,806 [Thread-232] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10...
2020-12-03 07:21:55,806 [Thread-231] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(199)) - Adding replicas to map for block pool BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9...
2020-12-03 07:21:55,809 [Thread-232] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10/current/BP-1344107566-172.17.0.11-1606980108837/current/replicas doesn't exist 
2020-12-03 07:21:55,808 [Thread-127] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1344107566-172.17.0.11-1606980108837: 5ms
2020-12-03 07:21:55,808 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12
2020-12-03 07:21:55,808 [Thread-230] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 2ms
2020-12-03 07:21:55,809 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7
2020-12-03 07:21:55,807 [Thread-228] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 1ms
2020-12-03 07:21:55,807 [Thread-227] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5: 0ms
2020-12-03 07:21:55,807 [Thread-226] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4: 1ms
2020-12-03 07:21:55,807 [Thread-229] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6: 0ms
2020-12-03 07:21:55,811 [Thread-83] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1344107566-172.17.0.11-1606980108837: 6ms
2020-12-03 07:21:55,811 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12, DS-c1ddbde3-64ec-4172-87a7-93013a994b4a): finished scanning block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:55,811 [Thread-59] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1344107566-172.17.0.11-1606980108837: 5ms
2020-12-03 07:21:55,811 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7, DS-7f1774f2-7b1d-42eb-be32-aeaf5b4b35f9): finished scanning block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:55,811 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11, DS-6cbd1a79-1320-44b1-8284-940a3e816e05): finished scanning block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:55,809 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8
2020-12-03 07:21:55,809 [Thread-232] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10: 1ms
2020-12-03 07:21:55,809 [Thread-231] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(876)) - Replica Cache file: /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9/current/BP-1344107566-172.17.0.11-1606980108837/current/replicas doesn't exist 
2020-12-03 07:21:55,812 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-12-03 07:21:55,811 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-12-03 07:21:55,811 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2020-12-03 07:21:55,811 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
2020-12-03 07:21:55,811 [Thread-105] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1344107566-172.17.0.11-1606980108837: 6ms
2020-12-03 07:21:55,819 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3, DS-058ff3c4-f186-458c-8f1b-9ea2f1373a45): finished scanning block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:55,821 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5
2020-12-03 07:21:55,815 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-5a3cdcb1-c70d-48fc-9151-fc286cea3570): finished scanning block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:55,821 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5, DS-5cbe80bb-e620-48de-90dc-09c79ddc8402): finished scanning block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:55,815 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-31c88246-3f1d-4052-a932-2eefddf8a505): finished scanning block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:55,813 [Thread-231] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(204)) - Time to add replicas to map for block pool BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9: 5ms
2020-12-03 07:21:55,822 [Thread-149] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(225)) - Total time to add all replicas to map for block pool BP-1344107566-172.17.0.11-1606980108837: 16ms
2020-12-03 07:21:55,812 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8, DS-fa19f5df-de0d-41f8-ae1e-004956343da7): finished scanning block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:55,821 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6
2020-12-03 07:21:55,821 [IPC Server handler 8 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:55,823 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6, DS-af93b569-157d-478e-8452-49963f34fadf): finished scanning block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:55,821 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, DS-c7c90efb-327f-4b73-88a6-34752b5003a8): finished scanning block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:55,823 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10
2020-12-03 07:21:55,823 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(381)) - Now scanning bpid BP-1344107566-172.17.0.11-1606980108837 on volume /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9
2020-12-03 07:21:55,823 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2782)) - dnInfo.length != numDataNodes
2020-12-03 07:21:55,824 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2703)) - Waiting for cluster to become active
2020-12-03 07:21:55,824 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9, DS-60aaafac-0dea-4e4b-8271-d136469da168): finished scanning block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:55,824 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(539)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10, DS-6ee6c00c-1ea1-4eca-be51-a4e9a83f8e11): finished scanning block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:55,838 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8, DS-fa19f5df-de0d-41f8-ae1e-004956343da7): no suitable block pools found to scan.  Waiting 1814399971 ms.
2020-12-03 07:21:55,839 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6, DS-af93b569-157d-478e-8452-49963f34fadf): no suitable block pools found to scan.  Waiting 1814399982 ms.
2020-12-03 07:21:55,839 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-5a3cdcb1-c70d-48fc-9151-fc286cea3570): no suitable block pools found to scan.  Waiting 1814399972 ms.
2020-12-03 07:21:55,839 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, DS-c7c90efb-327f-4b73-88a6-34752b5003a8): no suitable block pools found to scan.  Waiting 1814399972 ms.
2020-12-03 07:21:55,839 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3, DS-058ff3c4-f186-458c-8f1b-9ea2f1373a45): no suitable block pools found to scan.  Waiting 1814399972 ms.
2020-12-03 07:21:55,839 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11, DS-6cbd1a79-1320-44b1-8284-940a3e816e05): no suitable block pools found to scan.  Waiting 1814399968 ms.
2020-12-03 07:21:55,839 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5, DS-5cbe80bb-e620-48de-90dc-09c79ddc8402): no suitable block pools found to scan.  Waiting 1814399982 ms.
2020-12-03 07:21:55,838 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10, DS-6ee6c00c-1ea1-4eca-be51-a4e9a83f8e11): no suitable block pools found to scan.  Waiting 1814399985 ms.
2020-12-03 07:21:55,838 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-31c88246-3f1d-4052-a932-2eefddf8a505): no suitable block pools found to scan.  Waiting 1814399973 ms.
2020-12-03 07:21:55,838 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12, DS-c1ddbde3-64ec-4172-87a7-93013a994b4a): no suitable block pools found to scan.  Waiting 1814399970 ms.
2020-12-03 07:21:55,839 [Thread-83] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 7:47 AM with interval of 21600000ms
2020-12-03 07:21:55,839 [Thread-149] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 12:30 PM with interval of 21600000ms
2020-12-03 07:21:55,839 [Thread-127] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 7:25 AM with interval of 21600000ms
2020-12-03 07:21:55,839 [Thread-105] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 12:00 PM with interval of 21600000ms
2020-12-03 07:21:55,839 [Thread-59] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 9:17 AM with interval of 21600000ms
2020-12-03 07:21:55,839 [Thread-171] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(284)) - Periodic Directory Tree Verification scan starting at 12/3/20 10:00 AM with interval of 21600000ms
2020-12-03 07:21:55,839 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7, DS-7f1774f2-7b1d-42eb-be32-aeaf5b4b35f9): no suitable block pools found to scan.  Waiting 1814399970 ms.
2020-12-03 07:21:55,839 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(398)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9, DS-60aaafac-0dea-4e4b-8271-d136469da168): no suitable block pools found to scan.  Waiting 1814399984 ms.
2020-12-03 07:21:55,848 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1344107566-172.17.0.11-1606980108837 (Datanode Uuid 98225bc9-d9e5-48c4-b0b8-a893c3451b76) service to localhost/127.0.0.1:44708 beginning handshake with NN
2020-12-03 07:21:55,848 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1344107566-172.17.0.11-1606980108837 (Datanode Uuid e627ed49-82da-4339-b7cf-f2a6c12fcee4) service to localhost/127.0.0.1:44708 beginning handshake with NN
2020-12-03 07:21:55,848 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1344107566-172.17.0.11-1606980108837 (Datanode Uuid 5e2422dd-430a-4435-a145-829f854bf7e8) service to localhost/127.0.0.1:44708 beginning handshake with NN
2020-12-03 07:21:55,848 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1344107566-172.17.0.11-1606980108837 (Datanode Uuid 846ff15f-ed7d-4cf3-8112-e86de1180f07) service to localhost/127.0.0.1:44708 beginning handshake with NN
2020-12-03 07:21:55,848 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1344107566-172.17.0.11-1606980108837 (Datanode Uuid 0a567f53-c6f1-41b4-9ac9-d95bf8d50a27) service to localhost/127.0.0.1:44708 beginning handshake with NN
2020-12-03 07:21:55,848 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:register(767)) - Block pool BP-1344107566-172.17.0.11-1606980108837 (Datanode Uuid 622bf17c-c047-4086-9cd5-302fd27f9de3) service to localhost/127.0.0.1:44708 beginning handshake with NN
2020-12-03 07:21:55,861 [IPC Server handler 3 on default port 44708] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:42991, datanodeUuid=98225bc9-d9e5-48c4-b0b8-a893c3451b76, infoPort=37554, infoSecurePort=0, ipcPort=43194, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837) storage 98225bc9-d9e5-48c4-b0b8-a893c3451b76
2020-12-03 07:21:55,864 [IPC Server handler 3 on default port 44708] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:42991
2020-12-03 07:21:55,864 [IPC Server handler 3 on default port 44708] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 98225bc9-d9e5-48c4-b0b8-a893c3451b76 (127.0.0.1:42991).
2020-12-03 07:21:55,866 [IPC Server handler 4 on default port 44708] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:46034, datanodeUuid=5e2422dd-430a-4435-a145-829f854bf7e8, infoPort=39776, infoSecurePort=0, ipcPort=41652, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837) storage 5e2422dd-430a-4435-a145-829f854bf7e8
2020-12-03 07:21:55,867 [IPC Server handler 4 on default port 44708] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:46034
2020-12-03 07:21:55,867 [IPC Server handler 4 on default port 44708] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 5e2422dd-430a-4435-a145-829f854bf7e8 (127.0.0.1:46034).
2020-12-03 07:21:55,867 [IPC Server handler 0 on default port 44708] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:44891, datanodeUuid=e627ed49-82da-4339-b7cf-f2a6c12fcee4, infoPort=39970, infoSecurePort=0, ipcPort=35404, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837) storage e627ed49-82da-4339-b7cf-f2a6c12fcee4
2020-12-03 07:21:55,867 [IPC Server handler 0 on default port 44708] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:44891
2020-12-03 07:21:55,867 [IPC Server handler 0 on default port 44708] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN e627ed49-82da-4339-b7cf-f2a6c12fcee4 (127.0.0.1:44891).
2020-12-03 07:21:55,868 [IPC Server handler 2 on default port 44708] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:33403, datanodeUuid=0a567f53-c6f1-41b4-9ac9-d95bf8d50a27, infoPort=41687, infoSecurePort=0, ipcPort=42709, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837) storage 0a567f53-c6f1-41b4-9ac9-d95bf8d50a27
2020-12-03 07:21:55,868 [IPC Server handler 2 on default port 44708] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:33403
2020-12-03 07:21:55,868 [IPC Server handler 2 on default port 44708] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 0a567f53-c6f1-41b4-9ac9-d95bf8d50a27 (127.0.0.1:33403).
2020-12-03 07:21:55,868 [IPC Server handler 5 on default port 44708] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:33340, datanodeUuid=846ff15f-ed7d-4cf3-8112-e86de1180f07, infoPort=45658, infoSecurePort=0, ipcPort=38790, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837) storage 846ff15f-ed7d-4cf3-8112-e86de1180f07
2020-12-03 07:21:55,869 [IPC Server handler 5 on default port 44708] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:33340
2020-12-03 07:21:55,869 [IPC Server handler 5 on default port 44708] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 846ff15f-ed7d-4cf3-8112-e86de1180f07 (127.0.0.1:33340).
2020-12-03 07:21:55,869 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1344107566-172.17.0.11-1606980108837 (Datanode Uuid 0a567f53-c6f1-41b4-9ac9-d95bf8d50a27) service to localhost/127.0.0.1:44708 successfully registered with NN
2020-12-03 07:21:55,869 [IPC Server handler 6 on default port 44708] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1042)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:44597, datanodeUuid=622bf17c-c047-4086-9cd5-302fd27f9de3, infoPort=39415, infoSecurePort=0, ipcPort=35175, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837) storage 622bf17c-c047-4086-9cd5-302fd27f9de3
2020-12-03 07:21:55,869 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:44708 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=1000
2020-12-03 07:21:55,869 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1344107566-172.17.0.11-1606980108837 (Datanode Uuid 98225bc9-d9e5-48c4-b0b8-a893c3451b76) service to localhost/127.0.0.1:44708 successfully registered with NN
2020-12-03 07:21:55,870 [IPC Server handler 6 on default port 44708] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:44597
2020-12-03 07:21:55,870 [IPC Server handler 6 on default port 44708] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 622bf17c-c047-4086-9cd5-302fd27f9de3 (127.0.0.1:44597).
2020-12-03 07:21:55,870 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1344107566-172.17.0.11-1606980108837 (Datanode Uuid 5e2422dd-430a-4435-a145-829f854bf7e8) service to localhost/127.0.0.1:44708 successfully registered with NN
2020-12-03 07:21:55,870 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:44708 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=1000
2020-12-03 07:21:55,871 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:44708 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=1000
2020-12-03 07:21:55,871 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1344107566-172.17.0.11-1606980108837 (Datanode Uuid 622bf17c-c047-4086-9cd5-302fd27f9de3) service to localhost/127.0.0.1:44708 successfully registered with NN
2020-12-03 07:21:55,871 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:44708 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=1000
2020-12-03 07:21:55,870 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1344107566-172.17.0.11-1606980108837 (Datanode Uuid e627ed49-82da-4339-b7cf-f2a6c12fcee4) service to localhost/127.0.0.1:44708 successfully registered with NN
2020-12-03 07:21:55,870 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:register(786)) - Block pool Block pool BP-1344107566-172.17.0.11-1606980108837 (Datanode Uuid 846ff15f-ed7d-4cf3-8112-e86de1180f07) service to localhost/127.0.0.1:44708 successfully registered with NN
2020-12-03 07:21:55,872 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:44708 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=1000
2020-12-03 07:21:55,873 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:offerService(616)) - For namenode localhost/127.0.0.1:44708 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=1000
2020-12-03 07:21:55,892 [IPC Server handler 1 on default port 44708] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-60aaafac-0dea-4e4b-8271-d136469da168 for DN 127.0.0.1:44891
2020-12-03 07:21:55,893 [IPC Server handler 1 on default port 44708] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-6ee6c00c-1ea1-4eca-be51-a4e9a83f8e11 for DN 127.0.0.1:44891
2020-12-03 07:21:55,894 [IPC Server handler 2 on default port 44708] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-058ff3c4-f186-458c-8f1b-9ea2f1373a45 for DN 127.0.0.1:44597
2020-12-03 07:21:55,895 [IPC Server handler 2 on default port 44708] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-c7c90efb-327f-4b73-88a6-34752b5003a8 for DN 127.0.0.1:44597
2020-12-03 07:21:55,895 [IPC Server handler 3 on default port 44708] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-5a3cdcb1-c70d-48fc-9151-fc286cea3570 for DN 127.0.0.1:33340
2020-12-03 07:21:55,896 [IPC Server handler 3 on default port 44708] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-31c88246-3f1d-4052-a932-2eefddf8a505 for DN 127.0.0.1:33340
2020-12-03 07:21:55,896 [IPC Server handler 9 on default port 44708] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-6cbd1a79-1320-44b1-8284-940a3e816e05 for DN 127.0.0.1:46034
2020-12-03 07:21:55,897 [IPC Server handler 9 on default port 44708] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-c1ddbde3-64ec-4172-87a7-93013a994b4a for DN 127.0.0.1:46034
2020-12-03 07:21:55,897 [IPC Server handler 8 on default port 44708] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-5cbe80bb-e620-48de-90dc-09c79ddc8402 for DN 127.0.0.1:33403
2020-12-03 07:21:55,898 [IPC Server handler 8 on default port 44708] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-af93b569-157d-478e-8452-49963f34fadf for DN 127.0.0.1:33403
2020-12-03 07:21:55,898 [IPC Server handler 7 on default port 44708] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-7f1774f2-7b1d-42eb-be32-aeaf5b4b35f9 for DN 127.0.0.1:42991
2020-12-03 07:21:55,898 [IPC Server handler 7 on default port 44708] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-fa19f5df-de0d-41f8-ae1e-004956343da7 for DN 127.0.0.1:42991
2020-12-03 07:21:55,932 [IPC Server handler 7 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:55,933 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x7e2a985dacd78897: Processing first storage report for DS-058ff3c4-f186-458c-8f1b-9ea2f1373a45 from datanode 622bf17c-c047-4086-9cd5-302fd27f9de3
2020-12-03 07:21:55,935 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x7e2a985dacd78897: from storage DS-058ff3c4-f186-458c-8f1b-9ea2f1373a45 node DatanodeRegistration(127.0.0.1:44597, datanodeUuid=622bf17c-c047-4086-9cd5-302fd27f9de3, infoPort=39415, infoSecurePort=0, ipcPort=35175, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 0, hasStaleStorage: true, processing time: 2 msecs, invalidatedBlocks: 0
2020-12-03 07:21:55,935 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x5f71e6a65a67834b: Processing first storage report for DS-fa19f5df-de0d-41f8-ae1e-004956343da7 from datanode 98225bc9-d9e5-48c4-b0b8-a893c3451b76
2020-12-03 07:21:55,935 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x5f71e6a65a67834b: from storage DS-fa19f5df-de0d-41f8-ae1e-004956343da7 node DatanodeRegistration(127.0.0.1:42991, datanodeUuid=98225bc9-d9e5-48c4-b0b8-a893c3451b76, infoPort=37554, infoSecurePort=0, ipcPort=43194, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:21:55,935 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x452794627e9ce441: Processing first storage report for DS-5a3cdcb1-c70d-48fc-9151-fc286cea3570 from datanode 846ff15f-ed7d-4cf3-8112-e86de1180f07
2020-12-03 07:21:55,935 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x452794627e9ce441: from storage DS-5a3cdcb1-c70d-48fc-9151-fc286cea3570 node DatanodeRegistration(127.0.0.1:33340, datanodeUuid=846ff15f-ed7d-4cf3-8112-e86de1180f07, infoPort=45658, infoSecurePort=0, ipcPort=38790, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:21:55,936 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x21f01db7ce99a5d5: Processing first storage report for DS-6cbd1a79-1320-44b1-8284-940a3e816e05 from datanode 5e2422dd-430a-4435-a145-829f854bf7e8
2020-12-03 07:21:55,936 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x21f01db7ce99a5d5: from storage DS-6cbd1a79-1320-44b1-8284-940a3e816e05 node DatanodeRegistration(127.0.0.1:46034, datanodeUuid=5e2422dd-430a-4435-a145-829f854bf7e8, infoPort=39776, infoSecurePort=0, ipcPort=41652, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:21:55,936 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x13bb4922ba959792: Processing first storage report for DS-5cbe80bb-e620-48de-90dc-09c79ddc8402 from datanode 0a567f53-c6f1-41b4-9ac9-d95bf8d50a27
2020-12-03 07:21:55,936 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x13bb4922ba959792: from storage DS-5cbe80bb-e620-48de-90dc-09c79ddc8402 node DatanodeRegistration(127.0.0.1:33403, datanodeUuid=0a567f53-c6f1-41b4-9ac9-d95bf8d50a27, infoPort=41687, infoSecurePort=0, ipcPort=42709, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:21:55,936 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x15cc6f9b64a99494: Processing first storage report for DS-60aaafac-0dea-4e4b-8271-d136469da168 from datanode e627ed49-82da-4339-b7cf-f2a6c12fcee4
2020-12-03 07:21:55,936 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x15cc6f9b64a99494: from storage DS-60aaafac-0dea-4e4b-8271-d136469da168 node DatanodeRegistration(127.0.0.1:44891, datanodeUuid=e627ed49-82da-4339-b7cf-f2a6c12fcee4, infoPort=39970, infoSecurePort=0, ipcPort=35404, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:21:55,936 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x7e2a985dacd78897: Processing first storage report for DS-c7c90efb-327f-4b73-88a6-34752b5003a8 from datanode 622bf17c-c047-4086-9cd5-302fd27f9de3
2020-12-03 07:21:55,937 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x7e2a985dacd78897: from storage DS-c7c90efb-327f-4b73-88a6-34752b5003a8 node DatanodeRegistration(127.0.0.1:44597, datanodeUuid=622bf17c-c047-4086-9cd5-302fd27f9de3, infoPort=39415, infoSecurePort=0, ipcPort=35175, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:21:55,937 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x5f71e6a65a67834b: Processing first storage report for DS-7f1774f2-7b1d-42eb-be32-aeaf5b4b35f9 from datanode 98225bc9-d9e5-48c4-b0b8-a893c3451b76
2020-12-03 07:21:55,937 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x5f71e6a65a67834b: from storage DS-7f1774f2-7b1d-42eb-be32-aeaf5b4b35f9 node DatanodeRegistration(127.0.0.1:42991, datanodeUuid=98225bc9-d9e5-48c4-b0b8-a893c3451b76, infoPort=37554, infoSecurePort=0, ipcPort=43194, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:21:55,937 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x452794627e9ce441: Processing first storage report for DS-31c88246-3f1d-4052-a932-2eefddf8a505 from datanode 846ff15f-ed7d-4cf3-8112-e86de1180f07
2020-12-03 07:21:55,937 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x452794627e9ce441: from storage DS-31c88246-3f1d-4052-a932-2eefddf8a505 node DatanodeRegistration(127.0.0.1:33340, datanodeUuid=846ff15f-ed7d-4cf3-8112-e86de1180f07, infoPort=45658, infoSecurePort=0, ipcPort=38790, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:21:55,937 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x21f01db7ce99a5d5: Processing first storage report for DS-c1ddbde3-64ec-4172-87a7-93013a994b4a from datanode 5e2422dd-430a-4435-a145-829f854bf7e8
2020-12-03 07:21:55,937 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x21f01db7ce99a5d5: from storage DS-c1ddbde3-64ec-4172-87a7-93013a994b4a node DatanodeRegistration(127.0.0.1:46034, datanodeUuid=5e2422dd-430a-4435-a145-829f854bf7e8, infoPort=39776, infoSecurePort=0, ipcPort=41652, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:21:55,937 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x13bb4922ba959792: Processing first storage report for DS-af93b569-157d-478e-8452-49963f34fadf from datanode 0a567f53-c6f1-41b4-9ac9-d95bf8d50a27
2020-12-03 07:21:55,938 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x13bb4922ba959792: from storage DS-af93b569-157d-478e-8452-49963f34fadf node DatanodeRegistration(127.0.0.1:33403, datanodeUuid=0a567f53-c6f1-41b4-9ac9-d95bf8d50a27, infoPort=41687, infoSecurePort=0, ipcPort=42709, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:21:55,938 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2648)) - BLOCK* processReport 0x15cc6f9b64a99494: Processing first storage report for DS-6ee6c00c-1ea1-4eca-be51-a4e9a83f8e11 from datanode e627ed49-82da-4339-b7cf-f2a6c12fcee4
2020-12-03 07:21:55,938 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x15cc6f9b64a99494: from storage DS-6ee6c00c-1ea1-4eca-be51-a4e9a83f8e11 node DatanodeRegistration(127.0.0.1:44891, datanodeUuid=e627ed49-82da-4339-b7cf-f2a6c12fcee4, infoPort=39970, infoSecurePort=0, ipcPort=35404, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:21:55,943 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2758)) - Cluster is active
2020-12-03 07:21:55,953 [IPC Server handler 2 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:55,954 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2758)) - Cluster is active
2020-12-03 07:21:55,958 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x13bb4922ba959792,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 44 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:21:55,958 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x15cc6f9b64a99494,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 44 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:21:55,958 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x21f01db7ce99a5d5,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 44 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:21:55,959 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x452794627e9ce441,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 46 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:21:55,958 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x7e2a985dacd78897,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 44 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:21:55,961 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:55,961 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:55,961 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:55,961 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:55,961 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x5f71e6a65a67834b,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 46 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:21:55,961 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:55,962 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:55,980 [IPC Server handler 8 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/hot	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-12-03 07:21:55,999 [IPC Server handler 1 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/warm	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-12-03 07:21:56,001 [IPC Server handler 7 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/cold	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-12-03 07:21:56,006 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:offerService(698)) - Forcing a full block report to localhost/127.0.0.1:44708
2020-12-03 07:21:56,007 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x452794627e9ce442: from storage DS-5a3cdcb1-c70d-48fc-9151-fc286cea3570 node DatanodeRegistration(127.0.0.1:33340, datanodeUuid=846ff15f-ed7d-4cf3-8112-e86de1180f07, infoPort=45658, infoSecurePort=0, ipcPort=38790, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:21:56,008 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x452794627e9ce442: from storage DS-31c88246-3f1d-4052-a932-2eefddf8a505 node DatanodeRegistration(127.0.0.1:33340, datanodeUuid=846ff15f-ed7d-4cf3-8112-e86de1180f07, infoPort=45658, infoSecurePort=0, ipcPort=38790, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 0, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
2020-12-03 07:21:56,008 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x452794627e9ce442,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:21:56,008 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:56,107 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:offerService(698)) - Forcing a full block report to localhost/127.0.0.1:44708
2020-12-03 07:21:56,108 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x7e2a985dacd78898: from storage DS-058ff3c4-f186-458c-8f1b-9ea2f1373a45 node DatanodeRegistration(127.0.0.1:44597, datanodeUuid=622bf17c-c047-4086-9cd5-302fd27f9de3, infoPort=39415, infoSecurePort=0, ipcPort=35175, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:21:56,109 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x7e2a985dacd78898: from storage DS-c7c90efb-327f-4b73-88a6-34752b5003a8 node DatanodeRegistration(127.0.0.1:44597, datanodeUuid=622bf17c-c047-4086-9cd5-302fd27f9de3, infoPort=39415, infoSecurePort=0, ipcPort=35175, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:21:56,110 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x7e2a985dacd78898,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:21:56,110 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:56,207 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:offerService(698)) - Forcing a full block report to localhost/127.0.0.1:44708
2020-12-03 07:21:56,208 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x13bb4922ba959793: from storage DS-5cbe80bb-e620-48de-90dc-09c79ddc8402 node DatanodeRegistration(127.0.0.1:33403, datanodeUuid=0a567f53-c6f1-41b4-9ac9-d95bf8d50a27, infoPort=41687, infoSecurePort=0, ipcPort=42709, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:21:56,208 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x13bb4922ba959793: from storage DS-af93b569-157d-478e-8452-49963f34fadf node DatanodeRegistration(127.0.0.1:33403, datanodeUuid=0a567f53-c6f1-41b4-9ac9-d95bf8d50a27, infoPort=41687, infoSecurePort=0, ipcPort=42709, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:21:56,209 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x13bb4922ba959793,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:21:56,209 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:56,307 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:offerService(698)) - Forcing a full block report to localhost/127.0.0.1:44708
2020-12-03 07:21:56,308 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x5f71e6a65a67834c: from storage DS-fa19f5df-de0d-41f8-ae1e-004956343da7 node DatanodeRegistration(127.0.0.1:42991, datanodeUuid=98225bc9-d9e5-48c4-b0b8-a893c3451b76, infoPort=37554, infoSecurePort=0, ipcPort=43194, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:21:56,309 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x5f71e6a65a67834c: from storage DS-7f1774f2-7b1d-42eb-be32-aeaf5b4b35f9 node DatanodeRegistration(127.0.0.1:42991, datanodeUuid=98225bc9-d9e5-48c4-b0b8-a893c3451b76, infoPort=37554, infoSecurePort=0, ipcPort=43194, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:21:56,309 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x5f71e6a65a67834c,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:21:56,309 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:56,405 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:offerService(698)) - Forcing a full block report to localhost/127.0.0.1:44708
2020-12-03 07:21:56,407 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x15cc6f9b64a99495: from storage DS-60aaafac-0dea-4e4b-8271-d136469da168 node DatanodeRegistration(127.0.0.1:44891, datanodeUuid=e627ed49-82da-4339-b7cf-f2a6c12fcee4, infoPort=39970, infoSecurePort=0, ipcPort=35404, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:21:56,407 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x15cc6f9b64a99495: from storage DS-6ee6c00c-1ea1-4eca-be51-a4e9a83f8e11 node DatanodeRegistration(127.0.0.1:44891, datanodeUuid=e627ed49-82da-4339-b7cf-f2a6c12fcee4, infoPort=39970, infoSecurePort=0, ipcPort=35404, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:21:56,408 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x15cc6f9b64a99495,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 1 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:21:56,408 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:56,507 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:offerService(698)) - Forcing a full block report to localhost/127.0.0.1:44708
2020-12-03 07:21:56,508 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x21f01db7ce99a5d6: from storage DS-6cbd1a79-1320-44b1-8284-940a3e816e05 node DatanodeRegistration(127.0.0.1:46034, datanodeUuid=5e2422dd-430a-4435-a145-829f854bf7e8, infoPort=39776, infoSecurePort=0, ipcPort=41652, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:21:56,509 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x21f01db7ce99a5d6: from storage DS-c1ddbde3-64ec-4172-87a7-93013a994b4a node DatanodeRegistration(127.0.0.1:46034, datanodeUuid=5e2422dd-430a-4435-a145-829f854bf7e8, infoPort=39776, infoSecurePort=0, ipcPort=41652, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 0, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
2020-12-03 07:21:56,509 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x21f01db7ce99a5d6,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:21:56,509 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:21:56,614 [IPC Server handler 3 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/	dst=null	perm=null	proto=rpc
2020-12-03 07:21:56,623 [IPC Server handler 9 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/	dst=null	perm=null	proto=rpc
2020-12-03 07:21:56,628 [IPC Server handler 5 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/cold	dst=null	perm=null	proto=rpc
2020-12-03 07:21:56,629 [IPC Server handler 4 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/hot	dst=null	perm=null	proto=rpc
2020-12-03 07:21:56,630 [IPC Server handler 0 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/warm	dst=null	perm=null	proto=rpc
2020-12-03 07:21:56,640 [IPC Server handler 8 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setStoragePolicy	src=/cold	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-12-03 07:21:56,642 [IPC Server handler 1 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setStoragePolicy	src=/hot	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-12-03 07:21:56,644 [IPC Server handler 7 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setStoragePolicy	src=/warm	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-12-03 07:21:56,647 [Listener at localhost/41652] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:21:56,648 [Listener at localhost/41652] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.namenode.redundancy.interval.seconds(2) assuming SECONDS
2020-12-03 07:21:56,648 [Listener at localhost/41652] INFO  mover.Mover (Mover.java:run(642)) - namenodes = {hdfs://localhost:44708=null}
2020-12-03 07:21:56,714 [IPC Server handler 9 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/system/mover.id	dst=null	perm=null	proto=rpc
2020-12-03 07:21:56,743 [IPC Server handler 5 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/system/mover.id	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2020-12-03 07:21:56,766 [IPC Server handler 4 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/system/mover.id	dst=null	perm=null	proto=rpc
2020-12-03 07:21:56,769 [Listener at localhost/41652] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:21:56,769 [Listener at localhost/41652] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:21:56,784 [IPC Server handler 8 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getDatanodeStorageReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:56,794 [Listener at localhost/41652] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:33340
2020-12-03 07:21:56,794 [Listener at localhost/41652] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:33403
2020-12-03 07:21:56,795 [Listener at localhost/41652] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:42991
2020-12-03 07:21:56,795 [Listener at localhost/41652] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:44597
2020-12-03 07:21:56,795 [Listener at localhost/41652] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:44891
2020-12-03 07:21:56,795 [Listener at localhost/41652] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:46034
2020-12-03 07:21:56,803 [IPC Server handler 1 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=listSnapshottableDirectory	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:21:56,806 [IPC Server handler 7 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/	dst=null	perm=null	proto=rpc
2020-12-03 07:21:56,808 [IPC Server handler 2 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/cold/	dst=null	perm=null	proto=rpc
2020-12-03 07:21:56,809 [IPC Server handler 6 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/hot/	dst=null	perm=null	proto=rpc
2020-12-03 07:21:56,811 [IPC Server handler 3 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/system/	dst=null	perm=null	proto=rpc
2020-12-03 07:21:56,815 [IPC Server handler 9 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/warm/	dst=null	perm=null	proto=rpc
2020-12-03 07:21:56,821 [IPC Server handler 5 on default port 44708] INFO  hdfs.StateChange (FSNamesystem.java:completeFile(2948)) - DIR* completeFile: /system/mover.id is closed by DFSClient_NONMAPREDUCE_1921281077_1
2020-12-03 07:21:56,833 [IPC Server handler 4 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=delete	src=/system/mover.id	dst=null	perm=null	proto=rpc
Mover Successful: all blocks satisfy the specified storage policy. Exiting...
2020-12-03 07:22:05,839 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:offerService(698)) - Forcing a full block report to localhost/127.0.0.1:44708
2020-12-03 07:22:05,841 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x452794627e9ce443: from storage DS-5a3cdcb1-c70d-48fc-9151-fc286cea3570 node DatanodeRegistration(127.0.0.1:33340, datanodeUuid=846ff15f-ed7d-4cf3-8112-e86de1180f07, infoPort=45658, infoSecurePort=0, ipcPort=38790, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:22:05,842 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x452794627e9ce443: from storage DS-31c88246-3f1d-4052-a932-2eefddf8a505 node DatanodeRegistration(127.0.0.1:33340, datanodeUuid=846ff15f-ed7d-4cf3-8112-e86de1180f07, infoPort=45658, infoSecurePort=0, ipcPort=38790, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:22:05,842 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x452794627e9ce443,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:22:05,843 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:22:05,939 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:offerService(698)) - Forcing a full block report to localhost/127.0.0.1:44708
2020-12-03 07:22:05,941 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x7e2a985dacd78899: from storage DS-058ff3c4-f186-458c-8f1b-9ea2f1373a45 node DatanodeRegistration(127.0.0.1:44597, datanodeUuid=622bf17c-c047-4086-9cd5-302fd27f9de3, infoPort=39415, infoSecurePort=0, ipcPort=35175, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:22:05,941 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x7e2a985dacd78899: from storage DS-c7c90efb-327f-4b73-88a6-34752b5003a8 node DatanodeRegistration(127.0.0.1:44597, datanodeUuid=622bf17c-c047-4086-9cd5-302fd27f9de3, infoPort=39415, infoSecurePort=0, ipcPort=35175, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:22:05,941 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x7e2a985dacd78899,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:22:05,942 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:22:06,042 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:offerService(698)) - Forcing a full block report to localhost/127.0.0.1:44708
2020-12-03 07:22:06,044 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x13bb4922ba959794: from storage DS-5cbe80bb-e620-48de-90dc-09c79ddc8402 node DatanodeRegistration(127.0.0.1:33403, datanodeUuid=0a567f53-c6f1-41b4-9ac9-d95bf8d50a27, infoPort=41687, infoSecurePort=0, ipcPort=42709, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:22:06,044 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x13bb4922ba959794: from storage DS-af93b569-157d-478e-8452-49963f34fadf node DatanodeRegistration(127.0.0.1:33403, datanodeUuid=0a567f53-c6f1-41b4-9ac9-d95bf8d50a27, infoPort=41687, infoSecurePort=0, ipcPort=42709, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:22:06,045 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x13bb4922ba959794,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 3 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:22:06,045 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:22:06,140 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:offerService(698)) - Forcing a full block report to localhost/127.0.0.1:44708
2020-12-03 07:22:06,142 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x5f71e6a65a67834d: from storage DS-fa19f5df-de0d-41f8-ae1e-004956343da7 node DatanodeRegistration(127.0.0.1:42991, datanodeUuid=98225bc9-d9e5-48c4-b0b8-a893c3451b76, infoPort=37554, infoSecurePort=0, ipcPort=43194, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:22:06,142 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x5f71e6a65a67834d: from storage DS-7f1774f2-7b1d-42eb-be32-aeaf5b4b35f9 node DatanodeRegistration(127.0.0.1:42991, datanodeUuid=98225bc9-d9e5-48c4-b0b8-a893c3451b76, infoPort=37554, infoSecurePort=0, ipcPort=43194, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:22:06,145 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x5f71e6a65a67834d,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 4 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:22:06,145 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:22:06,239 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:offerService(698)) - Forcing a full block report to localhost/127.0.0.1:44708
2020-12-03 07:22:06,241 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x15cc6f9b64a99496: from storage DS-60aaafac-0dea-4e4b-8271-d136469da168 node DatanodeRegistration(127.0.0.1:44891, datanodeUuid=e627ed49-82da-4339-b7cf-f2a6c12fcee4, infoPort=39970, infoSecurePort=0, ipcPort=35404, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 0, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
2020-12-03 07:22:06,241 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x15cc6f9b64a99496: from storage DS-6ee6c00c-1ea1-4eca-be51-a4e9a83f8e11 node DatanodeRegistration(127.0.0.1:44891, datanodeUuid=e627ed49-82da-4339-b7cf-f2a6c12fcee4, infoPort=39970, infoSecurePort=0, ipcPort=35404, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:22:06,242 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x15cc6f9b64a99496,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 1 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:22:06,242 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:22:06,340 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:offerService(698)) - Forcing a full block report to localhost/127.0.0.1:44708
2020-12-03 07:22:06,342 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x21f01db7ce99a5d7: from storage DS-6cbd1a79-1320-44b1-8284-940a3e816e05 node DatanodeRegistration(127.0.0.1:46034, datanodeUuid=5e2422dd-430a-4435-a145-829f854bf7e8, infoPort=39776, infoSecurePort=0, ipcPort=41652, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:22:06,342 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x21f01db7ce99a5d7: from storage DS-c1ddbde3-64ec-4172-87a7-93013a994b4a node DatanodeRegistration(127.0.0.1:46034, datanodeUuid=5e2422dd-430a-4435-a145-829f854bf7e8, infoPort=39776, infoSecurePort=0, ipcPort=41652, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:22:06,343 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x21f01db7ce99a5d7,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 3 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:22:06,343 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:22:06,439 [IPC Server handler 1 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/	dst=null	perm=null	proto=rpc
2020-12-03 07:22:06,441 [IPC Server handler 7 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/	dst=null	perm=null	proto=rpc
2020-12-03 07:22:06,443 [IPC Server handler 2 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/cold	dst=null	perm=null	proto=rpc
2020-12-03 07:22:06,444 [IPC Server handler 6 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/hot	dst=null	perm=null	proto=rpc
2020-12-03 07:22:06,446 [IPC Server handler 3 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/system	dst=null	perm=null	proto=rpc
2020-12-03 07:22:06,447 [IPC Server handler 9 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/warm	dst=null	perm=null	proto=rpc
2020-12-03 07:22:06,448 [IPC Server handler 5 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/cold	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-12-03 07:22:06,452 [IPC Server handler 4 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/cold/file0	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2020-12-03 07:22:06,467 [IPC Server handler 0 on default port 44708] TRACE blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(427)) - storageTypes={ARCHIVE=3}
2020-12-03 07:22:06,468 [IPC Server handler 0 on default port 44708] DEBUG blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseRemoteRack(719)) - Failed to choose remote rack (location = ~/default-rack), fallback to local rack
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy$NotEnoughReplicasException: 
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:852)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRemoteRack(BlockPlacementPolicyDefault.java:714)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTargetInOrder(BlockPlacementPolicyDefault.java:519)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:437)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:309)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:149)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:174)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2211)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2789)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:892)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:574)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)
2020-12-03 07:22:06,471 [IPC Server handler 0 on default port 44708] DEBUG blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseRemoteRack(719)) - Failed to choose remote rack (location = ~/default-rack), fallback to local rack
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy$NotEnoughReplicasException: 
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:852)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRemoteRack(BlockPlacementPolicyDefault.java:714)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTargetInOrder(BlockPlacementPolicyDefault.java:528)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:437)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:309)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:149)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:174)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2211)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2789)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:892)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:574)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)
2020-12-03 07:22:06,476 [IPC Server handler 0 on default port 44708] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_1073741825_1001, replicas=127.0.0.1:33340, 127.0.0.1:44891, 127.0.0.1:33403 for /cold/file0
2020-12-03 07:22:06,494 [Thread-254] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:06,576 [Thread-254] TRACE datatransfer.DataTransferProtocol (Sender.java:send(79)) - Sending DataTransferOp OpWriteBlockProto: header {
  baseHeader {
    block {
      poolId: "BP-1344107566-172.17.0.11-1606980108837"
      blockId: 1073741825
      generationStamp: 1001
      numBytes: 1024
    }
    token {
      identifier: ""
      password: ""
      kind: ""
      service: ""
    }
  }
  clientName: "DFSClient_NONMAPREDUCE_1921281077_1"
}
targets {
  id {
    ipAddr: "127.0.0.1"
    hostName: "127.0.0.1"
    datanodeUuid: "e627ed49-82da-4339-b7cf-f2a6c12fcee4"
    xferPort: 44891
    infoPort: 39970
    ipcPort: 35404
    infoSecurePort: 0
  }
  capacity: 1922244395008
  dfsUsed: 49152
  remaining: 1198436343808
  blockPoolUsed: 49152
  lastUpdate: 1606980126239
  xceiverCount: 1
  location: "/default-rack"
  nonDfsUsed: 626116296704
  adminState: NORMAL
  cacheCapacity: 0
  cacheUsed: 0
  lastUpdateMonotonic: 141115162
  lastBlockReportTime: 1606980126241
  lastBlockReportMonotonic: 141115164
  numBlocks: 0
}
targets {
  id {
    ipAddr: "127.0.0.1"
    hostName: "127.0.0.1"
    datanodeUuid: "0a567f53-c6f1-41b4-9ac9-d95bf8d50a27"
    xferPort: 33403
    infoPort: 41687
    ipcPort: 42709
    infoSecurePort: 0
  }
  capacity: 1922244395008
  dfsUsed: 49152
  remaining: 1198436540416
  blockPoolUsed: 49152
  lastUpdate: 1606980126041
  xceiverCount: 1
  location: "/default-rack"
  nonDfsUsed: 626116100096
  adminState: NORMAL
  cacheCapacity: 0
  cacheUsed: 0
  lastUpdateMonotonic: 141114964
  lastBlockReportTime: 1606980126044
  lastBlockReportMonotonic: 141114967
  numBlocks: 0
}
stage: PIPELINE_SETUP_CREATE
pipelineSize: 3
minBytesRcvd: 0
maxBytesRcvd: 0
latestGenerationStamp: 0
requestedChecksum {
  type: CHECKSUM_CRC32C
  bytesPerChecksum: 512
}
cachingStrategy {
}
storageType: ARCHIVE
targetStorageTypes: ARCHIVE
targetStorageTypes: ARCHIVE
allowLazyPersist: false
pinning: false
targetPinnings: false
storageId: "DS-31c88246-3f1d-4052-a932-2eefddf8a505"
targetStorageIds: "DS-6ee6c00c-1ea1-4eca-be51-a4e9a83f8e11"
targetStorageIds: "DS-af93b569-157d-478e-8452-49963f34fadf"

2020-12-03 07:22:06,579 [DataXceiver for client DFSClient_NONMAPREDUCE_1921281077_1 at /127.0.0.1:60326 [Receiving block BP-1344107566-172.17.0.11-1606980108837:blk_1073741825_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1344107566-172.17.0.11-1606980108837:blk_1073741825_1001 src: /127.0.0.1:60326 dest: /127.0.0.1:33340
2020-12-03 07:22:06,603 [DataXceiver for client DFSClient_NONMAPREDUCE_1921281077_1 at /127.0.0.1:60326 [Receiving block BP-1344107566-172.17.0.11-1606980108837:blk_1073741825_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:06,607 [DataXceiver for client DFSClient_NONMAPREDUCE_1921281077_1 at /127.0.0.1:60326 [Receiving block BP-1344107566-172.17.0.11-1606980108837:blk_1073741825_1001]] TRACE datatransfer.DataTransferProtocol (Sender.java:send(79)) - Sending DataTransferOp OpWriteBlockProto: header {
  baseHeader {
    block {
      poolId: "BP-1344107566-172.17.0.11-1606980108837"
      blockId: 1073741825
      generationStamp: 1001
      numBytes: 1024
    }
    token {
      identifier: ""
      password: ""
      kind: ""
      service: ""
    }
  }
  clientName: "DFSClient_NONMAPREDUCE_1921281077_1"
}
targets {
  id {
    ipAddr: "127.0.0.1"
    hostName: "127.0.0.1"
    datanodeUuid: "0a567f53-c6f1-41b4-9ac9-d95bf8d50a27"
    xferPort: 33403
    infoPort: 41687
    ipcPort: 42709
    infoSecurePort: 0
  }
  capacity: 1922244395008
  dfsUsed: 49152
  remaining: 1198436540416
  blockPoolUsed: 49152
  lastUpdate: 1606980126041
  xceiverCount: 1
  location: "/default-rack"
  nonDfsUsed: 626116100096
  adminState: NORMAL
  cacheCapacity: 0
  cacheUsed: 0
  lastUpdateMonotonic: 141114964
  lastBlockReportTime: 1606980126044
  lastBlockReportMonotonic: 141114967
  numBlocks: 0
}
source {
  id {
    ipAddr: ""
    hostName: ""
    datanodeUuid: ""
    xferPort: 0
    infoPort: 0
    ipcPort: 0
    infoSecurePort: 0
  }
  capacity: 0
  dfsUsed: 0
  remaining: 0
  blockPoolUsed: 0
  lastUpdate: 0
  xceiverCount: 0
  nonDfsUsed: 0
  adminState: NORMAL
  cacheCapacity: 0
  cacheUsed: 0
  lastUpdateMonotonic: 0
  lastBlockReportTime: 0
  lastBlockReportMonotonic: 0
  numBlocks: 0
}
stage: PIPELINE_SETUP_CREATE
pipelineSize: 3
minBytesRcvd: 0
maxBytesRcvd: 0
latestGenerationStamp: 0
requestedChecksum {
  type: CHECKSUM_CRC32C
  bytesPerChecksum: 512
}
cachingStrategy {
}
storageType: ARCHIVE
targetStorageTypes: ARCHIVE
allowLazyPersist: false
pinning: false
storageId: "DS-6ee6c00c-1ea1-4eca-be51-a4e9a83f8e11"
targetStorageIds: "DS-af93b569-157d-478e-8452-49963f34fadf"

2020-12-03 07:22:06,608 [DataXceiver for client DFSClient_NONMAPREDUCE_1921281077_1 at /127.0.0.1:46030 [Receiving block BP-1344107566-172.17.0.11-1606980108837:blk_1073741825_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1344107566-172.17.0.11-1606980108837:blk_1073741825_1001 src: /127.0.0.1:46030 dest: /127.0.0.1:44891
2020-12-03 07:22:06,610 [DataXceiver for client DFSClient_NONMAPREDUCE_1921281077_1 at /127.0.0.1:46030 [Receiving block BP-1344107566-172.17.0.11-1606980108837:blk_1073741825_1001]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:06,613 [DataXceiver for client DFSClient_NONMAPREDUCE_1921281077_1 at /127.0.0.1:46030 [Receiving block BP-1344107566-172.17.0.11-1606980108837:blk_1073741825_1001]] TRACE datatransfer.DataTransferProtocol (Sender.java:send(79)) - Sending DataTransferOp OpWriteBlockProto: header {
  baseHeader {
    block {
      poolId: "BP-1344107566-172.17.0.11-1606980108837"
      blockId: 1073741825
      generationStamp: 1001
      numBytes: 1024
    }
    token {
      identifier: ""
      password: ""
      kind: ""
      service: ""
    }
  }
  clientName: "DFSClient_NONMAPREDUCE_1921281077_1"
}
source {
  id {
    ipAddr: ""
    hostName: ""
    datanodeUuid: ""
    xferPort: 0
    infoPort: 0
    ipcPort: 0
    infoSecurePort: 0
  }
  capacity: 0
  dfsUsed: 0
  remaining: 0
  blockPoolUsed: 0
  lastUpdate: 0
  xceiverCount: 0
  nonDfsUsed: 0
  adminState: NORMAL
  cacheCapacity: 0
  cacheUsed: 0
  lastUpdateMonotonic: 0
  lastBlockReportTime: 0
  lastBlockReportMonotonic: 0
  numBlocks: 0
}
stage: PIPELINE_SETUP_CREATE
pipelineSize: 3
minBytesRcvd: 0
maxBytesRcvd: 0
latestGenerationStamp: 0
requestedChecksum {
  type: CHECKSUM_CRC32C
  bytesPerChecksum: 512
}
cachingStrategy {
}
storageType: ARCHIVE
allowLazyPersist: false
pinning: false
storageId: "DS-af93b569-157d-478e-8452-49963f34fadf"

2020-12-03 07:22:06,614 [DataXceiver for client DFSClient_NONMAPREDUCE_1921281077_1 at /127.0.0.1:60018 [Receiving block BP-1344107566-172.17.0.11-1606980108837:blk_1073741825_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1344107566-172.17.0.11-1606980108837:blk_1073741825_1001 src: /127.0.0.1:60018 dest: /127.0.0.1:33403
2020-12-03 07:22:06,647 [PacketResponder: BP-1344107566-172.17.0.11-1606980108837:blk_1073741825_1001, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:60018, dest: /127.0.0.1:33403, bytes: 1024, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1921281077_1, offset: 0, srvID: 0a567f53-c6f1-41b4-9ac9-d95bf8d50a27, blockid: BP-1344107566-172.17.0.11-1606980108837:blk_1073741825_1001, duration(ns): 15756889
2020-12-03 07:22:06,648 [PacketResponder: BP-1344107566-172.17.0.11-1606980108837:blk_1073741825_1001, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1344107566-172.17.0.11-1606980108837:blk_1073741825_1001, type=LAST_IN_PIPELINE terminating
2020-12-03 07:22:06,652 [PacketResponder: BP-1344107566-172.17.0.11-1606980108837:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:33403]] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:46030, dest: /127.0.0.1:44891, bytes: 1024, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1921281077_1, offset: 0, srvID: e627ed49-82da-4339-b7cf-f2a6c12fcee4, blockid: BP-1344107566-172.17.0.11-1606980108837:blk_1073741825_1001, duration(ns): 23376818
2020-12-03 07:22:06,653 [PacketResponder: BP-1344107566-172.17.0.11-1606980108837:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:33403]] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1344107566-172.17.0.11-1606980108837:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:33403] terminating
2020-12-03 07:22:06,657 [PacketResponder: BP-1344107566-172.17.0.11-1606980108837:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[127.0.0.1:44891, 127.0.0.1:33403]] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:60326, dest: /127.0.0.1:33340, bytes: 1024, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1921281077_1, offset: 0, srvID: 846ff15f-ed7d-4cf3-8112-e86de1180f07, blockid: BP-1344107566-172.17.0.11-1606980108837:blk_1073741825_1001, duration(ns): 27145602
2020-12-03 07:22:06,657 [PacketResponder: BP-1344107566-172.17.0.11-1606980108837:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[127.0.0.1:44891, 127.0.0.1:33403]] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1344107566-172.17.0.11-1606980108837:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[127.0.0.1:44891, 127.0.0.1:33403] terminating
2020-12-03 07:22:06,665 [IPC Server handler 8 on default port 44708] INFO  namenode.FSNamesystem (FSNamesystem.java:checkBlocksComplete(2995)) - BLOCK* blk_1073741825_1001 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /cold/file0
2020-12-03 07:22:07,068 [IPC Server handler 5 on default port 44708] INFO  hdfs.StateChange (FSNamesystem.java:completeFile(2948)) - DIR* completeFile: /cold/file0 is closed by DFSClient_NONMAPREDUCE_1921281077_1
2020-12-03 07:22:07,069 [Listener at localhost/41652] INFO  mover.TestStorageMover (TestStorageMover.java:waitForAllReplicas(619)) - Waiting for replicas count 3, file name: /cold/file0
2020-12-03 07:22:07,075 [IPC Server handler 4 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/cold/file0	dst=null	perm=null	proto=rpc
2020-12-03 07:22:07,084 [IPC Server handler 0 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/cold	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-12-03 07:22:07,086 [IPC Server handler 8 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/cold/file1	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2020-12-03 07:22:07,090 [IPC Server handler 7 on default port 44708] TRACE blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(427)) - storageTypes={ARCHIVE=3}
2020-12-03 07:22:07,091 [IPC Server handler 7 on default port 44708] DEBUG blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseRemoteRack(719)) - Failed to choose remote rack (location = ~/default-rack), fallback to local rack
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy$NotEnoughReplicasException: 
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:852)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRemoteRack(BlockPlacementPolicyDefault.java:714)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTargetInOrder(BlockPlacementPolicyDefault.java:519)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:437)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:309)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:149)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:174)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2211)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2789)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:892)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:574)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)
2020-12-03 07:22:07,092 [IPC Server handler 7 on default port 44708] DEBUG blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseRemoteRack(719)) - Failed to choose remote rack (location = ~/default-rack), fallback to local rack
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy$NotEnoughReplicasException: 
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:852)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRemoteRack(BlockPlacementPolicyDefault.java:714)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTargetInOrder(BlockPlacementPolicyDefault.java:528)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:437)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:309)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:149)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:174)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2211)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2789)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:892)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:574)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)
2020-12-03 07:22:07,093 [IPC Server handler 7 on default port 44708] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_1073741826_1002, replicas=127.0.0.1:33403, 127.0.0.1:46034, 127.0.0.1:44597 for /cold/file1
2020-12-03 07:22:07,096 [Thread-262] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:07,100 [Thread-262] TRACE datatransfer.DataTransferProtocol (Sender.java:send(79)) - Sending DataTransferOp OpWriteBlockProto: header {
  baseHeader {
    block {
      poolId: "BP-1344107566-172.17.0.11-1606980108837"
      blockId: 1073741826
      generationStamp: 1002
      numBytes: 1024
    }
    token {
      identifier: ""
      password: ""
      kind: ""
      service: ""
    }
  }
  clientName: "DFSClient_NONMAPREDUCE_1921281077_1"
}
targets {
  id {
    ipAddr: "127.0.0.1"
    hostName: "127.0.0.1"
    datanodeUuid: "5e2422dd-430a-4435-a145-829f854bf7e8"
    xferPort: 46034
    infoPort: 39776
    ipcPort: 41652
    infoSecurePort: 0
  }
  capacity: 1922244395008
  dfsUsed: 49152
  remaining: 1198440448000
  blockPoolUsed: 49152
  lastUpdate: 1606980126339
  xceiverCount: 1
  location: "/default-rack"
  nonDfsUsed: 626112192512
  adminState: NORMAL
  cacheCapacity: 0
  cacheUsed: 0
  lastUpdateMonotonic: 141115262
  lastBlockReportTime: 1606980126342
  lastBlockReportMonotonic: 141115265
  numBlocks: 0
}
targets {
  id {
    ipAddr: "127.0.0.1"
    hostName: "127.0.0.1"
    datanodeUuid: "622bf17c-c047-4086-9cd5-302fd27f9de3"
    xferPort: 44597
    infoPort: 39415
    ipcPort: 35175
    infoSecurePort: 0
  }
  capacity: 1922244395008
  dfsUsed: 49152
  remaining: 1198451712000
  blockPoolUsed: 49152
  lastUpdate: 1606980126939
  xceiverCount: 1
  location: "/default-rack"
  nonDfsUsed: 626100928512
  adminState: NORMAL
  cacheCapacity: 0
  cacheUsed: 0
  lastUpdateMonotonic: 141115862
  lastBlockReportTime: 1606980125941
  lastBlockReportMonotonic: 141114864
  numBlocks: 0
}
stage: PIPELINE_SETUP_CREATE
pipelineSize: 3
minBytesRcvd: 0
maxBytesRcvd: 0
latestGenerationStamp: 0
requestedChecksum {
  type: CHECKSUM_CRC32C
  bytesPerChecksum: 512
}
cachingStrategy {
}
storageType: ARCHIVE
targetStorageTypes: ARCHIVE
targetStorageTypes: ARCHIVE
allowLazyPersist: false
pinning: false
targetPinnings: false
storageId: "DS-af93b569-157d-478e-8452-49963f34fadf"
targetStorageIds: "DS-c1ddbde3-64ec-4172-87a7-93013a994b4a"
targetStorageIds: "DS-c7c90efb-327f-4b73-88a6-34752b5003a8"

2020-12-03 07:22:07,101 [DataXceiver for client DFSClient_NONMAPREDUCE_1921281077_1 at /127.0.0.1:60022 [Receiving block BP-1344107566-172.17.0.11-1606980108837:blk_1073741826_1002]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1344107566-172.17.0.11-1606980108837:blk_1073741826_1002 src: /127.0.0.1:60022 dest: /127.0.0.1:33403
2020-12-03 07:22:07,104 [DataXceiver for client DFSClient_NONMAPREDUCE_1921281077_1 at /127.0.0.1:60022 [Receiving block BP-1344107566-172.17.0.11-1606980108837:blk_1073741826_1002]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:07,107 [DataXceiver for client DFSClient_NONMAPREDUCE_1921281077_1 at /127.0.0.1:60022 [Receiving block BP-1344107566-172.17.0.11-1606980108837:blk_1073741826_1002]] TRACE datatransfer.DataTransferProtocol (Sender.java:send(79)) - Sending DataTransferOp OpWriteBlockProto: header {
  baseHeader {
    block {
      poolId: "BP-1344107566-172.17.0.11-1606980108837"
      blockId: 1073741826
      generationStamp: 1002
      numBytes: 1024
    }
    token {
      identifier: ""
      password: ""
      kind: ""
      service: ""
    }
  }
  clientName: "DFSClient_NONMAPREDUCE_1921281077_1"
}
targets {
  id {
    ipAddr: "127.0.0.1"
    hostName: "127.0.0.1"
    datanodeUuid: "622bf17c-c047-4086-9cd5-302fd27f9de3"
    xferPort: 44597
    infoPort: 39415
    ipcPort: 35175
    infoSecurePort: 0
  }
  capacity: 1922244395008
  dfsUsed: 49152
  remaining: 1198451712000
  blockPoolUsed: 49152
  lastUpdate: 1606980126939
  xceiverCount: 1
  location: "/default-rack"
  nonDfsUsed: 626100928512
  adminState: NORMAL
  cacheCapacity: 0
  cacheUsed: 0
  lastUpdateMonotonic: 141115862
  lastBlockReportTime: 1606980125941
  lastBlockReportMonotonic: 141114864
  numBlocks: 0
}
source {
  id {
    ipAddr: ""
    hostName: ""
    datanodeUuid: ""
    xferPort: 0
    infoPort: 0
    ipcPort: 0
    infoSecurePort: 0
  }
  capacity: 0
  dfsUsed: 0
  remaining: 0
  blockPoolUsed: 0
  lastUpdate: 0
  xceiverCount: 0
  nonDfsUsed: 0
  adminState: NORMAL
  cacheCapacity: 0
  cacheUsed: 0
  lastUpdateMonotonic: 0
  lastBlockReportTime: 0
  lastBlockReportMonotonic: 0
  numBlocks: 0
}
stage: PIPELINE_SETUP_CREATE
pipelineSize: 3
minBytesRcvd: 0
maxBytesRcvd: 0
latestGenerationStamp: 0
requestedChecksum {
  type: CHECKSUM_CRC32C
  bytesPerChecksum: 512
}
cachingStrategy {
}
storageType: ARCHIVE
targetStorageTypes: ARCHIVE
allowLazyPersist: false
pinning: false
storageId: "DS-c1ddbde3-64ec-4172-87a7-93013a994b4a"
targetStorageIds: "DS-c7c90efb-327f-4b73-88a6-34752b5003a8"

2020-12-03 07:22:07,108 [DataXceiver for client DFSClient_NONMAPREDUCE_1921281077_1 at /127.0.0.1:46868 [Receiving block BP-1344107566-172.17.0.11-1606980108837:blk_1073741826_1002]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1344107566-172.17.0.11-1606980108837:blk_1073741826_1002 src: /127.0.0.1:46868 dest: /127.0.0.1:46034
2020-12-03 07:22:07,109 [DataXceiver for client DFSClient_NONMAPREDUCE_1921281077_1 at /127.0.0.1:46868 [Receiving block BP-1344107566-172.17.0.11-1606980108837:blk_1073741826_1002]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:07,111 [DataXceiver for client DFSClient_NONMAPREDUCE_1921281077_1 at /127.0.0.1:46868 [Receiving block BP-1344107566-172.17.0.11-1606980108837:blk_1073741826_1002]] TRACE datatransfer.DataTransferProtocol (Sender.java:send(79)) - Sending DataTransferOp OpWriteBlockProto: header {
  baseHeader {
    block {
      poolId: "BP-1344107566-172.17.0.11-1606980108837"
      blockId: 1073741826
      generationStamp: 1002
      numBytes: 1024
    }
    token {
      identifier: ""
      password: ""
      kind: ""
      service: ""
    }
  }
  clientName: "DFSClient_NONMAPREDUCE_1921281077_1"
}
source {
  id {
    ipAddr: ""
    hostName: ""
    datanodeUuid: ""
    xferPort: 0
    infoPort: 0
    ipcPort: 0
    infoSecurePort: 0
  }
  capacity: 0
  dfsUsed: 0
  remaining: 0
  blockPoolUsed: 0
  lastUpdate: 0
  xceiverCount: 0
  nonDfsUsed: 0
  adminState: NORMAL
  cacheCapacity: 0
  cacheUsed: 0
  lastUpdateMonotonic: 0
  lastBlockReportTime: 0
  lastBlockReportMonotonic: 0
  numBlocks: 0
}
stage: PIPELINE_SETUP_CREATE
pipelineSize: 3
minBytesRcvd: 0
maxBytesRcvd: 0
latestGenerationStamp: 0
requestedChecksum {
  type: CHECKSUM_CRC32C
  bytesPerChecksum: 512
}
cachingStrategy {
}
storageType: ARCHIVE
allowLazyPersist: false
pinning: false
storageId: "DS-c7c90efb-327f-4b73-88a6-34752b5003a8"

2020-12-03 07:22:07,112 [DataXceiver for client DFSClient_NONMAPREDUCE_1921281077_1 at /127.0.0.1:33898 [Receiving block BP-1344107566-172.17.0.11-1606980108837:blk_1073741826_1002]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1344107566-172.17.0.11-1606980108837:blk_1073741826_1002 src: /127.0.0.1:33898 dest: /127.0.0.1:44597
2020-12-03 07:22:07,124 [PacketResponder: BP-1344107566-172.17.0.11-1606980108837:blk_1073741826_1002, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:33898, dest: /127.0.0.1:44597, bytes: 1024, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1921281077_1, offset: 0, srvID: 622bf17c-c047-4086-9cd5-302fd27f9de3, blockid: BP-1344107566-172.17.0.11-1606980108837:blk_1073741826_1002, duration(ns): 8149186
2020-12-03 07:22:07,124 [PacketResponder: BP-1344107566-172.17.0.11-1606980108837:blk_1073741826_1002, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1344107566-172.17.0.11-1606980108837:blk_1073741826_1002, type=LAST_IN_PIPELINE terminating
2020-12-03 07:22:07,130 [PacketResponder: BP-1344107566-172.17.0.11-1606980108837:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:44597]] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:46868, dest: /127.0.0.1:46034, bytes: 1024, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1921281077_1, offset: 0, srvID: 5e2422dd-430a-4435-a145-829f854bf7e8, blockid: BP-1344107566-172.17.0.11-1606980108837:blk_1073741826_1002, duration(ns): 13873458
2020-12-03 07:22:07,131 [PacketResponder: BP-1344107566-172.17.0.11-1606980108837:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:44597]] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1344107566-172.17.0.11-1606980108837:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:44597] terminating
2020-12-03 07:22:07,132 [PacketResponder: BP-1344107566-172.17.0.11-1606980108837:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[127.0.0.1:46034, 127.0.0.1:44597]] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:60022, dest: /127.0.0.1:33403, bytes: 1024, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1921281077_1, offset: 0, srvID: 0a567f53-c6f1-41b4-9ac9-d95bf8d50a27, blockid: BP-1344107566-172.17.0.11-1606980108837:blk_1073741826_1002, duration(ns): 16428211
2020-12-03 07:22:07,133 [PacketResponder: BP-1344107566-172.17.0.11-1606980108837:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[127.0.0.1:46034, 127.0.0.1:44597]] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1344107566-172.17.0.11-1606980108837:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[127.0.0.1:46034, 127.0.0.1:44597] terminating
2020-12-03 07:22:07,135 [IPC Server handler 3 on default port 44708] INFO  hdfs.StateChange (FSNamesystem.java:completeFile(2948)) - DIR* completeFile: /cold/file1 is closed by DFSClient_NONMAPREDUCE_1921281077_1
2020-12-03 07:22:07,136 [Listener at localhost/41652] INFO  mover.TestStorageMover (TestStorageMover.java:waitForAllReplicas(619)) - Waiting for replicas count 3, file name: /cold/file1
2020-12-03 07:22:07,137 [IPC Server handler 9 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/cold/file1	dst=null	perm=null	proto=rpc
2020-12-03 07:22:07,140 [Listener at localhost/41652] INFO  mover.TestStorageMover (TestStorageMover.java:setVolumeFull(639)) - setCapacity to 0 for [ARCHIVE]DS-31c88246-3f1d-4052-a932-2eefddf8a505
2020-12-03 07:22:07,240 [Listener at localhost/41652] INFO  mover.TestStorageMover (TestStorageMover.java:setVolumeFull(639)) - setCapacity to 0 for [ARCHIVE]DS-c7c90efb-327f-4b73-88a6-34752b5003a8
2020-12-03 07:22:07,341 [Listener at localhost/41652] INFO  mover.TestStorageMover (TestStorageMover.java:setVolumeFull(639)) - setCapacity to 0 for [ARCHIVE]DS-af93b569-157d-478e-8452-49963f34fadf
2020-12-03 07:22:07,442 [Listener at localhost/41652] INFO  mover.TestStorageMover (TestStorageMover.java:setVolumeFull(639)) - setCapacity to 0 for [ARCHIVE]DS-fa19f5df-de0d-41f8-ae1e-004956343da7
2020-12-03 07:22:07,543 [Listener at localhost/41652] INFO  mover.TestStorageMover (TestStorageMover.java:setVolumeFull(639)) - setCapacity to 0 for [ARCHIVE]DS-6ee6c00c-1ea1-4eca-be51-a4e9a83f8e11
2020-12-03 07:22:07,643 [Listener at localhost/41652] INFO  mover.TestStorageMover (TestStorageMover.java:setVolumeFull(639)) - setCapacity to 0 for [ARCHIVE]DS-c1ddbde3-64ec-4172-87a7-93013a994b4a
2020-12-03 07:22:07,746 [IPC Server handler 9 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/cold/file0	dst=null	perm=null	proto=rpc
2020-12-03 07:22:07,753 [IPC Server handler 5 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setReplication	src=/cold/file0	dst=null	perm=null	proto=rpc
2020-12-03 07:22:07,758 [Listener at localhost/41652] INFO  mover.TestStorageMover (TestStorageMover.java:waitForAllReplicas(619)) - Waiting for replicas count 3, file name: /cold/file0
2020-12-03 07:22:07,760 [IPC Server handler 4 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/cold/file0	dst=null	perm=null	proto=rpc
2020-12-03 07:22:07,762 [IPC Server handler 0 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/cold/file0	dst=null	perm=null	proto=rpc
2020-12-03 07:22:07,765 [IPC Server handler 8 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/hot	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-12-03 07:22:07,768 [IPC Server handler 7 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/hot/foo	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2020-12-03 07:22:07,778 [IPC Server handler 1 on default port 44708] TRACE blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(427)) - storageTypes={DISK=3}
2020-12-03 07:22:07,779 [IPC Server handler 1 on default port 44708] DEBUG blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseRemoteRack(719)) - Failed to choose remote rack (location = ~/default-rack), fallback to local rack
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy$NotEnoughReplicasException: 
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:852)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRemoteRack(BlockPlacementPolicyDefault.java:714)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTargetInOrder(BlockPlacementPolicyDefault.java:519)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:437)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:309)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:149)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:174)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2211)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2789)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:892)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:574)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)
2020-12-03 07:22:07,779 [IPC Server handler 1 on default port 44708] DEBUG blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseRemoteRack(719)) - Failed to choose remote rack (location = ~/default-rack), fallback to local rack
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy$NotEnoughReplicasException: 
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:852)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRemoteRack(BlockPlacementPolicyDefault.java:714)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTargetInOrder(BlockPlacementPolicyDefault.java:528)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:437)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:309)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:149)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:174)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2211)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2789)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:892)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:574)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)
2020-12-03 07:22:07,781 [IPC Server handler 1 on default port 44708] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(798)) - BLOCK* allocate blk_1073741827_1003, replicas=127.0.0.1:33340, 127.0.0.1:44891, 127.0.0.1:44597 for /hot/foo
2020-12-03 07:22:07,783 [Thread-270] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:07,786 [Thread-270] TRACE datatransfer.DataTransferProtocol (Sender.java:send(79)) - Sending DataTransferOp OpWriteBlockProto: header {
  baseHeader {
    block {
      poolId: "BP-1344107566-172.17.0.11-1606980108837"
      blockId: 1073741827
      generationStamp: 1003
      numBytes: 1024
    }
    token {
      identifier: ""
      password: ""
      kind: ""
      service: ""
    }
  }
  clientName: "DFSClient_NONMAPREDUCE_1921281077_1"
}
targets {
  id {
    ipAddr: "127.0.0.1"
    hostName: "127.0.0.1"
    datanodeUuid: "e627ed49-82da-4339-b7cf-f2a6c12fcee4"
    xferPort: 44891
    infoPort: 39970
    ipcPort: 35404
    infoSecurePort: 0
  }
  capacity: 961122197504
  dfsUsed: 50191
  remaining: 599225884672
  blockPoolUsed: 50191
  lastUpdate: 1606980127545
  xceiverCount: 1
  location: "/default-rack"
  nonDfsUsed: 626100870129
  adminState: NORMAL
  cacheCapacity: 0
  cacheUsed: 0
  lastUpdateMonotonic: 141116468
  lastBlockReportTime: 1606980126241
  lastBlockReportMonotonic: 141115164
  numBlocks: 0
}
targets {
  id {
    ipAddr: "127.0.0.1"
    hostName: "127.0.0.1"
    datanodeUuid: "622bf17c-c047-4086-9cd5-302fd27f9de3"
    xferPort: 44597
    infoPort: 39415
    ipcPort: 35175
    infoSecurePort: 0
  }
  capacity: 961122197504
  dfsUsed: 50191
  remaining: 599225856000
  blockPoolUsed: 50191
  lastUpdate: 1606980127242
  xceiverCount: 1
  location: "/default-rack"
  nonDfsUsed: 626100927473
  adminState: NORMAL
  cacheCapacity: 0
  cacheUsed: 0
  lastUpdateMonotonic: 141116165
  lastBlockReportTime: 1606980125941
  lastBlockReportMonotonic: 141114864
  numBlocks: 0
}
stage: PIPELINE_SETUP_CREATE
pipelineSize: 3
minBytesRcvd: 0
maxBytesRcvd: 0
latestGenerationStamp: 0
requestedChecksum {
  type: CHECKSUM_CRC32C
  bytesPerChecksum: 512
}
cachingStrategy {
}
storageType: DISK
targetStorageTypes: DISK
targetStorageTypes: DISK
allowLazyPersist: false
pinning: false
targetPinnings: false
storageId: "DS-5a3cdcb1-c70d-48fc-9151-fc286cea3570"
targetStorageIds: "DS-60aaafac-0dea-4e4b-8271-d136469da168"
targetStorageIds: "DS-058ff3c4-f186-458c-8f1b-9ea2f1373a45"

2020-12-03 07:22:07,787 [DataXceiver for client DFSClient_NONMAPREDUCE_1921281077_1 at /127.0.0.1:60342 [Receiving block BP-1344107566-172.17.0.11-1606980108837:blk_1073741827_1003]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1344107566-172.17.0.11-1606980108837:blk_1073741827_1003 src: /127.0.0.1:60342 dest: /127.0.0.1:33340
2020-12-03 07:22:07,788 [DataXceiver for client DFSClient_NONMAPREDUCE_1921281077_1 at /127.0.0.1:60342 [Receiving block BP-1344107566-172.17.0.11-1606980108837:blk_1073741827_1003]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:07,790 [DataXceiver for client DFSClient_NONMAPREDUCE_1921281077_1 at /127.0.0.1:60342 [Receiving block BP-1344107566-172.17.0.11-1606980108837:blk_1073741827_1003]] TRACE datatransfer.DataTransferProtocol (Sender.java:send(79)) - Sending DataTransferOp OpWriteBlockProto: header {
  baseHeader {
    block {
      poolId: "BP-1344107566-172.17.0.11-1606980108837"
      blockId: 1073741827
      generationStamp: 1003
      numBytes: 1024
    }
    token {
      identifier: ""
      password: ""
      kind: ""
      service: ""
    }
  }
  clientName: "DFSClient_NONMAPREDUCE_1921281077_1"
}
targets {
  id {
    ipAddr: "127.0.0.1"
    hostName: "127.0.0.1"
    datanodeUuid: "622bf17c-c047-4086-9cd5-302fd27f9de3"
    xferPort: 44597
    infoPort: 39415
    ipcPort: 35175
    infoSecurePort: 0
  }
  capacity: 961122197504
  dfsUsed: 50191
  remaining: 599225856000
  blockPoolUsed: 50191
  lastUpdate: 1606980127242
  xceiverCount: 1
  location: "/default-rack"
  nonDfsUsed: 626100927473
  adminState: NORMAL
  cacheCapacity: 0
  cacheUsed: 0
  lastUpdateMonotonic: 141116165
  lastBlockReportTime: 1606980125941
  lastBlockReportMonotonic: 141114864
  numBlocks: 0
}
source {
  id {
    ipAddr: ""
    hostName: ""
    datanodeUuid: ""
    xferPort: 0
    infoPort: 0
    ipcPort: 0
    infoSecurePort: 0
  }
  capacity: 0
  dfsUsed: 0
  remaining: 0
  blockPoolUsed: 0
  lastUpdate: 0
  xceiverCount: 0
  nonDfsUsed: 0
  adminState: NORMAL
  cacheCapacity: 0
  cacheUsed: 0
  lastUpdateMonotonic: 0
  lastBlockReportTime: 0
  lastBlockReportMonotonic: 0
  numBlocks: 0
}
stage: PIPELINE_SETUP_CREATE
pipelineSize: 3
minBytesRcvd: 0
maxBytesRcvd: 0
latestGenerationStamp: 0
requestedChecksum {
  type: CHECKSUM_CRC32C
  bytesPerChecksum: 512
}
cachingStrategy {
}
storageType: DISK
targetStorageTypes: DISK
allowLazyPersist: false
pinning: false
storageId: "DS-60aaafac-0dea-4e4b-8271-d136469da168"
targetStorageIds: "DS-058ff3c4-f186-458c-8f1b-9ea2f1373a45"

2020-12-03 07:22:07,791 [DataXceiver for client DFSClient_NONMAPREDUCE_1921281077_1 at /127.0.0.1:46044 [Receiving block BP-1344107566-172.17.0.11-1606980108837:blk_1073741827_1003]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1344107566-172.17.0.11-1606980108837:blk_1073741827_1003 src: /127.0.0.1:46044 dest: /127.0.0.1:44891
2020-12-03 07:22:07,792 [DataXceiver for client DFSClient_NONMAPREDUCE_1921281077_1 at /127.0.0.1:46044 [Receiving block BP-1344107566-172.17.0.11-1606980108837:blk_1073741827_1003]] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:07,793 [DataXceiver for client DFSClient_NONMAPREDUCE_1921281077_1 at /127.0.0.1:46044 [Receiving block BP-1344107566-172.17.0.11-1606980108837:blk_1073741827_1003]] TRACE datatransfer.DataTransferProtocol (Sender.java:send(79)) - Sending DataTransferOp OpWriteBlockProto: header {
  baseHeader {
    block {
      poolId: "BP-1344107566-172.17.0.11-1606980108837"
      blockId: 1073741827
      generationStamp: 1003
      numBytes: 1024
    }
    token {
      identifier: ""
      password: ""
      kind: ""
      service: ""
    }
  }
  clientName: "DFSClient_NONMAPREDUCE_1921281077_1"
}
source {
  id {
    ipAddr: ""
    hostName: ""
    datanodeUuid: ""
    xferPort: 0
    infoPort: 0
    ipcPort: 0
    infoSecurePort: 0
  }
  capacity: 0
  dfsUsed: 0
  remaining: 0
  blockPoolUsed: 0
  lastUpdate: 0
  xceiverCount: 0
  nonDfsUsed: 0
  adminState: NORMAL
  cacheCapacity: 0
  cacheUsed: 0
  lastUpdateMonotonic: 0
  lastBlockReportTime: 0
  lastBlockReportMonotonic: 0
  numBlocks: 0
}
stage: PIPELINE_SETUP_CREATE
pipelineSize: 3
minBytesRcvd: 0
maxBytesRcvd: 0
latestGenerationStamp: 0
requestedChecksum {
  type: CHECKSUM_CRC32C
  bytesPerChecksum: 512
}
cachingStrategy {
}
storageType: DISK
allowLazyPersist: false
pinning: false
storageId: "DS-058ff3c4-f186-458c-8f1b-9ea2f1373a45"

2020-12-03 07:22:07,794 [DataXceiver for client DFSClient_NONMAPREDUCE_1921281077_1 at /127.0.0.1:33904 [Receiving block BP-1344107566-172.17.0.11-1606980108837:blk_1073741827_1003]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(747)) - Receiving BP-1344107566-172.17.0.11-1606980108837:blk_1073741827_1003 src: /127.0.0.1:33904 dest: /127.0.0.1:44597
2020-12-03 07:22:07,806 [PacketResponder: BP-1344107566-172.17.0.11-1606980108837:blk_1073741827_1003, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:33904, dest: /127.0.0.1:44597, bytes: 1024, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1921281077_1, offset: 0, srvID: 622bf17c-c047-4086-9cd5-302fd27f9de3, blockid: BP-1344107566-172.17.0.11-1606980108837:blk_1073741827_1003, duration(ns): 8512143
2020-12-03 07:22:07,807 [PacketResponder: BP-1344107566-172.17.0.11-1606980108837:blk_1073741827_1003, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1344107566-172.17.0.11-1606980108837:blk_1073741827_1003, type=LAST_IN_PIPELINE terminating
2020-12-03 07:22:07,811 [PacketResponder: BP-1344107566-172.17.0.11-1606980108837:blk_1073741827_1003, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:44597]] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:46044, dest: /127.0.0.1:44891, bytes: 1024, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1921281077_1, offset: 0, srvID: e627ed49-82da-4339-b7cf-f2a6c12fcee4, blockid: BP-1344107566-172.17.0.11-1606980108837:blk_1073741827_1003, duration(ns): 13426274
2020-12-03 07:22:07,812 [PacketResponder: BP-1344107566-172.17.0.11-1606980108837:blk_1073741827_1003, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:44597]] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1344107566-172.17.0.11-1606980108837:blk_1073741827_1003, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:44597] terminating
2020-12-03 07:22:07,814 [PacketResponder: BP-1344107566-172.17.0.11-1606980108837:blk_1073741827_1003, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[127.0.0.1:44891, 127.0.0.1:44597]] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:60342, dest: /127.0.0.1:33340, bytes: 1024, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1921281077_1, offset: 0, srvID: 846ff15f-ed7d-4cf3-8112-e86de1180f07, blockid: BP-1344107566-172.17.0.11-1606980108837:blk_1073741827_1003, duration(ns): 15239188
2020-12-03 07:22:07,814 [PacketResponder: BP-1344107566-172.17.0.11-1606980108837:blk_1073741827_1003, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[127.0.0.1:44891, 127.0.0.1:44597]] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1344107566-172.17.0.11-1606980108837:blk_1073741827_1003, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[127.0.0.1:44891, 127.0.0.1:44597] terminating
2020-12-03 07:22:07,816 [IPC Server handler 9 on default port 44708] INFO  hdfs.StateChange (FSNamesystem.java:completeFile(2948)) - DIR* completeFile: /hot/foo is closed by DFSClient_NONMAPREDUCE_1921281077_1
2020-12-03 07:22:07,829 [IPC Server handler 5 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=rename	src=/cold/file1	dst=/warm	perm=root:supergroup:rw-r--r--	proto=rpc
2020-12-03 07:22:07,833 [Listener at localhost/41652] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:22:07,833 [Listener at localhost/41652] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.namenode.redundancy.interval.seconds(2) assuming SECONDS
2020-12-03 07:22:07,834 [Listener at localhost/41652] INFO  mover.Mover (Mover.java:run(642)) - namenodes = {hdfs://localhost:44708=null}
2020-12-03 07:22:07,842 [IPC Server handler 8 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/system/mover.id	dst=null	perm=null	proto=rpc
2020-12-03 07:22:07,844 [IPC Server handler 7 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/system/mover.id	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2020-12-03 07:22:07,846 [IPC Server handler 1 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/system/mover.id	dst=null	perm=null	proto=rpc
2020-12-03 07:22:07,847 [Listener at localhost/41652] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:22:07,847 [Listener at localhost/41652] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1395)) - No unit for dfs.heartbeat.interval(1) assuming SECONDS
2020-12-03 07:22:07,849 [IPC Server handler 6 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getDatanodeStorageReport	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:22:07,851 [Listener at localhost/41652] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:44891
2020-12-03 07:22:07,851 [Listener at localhost/41652] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:33403
2020-12-03 07:22:07,852 [Listener at localhost/41652] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:46034
2020-12-03 07:22:07,852 [Listener at localhost/41652] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:42991
2020-12-03 07:22:07,852 [Listener at localhost/41652] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:33340
2020-12-03 07:22:07,852 [Listener at localhost/41652] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:44597
2020-12-03 07:22:07,853 [IPC Server handler 3 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=listSnapshottableDirectory	src=null	dst=null	perm=null	proto=rpc
2020-12-03 07:22:07,855 [IPC Server handler 9 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/	dst=null	perm=null	proto=rpc
2020-12-03 07:22:07,857 [IPC Server handler 5 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/cold/	dst=null	perm=null	proto=rpc
2020-12-03 07:22:07,860 [IPC Server handler 4 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/hot/	dst=null	perm=null	proto=rpc
2020-12-03 07:22:07,862 [IPC Server handler 0 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/system/	dst=null	perm=null	proto=rpc
2020-12-03 07:22:07,864 [IPC Server handler 8 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/warm/	dst=null	perm=null	proto=rpc
2020-12-03 07:22:07,868 [Listener at localhost/41652] DEBUG balancer.Dispatcher (Dispatcher.java:markMovedIfGoodBlock(294)) - Decided to move blk_1073741826_1002 with size=1024 from 127.0.0.1:46034:ARCHIVE to 127.0.0.1:46034:DISK through 127.0.0.1:46034
2020-12-03 07:22:07,869 [pool-83-thread-1] INFO  balancer.Dispatcher (Dispatcher.java:dispatch(361)) - Start moving blk_1073741826_1002 with size=1024 from 127.0.0.1:46034:ARCHIVE to 127.0.0.1:46034:DISK through 127.0.0.1:46034
2020-12-03 07:22:07,877 [pool-83-thread-1] INFO  sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(239)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2020-12-03 07:22:07,896 [pool-83-thread-1] TRACE datatransfer.DataTransferProtocol (Sender.java:send(79)) - Sending DataTransferOp OpReplaceBlockProto: header {
  block {
    poolId: "BP-1344107566-172.17.0.11-1606980108837"
    blockId: 1073741826
    generationStamp: 1002
    numBytes: 1024
  }
  token {
    identifier: ""
    password: ""
    kind: ""
    service: ""
  }
}
delHint: "5e2422dd-430a-4435-a145-829f854bf7e8"
source {
  id {
    ipAddr: "127.0.0.1"
    hostName: "127.0.0.1"
    datanodeUuid: "5e2422dd-430a-4435-a145-829f854bf7e8"
    xferPort: 46034
    infoPort: 39776
    ipcPort: 41652
    infoSecurePort: 0
  }
  capacity: 961122197504
  dfsUsed: 50191
  remaining: 599225917440
  blockPoolUsed: 50191
  lastUpdate: 1606980127645
  xceiverCount: 1
  location: "/default-rack"
  nonDfsUsed: 626100804593
  adminState: NORMAL
  cacheCapacity: 0
  cacheUsed: 0
  lastUpdateMonotonic: 141116568
  lastBlockReportTime: 1606980126342
  lastBlockReportMonotonic: 141115265
  numBlocks: 0
}
storageType: DISK

2020-12-03 07:22:07,902 [DataXceiver for client /127.0.0.1:46880 [Replacing block BP-1344107566-172.17.0.11-1606980108837:blk_1073741826_1002 from 5e2422dd-430a-4435-a145-829f854bf7e8]] INFO  datanode.DataNode (DataXceiver.java:replaceBlock(1186)) - Moved BP-1344107566-172.17.0.11-1606980108837:blk_1073741826_1002 from StorageType ARCHIVE to DISK
2020-12-03 07:22:07,902 [pool-83-thread-1] INFO  balancer.Dispatcher (Dispatcher.java:dispatch(396)) - Successfully moved blk_1073741826_1002 with size=1024 from 127.0.0.1:46034:ARCHIVE to 127.0.0.1:46034:DISK through 127.0.0.1:46034
2020-12-03 07:22:07,904 [Block report processor] WARN  BlockStateChange (BlockManager.java:addStoredBlock(3356)) - BLOCK* addStoredBlock: block blk_1073741826_1002 moved to storageType DISK on node 127.0.0.1:46034
2020-12-03 07:22:07,963 [RedundancyMonitor] TRACE blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(427)) - storageTypes={ARCHIVE=2}
2020-12-03 07:22:07,964 [RedundancyMonitor] DEBUG blockmanagement.BlockPlacementPolicy (DatanodeDescriptor.java:chooseStorage4Block(768)) - The node 127.0.0.1:44597 does not have enough ARCHIVE space (required=1024, scheduled=0, remaining=0).
2020-12-03 07:22:07,965 [RedundancyMonitor] DEBUG blockmanagement.BlockPlacementPolicy (DatanodeDescriptor.java:chooseStorage4Block(768)) - The node 127.0.0.1:42991 does not have enough ARCHIVE space (required=1024, scheduled=0, remaining=0).
2020-12-03 07:22:07,965 [RedundancyMonitor] DEBUG blockmanagement.BlockPlacementPolicy (DatanodeDescriptor.java:chooseStorage4Block(768)) - The node 127.0.0.1:46034 does not have enough ARCHIVE space (required=1024, scheduled=0, remaining=0).
2020-12-03 07:22:07,965 [RedundancyMonitor] INFO  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseRandom(850)) - Not enough replicas was chosen. Reason:{NOT_ENOUGH_STORAGE_SPACE=3}
2020-12-03 07:22:07,966 [RedundancyMonitor] TRACE blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(448)) - Failed to place enough replicas, still in need of 2 to reach 5 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{COLD:2, storageTypes=[ARCHIVE], creationFallbacks=[], replicationFallbacks=[]}, newBlock=false)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy$NotEnoughReplicasException: [
Node /default-rack/127.0.0.1:44597 [
  Datanode 127.0.0.1:44597 is not chosen since not enough storage space to place the block  for storage type ARCHIVE.
]
Node /default-rack/127.0.0.1:42991 [
  Datanode 127.0.0.1:42991 is not chosen since not enough storage space to place the block  for storage type ARCHIVE.
]
Node /default-rack/127.0.0.1:46034 [
  Datanode 127.0.0.1:46034 is not chosen since not enough storage space to place the block  for storage type ARCHIVE.
]
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:852)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTargetInOrder(BlockPlacementPolicyDefault.java:541)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:437)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:309)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:149)
	at org.apache.hadoop.hdfs.server.blockmanagement.ReplicationWork.chooseTargets(ReplicationWork.java:46)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReconstructionWorkForBlocks(BlockManager.java:1960)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeBlockReconstructionWork(BlockManager.java:1912)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:4813)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$RedundancyMonitor.run(BlockManager.java:4680)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:07,966 [RedundancyMonitor] WARN  protocol.BlockStoragePolicy (BlockStoragePolicy.java:chooseStorageTypes(161)) - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=5, selected=[], unavailable=[ARCHIVE], removed=[ARCHIVE, ARCHIVE], policy=BlockStoragePolicy{COLD:2, storageTypes=[ARCHIVE], creationFallbacks=[], replicationFallbacks=[]})
2020-12-03 07:22:07,966 [RedundancyMonitor] TRACE blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(427)) - storageTypes={}
2020-12-03 07:22:07,967 [RedundancyMonitor] TRACE blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(448)) - Failed to place enough replicas, still in need of 2 to reach 5 (unavailableStorages=[ARCHIVE], storagePolicy=BlockStoragePolicy{COLD:2, storageTypes=[ARCHIVE], creationFallbacks=[], replicationFallbacks=[]}, newBlock=false)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy$NotEnoughReplicasException: All required storage types are unavailable:  unavailableStorages=[ARCHIVE], storagePolicy=BlockStoragePolicy{COLD:2, storageTypes=[ARCHIVE], creationFallbacks=[], replicationFallbacks=[]}
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:432)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:486)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:309)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:149)
	at org.apache.hadoop.hdfs.server.blockmanagement.ReplicationWork.chooseTargets(ReplicationWork.java:46)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReconstructionWorkForBlocks(BlockManager.java:1960)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeBlockReconstructionWork(BlockManager.java:1912)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:4813)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$RedundancyMonitor.run(BlockManager.java:4680)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:08,870 [IPC Server handler 7 on default port 44708] INFO  hdfs.StateChange (FSNamesystem.java:completeFile(2948)) - DIR* completeFile: /system/mover.id is closed by DFSClient_NONMAPREDUCE_1921281077_1
2020-12-03 07:22:08,873 [IPC Server handler 1 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=delete	src=/system/mover.id	dst=null	perm=null	proto=rpc
2020-12-03 07:22:09,968 [RedundancyMonitor] TRACE blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(427)) - storageTypes={ARCHIVE=2}
2020-12-03 07:22:09,968 [RedundancyMonitor] DEBUG blockmanagement.BlockPlacementPolicy (DatanodeDescriptor.java:chooseStorage4Block(768)) - The node 127.0.0.1:44597 does not have enough ARCHIVE space (required=1024, scheduled=0, remaining=0).
2020-12-03 07:22:09,968 [RedundancyMonitor] DEBUG blockmanagement.BlockPlacementPolicy (DatanodeDescriptor.java:chooseStorage4Block(768)) - The node 127.0.0.1:46034 does not have enough ARCHIVE space (required=1024, scheduled=0, remaining=0).
2020-12-03 07:22:09,968 [RedundancyMonitor] DEBUG blockmanagement.BlockPlacementPolicy (DatanodeDescriptor.java:chooseStorage4Block(768)) - The node 127.0.0.1:42991 does not have enough ARCHIVE space (required=1024, scheduled=0, remaining=0).
2020-12-03 07:22:09,969 [RedundancyMonitor] INFO  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseRandom(850)) - Not enough replicas was chosen. Reason:{NOT_ENOUGH_STORAGE_SPACE=3}
2020-12-03 07:22:09,969 [RedundancyMonitor] TRACE blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(448)) - Failed to place enough replicas, still in need of 2 to reach 5 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{COLD:2, storageTypes=[ARCHIVE], creationFallbacks=[], replicationFallbacks=[]}, newBlock=false)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy$NotEnoughReplicasException: [
Node /default-rack/127.0.0.1:44597 [
  Datanode 127.0.0.1:44597 is not chosen since not enough storage space to place the block  for storage type ARCHIVE.
]
Node /default-rack/127.0.0.1:46034 [
  Datanode 127.0.0.1:46034 is not chosen since not enough storage space to place the block  for storage type ARCHIVE.
]
Node /default-rack/127.0.0.1:42991 [
  Datanode 127.0.0.1:42991 is not chosen since not enough storage space to place the block  for storage type ARCHIVE.
]
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:852)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTargetInOrder(BlockPlacementPolicyDefault.java:541)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:437)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:309)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:149)
	at org.apache.hadoop.hdfs.server.blockmanagement.ReplicationWork.chooseTargets(ReplicationWork.java:46)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReconstructionWorkForBlocks(BlockManager.java:1960)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeBlockReconstructionWork(BlockManager.java:1912)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:4813)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$RedundancyMonitor.run(BlockManager.java:4680)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:09,969 [RedundancyMonitor] WARN  protocol.BlockStoragePolicy (BlockStoragePolicy.java:chooseStorageTypes(161)) - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=5, selected=[], unavailable=[ARCHIVE], removed=[ARCHIVE, ARCHIVE], policy=BlockStoragePolicy{COLD:2, storageTypes=[ARCHIVE], creationFallbacks=[], replicationFallbacks=[]})
2020-12-03 07:22:09,969 [RedundancyMonitor] TRACE blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(427)) - storageTypes={}
2020-12-03 07:22:09,969 [RedundancyMonitor] TRACE blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(448)) - Failed to place enough replicas, still in need of 2 to reach 5 (unavailableStorages=[ARCHIVE], storagePolicy=BlockStoragePolicy{COLD:2, storageTypes=[ARCHIVE], creationFallbacks=[], replicationFallbacks=[]}, newBlock=false)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy$NotEnoughReplicasException: All required storage types are unavailable:  unavailableStorages=[ARCHIVE], storagePolicy=BlockStoragePolicy{COLD:2, storageTypes=[ARCHIVE], creationFallbacks=[], replicationFallbacks=[]}
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:432)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:486)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:309)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:149)
	at org.apache.hadoop.hdfs.server.blockmanagement.ReplicationWork.chooseTargets(ReplicationWork.java:46)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReconstructionWorkForBlocks(BlockManager.java:1960)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeBlockReconstructionWork(BlockManager.java:1912)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:4813)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$RedundancyMonitor.run(BlockManager.java:4680)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:11,970 [RedundancyMonitor] TRACE blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(427)) - storageTypes={ARCHIVE=2}
2020-12-03 07:22:11,971 [RedundancyMonitor] DEBUG blockmanagement.BlockPlacementPolicy (DatanodeDescriptor.java:chooseStorage4Block(768)) - The node 127.0.0.1:46034 does not have enough ARCHIVE space (required=1024, scheduled=0, remaining=0).
2020-12-03 07:22:11,971 [RedundancyMonitor] DEBUG blockmanagement.BlockPlacementPolicy (DatanodeDescriptor.java:chooseStorage4Block(768)) - The node 127.0.0.1:42991 does not have enough ARCHIVE space (required=1024, scheduled=0, remaining=0).
2020-12-03 07:22:11,971 [RedundancyMonitor] DEBUG blockmanagement.BlockPlacementPolicy (DatanodeDescriptor.java:chooseStorage4Block(768)) - The node 127.0.0.1:44597 does not have enough ARCHIVE space (required=1024, scheduled=0, remaining=0).
2020-12-03 07:22:11,971 [RedundancyMonitor] INFO  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseRandom(850)) - Not enough replicas was chosen. Reason:{NOT_ENOUGH_STORAGE_SPACE=3}
2020-12-03 07:22:11,972 [RedundancyMonitor] TRACE blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(448)) - Failed to place enough replicas, still in need of 2 to reach 5 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{COLD:2, storageTypes=[ARCHIVE], creationFallbacks=[], replicationFallbacks=[]}, newBlock=false)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy$NotEnoughReplicasException: [
Node /default-rack/127.0.0.1:46034 [
  Datanode 127.0.0.1:46034 is not chosen since not enough storage space to place the block  for storage type ARCHIVE.
]
Node /default-rack/127.0.0.1:42991 [
  Datanode 127.0.0.1:42991 is not chosen since not enough storage space to place the block  for storage type ARCHIVE.
]
Node /default-rack/127.0.0.1:44597 [
  Datanode 127.0.0.1:44597 is not chosen since not enough storage space to place the block  for storage type ARCHIVE.
]
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:852)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTargetInOrder(BlockPlacementPolicyDefault.java:541)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:437)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:309)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:149)
	at org.apache.hadoop.hdfs.server.blockmanagement.ReplicationWork.chooseTargets(ReplicationWork.java:46)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReconstructionWorkForBlocks(BlockManager.java:1960)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeBlockReconstructionWork(BlockManager.java:1912)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:4813)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$RedundancyMonitor.run(BlockManager.java:4680)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:11,972 [RedundancyMonitor] WARN  protocol.BlockStoragePolicy (BlockStoragePolicy.java:chooseStorageTypes(161)) - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=5, selected=[], unavailable=[ARCHIVE], removed=[ARCHIVE, ARCHIVE], policy=BlockStoragePolicy{COLD:2, storageTypes=[ARCHIVE], creationFallbacks=[], replicationFallbacks=[]})
2020-12-03 07:22:11,972 [RedundancyMonitor] TRACE blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(427)) - storageTypes={}
2020-12-03 07:22:11,972 [RedundancyMonitor] TRACE blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(448)) - Failed to place enough replicas, still in need of 2 to reach 5 (unavailableStorages=[ARCHIVE], storagePolicy=BlockStoragePolicy{COLD:2, storageTypes=[ARCHIVE], creationFallbacks=[], replicationFallbacks=[]}, newBlock=false)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy$NotEnoughReplicasException: All required storage types are unavailable:  unavailableStorages=[ARCHIVE], storagePolicy=BlockStoragePolicy{COLD:2, storageTypes=[ARCHIVE], creationFallbacks=[], replicationFallbacks=[]}
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:432)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:486)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:309)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:149)
	at org.apache.hadoop.hdfs.server.blockmanagement.ReplicationWork.chooseTargets(ReplicationWork.java:46)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReconstructionWorkForBlocks(BlockManager.java:1960)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeBlockReconstructionWork(BlockManager.java:1912)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:4813)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$RedundancyMonitor.run(BlockManager.java:4680)
	at java.lang.Thread.run(Thread.java:748)
Mover Successful: all blocks satisfy the specified storage policy. Exiting...
2020-12-03 07:22:13,973 [RedundancyMonitor] TRACE blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(427)) - storageTypes={ARCHIVE=2}
2020-12-03 07:22:13,974 [RedundancyMonitor] DEBUG blockmanagement.BlockPlacementPolicy (DatanodeDescriptor.java:chooseStorage4Block(768)) - The node 127.0.0.1:44597 does not have enough ARCHIVE space (required=1024, scheduled=0, remaining=0).
2020-12-03 07:22:13,974 [RedundancyMonitor] DEBUG blockmanagement.BlockPlacementPolicy (DatanodeDescriptor.java:chooseStorage4Block(768)) - The node 127.0.0.1:42991 does not have enough ARCHIVE space (required=1024, scheduled=0, remaining=0).
2020-12-03 07:22:13,974 [RedundancyMonitor] DEBUG blockmanagement.BlockPlacementPolicy (DatanodeDescriptor.java:chooseStorage4Block(768)) - The node 127.0.0.1:46034 does not have enough ARCHIVE space (required=1024, scheduled=0, remaining=0).
2020-12-03 07:22:13,974 [RedundancyMonitor] INFO  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseRandom(850)) - Not enough replicas was chosen. Reason:{NOT_ENOUGH_STORAGE_SPACE=3}
2020-12-03 07:22:13,974 [RedundancyMonitor] TRACE blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(448)) - Failed to place enough replicas, still in need of 2 to reach 5 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{COLD:2, storageTypes=[ARCHIVE], creationFallbacks=[], replicationFallbacks=[]}, newBlock=false)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy$NotEnoughReplicasException: [
Node /default-rack/127.0.0.1:44597 [
  Datanode 127.0.0.1:44597 is not chosen since not enough storage space to place the block  for storage type ARCHIVE.
]
Node /default-rack/127.0.0.1:42991 [
  Datanode 127.0.0.1:42991 is not chosen since not enough storage space to place the block  for storage type ARCHIVE.
]
Node /default-rack/127.0.0.1:46034 [
  Datanode 127.0.0.1:46034 is not chosen since not enough storage space to place the block  for storage type ARCHIVE.
]
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:852)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTargetInOrder(BlockPlacementPolicyDefault.java:541)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:437)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:309)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:149)
	at org.apache.hadoop.hdfs.server.blockmanagement.ReplicationWork.chooseTargets(ReplicationWork.java:46)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReconstructionWorkForBlocks(BlockManager.java:1960)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeBlockReconstructionWork(BlockManager.java:1912)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:4813)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$RedundancyMonitor.run(BlockManager.java:4680)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:13,975 [RedundancyMonitor] WARN  protocol.BlockStoragePolicy (BlockStoragePolicy.java:chooseStorageTypes(161)) - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=5, selected=[], unavailable=[ARCHIVE], removed=[ARCHIVE, ARCHIVE], policy=BlockStoragePolicy{COLD:2, storageTypes=[ARCHIVE], creationFallbacks=[], replicationFallbacks=[]})
2020-12-03 07:22:13,975 [RedundancyMonitor] TRACE blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(427)) - storageTypes={}
2020-12-03 07:22:13,975 [RedundancyMonitor] TRACE blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(448)) - Failed to place enough replicas, still in need of 2 to reach 5 (unavailableStorages=[ARCHIVE], storagePolicy=BlockStoragePolicy{COLD:2, storageTypes=[ARCHIVE], creationFallbacks=[], replicationFallbacks=[]}, newBlock=false)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy$NotEnoughReplicasException: All required storage types are unavailable:  unavailableStorages=[ARCHIVE], storagePolicy=BlockStoragePolicy{COLD:2, storageTypes=[ARCHIVE], creationFallbacks=[], replicationFallbacks=[]}
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:432)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:486)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:309)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:149)
	at org.apache.hadoop.hdfs.server.blockmanagement.ReplicationWork.chooseTargets(ReplicationWork.java:46)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReconstructionWorkForBlocks(BlockManager.java:1960)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeBlockReconstructionWork(BlockManager.java:1912)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:4813)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$RedundancyMonitor.run(BlockManager.java:4680)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:15,976 [RedundancyMonitor] TRACE blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(427)) - storageTypes={ARCHIVE=2}
2020-12-03 07:22:15,977 [RedundancyMonitor] DEBUG blockmanagement.BlockPlacementPolicy (DatanodeDescriptor.java:chooseStorage4Block(768)) - The node 127.0.0.1:44597 does not have enough ARCHIVE space (required=1024, scheduled=0, remaining=0).
2020-12-03 07:22:15,977 [RedundancyMonitor] DEBUG blockmanagement.BlockPlacementPolicy (DatanodeDescriptor.java:chooseStorage4Block(768)) - The node 127.0.0.1:42991 does not have enough ARCHIVE space (required=1024, scheduled=0, remaining=0).
2020-12-03 07:22:15,977 [RedundancyMonitor] DEBUG blockmanagement.BlockPlacementPolicy (DatanodeDescriptor.java:chooseStorage4Block(768)) - The node 127.0.0.1:46034 does not have enough ARCHIVE space (required=1024, scheduled=0, remaining=0).
2020-12-03 07:22:15,977 [RedundancyMonitor] INFO  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseRandom(850)) - Not enough replicas was chosen. Reason:{NOT_ENOUGH_STORAGE_SPACE=3}
2020-12-03 07:22:15,978 [RedundancyMonitor] TRACE blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(448)) - Failed to place enough replicas, still in need of 2 to reach 5 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{COLD:2, storageTypes=[ARCHIVE], creationFallbacks=[], replicationFallbacks=[]}, newBlock=false)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy$NotEnoughReplicasException: [
Node /default-rack/127.0.0.1:44597 [
  Datanode 127.0.0.1:44597 is not chosen since not enough storage space to place the block  for storage type ARCHIVE.
]
Node /default-rack/127.0.0.1:42991 [
  Datanode 127.0.0.1:42991 is not chosen since not enough storage space to place the block  for storage type ARCHIVE.
]
Node /default-rack/127.0.0.1:46034 [
  Datanode 127.0.0.1:46034 is not chosen since not enough storage space to place the block  for storage type ARCHIVE.
]
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:852)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTargetInOrder(BlockPlacementPolicyDefault.java:541)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:437)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:309)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:149)
	at org.apache.hadoop.hdfs.server.blockmanagement.ReplicationWork.chooseTargets(ReplicationWork.java:46)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReconstructionWorkForBlocks(BlockManager.java:1960)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeBlockReconstructionWork(BlockManager.java:1912)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:4813)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$RedundancyMonitor.run(BlockManager.java:4680)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:15,978 [RedundancyMonitor] WARN  protocol.BlockStoragePolicy (BlockStoragePolicy.java:chooseStorageTypes(161)) - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=5, selected=[], unavailable=[ARCHIVE], removed=[ARCHIVE, ARCHIVE], policy=BlockStoragePolicy{COLD:2, storageTypes=[ARCHIVE], creationFallbacks=[], replicationFallbacks=[]})
2020-12-03 07:22:15,978 [RedundancyMonitor] TRACE blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(427)) - storageTypes={}
2020-12-03 07:22:15,978 [RedundancyMonitor] TRACE blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(448)) - Failed to place enough replicas, still in need of 2 to reach 5 (unavailableStorages=[ARCHIVE], storagePolicy=BlockStoragePolicy{COLD:2, storageTypes=[ARCHIVE], creationFallbacks=[], replicationFallbacks=[]}, newBlock=false)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy$NotEnoughReplicasException: All required storage types are unavailable:  unavailableStorages=[ARCHIVE], storagePolicy=BlockStoragePolicy{COLD:2, storageTypes=[ARCHIVE], creationFallbacks=[], replicationFallbacks=[]}
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:432)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:486)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:309)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:149)
	at org.apache.hadoop.hdfs.server.blockmanagement.ReplicationWork.chooseTargets(ReplicationWork.java:46)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReconstructionWorkForBlocks(BlockManager.java:1960)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeBlockReconstructionWork(BlockManager.java:1912)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:4813)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$RedundancyMonitor.run(BlockManager.java:4680)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:17,876 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:offerService(698)) - Forcing a full block report to localhost/127.0.0.1:44708
2020-12-03 07:22:17,879 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x452794627e9ce444: from storage DS-5a3cdcb1-c70d-48fc-9151-fc286cea3570 node DatanodeRegistration(127.0.0.1:33340, datanodeUuid=846ff15f-ed7d-4cf3-8112-e86de1180f07, infoPort=45658, infoSecurePort=0, ipcPort=38790, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 1, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:22:17,879 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x452794627e9ce444: from storage DS-31c88246-3f1d-4052-a932-2eefddf8a505 node DatanodeRegistration(127.0.0.1:33340, datanodeUuid=846ff15f-ed7d-4cf3-8112-e86de1180f07, infoPort=45658, infoSecurePort=0, ipcPort=38790, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 1, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:22:17,880 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x452794627e9ce444,  containing 2 storage report(s), of which we sent 2. The reports had 2 total blocks and used 1 RPC(s). This took 1 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:22:17,880 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:22:17,976 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:offerService(698)) - Forcing a full block report to localhost/127.0.0.1:44708
2020-12-03 07:22:17,979 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x7e2a985dacd7889a: from storage DS-058ff3c4-f186-458c-8f1b-9ea2f1373a45 node DatanodeRegistration(127.0.0.1:44597, datanodeUuid=622bf17c-c047-4086-9cd5-302fd27f9de3, infoPort=39415, infoSecurePort=0, ipcPort=35175, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 1, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
2020-12-03 07:22:17,979 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x7e2a985dacd7889a: from storage DS-c7c90efb-327f-4b73-88a6-34752b5003a8 node DatanodeRegistration(127.0.0.1:44597, datanodeUuid=622bf17c-c047-4086-9cd5-302fd27f9de3, infoPort=39415, infoSecurePort=0, ipcPort=35175, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 1, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:22:17,980 [RedundancyMonitor] TRACE blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(427)) - storageTypes={ARCHIVE=2}
2020-12-03 07:22:17,980 [RedundancyMonitor] DEBUG blockmanagement.BlockPlacementPolicy (DatanodeDescriptor.java:chooseStorage4Block(768)) - The node 127.0.0.1:46034 does not have enough ARCHIVE space (required=1024, scheduled=0, remaining=0).
2020-12-03 07:22:17,980 [RedundancyMonitor] DEBUG blockmanagement.BlockPlacementPolicy (DatanodeDescriptor.java:chooseStorage4Block(768)) - The node 127.0.0.1:44597 does not have enough ARCHIVE space (required=1024, scheduled=0, remaining=0).
2020-12-03 07:22:17,981 [RedundancyMonitor] DEBUG blockmanagement.BlockPlacementPolicy (DatanodeDescriptor.java:chooseStorage4Block(768)) - The node 127.0.0.1:42991 does not have enough ARCHIVE space (required=1024, scheduled=0, remaining=0).
2020-12-03 07:22:17,981 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x7e2a985dacd7889a,  containing 2 storage report(s), of which we sent 2. The reports had 2 total blocks and used 1 RPC(s). This took 1 msec to generate and 4 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:22:17,981 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:22:17,981 [RedundancyMonitor] INFO  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseRandom(850)) - Not enough replicas was chosen. Reason:{NOT_ENOUGH_STORAGE_SPACE=3}
2020-12-03 07:22:17,982 [RedundancyMonitor] TRACE blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(448)) - Failed to place enough replicas, still in need of 2 to reach 5 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{COLD:2, storageTypes=[ARCHIVE], creationFallbacks=[], replicationFallbacks=[]}, newBlock=false)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy$NotEnoughReplicasException: [
Node /default-rack/127.0.0.1:46034 [
  Datanode 127.0.0.1:46034 is not chosen since not enough storage space to place the block  for storage type ARCHIVE.
]
Node /default-rack/127.0.0.1:44597 [
  Datanode 127.0.0.1:44597 is not chosen since not enough storage space to place the block  for storage type ARCHIVE.
]
Node /default-rack/127.0.0.1:42991 [
  Datanode 127.0.0.1:42991 is not chosen since not enough storage space to place the block  for storage type ARCHIVE.
]
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:852)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTargetInOrder(BlockPlacementPolicyDefault.java:541)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:437)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:309)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:149)
	at org.apache.hadoop.hdfs.server.blockmanagement.ReplicationWork.chooseTargets(ReplicationWork.java:46)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReconstructionWorkForBlocks(BlockManager.java:1960)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeBlockReconstructionWork(BlockManager.java:1912)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:4813)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$RedundancyMonitor.run(BlockManager.java:4680)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:17,982 [RedundancyMonitor] WARN  protocol.BlockStoragePolicy (BlockStoragePolicy.java:chooseStorageTypes(161)) - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=5, selected=[], unavailable=[ARCHIVE], removed=[ARCHIVE, ARCHIVE], policy=BlockStoragePolicy{COLD:2, storageTypes=[ARCHIVE], creationFallbacks=[], replicationFallbacks=[]})
2020-12-03 07:22:17,982 [RedundancyMonitor] TRACE blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(427)) - storageTypes={}
2020-12-03 07:22:17,982 [RedundancyMonitor] TRACE blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(448)) - Failed to place enough replicas, still in need of 2 to reach 5 (unavailableStorages=[ARCHIVE], storagePolicy=BlockStoragePolicy{COLD:2, storageTypes=[ARCHIVE], creationFallbacks=[], replicationFallbacks=[]}, newBlock=false)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy$NotEnoughReplicasException: All required storage types are unavailable:  unavailableStorages=[ARCHIVE], storagePolicy=BlockStoragePolicy{COLD:2, storageTypes=[ARCHIVE], creationFallbacks=[], replicationFallbacks=[]}
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:432)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:486)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:309)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:149)
	at org.apache.hadoop.hdfs.server.blockmanagement.ReplicationWork.chooseTargets(ReplicationWork.java:46)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReconstructionWorkForBlocks(BlockManager.java:1960)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeBlockReconstructionWork(BlockManager.java:1912)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:4813)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$RedundancyMonitor.run(BlockManager.java:4680)
	at java.lang.Thread.run(Thread.java:748)
2020-12-03 07:22:18,077 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:offerService(698)) - Forcing a full block report to localhost/127.0.0.1:44708
2020-12-03 07:22:18,078 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x13bb4922ba959795: from storage DS-5cbe80bb-e620-48de-90dc-09c79ddc8402 node DatanodeRegistration(127.0.0.1:33403, datanodeUuid=0a567f53-c6f1-41b4-9ac9-d95bf8d50a27, infoPort=41687, infoSecurePort=0, ipcPort=42709, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:22:18,079 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x13bb4922ba959795: from storage DS-af93b569-157d-478e-8452-49963f34fadf node DatanodeRegistration(127.0.0.1:33403, datanodeUuid=0a567f53-c6f1-41b4-9ac9-d95bf8d50a27, infoPort=41687, infoSecurePort=0, ipcPort=42709, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 2, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:22:18,079 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x13bb4922ba959795,  containing 2 storage report(s), of which we sent 2. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:22:18,079 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:22:18,177 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:offerService(698)) - Forcing a full block report to localhost/127.0.0.1:44708
2020-12-03 07:22:18,179 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x5f71e6a65a67834e: from storage DS-fa19f5df-de0d-41f8-ae1e-004956343da7 node DatanodeRegistration(127.0.0.1:42991, datanodeUuid=98225bc9-d9e5-48c4-b0b8-a893c3451b76, infoPort=37554, infoSecurePort=0, ipcPort=43194, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:22:18,179 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x5f71e6a65a67834e: from storage DS-7f1774f2-7b1d-42eb-be32-aeaf5b4b35f9 node DatanodeRegistration(127.0.0.1:42991, datanodeUuid=98225bc9-d9e5-48c4-b0b8-a893c3451b76, infoPort=37554, infoSecurePort=0, ipcPort=43194, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:22:18,180 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x5f71e6a65a67834e,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 3 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:22:18,180 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:22:18,277 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:offerService(698)) - Forcing a full block report to localhost/127.0.0.1:44708
2020-12-03 07:22:18,280 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x15cc6f9b64a99497: from storage DS-60aaafac-0dea-4e4b-8271-d136469da168 node DatanodeRegistration(127.0.0.1:44891, datanodeUuid=e627ed49-82da-4339-b7cf-f2a6c12fcee4, infoPort=39970, infoSecurePort=0, ipcPort=35404, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 1, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:22:18,280 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x15cc6f9b64a99497: from storage DS-6ee6c00c-1ea1-4eca-be51-a4e9a83f8e11 node DatanodeRegistration(127.0.0.1:44891, datanodeUuid=e627ed49-82da-4339-b7cf-f2a6c12fcee4, infoPort=39970, infoSecurePort=0, ipcPort=35404, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 1, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:22:18,281 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x15cc6f9b64a99497,  containing 2 storage report(s), of which we sent 2. The reports had 2 total blocks and used 1 RPC(s). This took 1 msec to generate and 3 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:22:18,281 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:22:18,378 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:offerService(698)) - Forcing a full block report to localhost/127.0.0.1:44708
2020-12-03 07:22:18,380 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x21f01db7ce99a5d8: from storage DS-6cbd1a79-1320-44b1-8284-940a3e816e05 node DatanodeRegistration(127.0.0.1:46034, datanodeUuid=5e2422dd-430a-4435-a145-829f854bf7e8, infoPort=39776, infoSecurePort=0, ipcPort=41652, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 1, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:22:18,380 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2677)) - BLOCK* processReport 0x21f01db7ce99a5d8: from storage DS-c1ddbde3-64ec-4172-87a7-93013a994b4a node DatanodeRegistration(127.0.0.1:46034, datanodeUuid=5e2422dd-430a-4435-a145-829f854bf7e8, infoPort=39776, infoSecurePort=0, ipcPort=41652, storageInfo=lv=-57;cid=testClusterID;nsid=333324072;c=1606980108837), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-12-03 07:22:18,381 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPServiceActor.java:blockReport(424)) - Successfully sent block report 0x21f01db7ce99a5d8,  containing 2 storage report(s), of which we sent 2. The reports had 1 total blocks and used 1 RPC(s). This took 0 msec to generate and 3 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-12-03 07:22:18,381 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:22:18,477 [IPC Server handler 3 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/	dst=null	perm=null	proto=rpc
2020-12-03 07:22:18,478 [IPC Server handler 9 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/	dst=null	perm=null	proto=rpc
2020-12-03 07:22:18,481 [IPC Server handler 5 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/cold	dst=null	perm=null	proto=rpc
2020-12-03 07:22:18,486 [IPC Server handler 4 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/hot	dst=null	perm=null	proto=rpc
2020-12-03 07:22:18,489 [IPC Server handler 0 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/system	dst=null	perm=null	proto=rpc
2020-12-03 07:22:18,491 [IPC Server handler 8 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/warm	dst=null	perm=null	proto=rpc
2020-12-03 07:22:18,493 [IPC Server handler 7 on default port 44708] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(8074)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/system/mover.id	dst=null	perm=null	proto=rpc
2020-12-03 07:22:18,495 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(2049)) - Shutting down the Mini HDFS Cluster
2020-12-03 07:22:18,495 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 5
2020-12-03 07:22:18,497 [Listener at localhost/41652] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:22:18,497 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@12f3afb5] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:22:18,502 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12, DS-c1ddbde3-64ec-4172-87a7-93013a994b4a) exiting.
2020-12-03 07:22:18,502 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11, DS-6cbd1a79-1320-44b1-8284-940a3e816e05) exiting.
2020-12-03 07:22:18,629 [Listener at localhost/41652] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@a23a01d{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:22:18,635 [Listener at localhost/41652] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@4acf72b6{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:22:18,636 [Listener at localhost/41652] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@3aaf4f07{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:22:18,640 [Listener at localhost/41652] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@27dc79f7{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:22:18,651 [Listener at localhost/41652] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 41652
2020-12-03 07:22:18,657 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:22:18,658 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:22:18,660 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:22:18,660 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1344107566-172.17.0.11-1606980108837 (Datanode Uuid 5e2422dd-430a-4435-a145-829f854bf7e8) service to localhost/127.0.0.1:44708
2020-12-03 07:22:18,660 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1344107566-172.17.0.11-1606980108837 (Datanode Uuid 5e2422dd-430a-4435-a145-829f854bf7e8)
2020-12-03 07:22:18,661 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:22:18,663 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11/current/BP-1344107566-172.17.0.11-1606980108837] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:18,663 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12/current/BP-1344107566-172.17.0.11-1606980108837] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:18,694 [Listener at localhost/41652] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:22:18,694 [Listener at localhost/41652] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:22:18,695 [Listener at localhost/41652] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:22:18,695 [Listener at localhost/41652] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:22:18,704 [Listener at localhost/41652] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:22:18,705 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 4
2020-12-03 07:22:18,707 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@6e0ff644] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:22:18,707 [Listener at localhost/41652] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:22:18,709 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9, DS-60aaafac-0dea-4e4b-8271-d136469da168) exiting.
2020-12-03 07:22:18,710 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10, DS-6ee6c00c-1ea1-4eca-be51-a4e9a83f8e11) exiting.
2020-12-03 07:22:18,764 [Listener at localhost/41652] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@88d6f9b{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:22:18,768 [Listener at localhost/41652] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@47d93e0d{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:22:18,771 [Listener at localhost/41652] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@1cfd1875{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:22:18,773 [Listener at localhost/41652] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@320e400{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:22:18,778 [Listener at localhost/41652] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 35404
2020-12-03 07:22:18,782 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:22:18,796 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:22:18,797 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:22:18,798 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1344107566-172.17.0.11-1606980108837 (Datanode Uuid e627ed49-82da-4339-b7cf-f2a6c12fcee4) service to localhost/127.0.0.1:44708
2020-12-03 07:22:18,798 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1344107566-172.17.0.11-1606980108837 (Datanode Uuid e627ed49-82da-4339-b7cf-f2a6c12fcee4)
2020-12-03 07:22:18,798 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:22:18,800 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9/current/BP-1344107566-172.17.0.11-1606980108837] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:18,800 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10/current/BP-1344107566-172.17.0.11-1606980108837] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:18,812 [Listener at localhost/41652] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:22:18,812 [Listener at localhost/41652] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:22:18,813 [Listener at localhost/41652] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:22:18,814 [Listener at localhost/41652] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:22:18,816 [Listener at localhost/41652] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:22:18,816 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 3
2020-12-03 07:22:18,817 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@32f61a31] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:22:18,818 [Listener at localhost/41652] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:22:18,820 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8, DS-fa19f5df-de0d-41f8-ae1e-004956343da7) exiting.
2020-12-03 07:22:18,820 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7, DS-7f1774f2-7b1d-42eb-be32-aeaf5b4b35f9) exiting.
2020-12-03 07:22:18,855 [Listener at localhost/41652] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@30c0ccff{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:22:18,856 [Listener at localhost/41652] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@581d969c{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:22:18,857 [Listener at localhost/41652] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@36ac8a63{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:22:18,857 [Listener at localhost/41652] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@344561e0{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:22:18,861 [Listener at localhost/41652] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 43194
2020-12-03 07:22:18,868 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:22:18,871 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:22:18,871 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:22:18,873 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1344107566-172.17.0.11-1606980108837 (Datanode Uuid 98225bc9-d9e5-48c4-b0b8-a893c3451b76) service to localhost/127.0.0.1:44708
2020-12-03 07:22:18,873 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1344107566-172.17.0.11-1606980108837 (Datanode Uuid 98225bc9-d9e5-48c4-b0b8-a893c3451b76)
2020-12-03 07:22:18,873 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:22:18,874 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7/current/BP-1344107566-172.17.0.11-1606980108837] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:18,874 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/current/BP-1344107566-172.17.0.11-1606980108837] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:18,879 [Listener at localhost/41652] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:22:18,880 [Listener at localhost/41652] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:22:18,881 [Listener at localhost/41652] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:22:18,881 [Listener at localhost/41652] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:22:18,884 [Listener at localhost/41652] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:22:18,884 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 2
2020-12-03 07:22:18,884 [Listener at localhost/41652] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:22:18,884 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@66238be2] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:22:18,886 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6, DS-af93b569-157d-478e-8452-49963f34fadf) exiting.
2020-12-03 07:22:18,886 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5, DS-5cbe80bb-e620-48de-90dc-09c79ddc8402) exiting.
2020-12-03 07:22:18,911 [Listener at localhost/41652] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@210386e0{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:22:18,912 [Listener at localhost/41652] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@3d4d3fe7{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:22:18,912 [Listener at localhost/41652] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@7544a1e4{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:22:18,913 [Listener at localhost/41652] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@63fd4873{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:22:18,918 [Listener at localhost/41652] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 42709
2020-12-03 07:22:18,923 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:22:18,923 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:22:18,929 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:22:18,929 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1344107566-172.17.0.11-1606980108837 (Datanode Uuid 0a567f53-c6f1-41b4-9ac9-d95bf8d50a27) service to localhost/127.0.0.1:44708
2020-12-03 07:22:18,929 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1344107566-172.17.0.11-1606980108837 (Datanode Uuid 0a567f53-c6f1-41b4-9ac9-d95bf8d50a27)
2020-12-03 07:22:18,929 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:22:18,935 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-1344107566-172.17.0.11-1606980108837] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:18,945 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/current/BP-1344107566-172.17.0.11-1606980108837] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:18,943 [Listener at localhost/41652] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:22:18,946 [Listener at localhost/41652] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:22:18,948 [Listener at localhost/41652] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:22:18,948 [Listener at localhost/41652] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:22:18,952 [Listener at localhost/41652] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:22:18,953 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 1
2020-12-03 07:22:18,954 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@41200e0c] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:22:18,954 [Listener at localhost/41652] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:22:18,957 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, DS-c7c90efb-327f-4b73-88a6-34752b5003a8) exiting.
2020-12-03 07:22:18,957 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3, DS-058ff3c4-f186-458c-8f1b-9ea2f1373a45) exiting.
2020-12-03 07:22:18,975 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1344107566-172.17.0.11-1606980108837 (Datanode Uuid 622bf17c-c047-4086-9cd5-302fd27f9de3) service to localhost/127.0.0.1:44708
2020-12-03 07:22:18,975 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1344107566-172.17.0.11-1606980108837 (Datanode Uuid 622bf17c-c047-4086-9cd5-302fd27f9de3)
2020-12-03 07:22:18,975 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:22:18,976 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1344107566-172.17.0.11-1606980108837] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:18,977 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1344107566-172.17.0.11-1606980108837] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:18,989 [Listener at localhost/41652] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@29ad44e3{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:22:18,999 [Listener at localhost/41652] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@15bcf458{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:22:19,000 [Listener at localhost/41652] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@aec50a1{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:22:19,001 [Listener at localhost/41652] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@24be2d9c{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:22:19,006 [Listener at localhost/41652] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 35175
2020-12-03 07:22:19,016 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:22:19,021 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:22:19,034 [Listener at localhost/41652] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:22:19,035 [Listener at localhost/41652] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:22:19,037 [Listener at localhost/41652] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:22:19,038 [Listener at localhost/41652] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:22:19,043 [Listener at localhost/41652] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:22:19,043 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2097)) - Shutting down DataNode 0
2020-12-03 07:22:19,043 [Listener at localhost/41652] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(343)) - DirectoryScanner: shutdown has been called
2020-12-03 07:22:19,043 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@4c6daf0] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-12-03 07:22:19,049 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-31c88246-3f1d-4052-a932-2eefddf8a505) exiting.
2020-12-03 07:22:19,053 [VolumeScannerThread(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(637)) - VolumeScanner(/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-5a3cdcb1-c70d-48fc-9151-fc286cea3570) exiting.
2020-12-03 07:22:19,084 [Listener at localhost/41652] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@61526469{/,null,UNAVAILABLE}{/datanode}
2020-12-03 07:22:19,085 [Listener at localhost/41652] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@274872f8{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:22:19,086 [Listener at localhost/41652] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@62e70ea3{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:22:19,086 [Listener at localhost/41652] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@7a7471ce{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:22:19,088 [Listener at localhost/41652] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 38790
2020-12-03 07:22:19,095 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:22:19,095 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:22:19,098 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-12-03 07:22:19,099 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] WARN  datanode.DataNode (BPServiceActor.java:run(860)) - Ending block pool service for: Block pool BP-1344107566-172.17.0.11-1606980108837 (Datanode Uuid 846ff15f-ed7d-4cf3-8112-e86de1180f07) service to localhost/127.0.0.1:44708
2020-12-03 07:22:19,099 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1344107566-172.17.0.11-1606980108837 (Datanode Uuid 846ff15f-ed7d-4cf3-8112-e86de1180f07)
2020-12-03 07:22:19,099 [BP-1344107566-172.17.0.11-1606980108837 heartbeating to localhost/127.0.0.1:44708] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2814)) - Removing block pool BP-1344107566-172.17.0.11-1606980108837
2020-12-03 07:22:19,100 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1344107566-172.17.0.11-1606980108837] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:19,100 [refreshUsed-/root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1344107566-172.17.0.11-1606980108837] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-12-03 07:22:19,111 [Listener at localhost/41652] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(193)) - Shutting down all async disk service threads
2020-12-03 07:22:19,112 [Listener at localhost/41652] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(201)) - All async disk service threads have been shut down
2020-12-03 07:22:19,114 [Listener at localhost/41652] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(177)) - Shutting down all async lazy persist service threads
2020-12-03 07:22:19,114 [Listener at localhost/41652] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(184)) - All async lazy persist service threads have been shut down
2020-12-03 07:22:19,119 [Listener at localhost/41652] INFO  datanode.DataNode (DataNode.java:shutdown(2164)) - Shutdown complete.
2020-12-03 07:22:19,120 [Listener at localhost/41652] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:stopAndJoinNameNode(2130)) - Shutting down the namenode
2020-12-03 07:22:19,120 [Listener at localhost/41652] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1334)) - Stopping services started for active state
2020-12-03 07:22:19,121 [Listener at localhost/41652] INFO  namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1410)) - Ending log segment 1, 31
2020-12-03 07:22:19,122 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@1b065145] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4107)) - NameNodeEditLogRoller was interrupted, exiting
2020-12-03 07:22:19,122 [Listener at localhost/41652] INFO  namenode.FSEditLog (FSEditLog.java:printStatistics(778)) - Number of transactions: 32 Total time for transactions(ms): 29 Number of transactions batched in Syncs: 2 Number of syncs: 31 SyncTimes(ms): 5 1 
2020-12-03 07:22:19,124 [Listener at localhost/41652] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(145)) - Finalizing edits file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_inprogress_0000000000000000001 -> /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000001-0000000000000000032
2020-12-03 07:22:19,124 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@45cff11c] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4198)) - LazyPersistFileScrubber was interrupted, exiting
2020-12-03 07:22:19,125 [Listener at localhost/41652] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(145)) - Finalizing edits file /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_inprogress_0000000000000000001 -> /root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000001-0000000000000000032
2020-12-03 07:22:19,126 [FSEditLogAsync] INFO  namenode.FSEditLog (FSEditLogAsync.java:run(253)) - FSEditLogAsync was interrupted, exiting
2020-12-03 07:22:19,127 [CacheReplicationMonitor(1134073148)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2020-12-03 07:22:19,129 [Listener at localhost/41652] INFO  ipc.Server (Server.java:stop(3359)) - Stopping server on 44708
2020-12-03 07:22:19,137 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1330)) - Stopping IPC Server listener on 0
2020-12-03 07:22:19,137 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1465)) - Stopping IPC Server Responder
2020-12-03 07:22:19,142 [RedundancyMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4687)) - Stopping RedundancyMonitor.
2020-12-03 07:22:19,142 [StorageInfoMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4722)) - Stopping thread.
2020-12-03 07:22:19,188 [Listener at localhost/41652] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1334)) - Stopping services started for active state
2020-12-03 07:22:19,188 [Listener at localhost/41652] INFO  namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1434)) - Stopping services started for standby state
2020-12-03 07:22:19,192 [Listener at localhost/41652] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@7a3793c7{/,null,UNAVAILABLE}{/hdfs}
2020-12-03 07:22:19,195 [Listener at localhost/41652] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@4c012563{HTTP/1.1,[http/1.1]}{localhost:0}
2020-12-03 07:22:19,195 [Listener at localhost/41652] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@71687585{/static,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-12-03 07:22:19,196 [Listener at localhost/41652] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@5c7bfdc1{/logs,file:///root/hadoop-3.2.1-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-12-03 07:22:19,197 [Listener at localhost/41652] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2020-12-03 07:22:19,224 [Listener at localhost/41652] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2020-12-03 07:22:19,225 [Listener at localhost/41652] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - DataNode metrics system shutdown complete.
msx-rc 0
